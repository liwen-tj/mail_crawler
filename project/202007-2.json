[
    {
        "id": "<55bb54d9.35e3.17331a62244.Coremail.aichiyuyiyi11@163.com>",
        "from": "爱吃鱼 &lt;aichiyuyiy...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 03:37:18 GMT",
        "subject": "flink时间窗口",
        "content": "你好，我最近业务上需要处理一个流式的时间处理窗口，部分业务代码如下&#010;SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&gt;&gt; operator = flatMap.keyBy(0,1)&#010;                .timeWindow(Time.minutes(1))&#010;                .process(new ProcessWindowFunction)&#010;当我运行的时候只有第一分钟的时间窗口会有数据进来，之后便没有数据进来了，业务逻辑代码也没有报错，请问这是什么原因。",
        "depth": "0",
        "reply": "<55bb54d9.35e3.17331a62244.Coremail.aichiyuyiyi11@163.com>"
    },
    {
        "id": "<tencent_78C91E0823567CDE656922341BB9B1812F0A@qq.com>",
        "from": "&quot;Yichao Yang&quot; &lt;1048262...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 05:09:32 GMT",
        "subject": "回复：flink时间窗口",
        "content": "Hi,&#013;&#010;&#013;&#010;&#013;&#010;根据你的keyby字段来看，你是根据 warningPojo + String 进行了keyby，可以看下是否相同的key只有一条相同数据。&#013;&#010;并且可以看下使用到的是处理时间还是事件时间？&#013;&#010;如果是事件时间，可以看下 timestamp assigner 是否正确，上游数据和时间戳是否符合预期。&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yichao Yang&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"爱吃鱼\"&lt;aichiyuyiyi11@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 中午11:37&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;flink时间窗口&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;你好，我最近业务上需要处理一个流式的时间处理窗口，部分业务代码如下&#013;&#010;SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&amp;gt;&amp;gt; operator = flatMap.keyBy(0,1)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.timeWindow(Time.minutes(1))&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.process(new ProcessWindowFunction)&#013;&#010;当我运行的时候只有第一分钟的时间窗口会有数据进来，之后便没有数据进来了，业务逻辑代码也没有报错，请问这是什么原因。",
        "depth": "1",
        "reply": "<55bb54d9.35e3.17331a62244.Coremail.aichiyuyiyi11@163.com>"
    },
    {
        "id": "<1e14191b.30ef.173321836f2.Coremail.aichiyuyiyi11@163.com>",
        "from": "爱吃鱼 &lt;aichiyuyiy...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 05:41:54 GMT",
        "subject": "Re:回复：flink时间窗口",
        "content": "&#010;&#010;&#010;Hi，&#010;&#010;&#010;&#010;&#010;&#010;&#010;因为业务原因具体的keyby字段没有写清楚，我是根据warningPojo类里面的字段进行排序，源数据&#010;是从kafka实时流传输过来的，每一分钟滑动窗口计算一次&#010;&#010;&#010;SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&gt;&gt; operator1 = env.addSource(stringFlinkKafkaConsumerBase)&#010;                .filter((String s) -&gt; (s.split(\",\", -1).length == 34))&#010;                .flatMap(new RichFlatMapFunction&lt;String, warningPojo&gt;() {&#010;                .keyBy(\"src\", \"msg\")&#010;                .timeWindow(Time.minutes(1))&#010;                .process(new ProcessWindowFunction&lt;warningPojo, Tuple2&lt;warningPojo,&#010;String&gt;, Tuple, TimeWindow&gt;() &#010;                .setParallelism(1);&#010;&#010;&#010;&#010;&#010;每次执行这段流代码就只有第一次的一分钟时间窗口有数据传输到es，之后就没有数据了。&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-09 13:09:32，\"Yichao Yang\" &lt;1048262223@qq.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;&#010;&gt;根据你的keyby字段来看，你是根据 warningPojo + String 进行了keyby，可以看下是否相同的key只有一条相同数据。&#010;&gt;并且可以看下使用到的是处理时间还是事件时间？&#010;&gt;如果是事件时间，可以看下 timestamp assigner 是否正确，上游数据和时间戳是否符合预期。&#010;&gt;&#010;&gt;&#010;&gt;Best,&#010;&gt;Yichao Yang&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt;发件人:&amp;nbsp;\"爱吃鱼\"&lt;aichiyuyiyi11@163.com&amp;gt;;&#010;&gt;发送时间:&amp;nbsp;2020年7月9日(星期四) 中午11:37&#010;&gt;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt;主题:&amp;nbsp;flink时间窗口&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;你好，我最近业务上需要处理一个流式的时间处理窗口，部分业务代码如下&#010;&gt;SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&amp;gt;&amp;gt; operator =&#010;flatMap.keyBy(0,1)&#010;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.timeWindow(Time.minutes(1))&#010;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.process(new ProcessWindowFunction)&#010;&gt;当我运行的时候只有第一分钟的时间窗口会有数据进来，之后便没有数据进来了，业务逻辑代码也没有报错，请问这是什么原因。&#010;",
        "depth": "2",
        "reply": "<55bb54d9.35e3.17331a62244.Coremail.aichiyuyiyi11@163.com>"
    },
    {
        "id": "<CAA8tFvtHBU5XfcO8_uEdNBQLOEcFLrw6JBsonS_JMGhDt+6SVA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 06:00:23 GMT",
        "subject": "Re: 回复：flink时间窗口",
        "content": "对于 window 来说，你需要判断下是没有数据进来，还是有数据进来但是&#010;window 没有触发。&#013;&#010;如果是数据没有进来，那么需要看 window 节点之前的逻辑，如果是数据进来了，但是没有触发，需要看下&#010;wateramrk 是不是符合预期&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;爱吃鱼 &lt;aichiyuyiyi11@163.com&gt; 于2020年7月9日周四 下午1:42写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi，&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 因为业务原因具体的keyby字段没有写清楚，我是根据warningPojo类里面的字段进行排序，源数据&#013;&#010;&gt; 是从kafka实时流传输过来的，每一分钟滑动窗口计算一次&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&gt;&gt; operator1 =&#013;&#010;&gt; env.addSource(stringFlinkKafkaConsumerBase)&#013;&#010;&gt;                 .filter((String s) -&gt; (s.split(\",\", -1).length == 34))&#013;&#010;&gt;                 .flatMap(new RichFlatMapFunction&lt;String, warningPojo&gt;() {&#013;&#010;&gt;                 .keyBy(\"src\", \"msg\")&#013;&#010;&gt;                 .timeWindow(Time.minutes(1))&#013;&#010;&gt;                 .process(new ProcessWindowFunction&lt;warningPojo,&#013;&#010;&gt; Tuple2&lt;warningPojo, String&gt;, Tuple, TimeWindow&gt;()&#013;&#010;&gt;                 .setParallelism(1);&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 每次执行这段流代码就只有第一次的一分钟时间窗口有数据传输到es，之后就没有数据了。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-09 13:09:32，\"Yichao Yang\" &lt;1048262223@qq.com&gt; 写道：&#013;&#010;&gt; &gt;Hi,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;根据你的keyby字段来看，你是根据 warningPojo + String 进行了keyby，可以看下是否相同的key只有一条相同数据。&#013;&#010;&gt; &gt;并且可以看下使用到的是处理时间还是事件时间？&#013;&#010;&gt; &gt;如果是事件时间，可以看下 timestamp assigner 是否正确，上游数据和时间戳是否符合预期。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Yichao Yang&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; &gt;发件人:&amp;nbsp;\"爱吃鱼\"&lt;aichiyuyiyi11@163.com&amp;gt;;&#013;&#010;&gt; &gt;发送时间:&amp;nbsp;2020年7月9日(星期四) 中午11:37&#013;&#010;&gt; &gt;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;主题:&amp;nbsp;flink时间窗口&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;你好，我最近业务上需要处理一个流式的时间处理窗口，部分业务代码如下&#013;&#010;&gt; &gt;SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&amp;gt;&amp;gt; operator&#010;=&#013;&#010;&gt; flatMap.keyBy(0,1)&#013;&#010;&gt; &gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; .timeWindow(Time.minutes(1))&#013;&#010;&gt; &gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; .process(new ProcessWindowFunction)&#013;&#010;&gt; &gt;当我运行的时候只有第一分钟的时间窗口会有数据进来，之后便没有数据进来了，业务逻辑代码也没有报错，请问这是什么原因。&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<55bb54d9.35e3.17331a62244.Coremail.aichiyuyiyi11@163.com>"
    },
    {
        "id": "<tencent_9A7A33D52ABE00E89E3635C2859AE76E7609@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 15:35:35 GMT",
        "subject": "回复：flink时间窗口",
        "content": "new ProcessWindowFunction是怎么处理的?&#013;&#010;是否设置了水印?&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;aichiyuyiyi11@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 中午11:37&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;flink时间窗口&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;你好，我最近业务上需要处理一个流式的时间处理窗口，部分业务代码如下&#013;&#010;SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&amp;gt;&amp;gt; operator = flatMap.keyBy(0,1)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.timeWindow(Time.minutes(1))&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.process(new ProcessWindowFunction)&#013;&#010;当我运行的时候只有第一分钟的时间窗口会有数据进来，之后便没有数据进来了，业务逻辑代码也没有报错，请问这是什么原因。",
        "depth": "1",
        "reply": "<55bb54d9.35e3.17331a62244.Coremail.aichiyuyiyi11@163.com>"
    },
    {
        "id": "<CAA8tFvs2UafuuvJcGUXGnnCsX9hYBjqL7zVejBvAdTw5wx+-NA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:14:23 GMT",
        "subject": "Re: flink时间窗口",
        "content": "Hi&#013;&#010;&#013;&#010;对于这个问题，可以尝试看添加相关日志能否在线上（或者测试环境）排查，另外可以使用&#010;watermark 相关的 metric[1] 查看下是否符合预期&#013;&#010;如果上面的不行，可以尝试看能否在 IDE 中进行复现，这样可以 debug&#010;进行追查&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-stable/monitoring/metrics.html#io&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;忝忝向仧 &lt;153488125@qq.com&gt; 于2020年7月9日周四 下午11:36写道：&#013;&#010;&#013;&#010;&gt; new ProcessWindowFunction是怎么处理的?&#013;&#010;&gt; 是否设置了水印?&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&#013;&#010;&gt;                                                   \"user-zh\"&#013;&#010;&gt;                                                                     &lt;&#013;&#010;&gt; aichiyuyiyi11@163.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月9日(星期四) 中午11:37&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;flink时间窗口&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你好，我最近业务上需要处理一个流式的时间处理窗口，部分业务代码如下&#013;&#010;&gt; SingleOutputStreamOperator&lt;Tuple2&lt;warningPojo, String&amp;gt;&amp;gt; operator&#010;=&#013;&#010;&gt; flatMap.keyBy(0,1)&#013;&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; .timeWindow(Time.minutes(1))&#013;&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; .process(new ProcessWindowFunction)&#013;&#010;&gt; 当我运行的时候只有第一分钟的时间窗口会有数据进来，之后便没有数据进来了，业务逻辑代码也没有报错，请问这是什么原因。&#013;&#010;",
        "depth": "2",
        "reply": "<55bb54d9.35e3.17331a62244.Coremail.aichiyuyiyi11@163.com>"
    },
    {
        "id": "<tencent_DC5ACBB225933CA5E114A6927BDAAFFED205@qq.com>",
        "from": "&quot;Evan&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 07:32:29 GMT",
        "subject": "回复：ddl es 报错",
        "content": "Hello，&#013;&#010;&#013;&#010;&#013;&#010;这个报错，在flink 1.11 最新版本我也遇见了，跟你同样的操作&#013;&#010;真正原因是这个ddl 是flink 的sink table，是数据写入端，不能打印数据。&#013;&#010;而tableEnv.toRetractStream(table, Row.class).print();&amp;nbsp;&#013;&#010;这个打印的数据方法只适合flink 的Source Table，也就是数据输入端，比如kafka&#010;table就可以正常使用。&#013;&#010;&#013;&#010;&#013;&#010;2020年7月9日15:31:56&amp;nbsp;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"出发\"&lt;573693104@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年3月23日(星期一) 晚上11:30&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;ddl es 报错&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;源码如下:&#013;&#010;CREATE TABLE buy_cnt_per_hour ( &#013;&#010; &amp;nbsp; hour_of_day BIGINT,&#013;&#010; &amp;nbsp; buy_cnt BIGINT&#013;&#010;) WITH (&#013;&#010; &amp;nbsp; 'connector.type' = 'elasticsearch',&#013;&#010; &amp;nbsp; 'connector.version' = '6',&#013;&#010; &amp;nbsp; 'connector.hosts' = 'http://localhost:9200',&#013;&#010; &amp;nbsp; 'connector.index' = 'buy_cnt_per_hour',&#013;&#010; &amp;nbsp; 'connector.document-type' = 'user_behavior',&#013;&#010; &amp;nbsp; 'connector.bulk-flush.max-actions' = '1',&#013;&#010; &amp;nbsp; 'format.type' = 'json',&#013;&#010; &amp;nbsp; 'update-mode' = 'append'&#013;&#010;)&#013;&#010;&#013;&#010;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#013;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#013;&#010;import org.apache.flink.table.api.Table;&#013;&#010;import org.apache.flink.table.api.java.StreamTableEnvironment;&#013;&#010;import org.apache.flink.types.Row;&#013;&#010;&#013;&#010;public class ESTest {&#013;&#010;&#013;&#010;public static void main(String[] args) throws Exception {&#013;&#010;&#013;&#010;//2、设置运行环境&#013;&#010;StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#013;&#010;StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv, settings);&#013;&#010;streamEnv.setParallelism(1);&#013;&#010;String sinkDDL = \" CREATE TABLE test_es ( hour_of_day BIGINT,buy_cnt BIGINT \"&#013;&#010;+ \") WITH ( 'connector.type' = 'elasticsearch','connector.version' = '6',\"&#013;&#010;+ \"'connector.hosts' = 'http://localhost:9200','connector.index' = 'buy_cnt_per_hour',\"&#013;&#010;+ \"'connector.document-type' = 'user_behavior',\"&#013;&#010;+ \"'connector.bulk-flush.max-actions' = '1',\\n\" + \"'format.type' = 'json',\"&#013;&#010;+ \"'update-mode' = 'append' )\";&#013;&#010;tableEnv.sqlUpdate(sinkDDL);&#013;&#010;Table table = tableEnv.sqlQuery(\"select * from test_es \");&#013;&#010;tableEnv.toRetractStream(table, Row.class).print();&#013;&#010;streamEnv.execute(\"\");&#013;&#010;}&#013;&#010;&#013;&#010;}&#013;&#010;&#013;&#010;具体error&#013;&#010;The matching candidates: org.apache.flink.table.sources.CsvAppendTableSourceFactory Mismatched&#010;properties: 'connector.type' expects 'filesystem', but is 'elasticsearch' 'format.type' expects&#010;'csv', but is 'json'The following properties are requested: connector.bulk-flush.max-actions=1&#010;connector.document-type=user_behavior connector.hosts=http://localhost:9200 connector.index=buy_cnt_per_hour&#010;connector.type=elasticsearch connector.version=6 format.type=json schema.0.data-type=BIGINT&#010;schema.0.name=hour_of_day schema.1.data-type=BIGINT schema.1.name=buy_cnt update-mode=append",
        "depth": "1",
        "reply": "<tencent_DC5ACBB225933CA5E114A6927BDAAFFED205@qq.com>"
    },
    {
        "id": "<22be617d.69a4.17332c7b030.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 08:53:34 GMT",
        "subject": "flink 双流join报错，java.lang.AssertionError",
        "content": "hi,&#010;我使用flink 1.10.1 blink-planner。运行以下SQL时，抛出异常。其中A和B分别是两个Kafka消息流。任务使用processtime。如果我把join的B表的select&#010;具体字段名 修改为 select *，貌似就可以执行。但是拿到的B表字段顺序貌似是错乱的。请问这个问题是bug么？&#010;&#010;&#010;select A.recvTime, A.khh, A.live_id, A.fund_code as product_code, A.fund_name as product_name,&#010;cast(B.balance as double) as balance&#010;from (&#010;select toLong(behaviorTime, true) as recvTime, user_id,&#010;cast(regexp_extract(btnTitle, 'zbid=\\{([^|]*)\\}', 1) as int) as live_id,&#010;regexp_extract(btnTitle, 'fundname=\\{([^|]*)\\}', 1) as fund_name,&#010;regexp_extract(btnTitle, 'fundcode=\\{([^|]*)\\}', 1) as fund_code, proctime from kafka_zl_etrack_event_stream&#010;where pageId = 'xxxx'&#010;    and eventId = 'click'&#010;    and btnId = 'xxxx&#010;    and CHARACTER_LENGTH(user_id) &gt; 4&#010;) A&#010;left join&#010;(&#010;select customerNumber, balance, fundCode, lastUpdateTime, proctime&#010;      from lscsp_sc_order_all&#010;       where `status` = '4'&#010;         and businessType IN ('4','5','14','16','17','18')&#010;         and fundCode IS NOT NULL&#010;         and balance IS NOT NULL&#010;         and lastUpdateTime IS NOT NULL&#010;) B&#010;on A.user_id = B.customerNumber and A.fund_code = B.fundCode&#010;group by  A.recvTime, A.user_id, A.live_id, A.fund_code, A.fund_name, cast(B.balance as double)&#010;&#010;&#010;&#010;&#010;&#010;&#010;Exception in thread \"main\" java.lang.AssertionError&#010;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:4448)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3765)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3737)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.access$2200(SqlToRelConverter.java:217)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4796)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4092)&#010;at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4656)&#010;at org.apache.calcite.sql2rel.StandardConvertletTable.convertCast(StandardConvertletTable.java:522)&#010;at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63)",
        "depth": "0",
        "reply": "<22be617d.69a4.17332c7b030.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<67c61841.5929.17332e7e086.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 09:28:43 GMT",
        "subject": "Re:flink 双流join报错，java.lang.AssertionError",
        "content": "&#010;&#010;&#010;hi,&#010;我切到最新的1.11 release版本，跑同样的sql，没有抛出异常。想问下这有相关的issue么？想确认下原因。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-09 16:53:34，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;hi,&#010;&gt;我使用flink 1.10.1 blink-planner。运行以下SQL时，抛出异常。其中A和B分别是两个Kafka消息流。任务使用processtime。如果我把join的B表的select&#010;具体字段名 修改为 select *，貌似就可以执行。但是拿到的B表字段顺序貌似是错乱的。请问这个问题是bug么？&#010;&gt;&#010;&gt;&#010;&gt;select A.recvTime, A.khh, A.live_id, A.fund_code as product_code, A.fund_name as product_name,&#010;cast(B.balance as double) as balance&#010;&gt;from (&#010;&gt;select toLong(behaviorTime, true) as recvTime, user_id,&#010;&gt;cast(regexp_extract(btnTitle, 'zbid=\\{([^|]*)\\}', 1) as int) as live_id,&#010;&gt;regexp_extract(btnTitle, 'fundname=\\{([^|]*)\\}', 1) as fund_name,&#010;&gt;regexp_extract(btnTitle, 'fundcode=\\{([^|]*)\\}', 1) as fund_code, proctime from kafka_zl_etrack_event_stream&#010;&gt;where pageId = 'xxxx'&#010;&gt;    and eventId = 'click'&#010;&gt;    and btnId = 'xxxx&#010;&gt;    and CHARACTER_LENGTH(user_id) &gt; 4&#010;&gt;) A&#010;&gt;left join&#010;&gt;(&#010;&gt;select customerNumber, balance, fundCode, lastUpdateTime, proctime&#010;&gt;      from lscsp_sc_order_all&#010;&gt;       where `status` = '4'&#010;&gt;         and businessType IN ('4','5','14','16','17','18')&#010;&gt;         and fundCode IS NOT NULL&#010;&gt;         and balance IS NOT NULL&#010;&gt;         and lastUpdateTime IS NOT NULL&#010;&gt;) B&#010;&gt;on A.user_id = B.customerNumber and A.fund_code = B.fundCode&#010;&gt;group by  A.recvTime, A.user_id, A.live_id, A.fund_code, A.fund_name, cast(B.balance as&#010;double)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;Exception in thread \"main\" java.lang.AssertionError&#010;&gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:4448)&#010;&gt;at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3765)&#010;&gt;at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3737)&#010;&gt;at org.apache.calcite.sql2rel.SqlToRelConverter.access$2200(SqlToRelConverter.java:217)&#010;&gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4796)&#010;&gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4092)&#010;&gt;at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317)&#010;&gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4656)&#010;&gt;at org.apache.calcite.sql2rel.StandardConvertletTable.convertCast(StandardConvertletTable.java:522)&#010;&gt;at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63)&#010;",
        "depth": "1",
        "reply": "<22be617d.69a4.17332c7b030.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAELO930UHemHBs+fQG3-51y=gNKGsjBo5UJHPg7LMuJQZR-2ZA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 02:42:12 GMT",
        "subject": "Re: flink 双流join报错，java.lang.AssertionError",
        "content": "cc @Danny Chan &lt;danny0405@apache.org&gt;  也许 Danny 老师知道。&#010;&#010;On Thu, 9 Jul 2020 at 17:29, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&#010;&gt;&#010;&gt; hi,&#010;&gt; 我切到最新的1.11 release版本，跑同样的sql，没有抛出异常。想问下这有相关的issue么？想确认下原因。&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-09 16:53:34，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt; &gt;hi,&#010;&gt; &gt;我使用flink 1.10.1 blink-planner。运行以下SQL时，抛出异常。其中A和B分别是两个Kafka消息流。任务使用processtime。如果我把join的B表的select&#010;具体字段名 修改为 select *，貌似就可以执行。但是拿到的B表字段顺序貌似是错乱的。请问这个问题是bug么？&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;select A.recvTime, A.khh, A.live_id, A.fund_code as product_code, A.fund_name as&#010;product_name, cast(B.balance as double) as balance&#010;&gt; &gt;from (&#010;&gt; &gt;select toLong(behaviorTime, true) as recvTime, user_id,&#010;&gt; &gt;cast(regexp_extract(btnTitle, 'zbid=\\{([^|]*)\\}', 1) as int) as live_id,&#010;&gt; &gt;regexp_extract(btnTitle, 'fundname=\\{([^|]*)\\}', 1) as fund_name,&#010;&gt; &gt;regexp_extract(btnTitle, 'fundcode=\\{([^|]*)\\}', 1) as fund_code, proctime from kafka_zl_etrack_event_stream&#010;&gt; &gt;where pageId = 'xxxx'&#010;&gt; &gt;    and eventId = 'click'&#010;&gt; &gt;    and btnId = 'xxxx&#010;&gt; &gt;    and CHARACTER_LENGTH(user_id) &gt; 4&#010;&gt; &gt;) A&#010;&gt; &gt;left join&#010;&gt; &gt;(&#010;&gt; &gt;select customerNumber, balance, fundCode, lastUpdateTime, proctime&#010;&gt; &gt;      from lscsp_sc_order_all&#010;&gt; &gt;       where `status` = '4'&#010;&gt; &gt;         and businessType IN ('4','5','14','16','17','18')&#010;&gt; &gt;         and fundCode IS NOT NULL&#010;&gt; &gt;         and balance IS NOT NULL&#010;&gt; &gt;         and lastUpdateTime IS NOT NULL&#010;&gt; &gt;) B&#010;&gt; &gt;on A.user_id = B.customerNumber and A.fund_code = B.fundCode&#010;&gt; &gt;group by  A.recvTime, A.user_id, A.live_id, A.fund_code, A.fund_name, cast(B.balance&#010;as double)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;Exception in thread \"main\" java.lang.AssertionError&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:4448)&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3765)&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3737)&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter.access$2200(SqlToRelConverter.java:217)&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4796)&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4092)&#010;&gt; &gt;at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317)&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4656)&#010;&gt; &gt;at org.apache.calcite.sql2rel.StandardConvertletTable.convertCast(StandardConvertletTable.java:522)&#010;&gt; &gt;at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<22be617d.69a4.17332c7b030.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<79ec8088.7530.1734d8daf86.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 13:39:57 GMT",
        "subject": "Re:Re: flink 双流join报错，java.lang.AssertionError",
        "content": "hi, &#010; @Danny Chan 我在1.10版本中确实触发到了这个bug，切到1.11版本貌似就没这问题了。简单解释下问题：双流join的case，右边流join后的结果字段在获取时貌似乱序了。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 10:42:12，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;cc @Danny Chan &lt;danny0405@apache.org&gt;  也许 Danny 老师知道。&#010;&gt;&#010;&gt;On Thu, 9 Jul 2020 at 17:29, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt;&#010;&gt;&gt; hi,&#010;&gt;&gt; 我切到最新的1.11 release版本，跑同样的sql，没有抛出异常。想问下这有相关的issue么？想确认下原因。&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-09 16:53:34，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt; &gt;hi,&#010;&gt;&gt; &gt;我使用flink 1.10.1 blink-planner。运行以下SQL时，抛出异常。其中A和B分别是两个Kafka消息流。任务使用processtime。如果我把join的B表的select&#010;具体字段名 修改为 select *，貌似就可以执行。但是拿到的B表字段顺序貌似是错乱的。请问这个问题是bug么？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;select A.recvTime, A.khh, A.live_id, A.fund_code as product_code, A.fund_name&#010;as product_name, cast(B.balance as double) as balance&#010;&gt;&gt; &gt;from (&#010;&gt;&gt; &gt;select toLong(behaviorTime, true) as recvTime, user_id,&#010;&gt;&gt; &gt;cast(regexp_extract(btnTitle, 'zbid=\\{([^|]*)\\}', 1) as int) as live_id,&#010;&gt;&gt; &gt;regexp_extract(btnTitle, 'fundname=\\{([^|]*)\\}', 1) as fund_name,&#010;&gt;&gt; &gt;regexp_extract(btnTitle, 'fundcode=\\{([^|]*)\\}', 1) as fund_code, proctime from&#010;kafka_zl_etrack_event_stream&#010;&gt;&gt; &gt;where pageId = 'xxxx'&#010;&gt;&gt; &gt;    and eventId = 'click'&#010;&gt;&gt; &gt;    and btnId = 'xxxx&#010;&gt;&gt; &gt;    and CHARACTER_LENGTH(user_id) &gt; 4&#010;&gt;&gt; &gt;) A&#010;&gt;&gt; &gt;left join&#010;&gt;&gt; &gt;(&#010;&gt;&gt; &gt;select customerNumber, balance, fundCode, lastUpdateTime, proctime&#010;&gt;&gt; &gt;      from lscsp_sc_order_all&#010;&gt;&gt; &gt;       where `status` = '4'&#010;&gt;&gt; &gt;         and businessType IN ('4','5','14','16','17','18')&#010;&gt;&gt; &gt;         and fundCode IS NOT NULL&#010;&gt;&gt; &gt;         and balance IS NOT NULL&#010;&gt;&gt; &gt;         and lastUpdateTime IS NOT NULL&#010;&gt;&gt; &gt;) B&#010;&gt;&gt; &gt;on A.user_id = B.customerNumber and A.fund_code = B.fundCode&#010;&gt;&gt; &gt;group by  A.recvTime, A.user_id, A.live_id, A.fund_code, A.fund_name, cast(B.balance&#010;as double)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Exception in thread \"main\" java.lang.AssertionError&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.getRootField(SqlToRelConverter.java:4448)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter.adjustInputRef(SqlToRelConverter.java:3765)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:3737)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter.access$2200(SqlToRelConverter.java:217)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4796)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.visit(SqlToRelConverter.java:4092)&#010;&gt;&gt; &gt;at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:317)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlToRelConverter$Blackboard.convertExpression(SqlToRelConverter.java:4656)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.StandardConvertletTable.convertCast(StandardConvertletTable.java:522)&#010;&gt;&gt; &gt;at org.apache.calcite.sql2rel.SqlNodeToRexConverterImpl.convertCall(SqlNodeToRexConverterImpl.java:63)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "3",
        "reply": "<22be617d.69a4.17332c7b030.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<1594284859667-0.post@n8.nabble.com>",
        "from": "liangji &lt;jiliang1...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 08:54:19 GMT",
        "subject": "flink 提交 offset 到 kafka",
        "content": "flink：1.6.2（部分集群未升级。）&#013;&#010;kafka：0.11&#013;&#010;作业从kafka中消费消息，并运行在yarn上，提供的作业未配置checkpoint，autoCommit设置为true。&#013;&#010;作业刚启动时通过kafka-console-consumer.sh可以正常观察到提交的offset，大概50分钟左右，通过kafka-console-consumer.sh就看不到相应的offset信息了（期间没有新消息），请问下flink是有什么机制吗？另外在flink&#010;web ui中看到的committed-offset metric一直显示的是 -915623761776，这是为什么？请大佬们指教，多谢。&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "0",
        "reply": "<1594284859667-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594288211058-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 09:50:11 GMT",
        "subject": "pyflink udf中发送rest api会导致udf被调用两次",
        "content": "Hi，&#010;&#010;我观察到一个现象：我定义了一个tumble window，调用一个python udf，在这个udf里面使用requests发送rest&#010;api。&#010;log显示这个udf会被调用两次。相隔不到一秒。这个是什么原因？requests库跟beam冲突了？&#010;&#010;2020-07-09 17:44:17,501 INFO  flink_test_stream_time_kafka.py:22                         &#010;&#010;[] - start to ad&#010;2020-07-09 17:44:17,530 INFO  flink_test_stream_time_kafka.py:63                         &#010;&#010;[] - start to send rest api.&#010;2020-07-09 17:44:17,532 INFO  flink_test_stream_time_kafka.py:69                         &#010;&#010;[] - Receive: {\"Received\": \"successful\"}&#010;2020-07-09 17:44:17,579 INFO &#010;/home/sysadmin/miniconda3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:564&#010;[] - Creating insecure state channel for localhost:57954.&#010;2020-07-09 17:44:17,580 INFO &#010;/home/sysadmin/miniconda3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:571&#010;[] - State channel established.&#010;2020-07-09 17:44:17,584 INFO &#010;/home/sysadmin/miniconda3/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:526&#010;[] - Creating client data channel for localhost:60902&#010;2020-07-09 17:44:17,591 INFO &#010;org.apache.beam.runners.fnexecution.data.GrpcDataService     [] - Beam Fn&#010;Data client connected.&#010;2020-07-09 17:44:17,761 INFO  flink_test_stream_time_kafka.py:22                         &#010;&#010;[] - start to ad&#010;2020-07-09 17:44:17,810 INFO  flink_test_stream_time_kafka.py:63                         &#010;&#010;[] - start to send rest api.&#010;2020-07-09 17:44:17,812 INFO  flink_test_stream_time_kafka.py:69                         &#010;&#010;[] - Receive: {\"Received\": \"successful\"}&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<9F5AFA00-B625-4C15-9932-3DA1A94B3D0E@gmail.com>",
        "from": "Dian Fu &lt;dian0511...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 11:23:14 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "Table API的作业在执行之前会经过一系列的rule优化，最终的执行计划，存在一个UDF调用多次的可能，你可以把执行计划打印出来看看(TableEnvironment#explain)。&#010;&#010;具体原因，需要看一下作业逻辑。可以发一下你的作业吗？可重现代码即可。&#010;&#010;&gt; 在 2020年7月9日，下午5:50，lgs &lt;9925174@qq.com&gt; 写道：&#010;&gt; &#010;&gt; Hi，&#010;&gt; &#010;&gt; 我观察到一个现象：我定义了一个tumble window，调用一个python udf，在这个udf里面使用requests发送rest&#010;api。&#010;&gt; log显示这个udf会被调用两次。相隔不到一秒。这个是什么原因？requests库跟beam冲突了？&#010;&gt; &#010;&gt; 2020-07-09 17:44:17,501 INFO  flink_test_stream_time_kafka.py:22                    &#010;     &#010;&gt; [] - start to ad&#010;&gt; 2020-07-09 17:44:17,530 INFO  flink_test_stream_time_kafka.py:63                    &#010;     &#010;&gt; [] - start to send rest api.&#010;&gt; 2020-07-09 17:44:17,532 INFO  flink_test_stream_time_kafka.py:69                    &#010;     &#010;&gt; [] - Receive: {\"Received\": \"successful\"}&#010;&gt; 2020-07-09 17:44:17,579 INFO &#010;&gt; /home/sysadmin/miniconda3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:564&#010;&gt; [] - Creating insecure state channel for localhost:57954.&#010;&gt; 2020-07-09 17:44:17,580 INFO &#010;&gt; /home/sysadmin/miniconda3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py:571&#010;&gt; [] - State channel established.&#010;&gt; 2020-07-09 17:44:17,584 INFO &#010;&gt; /home/sysadmin/miniconda3/lib/python3.7/site-packages/apache_beam/runners/worker/data_plane.py:526&#010;&gt; [] - Creating client data channel for localhost:60902&#010;&gt; 2020-07-09 17:44:17,591 INFO &#010;&gt; org.apache.beam.runners.fnexecution.data.GrpcDataService     [] - Beam Fn&#010;&gt; Data client connected.&#010;&gt; 2020-07-09 17:44:17,761 INFO  flink_test_stream_time_kafka.py:22                    &#010;     &#010;&gt; [] - start to ad&#010;&gt; 2020-07-09 17:44:17,810 INFO  flink_test_stream_time_kafka.py:63                    &#010;     &#010;&gt; [] - start to send rest api.&#010;&gt; 2020-07-09 17:44:17,812 INFO  flink_test_stream_time_kafka.py:69                    &#010;     &#010;&gt; [] - Receive: {\"Received\": \"successful\"}&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594346930158-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:08:50 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "谢谢提示。&#010;我打印出来explain，发现确实调用了两次udf，条件是那个eventtime.isNotNull：&#010;&#010;&#010;&#010;    st_env.scan(\"source\") \\&#010;         .where(\"action === 'Insert'\") \\&#010;        &#010;.window(Tumble.over(\"1.hour\").on(\"actionTime\").alias(\"hourlywindow\")) \\&#010;         .group_by(\"hourlywindow\") \\&#010;         .select(\"action.max as action1, conv_string(eventTime.collect) as&#010;etlist, hourlywindow.start as time1\") \\&#010;         .select(\"action1 as action, hbf_thres(etlist) as eventtime, time1&#010;as actiontime\") \\&#010;*         .filter(\"eventtime.isNotNull\") \\&#010;*         .insert_into(\"alarm_ad\")&#010;&#010;&#010;LegacySink(name=[`default_catalog`.`default_database`.`alarm_ad`],&#010;fields=[action, eventtime, actiontime])&#010;+- Calc(select=[EXPR$0 AS action, f0 AS eventtime, EXPR$2 AS actiontime])&#010;*   +- PythonCalc(select=[EXPR$0, EXPR$2, simple_udf(f0) AS f0])&#010;      +- Calc(select=[EXPR$0, EXPR$2, UDFLength(EXPR$1) AS f0], where=[IS&#010;NOT NULL(f0)])&#010;*         +- PythonCalc(select=[EXPR$0, EXPR$1, EXPR$2, simple_udf(f0) AS&#010;f0])&#010;            +- Calc(select=[EXPR$0, EXPR$1, EXPR$2, UDFLength(EXPR$1) AS&#010;f0])&#010;               +-&#010;GroupWindowAggregate(window=[TumblingGroupWindow('hourlywindow, actionTime,&#010;3600000)], properties=[EXPR$2], select=[MAX(action) AS EXPR$0,&#010;COLLECT(eventTime) AS EXPR$1, start('hourlywindow) AS EXPR$2])&#010;                  +- Exchange(distribution=[single])&#010;                     +- Calc(select=[recordId, action, originalState,&#010;newState, originalCause, newCause, ser_name, enb, eventTime, ceasedTime,&#010;duration, acked, pmdId, pmdTime, actionTime], where=[=(action,&#010;_UTF-16LE'Insert')])&#010;                        +- Reused(reference_id=[1])&#010;&#010;我这里是想过滤python udf的返回，如果返回是空，我就不要sink。是我的sql写错了吗？&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "2",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<6BA88751-D93B-45D0-B153-2063D99C2CE8@gmail.com>",
        "from": "Dian Fu &lt;dian0511...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 04:48:30 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "好的，针对你这个case，这个是个已知问题：https://issues.apache.org/jira/browse/FLINK-15973&#010;&lt;https://issues.apache.org/jira/browse/FLINK-15973&gt;，暂时还没有修复。&#010;&#010;&#010;你可以这样改写一下，应该可以绕过去这个问题：&#010;&#010; table = st_env.scan(\"source\") \\&#010;        .where(\"action === 'Insert'\") \\&#010;        .window(Tumble.over(\"1.hour\").on(\"actionTime\").alias(\"hourlywindow\")) \\&#010;        .group_by(\"hourlywindow\") \\&#010;        .select(\"action.max as action1, conv_string(eventTime.collect) as etlist, hourlywindow.start&#010;as time1\") \\&#010;        .select(\"action1 as action, hbf_thres(etlist) as eventtime, time1as actiontime\")&#010;&#010;st_env.create_temporary_view(\"tmp\", table)&#010;st_env.scan(\"tmp\").filter(\"eventtime.isNotNull\").insert_into(\"alarm_ad\")&#010;&#010;&#010;&gt; 在 2020年7月10日，上午10:08，lgs &lt;9925174@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 谢谢提示。&#010;&gt; 我打印出来explain，发现确实调用了两次udf，条件是那个eventtime.isNotNull：&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt;    st_env.scan(\"source\") \\&#010;&gt;         .where(\"action === 'Insert'\") \\&#010;&gt; &#010;&gt; .window(Tumble.over(\"1.hour\").on(\"actionTime\").alias(\"hourlywindow\")) \\&#010;&gt;         .group_by(\"hourlywindow\") \\&#010;&gt;         .select(\"action.max as action1, conv_string(eventTime.collect) as&#010;&gt; etlist, hourlywindow.start as time1\") \\&#010;&gt;         .select(\"action1 as action, hbf_thres(etlist) as eventtime, time1&#010;&gt; as actiontime\") \\&#010;&gt; *         .filter(\"eventtime.isNotNull\") \\&#010;&gt; *         .insert_into(\"alarm_ad\")&#010;&gt; &#010;&gt; &#010;&gt; LegacySink(name=[`default_catalog`.`default_database`.`alarm_ad`],&#010;&gt; fields=[action, eventtime, actiontime])&#010;&gt; +- Calc(select=[EXPR$0 AS action, f0 AS eventtime, EXPR$2 AS actiontime])&#010;&gt; *   +- PythonCalc(select=[EXPR$0, EXPR$2, simple_udf(f0) AS f0])&#010;&gt;      +- Calc(select=[EXPR$0, EXPR$2, UDFLength(EXPR$1) AS f0], where=[IS&#010;&gt; NOT NULL(f0)])&#010;&gt; *         +- PythonCalc(select=[EXPR$0, EXPR$1, EXPR$2, simple_udf(f0) AS&#010;&gt; f0])&#010;&gt;            +- Calc(select=[EXPR$0, EXPR$1, EXPR$2, UDFLength(EXPR$1) AS&#010;&gt; f0])&#010;&gt;               +-&#010;&gt; GroupWindowAggregate(window=[TumblingGroupWindow('hourlywindow, actionTime,&#010;&gt; 3600000)], properties=[EXPR$2], select=[MAX(action) AS EXPR$0,&#010;&gt; COLLECT(eventTime) AS EXPR$1, start('hourlywindow) AS EXPR$2])&#010;&gt;                  +- Exchange(distribution=[single])&#010;&gt;                     +- Calc(select=[recordId, action, originalState,&#010;&gt; newState, originalCause, newCause, ser_name, enb, eventTime, ceasedTime,&#010;&gt; duration, acked, pmdId, pmdTime, actionTime], where=[=(action,&#010;&gt; _UTF-16LE'Insert')])&#010;&gt;                        +- Reused(reference_id=[1])&#010;&gt; &#010;&gt; 我这里是想过滤python udf的返回，如果返回是空，我就不要sink。是我的sql写错了吗？&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594363441143-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 06:44:01 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "谢谢建议。&#013;&#010;我照着代码试了一下，发现还是一样的结果。&#013;&#010;udf还是会被调用两次&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "4",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<E935EAB8-83E0-463F-B72B-6D199788A4D5@gmail.com>",
        "from": "Dian Fu &lt;dian0511...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 07:13:35 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "这样再试试？&#010;&#010;tmp_table = st_env.scan(\"source\") \\&#010;        .where(\"action === 'Insert'\") \\&#010;        .window(Tumble.over(\"1.hour\").on(\"actionTime\").alias(\"hourlywindow\")) \\&#010;        .group_by(\"hourlywindow\") \\&#010;        .select(\"action.max as action1, conv_string(eventTime.collect) as etlist, hourlywindow.start&#010;as time1\") \\&#010;        .select(\"action1 as action, hbf_thres(etlist) as eventtime, time1 as actiontime\")&#010;&#010;ds = st_env._j_tenv.toAppendStream(tmp_table._j_table, tmp_table._j_table.getSchema().toRowType())&#010;table = Table(st_env._j_tenv.fromDataStream(ds, \"action, eventtime, actiontime\"))&#010;table.filter(\"eventtime.isNotNull\").insert_into(\"alarm_ad\")&#010;&#010;&gt; 在 2020年7月10日，下午2:44，lgs &lt;9925174@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 谢谢建议。&#010;&gt; 我照着代码试了一下，发现还是一样的结果。&#010;&gt; udf还是会被调用两次&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;&#010;",
        "depth": "5",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594368617747-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 08:10:17 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "这次可以了。谢谢&#013;&#010;&#013;&#010;另外还有一个问题请教一下：&#013;&#010;我实际上是有另一个sink，source是同一个。&#013;&#010;第一个sink是直接保存kafka数据到DB。&#013;&#010;第二个sink是读取kafka，tumble window，然后在udf里面去读取DB。&#013;&#010;&#013;&#010;要怎么样保证第一个sink写完了DB，然后第二个sink的udf能读取到最新的数据？&#013;&#010;&#013;&#010;代码的顺序就能保证吗？&#013;&#010;&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "6",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<CF4CF773-0287-467F-8F7A-E8C52F493E82@gmail.com>",
        "from": "Dian Fu &lt;dian0511...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 08:46:15 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "我不太明白你说的“代码顺序”指的什么？&#013;&#010;&#013;&#010;据我所知，应该没有什么太好的办法。从执行图上来看，这2个之间没有依赖关系，所以也就无法保证先后顺序。&#013;&#010;&#013;&#010;如果必须这样干的话，你得从业务的角度想一下，改造一下业务逻辑。&#013;&#010;&#013;&#010;&#013;&#010;&gt; 在 2020年7月10日，下午4:10，lgs &lt;9925174@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 这次可以了。谢谢&#013;&#010;&gt; &#013;&#010;&gt; 另外还有一个问题请教一下：&#013;&#010;&gt; 我实际上是有另一个sink，source是同一个。&#013;&#010;&gt; 第一个sink是直接保存kafka数据到DB。&#013;&#010;&gt; 第二个sink是读取kafka，tumble window，然后在udf里面去读取DB。&#013;&#010;&gt; &#013;&#010;&gt; 要怎么样保证第一个sink写完了DB，然后第二个sink的udf能读取到最新的数据？&#013;&#010;&gt; &#013;&#010;&gt; 代码的顺序就能保证吗？&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&#013;&#010;",
        "depth": "7",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594372109476-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 09:08:29 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "代码顺序是指我先写第一个sink的代码，再写第二个sink的代码。&#013;&#010;&#013;&#010;我设置了'connector.write.flush.max-rows' = '1'&#013;&#010;第一个sink没有窗口，所以直接写了&#013;&#010;&#013;&#010;第二个sink有窗口，所以是会在一个小时的最后触发。&#013;&#010;&#013;&#010;可能这样就能保证第二个sink能够读到最新的数据。&#013;&#010;&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "8",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<1A70528F-D60B-4594-9A11-BB2DAF96D790@gmail.com>",
        "from": "Dian Fu &lt;dian0511...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 11:57:36 GMT",
        "subject": "Re: pyflink udf中发送rest api会导致udf被调用两次",
        "content": "大部分情况下，可以work，但是有一些边界的情况，可能会有问题。比如第一个sink的作业，由于某种原因，处理得比较慢，延迟比较大？&#013;&#010;&#013;&#010;也就是说，通常情况下可能没有问题，但是由于这2个作业之间没有任何依赖关系，这个先后顺序是得不到保证的。&#013;&#010;&#013;&#010;我觉得你可以测一下，如果能接受那些极端情况，就可以。&#013;&#010;&#013;&#010;&gt; 在 2020年7月10日，下午5:08，lgs &lt;9925174@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 代码顺序是指我先写第一个sink的代码，再写第二个sink的代码。&#013;&#010;&gt; &#013;&#010;&gt; 我设置了'connector.write.flush.max-rows' = '1'&#013;&#010;&gt; 第一个sink没有窗口，所以直接写了&#013;&#010;&gt; &#013;&#010;&gt; 第二个sink有窗口，所以是会在一个小时的最后触发。&#013;&#010;&gt; &#013;&#010;&gt; 可能这样就能保证第二个sink能够读到最新的数据。&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&#013;&#010;",
        "depth": "9",
        "reply": "<1594288211058-0.post@n8.nabble.com>"
    },
    {
        "id": "<202007091956412586872@sinoiov.com>",
        "from": "&quot;maqi@sinoiov.com&quot; &lt;m...@sinoiov.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 11:57:22 GMT",
        "subject": "flink1.10.1在yarn上无法写入kafka的问题",
        "content": "&#013;&#010;请教各位：&#013;&#010;flink任务在本机写入测试环境kafka集群没问题，&#013;&#010;&#013;&#010;但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#013;&#010;&#013;&#010;异常信息如下：&#013;&#010;&#013;&#010;2020-07-09 19:17:33,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph     &#010;  - KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from DEPLOYING to RUNNING.&#013;&#010;2020-07-09 19:17:33,164 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph     &#010;  - KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from DEPLOYING to RUNNING.&#013;&#010;2020-07-09 19:17:39,049 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph     &#010;  - async wait operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f) switched&#010;from RUNNING to FAILE&#013;&#010;D.&#013;&#010;org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka:&#010;&#013;&#010;at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#013;&#010;at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#013;&#010;at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#013;&#010;at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#013;&#010;at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#013;&#010;at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#013;&#010;at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#013;&#010;at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#013;&#010;at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#013;&#010;at org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#013;&#010;at org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<CAMhjQvgzJFgsnO=a+97ceVfF2YULDJ0LMg=AmO=SP4eGPpjcJw@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 13:06:26 GMT",
        "subject": "Re: flink1.10.1在yarn上无法写入kafka的问题",
        "content": "hi，maqi&#010;&#010;有完整的日志吗？在这个异常之前还有其他的异常信息吗？如果有，可以提供一下！&#010;&#010;Best，&#010;zhisheng&#010;&#010;maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午7:57写道：&#010;&#010;&gt;&#010;&gt; 请教各位：&#010;&gt; flink任务在本机写入测试环境kafka集群没问题，&#010;&gt;&#010;&gt; 但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#010;&gt;&#010;&gt; 异常信息如下：&#010;&gt;&#010;&gt; 2020-07-09 19:17:33,126 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt; KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from&#010;&gt; DEPLOYING to RUNNING.&#010;&gt; 2020-07-09 19:17:33,164 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt; KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from&#010;&gt; DEPLOYING to RUNNING.&#010;&gt; 2020-07-09 19:17:39,049 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        - async wait&#010;&gt; operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f) switched&#010;&gt; from RUNNING to FAILE&#010;&gt; D.&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to&#010;&gt; send data to Kafka:&#010;&gt; at&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<202007092121348967469@sinoiov.com>",
        "from": "&quot;maqi@sinoiov.com&quot; &lt;m...@sinoiov.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 13:21:35 GMT",
        "subject": "Re: Re: flink1.10.1在yarn上无法写入kafka的问题",
        "content": "hi：zhisheng：&#013;&#010;&#013;&#010;这是TM日志，在这之前没有任何错误日志，&#013;&#010;&#013;&#010;代码逻辑很简单：&#013;&#010;SingleOutputStreamOperator&lt;ConcurrentLinkedQueue&lt;ProtocolEvent&gt;&gt; sourceStream&#010;= env.addSource(source)&#013;&#010;        .setParallelism(2)&#013;&#010;        .uid(\"DataProcessSource\")&#013;&#010;        .flatMap(new DataConvertFunction())&#013;&#010;        .setParallelism(2)&#013;&#010;        .uid(\"DataProcessDataCovert\")&#013;&#010;        .keyBy(new KeySelectorFunction())&#013;&#010;        .process(new DataCleanFunction())&#013;&#010;        .setParallelism(2)&#013;&#010;        .uid(\"DataProcessDataProcess\");&#013;&#010;&#013;&#010;AsyncDataStream.orderedWait(&#013;&#010;        sourceStream,&#013;&#010;        new AsyncDataCleanFunction(),&#013;&#010;        EnvUtil.TOOL.getLong(Constant.ASYNC_TOMEOUT),&#013;&#010;        TimeUnit.MILLISECONDS,&#013;&#010;        EnvUtil.TOOL.getInt(Constant.ASYNC_CAPACITY)&#013;&#010;).uid(\"DataProcessAsync\")&#013;&#010;        .setParallelism(2)&#013;&#010;        .addSink(sink)&#013;&#010;        .uid(\"DataProcessSinkKafka\")&#013;&#010;        .setParallelism(2);&#013;&#010;&#013;&#010;2020-07-09 19:33:37,291 WARN org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;'gps.kafka.sasl' was supplied but isn't a known config.&#013;&#010;2020-07-09 19:33:37,291 WARN org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;'java.ext.dirs' was supplied but isn't a known config.&#013;&#010;2020-07-09 19:33:37,291 WARN org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;'java.class.version' was supplied but isn't a known config.&#013;&#010;2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version:&#010;2.2.0&#013;&#010;2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId:&#010;05fcfde8f69b0349&#013;&#010;2020-07-09 19:33:38,482 INFO com.sinoi.rt.common.protocol.HttpPoolUtil - http pool init,maxTotal:18,maxPerRoute:6&#013;&#010;2020-07-09 19:33:38,971 WARN org.apache.kafka.clients.NetworkClient - [Producer clientId=producer-1]&#010;Error while fetching metadata with correlation id 8 : {=INVALID_TOPIC_EXCEPTION}&#013;&#010;2020-07-09 19:33:38,974 INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-1]&#010;Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.&#013;&#010;2020-07-09 19:33:41,612 INFO org.apache.flink.runtime.taskmanager.Task - async wait operator&#010;-&gt; Sink: Unnamed (1/2) (cdbe008dcdb76813f88c4a48b9907d77) switched from RUNNING to FAILED.&#013;&#010;org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka:&#010;&#013;&#010;    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#013;&#010;    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#013;&#010;    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#013;&#010;    at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#013;&#010;    at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#013;&#010;    at org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#013;&#010;    at org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#013;&#010;    at org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#013;&#010;    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:279)&#013;&#010;    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:76)&#013;&#010;    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:351)&#013;&#010;    at org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:336)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#013;&#010;    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#013;&#010;    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#013;&#010;    at java.lang.Thread.run(Thread.java:748)&#013;&#010;Caused by: org.apache.kafka.common.errors.InvalidTopicException: &#013;&#010;2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task - Freeing task resources&#010;for async wait operator -&gt; Sink: Unnamed (1/2) (cdbe008dcdb76813f88c4a48b9907d77).&#013;&#010;2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task - Ensuring all FileSystem&#010;streams are closed for task async wait operator -&gt; Sink: Unnamed (1/2) &#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010; &#013;&#010;发件人： zhisheng&#013;&#010;发送时间： 2020-07-09 21:06&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: flink1.10.1在yarn上无法写入kafka的问题&#013;&#010;hi，maqi&#013;&#010; &#013;&#010;有完整的日志吗？在这个异常之前还有其他的异常信息吗？如果有，可以提供一下！&#013;&#010; &#013;&#010;Best，&#013;&#010;zhisheng&#013;&#010; &#013;&#010;maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午7:57写道：&#013;&#010; &#013;&#010;&gt;&#013;&#010;&gt; 请教各位：&#013;&#010;&gt; flink任务在本机写入测试环境kafka集群没问题，&#013;&#010;&gt;&#013;&#010;&gt; 但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#013;&#010;&gt;&#013;&#010;&gt; 异常信息如下：&#013;&#010;&gt;&#013;&#010;&gt; 2020-07-09 19:17:33,126 INFO&#013;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#013;&#010;&gt; KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from&#013;&#010;&gt; DEPLOYING to RUNNING.&#013;&#010;&gt; 2020-07-09 19:17:33,164 INFO&#013;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#013;&#010;&gt; KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from&#013;&#010;&gt; DEPLOYING to RUNNING.&#013;&#010;&gt; 2020-07-09 19:17:39,049 INFO&#013;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        - async wait&#013;&#010;&gt; operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f) switched&#013;&#010;&gt; from RUNNING to FAILE&#013;&#010;&gt; D.&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to&#013;&#010;&gt; send data to Kafka:&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<CACaQKu61DWxspigWT_mqhc68QUJZKsGShbb=WNv_+fq6jmweQw@mail.gmail.com>",
        "from": "LakeShen &lt;shenleifight...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:08:42 GMT",
        "subject": "Re: Re: flink1.10.1在yarn上无法写入kafka的问题",
        "content": "Hi,&#010;&#010;从日志看出，应该是你提交到 Yarn 的环境，这个环境和你的测试环境的&#010;kafka 连接不上，获取不到元数据。&#010;&#010;这里你检查一下你的 Yarn 环境，Flink kafka broker 地址是否是测试环境的&#010;kafka broker 地址。&#010;&#010;Best,&#010;LakeShen&#010;&#010;maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午9:21写道：&#010;&#010;&gt; hi：zhisheng：&#010;&gt;&#010;&gt; 这是TM日志，在这之前没有任何错误日志，&#010;&gt;&#010;&gt; 代码逻辑很简单：&#010;&gt; SingleOutputStreamOperator&lt;ConcurrentLinkedQueue&lt;ProtocolEvent&gt;&gt;&#010;&gt; sourceStream = env.addSource(source)&#010;&gt;         .setParallelism(2)&#010;&gt;         .uid(\"DataProcessSource\")&#010;&gt;         .flatMap(new DataConvertFunction())&#010;&gt;         .setParallelism(2)&#010;&gt;         .uid(\"DataProcessDataCovert\")&#010;&gt;         .keyBy(new KeySelectorFunction())&#010;&gt;         .process(new DataCleanFunction())&#010;&gt;         .setParallelism(2)&#010;&gt;         .uid(\"DataProcessDataProcess\");&#010;&gt;&#010;&gt; AsyncDataStream.orderedWait(&#010;&gt;         sourceStream,&#010;&gt;         new AsyncDataCleanFunction(),&#010;&gt;         EnvUtil.TOOL.getLong(Constant.ASYNC_TOMEOUT),&#010;&gt;         TimeUnit.MILLISECONDS,&#010;&gt;         EnvUtil.TOOL.getInt(Constant.ASYNC_CAPACITY)&#010;&gt; ).uid(\"DataProcessAsync\")&#010;&gt;         .setParallelism(2)&#010;&gt;         .addSink(sink)&#010;&gt;         .uid(\"DataProcessSinkKafka\")&#010;&gt;         .setParallelism(2);&#010;&gt;&#010;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt; 'gps.kafka.sasl' was supplied but isn't a known config.&#010;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt; 'java.ext.dirs' was supplied but isn't a known config.&#010;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt; 'java.class.version' was supplied but isn't a known config.&#010;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser -&#010;&gt; Kafka version: 2.2.0&#010;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser -&#010;&gt; Kafka commitId: 05fcfde8f69b0349&#010;&gt; 2020-07-09 19:33:38,482 INFO com.sinoi.rt.common.protocol.HttpPoolUtil -&#010;&gt; http pool init,maxTotal:18,maxPerRoute:6&#010;&gt; 2020-07-09 19:33:38,971 WARN org.apache.kafka.clients.NetworkClient -&#010;&gt; [Producer clientId=producer-1] Error while fetching metadata with&#010;&gt; correlation id 8 : {=INVALID_TOPIC_EXCEPTION}&#010;&gt; 2020-07-09 19:33:38,974 INFO&#010;&gt; org.apache.kafka.clients.producer.KafkaProducer - [Producer&#010;&gt; clientId=producer-1] Closing the Kafka producer with timeoutMillis =&#010;&gt; 9223372036854775807 ms.&#010;&gt; 2020-07-09 19:33:41,612 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt; async wait operator -&gt; Sink: Unnamed (1/2)&#010;&gt; (cdbe008dcdb76813f88c4a48b9907d77) switched from RUNNING to FAILED.&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed to&#010;&gt; send data to Kafka:&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:279)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:76)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:351)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:336)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt;     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt;     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt;     at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: org.apache.kafka.common.errors.InvalidTopicException:&#010;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt; Freeing task resources for async wait operator -&gt; Sink: Unnamed (1/2)&#010;&gt; (cdbe008dcdb76813f88c4a48b9907d77).&#010;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt; Ensuring all FileSystem streams are closed for task async wait operator -&gt;&#010;&gt; Sink: Unnamed (1/2)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 发件人： zhisheng&#010;&gt; 发送时间： 2020-07-09 21:06&#010;&gt; 收件人： user-zh&#010;&gt; 主题： Re: flink1.10.1在yarn上无法写入kafka的问题&#010;&gt; hi，maqi&#010;&gt;&#010;&gt; 有完整的日志吗？在这个异常之前还有其他的异常信息吗？如果有，可以提供一下！&#010;&gt;&#010;&gt; Best，&#010;&gt; zhisheng&#010;&gt;&#010;&gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午7:57写道：&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; 请教各位：&#010;&gt; &gt; flink任务在本机写入测试环境kafka集群没问题，&#010;&gt; &gt;&#010;&gt; &gt; 但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#010;&gt; &gt;&#010;&gt; &gt; 异常信息如下：&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-09 19:17:33,126 INFO&#010;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt; &gt; KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from&#010;&gt; &gt; DEPLOYING to RUNNING.&#010;&gt; &gt; 2020-07-09 19:17:33,164 INFO&#010;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt; &gt; KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from&#010;&gt; &gt; DEPLOYING to RUNNING.&#010;&gt; &gt; 2020-07-09 19:17:39,049 INFO&#010;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        - async&#010;&gt; wait&#010;&gt; &gt; operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f)&#010;&gt; switched&#010;&gt; &gt; from RUNNING to FAILE&#010;&gt; &gt; D.&#010;&gt; &gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed&#010;&gt; to&#010;&gt; &gt; send data to Kafka:&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<CACaQKu6j8w36RM43SLVopMpH0hgRLVP3Jieqf=v-zPZyftZBAQ@mail.gmail.com>",
        "from": "LakeShen &lt;shenleifight...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:10:03 GMT",
        "subject": "Re: Re: flink1.10.1在yarn上无法写入kafka的问题",
        "content": "你的 Yarn 环境，Flink 任务使用的 Kafka 地址，应该是 Yarn 环境的 kafka broker&#010;地址。&#010;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:08写道：&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; 从日志看出，应该是你提交到 Yarn 的环境，这个环境和你的测试环境的&#010;kafka 连接不上，获取不到元数据。&#010;&gt;&#010;&gt; 这里你检查一下你的 Yarn 环境，Flink kafka broker 地址是否是测试环境的&#010;kafka broker 地址。&#010;&gt;&#010;&gt; Best,&#010;&gt; LakeShen&#010;&gt;&#010;&gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午9:21写道：&#010;&gt;&#010;&gt;&gt; hi：zhisheng：&#010;&gt;&gt;&#010;&gt;&gt; 这是TM日志，在这之前没有任何错误日志，&#010;&gt;&gt;&#010;&gt;&gt; 代码逻辑很简单：&#010;&gt;&gt; SingleOutputStreamOperator&lt;ConcurrentLinkedQueue&lt;ProtocolEvent&gt;&gt;&#010;&gt;&gt; sourceStream = env.addSource(source)&#010;&gt;&gt;         .setParallelism(2)&#010;&gt;&gt;         .uid(\"DataProcessSource\")&#010;&gt;&gt;         .flatMap(new DataConvertFunction())&#010;&gt;&gt;         .setParallelism(2)&#010;&gt;&gt;         .uid(\"DataProcessDataCovert\")&#010;&gt;&gt;         .keyBy(new KeySelectorFunction())&#010;&gt;&gt;         .process(new DataCleanFunction())&#010;&gt;&gt;         .setParallelism(2)&#010;&gt;&gt;         .uid(\"DataProcessDataProcess\");&#010;&gt;&gt;&#010;&gt;&gt; AsyncDataStream.orderedWait(&#010;&gt;&gt;         sourceStream,&#010;&gt;&gt;         new AsyncDataCleanFunction(),&#010;&gt;&gt;         EnvUtil.TOOL.getLong(Constant.ASYNC_TOMEOUT),&#010;&gt;&gt;         TimeUnit.MILLISECONDS,&#010;&gt;&gt;         EnvUtil.TOOL.getInt(Constant.ASYNC_CAPACITY)&#010;&gt;&gt; ).uid(\"DataProcessAsync\")&#010;&gt;&gt;         .setParallelism(2)&#010;&gt;&gt;         .addSink(sink)&#010;&gt;&gt;         .uid(\"DataProcessSinkKafka\")&#010;&gt;&gt;         .setParallelism(2);&#010;&gt;&gt;&#010;&gt;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt;&gt; 'gps.kafka.sasl' was supplied but isn't a known config.&#010;&gt;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt;&gt; 'java.ext.dirs' was supplied but isn't a known config.&#010;&gt;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt;&gt; 'java.class.version' was supplied but isn't a known config.&#010;&gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#010;&gt;&gt; - Kafka version: 2.2.0&#010;&gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#010;&gt;&gt; - Kafka commitId: 05fcfde8f69b0349&#010;&gt;&gt; 2020-07-09 19:33:38,482 INFO com.sinoi.rt.common.protocol.HttpPoolUtil -&#010;&gt;&gt; http pool init,maxTotal:18,maxPerRoute:6&#010;&gt;&gt; 2020-07-09 19:33:38,971 WARN org.apache.kafka.clients.NetworkClient -&#010;&gt;&gt; [Producer clientId=producer-1] Error while fetching metadata with&#010;&gt;&gt; correlation id 8 : {=INVALID_TOPIC_EXCEPTION}&#010;&gt;&gt; 2020-07-09 19:33:38,974 INFO&#010;&gt;&gt; org.apache.kafka.clients.producer.KafkaProducer - [Producer&#010;&gt;&gt; clientId=producer-1] Closing the Kafka producer with timeoutMillis =&#010;&gt;&gt; 9223372036854775807 ms.&#010;&gt;&gt; 2020-07-09 19:33:41,612 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt;&gt; async wait operator -&gt; Sink: Unnamed (1/2)&#010;&gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77) switched from RUNNING to FAILED.&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed&#010;&gt;&gt; to send data to Kafka:&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:279)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:76)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:351)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:336)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt;&gt;     at java.lang.Thread.run(Thread.java:748)&#010;&gt;&gt; Caused by: org.apache.kafka.common.errors.InvalidTopicException:&#010;&gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt;&gt; Freeing task resources for async wait operator -&gt; Sink: Unnamed (1/2)&#010;&gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77).&#010;&gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt;&gt; Ensuring all FileSystem streams are closed for task async wait operator -&gt;&#010;&gt;&gt; Sink: Unnamed (1/2)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 发件人： zhisheng&#010;&gt;&gt; 发送时间： 2020-07-09 21:06&#010;&gt;&gt; 收件人： user-zh&#010;&gt;&gt; 主题： Re: flink1.10.1在yarn上无法写入kafka的问题&#010;&gt;&gt; hi，maqi&#010;&gt;&gt;&#010;&gt;&gt; 有完整的日志吗？在这个异常之前还有其他的异常信息吗？如果有，可以提供一下！&#010;&gt;&gt;&#010;&gt;&gt; Best，&#010;&gt;&gt; zhisheng&#010;&gt;&gt;&#010;&gt;&gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午7:57写道：&#010;&gt;&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 请教各位：&#010;&gt;&gt; &gt; flink任务在本机写入测试环境kafka集群没问题，&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 异常信息如下：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 2020-07-09 19:17:33,126 INFO&#010;&gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt;&gt; &gt; KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from&#010;&gt;&gt; &gt; DEPLOYING to RUNNING.&#010;&gt;&gt; &gt; 2020-07-09 19:17:33,164 INFO&#010;&gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt;&gt; &gt; KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from&#010;&gt;&gt; &gt; DEPLOYING to RUNNING.&#010;&gt;&gt; &gt; 2020-07-09 19:17:39,049 INFO&#010;&gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        - async&#010;&gt;&gt; wait&#010;&gt;&gt; &gt; operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f)&#010;&gt;&gt; switched&#010;&gt;&gt; &gt; from RUNNING to FAILE&#010;&gt;&gt; &gt; D.&#010;&gt;&gt; &gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed&#010;&gt;&gt; to&#010;&gt;&gt; &gt; send data to Kafka:&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<CAA8tFvs3SSibNLjZPCkMmAkSPzGH6xifLsCoD0z2h1w_7e73Vg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:20:04 GMT",
        "subject": "Re: Re: flink1.10.1在yarn上无法写入kafka的问题",
        "content": "Hi&#010;从 org.apache.kafka.common.errors.InvalidTopicException: 这个异常来看，是 topic&#010;invalid 导致，具体的可以看一下 InvalidTopicException 的介绍[1], 这上面说的有可能是&#010;名字太长，或者有非法字符等，这也可以查看一下&#010;&#010;[1]&#010;https://www.javadoc.io/doc/org.apache.kafka/kafka-clients/2.0.0/org/apache/kafka/common/errors/InvalidTopicException.html&#010;Best,&#010;Congxian&#010;&#010;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:10写道：&#010;&#010;&gt; 你的 Yarn 环境，Flink 任务使用的 Kafka 地址，应该是 Yarn 环境的 kafka&#010;broker 地址。&#010;&gt;&#010;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:08写道：&#010;&gt;&#010;&gt; &gt; Hi,&#010;&gt; &gt;&#010;&gt; &gt; 从日志看出，应该是你提交到 Yarn 的环境，这个环境和你的测试环境的&#010;kafka 连接不上，获取不到元数据。&#010;&gt; &gt;&#010;&gt; &gt; 这里你检查一下你的 Yarn 环境，Flink kafka broker 地址是否是测试环境的&#010;kafka broker 地址。&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; LakeShen&#010;&gt; &gt;&#010;&gt; &gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午9:21写道：&#010;&gt; &gt;&#010;&gt; &gt;&gt; hi：zhisheng：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 这是TM日志，在这之前没有任何错误日志，&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 代码逻辑很简单：&#010;&gt; &gt;&gt; SingleOutputStreamOperator&lt;ConcurrentLinkedQueue&lt;ProtocolEvent&gt;&gt;&#010;&gt; &gt;&gt; sourceStream = env.addSource(source)&#010;&gt; &gt;&gt;         .setParallelism(2)&#010;&gt; &gt;&gt;         .uid(\"DataProcessSource\")&#010;&gt; &gt;&gt;         .flatMap(new DataConvertFunction())&#010;&gt; &gt;&gt;         .setParallelism(2)&#010;&gt; &gt;&gt;         .uid(\"DataProcessDataCovert\")&#010;&gt; &gt;&gt;         .keyBy(new KeySelectorFunction())&#010;&gt; &gt;&gt;         .process(new DataCleanFunction())&#010;&gt; &gt;&gt;         .setParallelism(2)&#010;&gt; &gt;&gt;         .uid(\"DataProcessDataProcess\");&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; AsyncDataStream.orderedWait(&#010;&gt; &gt;&gt;         sourceStream,&#010;&gt; &gt;&gt;         new AsyncDataCleanFunction(),&#010;&gt; &gt;&gt;         EnvUtil.TOOL.getLong(Constant.ASYNC_TOMEOUT),&#010;&gt; &gt;&gt;         TimeUnit.MILLISECONDS,&#010;&gt; &gt;&gt;         EnvUtil.TOOL.getInt(Constant.ASYNC_CAPACITY)&#010;&gt; &gt;&gt; ).uid(\"DataProcessAsync\")&#010;&gt; &gt;&gt;         .setParallelism(2)&#010;&gt; &gt;&gt;         .addSink(sink)&#010;&gt; &gt;&gt;         .uid(\"DataProcessSinkKafka\")&#010;&gt; &gt;&gt;         .setParallelism(2);&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt; &gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt; &gt;&gt; 'gps.kafka.sasl' was supplied but isn't a known config.&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt; &gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt; &gt;&gt; 'java.ext.dirs' was supplied but isn't a known config.&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 WARN&#010;&gt; &gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#010;&gt; &gt;&gt; 'java.class.version' was supplied but isn't a known config.&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#010;&gt; &gt;&gt; - Kafka version: 2.2.0&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#010;&gt; &gt;&gt; - Kafka commitId: 05fcfde8f69b0349&#010;&gt; &gt;&gt; 2020-07-09 19:33:38,482 INFO com.sinoi.rt.common.protocol.HttpPoolUtil -&#010;&gt; &gt;&gt; http pool init,maxTotal:18,maxPerRoute:6&#010;&gt; &gt;&gt; 2020-07-09 19:33:38,971 WARN org.apache.kafka.clients.NetworkClient -&#010;&gt; &gt;&gt; [Producer clientId=producer-1] Error while fetching metadata with&#010;&gt; &gt;&gt; correlation id 8 : {=INVALID_TOPIC_EXCEPTION}&#010;&gt; &gt;&gt; 2020-07-09 19:33:38,974 INFO&#010;&gt; &gt;&gt; org.apache.kafka.clients.producer.KafkaProducer - [Producer&#010;&gt; &gt;&gt; clientId=producer-1] Closing the Kafka producer with timeoutMillis =&#010;&gt; &gt;&gt; 9223372036854775807 ms.&#010;&gt; &gt;&gt; 2020-07-09 19:33:41,612 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt; &gt;&gt; async wait operator -&gt; Sink: Unnamed (1/2)&#010;&gt; &gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77) switched from RUNNING to FAILED.&#010;&gt; &gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed&#010;&gt; &gt;&gt; to send data to Kafka:&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:279)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:76)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:351)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:336)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt; &gt;&gt;     at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt; &gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt; &gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt; &gt;&gt;     at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt;&gt; Caused by: org.apache.kafka.common.errors.InvalidTopicException:&#010;&gt; &gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt; &gt;&gt; Freeing task resources for async wait operator -&gt; Sink: Unnamed (1/2)&#010;&gt; &gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77).&#010;&gt; &gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#010;&gt; &gt;&gt; Ensuring all FileSystem streams are closed for task async wait operator&#010;&gt; -&gt;&#010;&gt; &gt;&gt; Sink: Unnamed (1/2)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 发件人： zhisheng&#010;&gt; &gt;&gt; 发送时间： 2020-07-09 21:06&#010;&gt; &gt;&gt; 收件人： user-zh&#010;&gt; &gt;&gt; 主题： Re: flink1.10.1在yarn上无法写入kafka的问题&#010;&gt; &gt;&gt; hi，maqi&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 有完整的日志吗？在这个异常之前还有其他的异常信息吗？如果有，可以提供一下！&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Best，&#010;&gt; &gt;&gt; zhisheng&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午7:57写道：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; 请教各位：&#010;&gt; &gt;&gt; &gt; flink任务在本机写入测试环境kafka集群没问题，&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; 但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; 异常信息如下：&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; 2020-07-09 19:17:33,126 INFO&#010;&gt; &gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt; &gt;&gt; &gt; KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from&#010;&gt; &gt;&gt; &gt; DEPLOYING to RUNNING.&#010;&gt; &gt;&gt; &gt; 2020-07-09 19:17:33,164 INFO&#010;&gt; &gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#010;&gt; &gt;&gt; &gt; KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from&#010;&gt; &gt;&gt; &gt; DEPLOYING to RUNNING.&#010;&gt; &gt;&gt; &gt; 2020-07-09 19:17:39,049 INFO&#010;&gt; &gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        - async&#010;&gt; &gt;&gt; wait&#010;&gt; &gt;&gt; &gt; operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f)&#010;&gt; &gt;&gt; switched&#010;&gt; &gt;&gt; &gt; from RUNNING to FAILE&#010;&gt; &gt;&gt; &gt; D.&#010;&gt; &gt;&gt; &gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#010;&gt; Failed&#010;&gt; &gt;&gt; to&#010;&gt; &gt;&gt; &gt; send data to Kafka:&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#010;&gt; &gt;&gt; &gt; at&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<2020071010192319551712@sinoiov.com>",
        "from": "&quot;maqi@sinoiov.com&quot; &lt;m...@sinoiov.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:19:23 GMT",
        "subject": "Re: Re: flink1.10.1在yarn上无法写入kafka的问题",
        "content": "hi ，LakeShen&#013;&#010;&#013;&#010;对，测试环境包括yarn集群和kafka集群，他们想联通的&#013;&#010;&#013;&#010;配置的是测试环境的kafka broker的地址&#013;&#010;&#013;&#010;road.kafka.brokers=172.17.47.134:9092,172.17.47.135:9092,172.17.47.136:9092&#013;&#010;road.kafka.topic=road-map&#013;&#010;road.kafka.group.id=ins-001&#013;&#010;road.kafka.transaction.timeout.ms=300000&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;马琪&#013;&#010;研发中心&#013;&#010;北京中交兴路车联网科技有限公司&#013;&#010;&#013;&#010;&#013;&#010;T. 010-50822710          M. 13701177502&#013;&#010;F. 010-50822899          E. maqi@sinoiov.com&#013;&#010;地址：北京市海淀区东北旺西路8号中关村软件园27号院千方科技大厦A座（100085）&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010; &#013;&#010;发件人： LakeShen&#013;&#010;发送时间： 2020-07-10 10:10&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: Re: flink1.10.1在yarn上无法写入kafka的问题&#013;&#010;你的 Yarn 环境，Flink 任务使用的 Kafka 地址，应该是 Yarn 环境的 kafka broker&#010;地址。&#013;&#010; &#013;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:08写道：&#013;&#010; &#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 从日志看出，应该是你提交到 Yarn 的环境，这个环境和你的测试环境的&#010;kafka 连接不上，获取不到元数据。&#013;&#010;&gt;&#013;&#010;&gt; 这里你检查一下你的 Yarn 环境，Flink kafka broker 地址是否是测试环境的&#010;kafka broker 地址。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; LakeShen&#013;&#010;&gt;&#013;&#010;&gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午9:21写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; hi：zhisheng：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 这是TM日志，在这之前没有任何错误日志，&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 代码逻辑很简单：&#013;&#010;&gt;&gt; SingleOutputStreamOperator&lt;ConcurrentLinkedQueue&lt;ProtocolEvent&gt;&gt;&#013;&#010;&gt;&gt; sourceStream = env.addSource(source)&#013;&#010;&gt;&gt;         .setParallelism(2)&#013;&#010;&gt;&gt;         .uid(\"DataProcessSource\")&#013;&#010;&gt;&gt;         .flatMap(new DataConvertFunction())&#013;&#010;&gt;&gt;         .setParallelism(2)&#013;&#010;&gt;&gt;         .uid(\"DataProcessDataCovert\")&#013;&#010;&gt;&gt;         .keyBy(new KeySelectorFunction())&#013;&#010;&gt;&gt;         .process(new DataCleanFunction())&#013;&#010;&gt;&gt;         .setParallelism(2)&#013;&#010;&gt;&gt;         .uid(\"DataProcessDataProcess\");&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; AsyncDataStream.orderedWait(&#013;&#010;&gt;&gt;         sourceStream,&#013;&#010;&gt;&gt;         new AsyncDataCleanFunction(),&#013;&#010;&gt;&gt;         EnvUtil.TOOL.getLong(Constant.ASYNC_TOMEOUT),&#013;&#010;&gt;&gt;         TimeUnit.MILLISECONDS,&#013;&#010;&gt;&gt;         EnvUtil.TOOL.getInt(Constant.ASYNC_CAPACITY)&#013;&#010;&gt;&gt; ).uid(\"DataProcessAsync\")&#013;&#010;&gt;&gt;         .setParallelism(2)&#013;&#010;&gt;&gt;         .addSink(sink)&#013;&#010;&gt;&gt;         .uid(\"DataProcessSinkKafka\")&#013;&#010;&gt;&gt;         .setParallelism(2);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 2020-07-09 19:33:37,291 WARN&#013;&#010;&gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#013;&#010;&gt;&gt; 'gps.kafka.sasl' was supplied but isn't a known config.&#013;&#010;&gt;&gt; 2020-07-09 19:33:37,291 WARN&#013;&#010;&gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#013;&#010;&gt;&gt; 'java.ext.dirs' was supplied but isn't a known config.&#013;&#010;&gt;&gt; 2020-07-09 19:33:37,291 WARN&#013;&#010;&gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#013;&#010;&gt;&gt; 'java.class.version' was supplied but isn't a known config.&#013;&#010;&gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#013;&#010;&gt;&gt; - Kafka version: 2.2.0&#013;&#010;&gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#013;&#010;&gt;&gt; - Kafka commitId: 05fcfde8f69b0349&#013;&#010;&gt;&gt; 2020-07-09 19:33:38,482 INFO com.sinoi.rt.common.protocol.HttpPoolUtil -&#013;&#010;&gt;&gt; http pool init,maxTotal:18,maxPerRoute:6&#013;&#010;&gt;&gt; 2020-07-09 19:33:38,971 WARN org.apache.kafka.clients.NetworkClient -&#013;&#010;&gt;&gt; [Producer clientId=producer-1] Error while fetching metadata with&#013;&#010;&gt;&gt; correlation id 8 : {=INVALID_TOPIC_EXCEPTION}&#013;&#010;&gt;&gt; 2020-07-09 19:33:38,974 INFO&#013;&#010;&gt;&gt; org.apache.kafka.clients.producer.KafkaProducer - [Producer&#013;&#010;&gt;&gt; clientId=producer-1] Closing the Kafka producer with timeoutMillis =&#013;&#010;&gt;&gt; 9223372036854775807 ms.&#013;&#010;&gt;&gt; 2020-07-09 19:33:41,612 INFO org.apache.flink.runtime.taskmanager.Task -&#013;&#010;&gt;&gt; async wait operator -&gt; Sink: Unnamed (1/2)&#013;&#010;&gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77) switched from RUNNING to FAILED.&#013;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed&#013;&#010;&gt;&gt; to send data to Kafka:&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:279)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:76)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:351)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:336)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#013;&#010;&gt;&gt;     at&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#013;&#010;&gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#013;&#010;&gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#013;&#010;&gt;&gt;     at java.lang.Thread.run(Thread.java:748)&#013;&#010;&gt;&gt; Caused by: org.apache.kafka.common.errors.InvalidTopicException:&#013;&#010;&gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#013;&#010;&gt;&gt; Freeing task resources for async wait operator -&gt; Sink: Unnamed (1/2)&#013;&#010;&gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77).&#013;&#010;&gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#013;&#010;&gt;&gt; Ensuring all FileSystem streams are closed for task async wait operator -&gt;&#013;&#010;&gt;&gt; Sink: Unnamed (1/2)&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 发件人： zhisheng&#013;&#010;&gt;&gt; 发送时间： 2020-07-09 21:06&#013;&#010;&gt;&gt; 收件人： user-zh&#013;&#010;&gt;&gt; 主题： Re: flink1.10.1在yarn上无法写入kafka的问题&#013;&#010;&gt;&gt; hi，maqi&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 有完整的日志吗？在这个异常之前还有其他的异常信息吗？如果有，可以提供一下！&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best，&#013;&#010;&gt;&gt; zhisheng&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午7:57写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 请教各位：&#013;&#010;&gt;&gt; &gt; flink任务在本机写入测试环境kafka集群没问题，&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 异常信息如下：&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 2020-07-09 19:17:33,126 INFO&#013;&#010;&gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#013;&#010;&gt;&gt; &gt; KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from&#013;&#010;&gt;&gt; &gt; DEPLOYING to RUNNING.&#013;&#010;&gt;&gt; &gt; 2020-07-09 19:17:33,164 INFO&#013;&#010;&gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#013;&#010;&gt;&gt; &gt; KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from&#013;&#010;&gt;&gt; &gt; DEPLOYING to RUNNING.&#013;&#010;&gt;&gt; &gt; 2020-07-09 19:17:39,049 INFO&#013;&#010;&gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        - async&#013;&#010;&gt;&gt; wait&#013;&#010;&gt;&gt; &gt; operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f)&#013;&#010;&gt;&gt; switched&#013;&#010;&gt;&gt; &gt; from RUNNING to FAILE&#013;&#010;&gt;&gt; &gt; D.&#013;&#010;&gt;&gt; &gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed&#013;&#010;&gt;&gt; to&#013;&#010;&gt;&gt; &gt; send data to Kafka:&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#013;&#010;&gt;&gt; &gt; at&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<2020071011444738442615@sinoiov.com>",
        "from": "&quot;maqi@sinoiov.com&quot; &lt;m...@sinoiov.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:44:47 GMT",
        "subject": "Re: Re: flink1.10.1在yarn上无法写入kafka的问题",
        "content": "hi：Congxian qiu：&#013;&#010;&#013;&#010;topic没问题，用kafka指令创建，其他应用也能写入，我换一个kafka集群也不行&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;马琪&#013;&#010;研发中心&#013;&#010;北京中交兴路车联网科技有限公司&#013;&#010;&#013;&#010;&#013;&#010;T. 010-50822710          M. 13701177502&#013;&#010;F. 010-50822899          E. maqi@sinoiov.com&#013;&#010;地址：北京市海淀区东北旺西路8号中关村软件园27号院千方科技大厦A座（100085）&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010; &#013;&#010;发件人： Congxian Qiu&#013;&#010;发送时间： 2020-07-10 10:20&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: Re: flink1.10.1在yarn上无法写入kafka的问题&#013;&#010;Hi&#013;&#010;从 org.apache.kafka.common.errors.InvalidTopicException: 这个异常来看，是 topic&#013;&#010;invalid 导致，具体的可以看一下 InvalidTopicException 的介绍[1], 这上面说的有可能是&#013;&#010;名字太长，或者有非法字符等，这也可以查看一下&#013;&#010; &#013;&#010;[1]&#013;&#010;https://www.javadoc.io/doc/org.apache.kafka/kafka-clients/2.0.0/org/apache/kafka/common/errors/InvalidTopicException.html&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010; &#013;&#010; &#013;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:10写道：&#013;&#010; &#013;&#010;&gt; 你的 Yarn 环境，Flink 任务使用的 Kafka 地址，应该是 Yarn 环境的 kafka&#010;broker 地址。&#013;&#010;&gt;&#013;&#010;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:08写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 从日志看出，应该是你提交到 Yarn 的环境，这个环境和你的测试环境的&#010;kafka 连接不上，获取不到元数据。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 这里你检查一下你的 Yarn 环境，Flink kafka broker 地址是否是测试环境的&#010;kafka broker 地址。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; LakeShen&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午9:21写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; hi：zhisheng：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 这是TM日志，在这之前没有任何错误日志，&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 代码逻辑很简单：&#013;&#010;&gt; &gt;&gt; SingleOutputStreamOperator&lt;ConcurrentLinkedQueue&lt;ProtocolEvent&gt;&gt;&#013;&#010;&gt; &gt;&gt; sourceStream = env.addSource(source)&#013;&#010;&gt; &gt;&gt;         .setParallelism(2)&#013;&#010;&gt; &gt;&gt;         .uid(\"DataProcessSource\")&#013;&#010;&gt; &gt;&gt;         .flatMap(new DataConvertFunction())&#013;&#010;&gt; &gt;&gt;         .setParallelism(2)&#013;&#010;&gt; &gt;&gt;         .uid(\"DataProcessDataCovert\")&#013;&#010;&gt; &gt;&gt;         .keyBy(new KeySelectorFunction())&#013;&#010;&gt; &gt;&gt;         .process(new DataCleanFunction())&#013;&#010;&gt; &gt;&gt;         .setParallelism(2)&#013;&#010;&gt; &gt;&gt;         .uid(\"DataProcessDataProcess\");&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; AsyncDataStream.orderedWait(&#013;&#010;&gt; &gt;&gt;         sourceStream,&#013;&#010;&gt; &gt;&gt;         new AsyncDataCleanFunction(),&#013;&#010;&gt; &gt;&gt;         EnvUtil.TOOL.getLong(Constant.ASYNC_TOMEOUT),&#013;&#010;&gt; &gt;&gt;         TimeUnit.MILLISECONDS,&#013;&#010;&gt; &gt;&gt;         EnvUtil.TOOL.getInt(Constant.ASYNC_CAPACITY)&#013;&#010;&gt; &gt;&gt; ).uid(\"DataProcessAsync\")&#013;&#010;&gt; &gt;&gt;         .setParallelism(2)&#013;&#010;&gt; &gt;&gt;         .addSink(sink)&#013;&#010;&gt; &gt;&gt;         .uid(\"DataProcessSinkKafka\")&#013;&#010;&gt; &gt;&gt;         .setParallelism(2);&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 WARN&#013;&#010;&gt; &gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#013;&#010;&gt; &gt;&gt; 'gps.kafka.sasl' was supplied but isn't a known config.&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 WARN&#013;&#010;&gt; &gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#013;&#010;&gt; &gt;&gt; 'java.ext.dirs' was supplied but isn't a known config.&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 WARN&#013;&#010;&gt; &gt;&gt; org.apache.kafka.clients.consumer.ConsumerConfig - The configuration&#013;&#010;&gt; &gt;&gt; 'java.class.version' was supplied but isn't a known config.&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#013;&#010;&gt; &gt;&gt; - Kafka version: 2.2.0&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:37,291 INFO org.apache.kafka.common.utils.AppInfoParser&#013;&#010;&gt; &gt;&gt; - Kafka commitId: 05fcfde8f69b0349&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:38,482 INFO com.sinoi.rt.common.protocol.HttpPoolUtil -&#013;&#010;&gt; &gt;&gt; http pool init,maxTotal:18,maxPerRoute:6&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:38,971 WARN org.apache.kafka.clients.NetworkClient -&#013;&#010;&gt; &gt;&gt; [Producer clientId=producer-1] Error while fetching metadata with&#013;&#010;&gt; &gt;&gt; correlation id 8 : {=INVALID_TOPIC_EXCEPTION}&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:38,974 INFO&#013;&#010;&gt; &gt;&gt; org.apache.kafka.clients.producer.KafkaProducer - [Producer&#013;&#010;&gt; &gt;&gt; clientId=producer-1] Closing the Kafka producer with timeoutMillis =&#013;&#010;&gt; &gt;&gt; 9223372036854775807 ms.&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:41,612 INFO org.apache.flink.runtime.taskmanager.Task -&#013;&#010;&gt; &gt;&gt; async wait operator -&gt; Sink: Unnamed (1/2)&#013;&#010;&gt; &gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77) switched from RUNNING to FAILED.&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException: Failed&#013;&#010;&gt; &gt;&gt; to send data to Kafka:&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.outputCompletedElement(AsyncWaitOperator.java:279)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator.access$100(AsyncWaitOperator.java:76)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.processResults(AsyncWaitOperator.java:351)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.AsyncWaitOperator$ResultHandler.lambda$processInMailbox$0(AsyncWaitOperator.java:336)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:255)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#013;&#010;&gt; &gt;&gt;     at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#013;&#010;&gt; &gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#013;&#010;&gt; &gt;&gt;     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#013;&#010;&gt; &gt;&gt;     at java.lang.Thread.run(Thread.java:748)&#013;&#010;&gt; &gt;&gt; Caused by: org.apache.kafka.common.errors.InvalidTopicException:&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#013;&#010;&gt; &gt;&gt; Freeing task resources for async wait operator -&gt; Sink: Unnamed (1/2)&#013;&#010;&gt; &gt;&gt; (cdbe008dcdb76813f88c4a48b9907d77).&#013;&#010;&gt; &gt;&gt; 2020-07-09 19:33:41,615 INFO org.apache.flink.runtime.taskmanager.Task -&#013;&#010;&gt; &gt;&gt; Ensuring all FileSystem streams are closed for task async wait operator&#013;&#010;&gt; -&gt;&#013;&#010;&gt; &gt;&gt; Sink: Unnamed (1/2)&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 发件人： zhisheng&#013;&#010;&gt; &gt;&gt; 发送时间： 2020-07-09 21:06&#013;&#010;&gt; &gt;&gt; 收件人： user-zh&#013;&#010;&gt; &gt;&gt; 主题： Re: flink1.10.1在yarn上无法写入kafka的问题&#013;&#010;&gt; &gt;&gt; hi，maqi&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 有完整的日志吗？在这个异常之前还有其他的异常信息吗？如果有，可以提供一下！&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best，&#013;&#010;&gt; &gt;&gt; zhisheng&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; maqi@sinoiov.com &lt;maqi@sinoiov.com&gt; 于2020年7月9日周四 下午7:57写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 请教各位：&#013;&#010;&gt; &gt;&gt; &gt; flink任务在本机写入测试环境kafka集群没问题，&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 但是上传到yarn环境，就是写不进去，其他job运行在yarn可以写入测试环境的kafka&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 异常信息如下：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 2020-07-09 19:17:33,126 INFO&#013;&#010;&gt; &gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#013;&#010;&gt; &gt;&gt; &gt; KeyedProcess (1/2) (9449b1e3b758a40fb5e1e60cf84fd844) switched from&#013;&#010;&gt; &gt;&gt; &gt; DEPLOYING to RUNNING.&#013;&#010;&gt; &gt;&gt; &gt; 2020-07-09 19:17:33,164 INFO&#013;&#010;&gt; &gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        -&#013;&#010;&gt; &gt;&gt; &gt; KeyedProcess (2/2) (bc6eefd911cf44412121939d0afa6a81) switched from&#013;&#010;&gt; &gt;&gt; &gt; DEPLOYING to RUNNING.&#013;&#010;&gt; &gt;&gt; &gt; 2020-07-09 19:17:39,049 INFO&#013;&#010;&gt; &gt;&gt; &gt; org.apache.flink.runtime.executiongraph.ExecutionGraph        - async&#013;&#010;&gt; &gt;&gt; wait&#013;&#010;&gt; &gt;&gt; &gt; operator -&gt; Sink: Unnamed (1/2) (cfc31005099a8ad7e44a94dc617dd45f)&#013;&#010;&gt; &gt;&gt; switched&#013;&#010;&gt; &gt;&gt; &gt; from RUNNING to FAILE&#013;&#010;&gt; &gt;&gt; &gt; D.&#013;&#010;&gt; &gt;&gt; &gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#013;&#010;&gt; Failed&#013;&#010;&gt; &gt;&gt; to&#013;&#010;&gt; &gt;&gt; &gt; send data to Kafka:&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1225)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:767)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:99)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:235)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:56)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.TimestampedCollector.collect(TimestampedCollector.java:53)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.StreamRecordQueueEntry.emitResult(StreamRecordQueueEntry.java:65)&#013;&#010;&gt; &gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.api.operators.async.queue.OrderedStreamElementQueue.emitCompletedElement(OrderedStreamElementQueue.java:71)&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<202007091956412586872@sinoiov.com>"
    },
    {
        "id": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 13:23:07 GMT",
        "subject": "flink1.10升级到flink1.11 提交到yarn失败",
        "content": "hi all，&#010;原来用1.10使用per job模式，可以提交的作业，现在用1.11使用应用模式提交失败，看日志，也不清楚原因，&#010;yarn log：&#010;Log Type: jobmanager.err&#010;&#010;&#010;Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&#010;&#010;Log Length: 785&#010;&#010;&#010;SLF4J: Class path contains multiple SLF4J bindings.&#010;SLF4J: Found binding in [jar:file:/yarn/nm/usercache/hdfs/appcache/application_1594271580406_0010/filecache/11/data-flow-1.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.&#010;SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#010;log4j:WARN No appenders could be found for logger (org.apache.flink.runtime.entrypoint.ClusterEntrypoint).&#010;log4j:WARN Please initialize the log4j system properly.&#010;log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.&#010;&#010;&#010;Log Type: jobmanager.out&#010;&#010;&#010;Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&#010;&#010;Log Length: 0&#010;&#010;&#010;&#010;&#010;Log Type: prelaunch.err&#010;&#010;&#010;Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&#010;&#010;Log Length: 0&#010;&#010;&#010;&#010;&#010;Log Type: prelaunch.out&#010;&#010;&#010;Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&#010;&#010;Log Length: 70&#010;&#010;&#010;Setting up env variables&#010;Setting up job resources&#010;Launching container&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;本地log：&#010;2020-07-09 21:02:41,015 INFO  org.apache.flink.client.cli.CliFrontend                    &#010; [] - --------------------------------------------------------------------------------&#010;2020-07-09 21:02:41,020 INFO  org.apache.flink.configuration.GlobalConfiguration         &#010; [] - Loading configuration property: jobmanager.rpc.address, localhost&#010;2020-07-09 21:02:41,020 INFO  org.apache.flink.configuration.GlobalConfiguration         &#010; [] - Loading configuration property: jobmanager.rpc.port, 6123&#010;2020-07-09 21:02:41,021 INFO  org.apache.flink.configuration.GlobalConfiguration         &#010; [] - Loading configuration property: jobmanager.memory.process.size, 1600m&#010;2020-07-09 21:02:41,021 INFO  org.apache.flink.configuration.GlobalConfiguration         &#010; [] - Loading configuration property: taskmanager.memory.process.size, 1728m&#010;2020-07-09 21:02:41,021 INFO  org.apache.flink.configuration.GlobalConfiguration         &#010; [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1&#010;2020-07-09 21:02:41,021 INFO  org.apache.flink.configuration.GlobalConfiguration         &#010; [] - Loading configuration property: parallelism.default, 1&#010;2020-07-09 21:02:41,021 INFO  org.apache.flink.configuration.GlobalConfiguration         &#010; [] - Loading configuration property: jobmanager.execution.failover-strategy, region&#010;2020-07-09 21:02:41,164 INFO  org.apache.flink.runtime.security.modules.HadoopModule     &#010; [] - Hadoop user set to hdfs (auth:SIMPLE)&#010;2020-07-09 21:02:41,172 INFO  org.apache.flink.runtime.security.modules.JaasModule       &#010; [] - Jaas file will be created as /tmp/jaas-2213111423022415421.conf.&#010;2020-07-09 21:02:41,181 INFO  org.apache.flink.client.cli.CliFrontend                    &#010; [] - Running 'run-application' command.&#010;2020-07-09 21:02:41,194 INFO  org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer&#010;[] - Submitting application in 'Application Mode'.&#010;2020-07-09 21:02:41,201 WARN  org.apache.flink.yarn.configuration.YarnLogConfigUtil      &#010; [] - The configuration directory ('/opt/flink-1.11.0/conf') already contains a LOG4J config&#010;file.If you want to use logback, then please delete or rename the log configuration file.&#010;2020-07-09 21:02:41,537 INFO  org.apache.flink.yarn.YarnClusterDescriptor                &#010; [] - No path for the flink jar passed. Using the location of class org.apache.flink.yarn.YarnClusterDescriptor&#010;to locate the jar&#010;2020-07-09 21:02:41,665 INFO  org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider&#010;[] - Failing over to rm220&#010;2020-07-09 21:02:41,717 INFO  org.apache.hadoop.conf.Configuration                       &#010; [] - resource-types.xml not found&#010;2020-07-09 21:02:41,718 INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils         &#010; [] - Unable to find 'resource-types.xml'.&#010;2020-07-09 21:02:41,755 INFO  org.apache.flink.yarn.YarnClusterDescriptor                &#010; [] - Cluster specification: ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=4096,&#010;slotsPerTaskManager=1}&#010;2020-07-09 21:02:42,723 INFO  org.apache.flink.yarn.YarnClusterDescriptor                &#010; [] - Submitting application master application_1594271580406_0010&#010;2020-07-09 21:02:42,969 INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl      &#010; [] - Submitted application application_1594271580406_0010&#010;2020-07-09 21:02:42,969 INFO  org.apache.flink.yarn.YarnClusterDescriptor                &#010; [] - Waiting for the cluster to be allocated&#010;2020-07-09 21:02:42,971 INFO  org.apache.flink.yarn.YarnClusterDescriptor                &#010; [] - Deploying cluster, current state ACCEPTED&#010;2020-07-09 21:02:47,619 INFO  org.apache.flink.yarn.YarnClusterDescriptor                &#010; [] - YARN application has been deployed successfully.&#010;2020-07-09 21:02:47,620 INFO  org.apache.flink.yarn.YarnClusterDescriptor                &#010; [] - Found Web Interface cdh003:38716 of application 'application_1594271580406_0010'",
        "depth": "0",
        "reply": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>"
    },
    {
        "id": "<CAA8tFvvG2UT+KVgqbJ2g96=f6WmevpGTB-v04sHfzR7zV0CQjg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:24:02 GMT",
        "subject": "Re: flink1.10升级到flink1.11 提交到yarn失败",
        "content": "Hi&#010;&#010;这个看上去是提交到 Yarn 了，具体的原因需要看下 JM log 是啥原因。另外是否是日志没有贴全，这里只看到本地&#010;log，其他的就只有小部分&#010;jobmanager.err 的 log。&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月9日周四 下午9:23写道：&#010;&#010;&gt; hi all，&#010;&gt; 原来用1.10使用per job模式，可以提交的作业，现在用1.11使用应用模式提交失败，看日志，也不清楚原因，&#010;&gt; yarn log：&#010;&gt; Log Type: jobmanager.err&#010;&gt;&#010;&gt;&#010;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&#010;&gt;&#010;&gt; Log Length: 785&#010;&gt;&#010;&gt;&#010;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt; SLF4J: Found binding in&#010;&gt; [jar:file:/yarn/nm/usercache/hdfs/appcache/application_1594271580406_0010/filecache/11/data-flow-1.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; SLF4J: Found binding in&#010;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt; explanation.&#010;&gt; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#010;&gt; log4j:WARN No appenders could be found for logger&#010;&gt; (org.apache.flink.runtime.entrypoint.ClusterEntrypoint).&#010;&gt; log4j:WARN Please initialize the log4j system properly.&#010;&gt; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for&#010;&gt; more info.&#010;&gt;&#010;&gt;&#010;&gt; Log Type: jobmanager.out&#010;&gt;&#010;&gt;&#010;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&#010;&gt;&#010;&gt; Log Length: 0&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Log Type: prelaunch.err&#010;&gt;&#010;&gt;&#010;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&#010;&gt;&#010;&gt; Log Length: 0&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Log Type: prelaunch.out&#010;&gt;&#010;&gt;&#010;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&#010;&gt;&#010;&gt; Log Length: 70&#010;&gt;&#010;&gt;&#010;&gt; Setting up env variables&#010;&gt; Setting up job resources&#010;&gt; Launching container&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 本地log：&#010;&gt; 2020-07-09 21:02:41,015 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;                 [] -&#010;&gt; --------------------------------------------------------------------------------&#010;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: parallelism.default, 1&#010;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; 2020-07-09 21:02:41,164 INFO&#010;&gt; org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop&#010;&gt; user set to hdfs (auth:SIMPLE)&#010;&gt; 2020-07-09 21:02:41,172 INFO&#010;&gt; org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file&#010;&gt; will be created as /tmp/jaas-2213111423022415421.conf.&#010;&gt; 2020-07-09 21:02:41,181 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;                 [] - Running 'run-application' command.&#010;&gt; 2020-07-09 21:02:41,194 INFO&#010;&gt; org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer&#010;&gt; [] - Submitting application in 'Application Mode'.&#010;&gt; 2020-07-09 21:02:41,201 WARN&#010;&gt; org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The&#010;&gt; configuration directory ('/opt/flink-1.11.0/conf') already contains a LOG4J&#010;&gt; config file.If you want to use logback, then please delete or rename the&#010;&gt; log configuration file.&#010;&gt; 2020-07-09 21:02:41,537 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;                 [] - No path for the flink jar passed. Using the location&#010;&gt; of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar&#010;&gt; 2020-07-09 21:02:41,665 INFO&#010;&gt; org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] -&#010;&gt; Failing over to rm220&#010;&gt; 2020-07-09 21:02:41,717 INFO  org.apache.hadoop.conf.Configuration&#010;&gt;                  [] - resource-types.xml not found&#010;&gt; 2020-07-09 21:02:41,718 INFO&#010;&gt; org.apache.hadoop.yarn.util.resource.ResourceUtils           [] - Unable to&#010;&gt; find 'resource-types.xml'.&#010;&gt; 2020-07-09 21:02:41,755 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;                 [] - Cluster specification:&#010;&gt; ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=4096,&#010;&gt; slotsPerTaskManager=1}&#010;&gt; 2020-07-09 21:02:42,723 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;                 [] - Submitting application master&#010;&gt; application_1594271580406_0010&#010;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt; org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted&#010;&gt; application application_1594271580406_0010&#010;&gt; 2020-07-09 21:02:42,969 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;                 [] - Waiting for the cluster to be allocated&#010;&gt; 2020-07-09 21:02:42,971 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;                 [] - Deploying cluster, current state ACCEPTED&#010;&gt; 2020-07-09 21:02:47,619 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;                 [] - YARN application has been deployed successfully.&#010;&gt; 2020-07-09 21:02:47,620 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;                 [] - Found Web Interface cdh003:38716 of application&#010;&gt; 'application_1594271580406_0010'&#010;&#010;",
        "depth": "1",
        "reply": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>"
    },
    {
        "id": "<2fe6bd72.2c2d.17336978f45.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:39:29 GMT",
        "subject": "Re:Re: flink1.10升级到flink1.11 提交到yarn失败",
        "content": "日志贴全了的，这是从yarn ui贴的full log，用yarn logs命令也是这些log，太简短，看不出错误在哪。。。&#010;&#010;&#010;我又提交了另外之前用flink1.10跑过的任务，现在用flink1.11跑，报了异常：&#010;&#010;&#010;SLF4J: Class path contains multiple SLF4J bindings.&#010;SLF4J: Found binding in [jar:file:/opt/flink-1.11.0/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.&#010;SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]&#010;&#010;&#010;------------------------------------------------------------&#010; The program finished with the following exception:&#010;&#010;&#010;org.apache.flink.client.program.ProgramInvocationException: The main method caused an error:&#010;findAndCreateTableSource failed.&#010;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;at java.security.AccessController.doPrivileged(Native Method)&#010;at javax.security.auth.Subject.doAs(Subject.java:422)&#010;at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)&#010;at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;Caused by: org.apache.flink.table.api.TableException: findAndCreateTableSource failed.&#010;at org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:49)&#010;at org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.findAndCreateLegacyTableSource(LegacyCatalogSourceTable.scala:190)&#010;at org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.toRel(LegacyCatalogSourceTable.scala:89)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#010;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:747)&#010;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV$.main(FromKafkaSinkJdbcForUserUV.scala:78)&#010;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV.main(FromKafkaSinkJdbcForUserUV.scala)&#010;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;at java.lang.reflect.Method.invoke(Method.java:498)&#010;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;... 11 more&#010;Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException: Could not find a suitable&#010;table factory for 'org.apache.flink.table.factories.TableSourceFactory' in&#010;the classpath.&#010;&#010;&#010;Reason: Required context properties mismatch.&#010;&#010;&#010;The following properties are requested:&#010;connector.properties.bootstrap.servers=cdh1:9092,cdh2:9092,cdh3:9092&#010;connector.properties.group.id=user_flink&#010;connector.properties.zookeeper.connect=cdh1:2181,cdh2:2181,cdh3:2181&#010;connector.startup-mode=latest-offset&#010;connector.topic=user&#010;connector.type=kafka&#010;connector.version=universal&#010;format.derive-schema=true&#010;format.type=json&#010;schema.0.data-type=VARCHAR(2147483647)&#010;schema.0.name=uid&#010;schema.1.data-type=VARCHAR(2147483647)&#010;schema.1.name=sex&#010;schema.2.data-type=INT&#010;schema.2.name=age&#010;schema.3.data-type=TIMESTAMP(3)&#010;schema.3.name=created_time&#010;schema.4.data-type=TIMESTAMP(3) NOT NULL&#010;schema.4.expr=PROCTIME()&#010;schema.4.name=proctime&#010;schema.watermark.0.rowtime=created_time&#010;schema.watermark.0.strategy.data-type=TIMESTAMP(3)&#010;schema.watermark.0.strategy.expr=`created_time` - INTERVAL '3' SECOND&#010;&#010;&#010;The following factories have been considered:&#010;org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;org.apache.flink.table.filesystem.FileSystemTableFactory&#010;at org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;at org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;at org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;at org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;at org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;... 37 more&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;我把maven依赖的provide范围全部去掉了：&#010;&lt;properties&gt;&#010;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#010;        &lt;flink.version&gt;1.11.0&lt;/flink.version&gt;&#010;        &lt;hive.version&gt;2.1.1&lt;/hive.version&gt;&#010;        &lt;java.version&gt;1.8&lt;/java.version&gt;&#010;        &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;&#010;        &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;&#010;        &lt;maven.compiler.source&gt;${java.version}&lt;/maven.compiler.source&gt;&#010;        &lt;maven.compiler.target&gt;${java.version}&lt;/maven.compiler.target&gt;&#010;    &lt;/properties&gt;&#010;&#010;&#010;    &lt;repositories&gt;&#010;        &lt;repository&gt;&#010;            &lt;id&gt;maven-net-cn&lt;/id&gt;&#010;            &lt;name&gt;Maven China Mirror&lt;/name&gt;&#010;            &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;&#010;            &lt;releases&gt;&#010;                &lt;enabled&gt;true&lt;/enabled&gt;&#010;            &lt;/releases&gt;&#010;            &lt;snapshots&gt;&#010;                &lt;enabled&gt;false&lt;/enabled&gt;&#010;            &lt;/snapshots&gt;&#010;        &lt;/repository&gt;&#010;&#010;&#010;        &lt;repository&gt;&#010;            &lt;id&gt;apache.snapshots&lt;/id&gt;&#010;            &lt;name&gt;Apache Development Snapshot Repository&lt;/name&gt;&#010;            &lt;url&gt;https://repository.apache.org/content/repositories/snapshots/&lt;/url&gt;&#010;            &lt;releases&gt;&#010;                &lt;enabled&gt;false&lt;/enabled&gt;&#010;            &lt;/releases&gt;&#010;            &lt;snapshots&gt;&#010;                &lt;enabled&gt;true&lt;/enabled&gt;&#010;            &lt;/snapshots&gt;&#010;        &lt;/repository&gt;&#010;    &lt;/repositories&gt;&#010;&#010;&#010;    &lt;dependencies&gt;&#010;        &lt;!-- Apache Flink dependencies --&gt;&#010;        &lt;!-- These dependencies are provided, because they should not be packaged into&#010;the JAR file. --&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-avro&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;            &lt;scope&gt;provided&lt;/scope&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;            &lt;scope&gt;provided&lt;/scope&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;1.0&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;            &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&#010;            &lt;version&gt;2.8.0&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;redis.clients&lt;/groupId&gt;&#010;            &lt;artifactId&gt;jedis&lt;/artifactId&gt;&#010;            &lt;version&gt;3.3.0&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-connector-hbase_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;1.11-SNAPSHOT&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&#010;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;&#010;            &lt;version&gt;2.1.0&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&#010;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;&#010;            &lt;version&gt;1.18.12&lt;/version&gt;&#010;            &lt;scope&gt;provided&lt;/scope&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;io.lettuce&lt;/groupId&gt;&#010;            &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;&#010;            &lt;version&gt;5.3.1.RELEASE&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;junit&lt;/groupId&gt;&#010;            &lt;artifactId&gt;junit&lt;/artifactId&gt;&#010;            &lt;version&gt;4.13&lt;/version&gt;&#010;            &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;            &lt;artifactId&gt;commons-email&lt;/artifactId&gt;&#010;            &lt;version&gt;1.5&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&#010;            &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&#010;            &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-connector-hive_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;&#010;            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;&#010;            &lt;version&gt;${hive.version}&lt;/version&gt;&#010;            &lt;scope&gt;provided&lt;/scope&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;!-- Add logging framework, to produce console output when running in the IDE.&#010;--&gt;&#010;        &lt;!-- These dependencies are excluded from the application JAR by default. --&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&#010;            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&#010;            &lt;version&gt;1.7.7&lt;/version&gt;&#010;            &lt;scope&gt;runtime&lt;/scope&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;log4j&lt;/groupId&gt;&#010;            &lt;artifactId&gt;log4j&lt;/artifactId&gt;&#010;            &lt;version&gt;1.2.17&lt;/version&gt;&#010;            &lt;scope&gt;runtime&lt;/scope&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;&#010;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;&#010;            &lt;version&gt;1.2.68&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;com.jayway.jsonpath&lt;/groupId&gt;&#010;            &lt;artifactId&gt;json-path&lt;/artifactId&gt;&#010;            &lt;version&gt;2.4.0&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-connector-jdbc_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;mysql&lt;/groupId&gt;&#010;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&#010;            &lt;version&gt;5.1.46&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;            &lt;artifactId&gt;vertx-core&lt;/artifactId&gt;&#010;            &lt;version&gt;3.9.1&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;            &lt;artifactId&gt;vertx-jdbc-client&lt;/artifactId&gt;&#010;            &lt;version&gt;3.9.1&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;&#010;    &lt;/dependencies&gt;&#010;&#010;&#010;&#010;&#010;&#010;&#010;集群节点flink-1.11.0/lib/:&#010;-rw-r--r-- 1 root root    197597 6月  30 10:28 flink-clients_2.11-1.11.0.jar&#010;-rw-r--r-- 1 root root     90782 6月  30 17:46 flink-csv-1.11.0.jar&#010;-rw-r--r-- 1 root root 108349203 6月  30 17:52 flink-dist_2.11-1.11.0.jar&#010;-rw-r--r-- 1 root root     94863 6月  30 17:45 flink-json-1.11.0.jar&#010;-rw-r--r-- 1 root root   7712156 6月  18 10:42 flink-shaded-zookeeper-3.4.14.jar&#010;-rw-r--r-- 1 root root  33325754 6月  30 17:50 flink-table_2.11-1.11.0.jar&#010;-rw-r--r-- 1 root root     47333 6月  30 10:38 flink-table-api-scala-bridge_2.11-1.11.0.jar&#010;-rw-r--r-- 1 root root  37330521 6月  30 17:50 flink-table-blink_2.11-1.11.0.jar&#010;-rw-r--r-- 1 root root    754983 6月  30 12:29 flink-table-common-1.11.0.jar&#010;-rw-r--r-- 1 root root     67114 4月  20 20:47 log4j-1.2-api-2.12.1.jar&#010;-rw-r--r-- 1 root root    276771 4月  20 20:47 log4j-api-2.12.1.jar&#010;-rw-r--r-- 1 root root   1674433 4月  20 20:47 log4j-core-2.12.1.jar&#010;-rw-r--r-- 1 root root     23518 4月  20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;&#010;&#010;把table相关的包都下载下来了，还是报同样的错，好奇怪。。。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-10 10:24:02，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;Hi&#010;&gt;&#010;&gt;这个看上去是提交到 Yarn 了，具体的原因需要看下 JM log 是啥原因。另外是否是日志没有贴全，这里只看到本地&#010;log，其他的就只有小部分&#010;&gt;jobmanager.err 的 log。&#010;&gt;&#010;&gt;Best,&#010;&gt;Congxian&#010;&gt;&#010;&gt;&#010;&gt;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月9日周四 下午9:23写道：&#010;&gt;&#010;&gt;&gt; hi all，&#010;&gt;&gt; 原来用1.10使用per job模式，可以提交的作业，现在用1.11使用应用模式提交失败，看日志，也不清楚原因，&#010;&gt;&gt; yarn log：&#010;&gt;&gt; Log Type: jobmanager.err&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Length: 785&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt; [jar:file:/yarn/nm/usercache/hdfs/appcache/application_1594271580406_0010/filecache/11/data-flow-1.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt;&gt; explanation.&#010;&gt;&gt; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#010;&gt;&gt; log4j:WARN No appenders could be found for logger&#010;&gt;&gt; (org.apache.flink.runtime.entrypoint.ClusterEntrypoint).&#010;&gt;&gt; log4j:WARN Please initialize the log4j system properly.&#010;&gt;&gt; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for&#010;&gt;&gt; more info.&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Type: jobmanager.out&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Length: 0&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Type: prelaunch.err&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Length: 0&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Type: prelaunch.out&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Log Length: 70&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Setting up env variables&#010;&gt;&gt; Setting up job resources&#010;&gt;&gt; Launching container&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 本地log：&#010;&gt;&gt; 2020-07-09 21:02:41,015 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;&gt;                 [] -&#010;&gt;&gt; --------------------------------------------------------------------------------&#010;&gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt;&gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt;&gt; configuration property: parallelism.default, 1&#010;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt;&gt; 2020-07-09 21:02:41,164 INFO&#010;&gt;&gt; org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop&#010;&gt;&gt; user set to hdfs (auth:SIMPLE)&#010;&gt;&gt; 2020-07-09 21:02:41,172 INFO&#010;&gt;&gt; org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file&#010;&gt;&gt; will be created as /tmp/jaas-2213111423022415421.conf.&#010;&gt;&gt; 2020-07-09 21:02:41,181 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;&gt;                 [] - Running 'run-application' command.&#010;&gt;&gt; 2020-07-09 21:02:41,194 INFO&#010;&gt;&gt; org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer&#010;&gt;&gt; [] - Submitting application in 'Application Mode'.&#010;&gt;&gt; 2020-07-09 21:02:41,201 WARN&#010;&gt;&gt; org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The&#010;&gt;&gt; configuration directory ('/opt/flink-1.11.0/conf') already contains a LOG4J&#010;&gt;&gt; config file.If you want to use logback, then please delete or rename the&#010;&gt;&gt; log configuration file.&#010;&gt;&gt; 2020-07-09 21:02:41,537 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;                 [] - No path for the flink jar passed. Using the location&#010;&gt;&gt; of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar&#010;&gt;&gt; 2020-07-09 21:02:41,665 INFO&#010;&gt;&gt; org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] -&#010;&gt;&gt; Failing over to rm220&#010;&gt;&gt; 2020-07-09 21:02:41,717 INFO  org.apache.hadoop.conf.Configuration&#010;&gt;&gt;                  [] - resource-types.xml not found&#010;&gt;&gt; 2020-07-09 21:02:41,718 INFO&#010;&gt;&gt; org.apache.hadoop.yarn.util.resource.ResourceUtils           [] - Unable to&#010;&gt;&gt; find 'resource-types.xml'.&#010;&gt;&gt; 2020-07-09 21:02:41,755 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;                 [] - Cluster specification:&#010;&gt;&gt; ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=4096,&#010;&gt;&gt; slotsPerTaskManager=1}&#010;&gt;&gt; 2020-07-09 21:02:42,723 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;                 [] - Submitting application master&#010;&gt;&gt; application_1594271580406_0010&#010;&gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt;&gt; org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Submitted&#010;&gt;&gt; application application_1594271580406_0010&#010;&gt;&gt; 2020-07-09 21:02:42,969 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;                 [] - Waiting for the cluster to be allocated&#010;&gt;&gt; 2020-07-09 21:02:42,971 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;                 [] - Deploying cluster, current state ACCEPTED&#010;&gt;&gt; 2020-07-09 21:02:47,619 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;                 [] - YARN application has been deployed successfully.&#010;&gt;&gt; 2020-07-09 21:02:47,620 INFO  org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;                 [] - Found Web Interface cdh003:38716 of application&#010;&gt;&gt; 'application_1594271580406_0010'&#010;",
        "depth": "2",
        "reply": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>"
    },
    {
        "id": "<CAA8tFvsmXUE00FSj4x=JXa+6odQNxZH2zXfH3v4Q4QUi8xj8LQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:56:42 GMT",
        "subject": "Re: Re: flink1.10升级到flink1.11 提交到yarn失败",
        "content": "Hi&#010;&#010;从异常看，可能是某个包没有引入导致的，和这个[1]比较像，可能你需要对比一下需要的是哪个包没有引入。&#010;&#010;PS 从栈那里看到是 csv 相关的，可以优先考虑下 cvs 相关的包&#010;&#010;```&#010;The following factories have been considered:&#010;org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;org.apache.flink.table.filesystem.FileSystemTableFactory&#010;at&#010;org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;at&#010;org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;at&#010;org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;at&#010;org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;at&#010;org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;... 37 more&#010;```&#010;&#010;[1] http://apache-flink.147419.n8.nabble.com/flink-1-11-td4471.html&#010;Best,&#010;Congxian&#010;&#010;&#010;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月10日周五 上午10:39写道：&#010;&#010;&gt; 日志贴全了的，这是从yarn ui贴的full log，用yarn logs命令也是这些log，太简短，看不出错误在哪。。。&#010;&gt;&#010;&gt;&#010;&gt; 我又提交了另外之前用flink1.10跑过的任务，现在用flink1.11跑，报了异常：&#010;&gt;&#010;&gt;&#010;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt; SLF4J: Found binding in&#010;&gt; [jar:file:/opt/flink-1.11.0/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; SLF4J: Found binding in&#010;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt; explanation.&#010;&gt; SLF4J: Actual binding is of type&#010;&gt; [org.apache.logging.slf4j.Log4jLoggerFactory]&#010;&gt;&#010;&gt;&#010;&gt; ------------------------------------------------------------&#010;&gt;  The program finished with the following exception:&#010;&gt;&#010;&gt;&#010;&gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt; method caused an error: findAndCreateTableSource failed.&#010;&gt; at&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; at&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; at&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; Caused by: org.apache.flink.table.api.TableException:&#010;&gt; findAndCreateTableSource failed.&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:49)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.findAndCreateLegacyTableSource(LegacyCatalogSourceTable.scala:190)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.toRel(LegacyCatalogSourceTable.scala:89)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:747)&#010;&gt; at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV$.main(FromKafkaSinkJdbcForUserUV.scala:78)&#010;&gt; at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV.main(FromKafkaSinkJdbcForUserUV.scala)&#010;&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; at&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; ... 11 more&#010;&gt; Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException:&#010;&gt; Could not find a suitable table factory for&#010;&gt; 'org.apache.flink.table.factories.TableSourceFactory' in&#010;&gt; the classpath.&#010;&gt;&#010;&gt;&#010;&gt; Reason: Required context properties mismatch.&#010;&gt;&#010;&gt;&#010;&gt; The following properties are requested:&#010;&gt; connector.properties.bootstrap.servers=cdh1:9092,cdh2:9092,cdh3:9092&#010;&gt; connector.properties.group.id=user_flink&#010;&gt; connector.properties.zookeeper.connect=cdh1:2181,cdh2:2181,cdh3:2181&#010;&gt; connector.startup-mode=latest-offset&#010;&gt; connector.topic=user&#010;&gt; connector.type=kafka&#010;&gt; connector.version=universal&#010;&gt; format.derive-schema=true&#010;&gt; format.type=json&#010;&gt; schema.0.data-type=VARCHAR(2147483647)&#010;&gt; schema.0.name=uid&#010;&gt; schema.1.data-type=VARCHAR(2147483647)&#010;&gt; schema.1.name=sex&#010;&gt; schema.2.data-type=INT&#010;&gt; schema.2.name=age&#010;&gt; schema.3.data-type=TIMESTAMP(3)&#010;&gt; schema.3.name=created_time&#010;&gt; schema.4.data-type=TIMESTAMP(3) NOT NULL&#010;&gt; schema.4.expr=PROCTIME()&#010;&gt; schema.4.name=proctime&#010;&gt; schema.watermark.0.rowtime=created_time&#010;&gt; schema.watermark.0.strategy.data-type=TIMESTAMP(3)&#010;&gt; schema.watermark.0.strategy.expr=`created_time` - INTERVAL '3' SECOND&#010;&gt;&#010;&gt;&#010;&gt; The following factories have been considered:&#010;&gt; org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;&gt; org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;&gt; org.apache.flink.table.filesystem.FileSystemTableFactory&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;&gt; ... 37 more&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 我把maven依赖的provide范围全部去掉了：&#010;&gt; &lt;properties&gt;&#010;&gt;         &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#010;&gt;         &lt;flink.version&gt;1.11.0&lt;/flink.version&gt;&#010;&gt;         &lt;hive.version&gt;2.1.1&lt;/hive.version&gt;&#010;&gt;         &lt;java.version&gt;1.8&lt;/java.version&gt;&#010;&gt;         &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;&#010;&gt;         &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;&#010;&gt;         &lt;maven.compiler.source&gt;${java.version}&lt;/maven.compiler.source&gt;&#010;&gt;         &lt;maven.compiler.target&gt;${java.version}&lt;/maven.compiler.target&gt;&#010;&gt;     &lt;/properties&gt;&#010;&gt;&#010;&gt;&#010;&gt;     &lt;repositories&gt;&#010;&gt;         &lt;repository&gt;&#010;&gt;             &lt;id&gt;maven-net-cn&lt;/id&gt;&#010;&gt;             &lt;name&gt;Maven China Mirror&lt;/name&gt;&#010;&gt;             &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&#010;&gt; &lt;/url&gt;&#010;&gt;             &lt;releases&gt;&#010;&gt;                 &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt;             &lt;/releases&gt;&#010;&gt;             &lt;snapshots&gt;&#010;&gt;                 &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt;             &lt;/snapshots&gt;&#010;&gt;         &lt;/repository&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;repository&gt;&#010;&gt;             &lt;id&gt;apache.snapshots&lt;/id&gt;&#010;&gt;             &lt;name&gt;Apache Development Snapshot Repository&lt;/name&gt;&#010;&gt;             &lt;url&gt;&#010;&gt; https://repository.apache.org/content/repositories/snapshots/&lt;/url&gt;&#010;&gt;             &lt;releases&gt;&#010;&gt;                 &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt;             &lt;/releases&gt;&#010;&gt;             &lt;snapshots&gt;&#010;&gt;                 &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt;             &lt;/snapshots&gt;&#010;&gt;         &lt;/repository&gt;&#010;&gt;     &lt;/repositories&gt;&#010;&gt;&#010;&gt;&#010;&gt;     &lt;dependencies&gt;&#010;&gt;         &lt;!-- Apache Flink dependencies --&gt;&#010;&gt;         &lt;!-- These dependencies are provided, because they should not be&#010;&gt; packaged into the JAR file. --&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-avro&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;1.0&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;2.8.0&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;redis.clients&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;jedis&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;3.3.0&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-connector-hbase_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;1.11-SNAPSHOT&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;2.1.0&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;lombok&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;1.18.12&lt;/version&gt;&#010;&gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;io.lettuce&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;5.3.1.RELEASE&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;junit&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;junit&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;4.13&lt;/version&gt;&#010;&gt;             &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;commons-email&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;1.5&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-connector-hive_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${hive.version}&lt;/version&gt;&#010;&gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;!-- Add logging framework, to produce console output when running&#010;&gt; in the IDE. --&gt;&#010;&gt;         &lt;!-- These dependencies are excluded from the application JAR by&#010;&gt; default. --&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;1.7.7&lt;/version&gt;&#010;&gt;             &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;log4j&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;log4j&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;1.2.17&lt;/version&gt;&#010;&gt;             &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;com.alibaba&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;fastjson&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;1.2.68&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;com.jayway.jsonpath&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;json-path&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;2.4.0&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-connector-jdbc_2.11&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;mysql&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;5.1.46&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;vertx-core&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;vertx-jdbc-client&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;&#010;&gt;&#010;&gt;     &lt;/dependencies&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 集群节点flink-1.11.0/lib/:&#010;&gt; -rw-r--r-- 1 root root    197597 6月  30 10:28 flink-clients_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root     90782 6月  30 17:46 flink-csv-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root 108349203 6月  30 17:52 flink-dist_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root     94863 6月  30 17:45 flink-json-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root   7712156 6月  18 10:42&#010;&gt; flink-shaded-zookeeper-3.4.14.jar&#010;&gt; -rw-r--r-- 1 root root  33325754 6月  30 17:50 flink-table_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root     47333 6月  30 10:38&#010;&gt; flink-table-api-scala-bridge_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root  37330521 6月  30 17:50&#010;&gt; flink-table-blink_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root    754983 6月  30 12:29 flink-table-common-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root root     67114 4月  20 20:47 log4j-1.2-api-2.12.1.jar&#010;&gt; -rw-r--r-- 1 root root    276771 4月  20 20:47 log4j-api-2.12.1.jar&#010;&gt; -rw-r--r-- 1 root root   1674433 4月  20 20:47 log4j-core-2.12.1.jar&#010;&gt; -rw-r--r-- 1 root root     23518 4月  20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;&gt;&#010;&gt;&#010;&gt; 把table相关的包都下载下来了，还是报同样的错，好奇怪。。。&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-10 10:24:02，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt; &gt;Hi&#010;&gt; &gt;&#010;&gt; &gt;这个看上去是提交到 Yarn 了，具体的原因需要看下 JM log 是啥原因。另外是否是日志没有贴全，这里只看到本地&#010;log，其他的就只有小部分&#010;&gt; &gt;jobmanager.err 的 log。&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Congxian&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月9日周四 下午9:23写道：&#010;&gt; &gt;&#010;&gt; &gt;&gt; hi all，&#010;&gt; &gt;&gt; 原来用1.10使用per job模式，可以提交的作业，现在用1.11使用应用模式提交失败，看日志，也不清楚原因，&#010;&gt; &gt;&gt; yarn log：&#010;&gt; &gt;&gt; Log Type: jobmanager.err&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Length: 785&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt; &gt;&gt; SLF4J: Found binding in&#010;&gt; &gt;&gt;&#010;&gt; [jar:file:/yarn/nm/usercache/hdfs/appcache/application_1594271580406_0010/filecache/11/data-flow-1.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; &gt;&gt; SLF4J: Found binding in&#010;&gt; &gt;&gt;&#010;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; &gt;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt; &gt;&gt; explanation.&#010;&gt; &gt;&gt; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#010;&gt; &gt;&gt; log4j:WARN No appenders could be found for logger&#010;&gt; &gt;&gt; (org.apache.flink.runtime.entrypoint.ClusterEntrypoint).&#010;&gt; &gt;&gt; log4j:WARN Please initialize the log4j system properly.&#010;&gt; &gt;&gt; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig&#010;&gt; for&#010;&gt; &gt;&gt; more info.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Type: jobmanager.out&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Length: 0&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Type: prelaunch.err&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Length: 0&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Type: prelaunch.out&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Log Length: 70&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Setting up env variables&#010;&gt; &gt;&gt; Setting up job resources&#010;&gt; &gt;&gt; Launching container&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 本地log：&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,015 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt; &gt;&gt;                 [] -&#010;&gt; &gt;&gt;&#010;&gt; --------------------------------------------------------------------------------&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt;&gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt;&gt; configuration property: parallelism.default, 1&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,164 INFO&#010;&gt; &gt;&gt; org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop&#010;&gt; &gt;&gt; user set to hdfs (auth:SIMPLE)&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,172 INFO&#010;&gt; &gt;&gt; org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas&#010;&gt; file&#010;&gt; &gt;&gt; will be created as /tmp/jaas-2213111423022415421.conf.&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,181 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt; &gt;&gt;                 [] - Running 'run-application' command.&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,194 INFO&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer&#010;&gt; &gt;&gt; [] - Submitting application in 'Application Mode'.&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,201 WARN&#010;&gt; &gt;&gt; org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The&#010;&gt; &gt;&gt; configuration directory ('/opt/flink-1.11.0/conf') already contains a&#010;&gt; LOG4J&#010;&gt; &gt;&gt; config file.If you want to use logback, then please delete or rename the&#010;&gt; &gt;&gt; log configuration file.&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,537 INFO&#010;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt;&gt;                 [] - No path for the flink jar passed. Using the&#010;&gt; location&#010;&gt; &gt;&gt; of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,665 INFO&#010;&gt; &gt;&gt; org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] -&#010;&gt; &gt;&gt; Failing over to rm220&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,717 INFO  org.apache.hadoop.conf.Configuration&#010;&gt; &gt;&gt;                  [] - resource-types.xml not found&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,718 INFO&#010;&gt; &gt;&gt; org.apache.hadoop.yarn.util.resource.ResourceUtils           [] -&#010;&gt; Unable to&#010;&gt; &gt;&gt; find 'resource-types.xml'.&#010;&gt; &gt;&gt; 2020-07-09 21:02:41,755 INFO&#010;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt;&gt;                 [] - Cluster specification:&#010;&gt; &gt;&gt; ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=4096,&#010;&gt; &gt;&gt; slotsPerTaskManager=1}&#010;&gt; &gt;&gt; 2020-07-09 21:02:42,723 INFO&#010;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt;&gt;                 [] - Submitting application master&#010;&gt; &gt;&gt; application_1594271580406_0010&#010;&gt; &gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt; &gt;&gt; org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] -&#010;&gt; Submitted&#010;&gt; &gt;&gt; application application_1594271580406_0010&#010;&gt; &gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt;&gt;                 [] - Waiting for the cluster to be allocated&#010;&gt; &gt;&gt; 2020-07-09 21:02:42,971 INFO&#010;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt;&gt;                 [] - Deploying cluster, current state ACCEPTED&#010;&gt; &gt;&gt; 2020-07-09 21:02:47,619 INFO&#010;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt;&gt;                 [] - YARN application has been deployed successfully.&#010;&gt; &gt;&gt; 2020-07-09 21:02:47,620 INFO&#010;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt;&gt;                 [] - Found Web Interface cdh003:38716 of application&#010;&gt; &gt;&gt; 'application_1594271580406_0010'&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABvJ6uXdYW-tKzoRS88zk+FnSxmQbg9QKB_tk0DkQ97_wocB3g@mail.gmail.com>",
        "from": "Shuiqiang Chen &lt;acqua....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:08:42 GMT",
        "subject": "Re: Re: flink1.10升级到flink1.11 提交到yarn失败",
        "content": "Hi，&#010;看样子是kafka table source没有成功创建，也许你需要将&#010;&lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010; &lt;/dependency&gt;&#010;&#010;这个jar 放到 FLINK_HOME/lib 目录下&#010;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月10日周五 上午10:57写道：&#010;&#010;&gt; Hi&#010;&gt;&#010;&gt; 从异常看，可能是某个包没有引入导致的，和这个[1]比较像，可能你需要对比一下需要的是哪个包没有引入。&#010;&gt;&#010;&gt; PS 从栈那里看到是 csv 相关的，可以优先考虑下 cvs 相关的包&#010;&gt;&#010;&gt; ```&#010;&gt; The following factories have been considered:&#010;&gt; org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;&gt; org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;&gt; org.apache.flink.table.filesystem.FileSystemTableFactory&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;&gt; ... 37 more&#010;&gt; ```&#010;&gt;&#010;&gt; [1] http://apache-flink.147419.n8.nabble.com/flink-1-11-td4471.html&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; Zhou Zach &lt;wander669@163.com&gt; 于2020年7月10日周五 上午10:39写道：&#010;&gt;&#010;&gt; &gt; 日志贴全了的，这是从yarn ui贴的full log，用yarn logs命令也是这些log，太简短，看不出错误在哪。。。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 我又提交了另外之前用flink1.10跑过的任务，现在用flink1.11跑，报了异常：&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt; &gt; SLF4J: Found binding in&#010;&gt; &gt;&#010;&gt; [jar:file:/opt/flink-1.11.0/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; &gt; SLF4J: Found binding in&#010;&gt; &gt;&#010;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; &gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt; &gt; explanation.&#010;&gt; &gt; SLF4J: Actual binding is of type&#010;&gt; &gt; [org.apache.logging.slf4j.Log4jLoggerFactory]&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; ------------------------------------------------------------&#010;&gt; &gt;  The program finished with the following exception:&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt; &gt; method caused an error: findAndCreateTableSource failed.&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt; at&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; &gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; &gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; &gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; &gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; &gt; Caused by: org.apache.flink.table.api.TableException:&#010;&gt; &gt; findAndCreateTableSource failed.&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:49)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.findAndCreateLegacyTableSource(LegacyCatalogSourceTable.scala:190)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.toRel(LegacyCatalogSourceTable.scala:89)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt; &gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:747)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV$.main(FromKafkaSinkJdbcForUserUV.scala:78)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV.main(FromKafkaSinkJdbcForUserUV.scala)&#010;&gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt; ... 11 more&#010;&gt; &gt; Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException:&#010;&gt; &gt; Could not find a suitable table factory for&#010;&gt; &gt; 'org.apache.flink.table.factories.TableSourceFactory' in&#010;&gt; &gt; the classpath.&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Reason: Required context properties mismatch.&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; The following properties are requested:&#010;&gt; &gt; connector.properties.bootstrap.servers=cdh1:9092,cdh2:9092,cdh3:9092&#010;&gt; &gt; connector.properties.group.id=user_flink&#010;&gt; &gt; connector.properties.zookeeper.connect=cdh1:2181,cdh2:2181,cdh3:2181&#010;&gt; &gt; connector.startup-mode=latest-offset&#010;&gt; &gt; connector.topic=user&#010;&gt; &gt; connector.type=kafka&#010;&gt; &gt; connector.version=universal&#010;&gt; &gt; format.derive-schema=true&#010;&gt; &gt; format.type=json&#010;&gt; &gt; schema.0.data-type=VARCHAR(2147483647)&#010;&gt; &gt; schema.0.name=uid&#010;&gt; &gt; schema.1.data-type=VARCHAR(2147483647)&#010;&gt; &gt; schema.1.name=sex&#010;&gt; &gt; schema.2.data-type=INT&#010;&gt; &gt; schema.2.name=age&#010;&gt; &gt; schema.3.data-type=TIMESTAMP(3)&#010;&gt; &gt; schema.3.name=created_time&#010;&gt; &gt; schema.4.data-type=TIMESTAMP(3) NOT NULL&#010;&gt; &gt; schema.4.expr=PROCTIME()&#010;&gt; &gt; schema.4.name=proctime&#010;&gt; &gt; schema.watermark.0.rowtime=created_time&#010;&gt; &gt; schema.watermark.0.strategy.data-type=TIMESTAMP(3)&#010;&gt; &gt; schema.watermark.0.strategy.expr=`created_time` - INTERVAL '3' SECOND&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; The following factories have been considered:&#010;&gt; &gt; org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;&gt; &gt; org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;&gt; &gt; org.apache.flink.table.filesystem.FileSystemTableFactory&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;&gt; &gt; ... 37 more&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 我把maven依赖的provide范围全部去掉了：&#010;&gt; &gt; &lt;properties&gt;&#010;&gt; &gt;&#010;&gt;  &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#010;&gt; &gt;         &lt;flink.version&gt;1.11.0&lt;/flink.version&gt;&#010;&gt; &gt;         &lt;hive.version&gt;2.1.1&lt;/hive.version&gt;&#010;&gt; &gt;         &lt;java.version&gt;1.8&lt;/java.version&gt;&#010;&gt; &gt;         &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;&#010;&gt; &gt;         &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;&#010;&gt; &gt;         &lt;maven.compiler.source&gt;${java.version}&lt;/maven.compiler.source&gt;&#010;&gt; &gt;         &lt;maven.compiler.target&gt;${java.version}&lt;/maven.compiler.target&gt;&#010;&gt; &gt;     &lt;/properties&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;     &lt;repositories&gt;&#010;&gt; &gt;         &lt;repository&gt;&#010;&gt; &gt;             &lt;id&gt;maven-net-cn&lt;/id&gt;&#010;&gt; &gt;             &lt;name&gt;Maven China Mirror&lt;/name&gt;&#010;&gt; &gt;             &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&#010;&gt; &gt; &lt;/url&gt;&#010;&gt; &gt;             &lt;releases&gt;&#010;&gt; &gt;                 &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt; &gt;             &lt;/releases&gt;&#010;&gt; &gt;             &lt;snapshots&gt;&#010;&gt; &gt;                 &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt; &gt;             &lt;/snapshots&gt;&#010;&gt; &gt;         &lt;/repository&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;repository&gt;&#010;&gt; &gt;             &lt;id&gt;apache.snapshots&lt;/id&gt;&#010;&gt; &gt;             &lt;name&gt;Apache Development Snapshot Repository&lt;/name&gt;&#010;&gt; &gt;             &lt;url&gt;&#010;&gt; &gt; https://repository.apache.org/content/repositories/snapshots/&lt;/url&gt;&#010;&gt; &gt;             &lt;releases&gt;&#010;&gt; &gt;                 &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt; &gt;             &lt;/releases&gt;&#010;&gt; &gt;             &lt;snapshots&gt;&#010;&gt; &gt;                 &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt; &gt;             &lt;/snapshots&gt;&#010;&gt; &gt;         &lt;/repository&gt;&#010;&gt; &gt;     &lt;/repositories&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;     &lt;dependencies&gt;&#010;&gt; &gt;         &lt;!-- Apache Flink dependencies --&gt;&#010;&gt; &gt;         &lt;!-- These dependencies are provided, because they should not be&#010;&gt; &gt; packaged into the JAR file. --&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-avro&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;1.0&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;2.8.0&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;redis.clients&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;jedis&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;3.3.0&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-connector-hbase_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;1.11-SNAPSHOT&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;2.1.0&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;lombok&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;1.18.12&lt;/version&gt;&#010;&gt; &gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;io.lettuce&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;5.3.1.RELEASE&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;junit&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;junit&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;4.13&lt;/version&gt;&#010;&gt; &gt;             &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;commons-email&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;1.5&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-connector-hive_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${hive.version}&lt;/version&gt;&#010;&gt; &gt;             &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;!-- Add logging framework, to produce console output when&#010;&gt; running&#010;&gt; &gt; in the IDE. --&gt;&#010;&gt; &gt;         &lt;!-- These dependencies are excluded from the application JAR by&#010;&gt; &gt; default. --&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;1.7.7&lt;/version&gt;&#010;&gt; &gt;             &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;log4j&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;log4j&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;1.2.17&lt;/version&gt;&#010;&gt; &gt;             &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;com.alibaba&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;fastjson&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;1.2.68&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;com.jayway.jsonpath&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;json-path&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;2.4.0&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;flink-connector-jdbc_2.11&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;mysql&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;5.1.46&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;vertx-core&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;         &lt;dependency&gt;&#010;&gt; &gt;             &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt; &gt;             &lt;artifactId&gt;vertx-jdbc-client&lt;/artifactId&gt;&#010;&gt; &gt;             &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt; &gt;         &lt;/dependency&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;     &lt;/dependencies&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 集群节点flink-1.11.0/lib/:&#010;&gt; &gt; -rw-r--r-- 1 root root    197597 6月  30 10:28&#010;&gt; flink-clients_2.11-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root     90782 6月  30 17:46 flink-csv-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root 108349203 6月  30 17:52 flink-dist_2.11-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root     94863 6月  30 17:45 flink-json-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root   7712156 6月  18 10:42&#010;&gt; &gt; flink-shaded-zookeeper-3.4.14.jar&#010;&gt; &gt; -rw-r--r-- 1 root root  33325754 6月  30 17:50 flink-table_2.11-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root     47333 6月  30 10:38&#010;&gt; &gt; flink-table-api-scala-bridge_2.11-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root  37330521 6月  30 17:50&#010;&gt; &gt; flink-table-blink_2.11-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root    754983 6月  30 12:29&#010;&gt; flink-table-common-1.11.0.jar&#010;&gt; &gt; -rw-r--r-- 1 root root     67114 4月  20 20:47 log4j-1.2-api-2.12.1.jar&#010;&gt; &gt; -rw-r--r-- 1 root root    276771 4月  20 20:47 log4j-api-2.12.1.jar&#010;&gt; &gt; -rw-r--r-- 1 root root   1674433 4月  20 20:47 log4j-core-2.12.1.jar&#010;&gt; &gt; -rw-r--r-- 1 root root     23518 4月  20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 把table相关的包都下载下来了，还是报同样的错，好奇怪。。。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 在 2020-07-10 10:24:02，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt; &gt; &gt;Hi&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;这个看上去是提交到 Yarn 了，具体的原因需要看下 JM log 是啥原因。另外是否是日志没有贴全，这里只看到本地&#010;log，其他的就只有小部分&#010;&gt; &gt; &gt;jobmanager.err 的 log。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;Best,&#010;&gt; &gt; &gt;Congxian&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月9日周四 下午9:23写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&gt; hi all，&#010;&gt; &gt; &gt;&gt; 原来用1.10使用per job模式，可以提交的作业，现在用1.11使用应用模式提交失败，看日志，也不清楚原因，&#010;&gt; &gt; &gt;&gt; yarn log：&#010;&gt; &gt; &gt;&gt; Log Type: jobmanager.err&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Length: 785&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt; &gt; &gt;&gt; SLF4J: Found binding in&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; [jar:file:/yarn/nm/usercache/hdfs/appcache/application_1594271580406_0010/filecache/11/data-flow-1.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; &gt; &gt;&gt; SLF4J: Found binding in&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt; &gt; &gt;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt; &gt; &gt;&gt; explanation.&#010;&gt; &gt; &gt;&gt; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#010;&gt; &gt; &gt;&gt; log4j:WARN No appenders could be found for logger&#010;&gt; &gt; &gt;&gt; (org.apache.flink.runtime.entrypoint.ClusterEntrypoint).&#010;&gt; &gt; &gt;&gt; log4j:WARN Please initialize the log4j system properly.&#010;&gt; &gt; &gt;&gt; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig&#010;&gt; &gt; for&#010;&gt; &gt; &gt;&gt; more info.&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Type: jobmanager.out&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Length: 0&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Type: prelaunch.err&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Length: 0&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Type: prelaunch.out&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Log Length: 70&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Setting up env variables&#010;&gt; &gt; &gt;&gt; Setting up job resources&#010;&gt; &gt; &gt;&gt; Launching container&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; 本地log：&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,015 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt; &gt; &gt;&gt;                 [] -&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; --------------------------------------------------------------------------------&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; &gt; Loading&#010;&gt; &gt; &gt;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; &gt; Loading&#010;&gt; &gt; &gt;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; &gt; Loading&#010;&gt; &gt; &gt;&gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; &gt; Loading&#010;&gt; &gt; &gt;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; &gt; Loading&#010;&gt; &gt; &gt;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; &gt; Loading&#010;&gt; &gt; &gt;&gt; configuration property: parallelism.default, 1&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; &gt; Loading&#010;&gt; &gt; &gt;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,164 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.runtime.security.modules.HadoopModule       [] -&#010;&gt; Hadoop&#010;&gt; &gt; &gt;&gt; user set to hdfs (auth:SIMPLE)&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,172 INFO&#010;&gt; &gt; &gt;&gt; org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas&#010;&gt; &gt; file&#010;&gt; &gt; &gt;&gt; will be created as /tmp/jaas-2213111423022415421.conf.&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,181 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt; &gt; &gt;&gt;                 [] - Running 'run-application' command.&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,194 INFO&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer&#010;&gt; &gt; &gt;&gt; [] - Submitting application in 'Application Mode'.&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,201 WARN&#010;&gt; &gt; &gt;&gt; org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The&#010;&gt; &gt; &gt;&gt; configuration directory ('/opt/flink-1.11.0/conf') already contains a&#010;&gt; &gt; LOG4J&#010;&gt; &gt; &gt;&gt; config file.If you want to use logback, then please delete or rename&#010;&gt; the&#010;&gt; &gt; &gt;&gt; log configuration file.&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,537 INFO&#010;&gt; &gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt; &gt;&gt;                 [] - No path for the flink jar passed. Using the&#010;&gt; &gt; location&#010;&gt; &gt; &gt;&gt; of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,665 INFO&#010;&gt; &gt; &gt;&gt; org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] -&#010;&gt; &gt; &gt;&gt; Failing over to rm220&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,717 INFO  org.apache.hadoop.conf.Configuration&#010;&gt; &gt; &gt;&gt;                  [] - resource-types.xml not found&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,718 INFO&#010;&gt; &gt; &gt;&gt; org.apache.hadoop.yarn.util.resource.ResourceUtils           [] -&#010;&gt; &gt; Unable to&#010;&gt; &gt; &gt;&gt; find 'resource-types.xml'.&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:41,755 INFO&#010;&gt; &gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt; &gt;&gt;                 [] - Cluster specification:&#010;&gt; &gt; &gt;&gt; ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=4096,&#010;&gt; &gt; &gt;&gt; slotsPerTaskManager=1}&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:42,723 INFO&#010;&gt; &gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt; &gt;&gt;                 [] - Submitting application master&#010;&gt; &gt; &gt;&gt; application_1594271580406_0010&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt; &gt; &gt;&gt; org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] -&#010;&gt; &gt; Submitted&#010;&gt; &gt; &gt;&gt; application application_1594271580406_0010&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt; &gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt; &gt;&gt;                 [] - Waiting for the cluster to be allocated&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:42,971 INFO&#010;&gt; &gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt; &gt;&gt;                 [] - Deploying cluster, current state ACCEPTED&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:47,619 INFO&#010;&gt; &gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt; &gt;&gt;                 [] - YARN application has been deployed successfully.&#010;&gt; &gt; &gt;&gt; 2020-07-09 21:02:47,620 INFO&#010;&gt; &gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt; &gt; &gt;&gt;                 [] - Found Web Interface cdh003:38716 of application&#010;&gt; &gt; &gt;&gt; 'application_1594271580406_0010'&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>"
    },
    {
        "id": "<16DFB1ED-1CFD-476D-85AD-5E7B039E333F@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:31:39 GMT",
        "subject": "Re: flink1.10升级到flink1.11 提交到yarn失败",
        "content": "Hello，Zach&#010;&#010;&gt;&gt;&gt; Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException:&#010;&gt;&gt;&gt; Could not find a suitable table factory for&#010;&gt;&gt;&gt; 'org.apache.flink.table.factories.TableSourceFactory' in&#010;&gt;&gt;&gt; the classpath.&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Reason: Required context properties mismatch.&#010;这个错误，一般是SQL 程序缺少了SQL connector 或 format的依赖，你pom里下面的这两个依赖，&#010;&#010;      &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;&#010;放在一起是会冲突的，flink-sql-connector-kafka_2.11 shaded 了kafka的依赖， flink-connector-kafka_2.11&#010;是没有shade的。&#010;你根据你的需要，如果是SQL 程序用第一个， 如果是 dataStream 作业 使用第二个。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月10日，11:08，Shuiqiang Chen &lt;acqua.csq@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Hi，&#010;&gt; 看样子是kafka table source没有成功创建，也许你需要将&#010;&gt; &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;            &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &lt;/dependency&gt;&#010;&gt; &#010;&gt; 这个jar 放到 FLINK_HOME/lib 目录下&#010;&gt; &#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月10日周五 上午10:57写道：&#010;&gt; &#010;&gt;&gt; Hi&#010;&gt;&gt; &#010;&gt;&gt; 从异常看，可能是某个包没有引入导致的，和这个[1]比较像，可能你需要对比一下需要的是哪个包没有引入。&#010;&gt;&gt; &#010;&gt;&gt; PS 从栈那里看到是 csv 相关的，可以优先考虑下 cvs 相关的包&#010;&gt;&gt; &#010;&gt;&gt; ```&#010;&gt;&gt; The following factories have been considered:&#010;&gt;&gt; org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;&gt;&gt; org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;&gt;&gt; org.apache.flink.table.filesystem.FileSystemTableFactory&#010;&gt;&gt; at&#010;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;&gt;&gt; at&#010;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;&gt;&gt; at&#010;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;&gt;&gt; at&#010;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;&gt;&gt; at&#010;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;&gt;&gt; ... 37 more&#010;&gt;&gt; ```&#010;&gt;&gt; &#010;&gt;&gt; [1] http://apache-flink.147419.n8.nabble.com/flink-1-11-td4471.html&#010;&gt;&gt; Best,&#010;&gt;&gt; Congxian&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; Zhou Zach &lt;wander669@163.com&gt; 于2020年7月10日周五 上午10:39写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt; 日志贴全了的，这是从yarn ui贴的full log，用yarn logs命令也是这些log，太简短，看不出错误在哪。。。&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 我又提交了另外之前用flink1.10跑过的任务，现在用flink1.11跑，报了异常：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt; &#010;&gt;&gt; [jar:file:/opt/flink-1.11.0/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt; &#010;&gt;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt;&gt;&gt; explanation.&#010;&gt;&gt;&gt; SLF4J: Actual binding is of type&#010;&gt;&gt;&gt; [org.apache.logging.slf4j.Log4jLoggerFactory]&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; ------------------------------------------------------------&#010;&gt;&gt;&gt; The program finished with the following exception:&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt;&gt;&gt; method caused an error: findAndCreateTableSource failed.&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt;&gt; at&#010;&gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt;&gt;&gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt;&gt;&gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt;&gt;&gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt;&gt;&gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt;&gt;&gt; Caused by: org.apache.flink.table.api.TableException:&#010;&gt;&gt;&gt; findAndCreateTableSource failed.&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:49)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.findAndCreateLegacyTableSource(LegacyCatalogSourceTable.scala:190)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.toRel(LegacyCatalogSourceTable.scala:89)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt;&gt;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt;&gt; &#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:747)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV$.main(FromKafkaSinkJdbcForUserUV.scala:78)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV.main(FromKafkaSinkJdbcForUserUV.scala)&#010;&gt;&gt;&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt;&gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt;&gt; ... 11 more&#010;&gt;&gt;&gt; Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException:&#010;&gt;&gt;&gt; Could not find a suitable table factory for&#010;&gt;&gt;&gt; 'org.apache.flink.table.factories.TableSourceFactory' in&#010;&gt;&gt;&gt; the classpath.&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Reason: Required context properties mismatch.&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; The following properties are requested:&#010;&gt;&gt;&gt; connector.properties.bootstrap.servers=cdh1:9092,cdh2:9092,cdh3:9092&#010;&gt;&gt;&gt; connector.properties.group.id=user_flink&#010;&gt;&gt;&gt; connector.properties.zookeeper.connect=cdh1:2181,cdh2:2181,cdh3:2181&#010;&gt;&gt;&gt; connector.startup-mode=latest-offset&#010;&gt;&gt;&gt; connector.topic=user&#010;&gt;&gt;&gt; connector.type=kafka&#010;&gt;&gt;&gt; connector.version=universal&#010;&gt;&gt;&gt; format.derive-schema=true&#010;&gt;&gt;&gt; format.type=json&#010;&gt;&gt;&gt; schema.0.data-type=VARCHAR(2147483647)&#010;&gt;&gt;&gt; schema.0.name=uid&#010;&gt;&gt;&gt; schema.1.data-type=VARCHAR(2147483647)&#010;&gt;&gt;&gt; schema.1.name=sex&#010;&gt;&gt;&gt; schema.2.data-type=INT&#010;&gt;&gt;&gt; schema.2.name=age&#010;&gt;&gt;&gt; schema.3.data-type=TIMESTAMP(3)&#010;&gt;&gt;&gt; schema.3.name=created_time&#010;&gt;&gt;&gt; schema.4.data-type=TIMESTAMP(3) NOT NULL&#010;&gt;&gt;&gt; schema.4.expr=PROCTIME()&#010;&gt;&gt;&gt; schema.4.name=proctime&#010;&gt;&gt;&gt; schema.watermark.0.rowtime=created_time&#010;&gt;&gt;&gt; schema.watermark.0.strategy.data-type=TIMESTAMP(3)&#010;&gt;&gt;&gt; schema.watermark.0.strategy.expr=`created_time` - INTERVAL '3' SECOND&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; The following factories have been considered:&#010;&gt;&gt;&gt; org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;&gt;&gt;&gt; org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;&gt;&gt;&gt; org.apache.flink.table.filesystem.FileSystemTableFactory&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;&gt;&gt;&gt; ... 37 more&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 我把maven依赖的provide范围全部去掉了：&#010;&gt;&gt;&gt; &lt;properties&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#010;&gt;&gt;&gt;        &lt;flink.version&gt;1.11.0&lt;/flink.version&gt;&#010;&gt;&gt;&gt;        &lt;hive.version&gt;2.1.1&lt;/hive.version&gt;&#010;&gt;&gt;&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;&#010;&gt;&gt;&gt;        &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;&#010;&gt;&gt;&gt;        &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;&#010;&gt;&gt;&gt;        &lt;maven.compiler.source&gt;${java.version}&lt;/maven.compiler.source&gt;&#010;&gt;&gt;&gt;        &lt;maven.compiler.target&gt;${java.version}&lt;/maven.compiler.target&gt;&#010;&gt;&gt;&gt;    &lt;/properties&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    &lt;repositories&gt;&#010;&gt;&gt;&gt;        &lt;repository&gt;&#010;&gt;&gt;&gt;            &lt;id&gt;maven-net-cn&lt;/id&gt;&#010;&gt;&gt;&gt;            &lt;name&gt;Maven China Mirror&lt;/name&gt;&#010;&gt;&gt;&gt;            &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&#010;&gt;&gt;&gt; &lt;/url&gt;&#010;&gt;&gt;&gt;            &lt;releases&gt;&#010;&gt;&gt;&gt;                &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt;&gt;&gt;            &lt;/releases&gt;&#010;&gt;&gt;&gt;            &lt;snapshots&gt;&#010;&gt;&gt;&gt;                &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt;&gt;&gt;            &lt;/snapshots&gt;&#010;&gt;&gt;&gt;        &lt;/repository&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;repository&gt;&#010;&gt;&gt;&gt;            &lt;id&gt;apache.snapshots&lt;/id&gt;&#010;&gt;&gt;&gt;            &lt;name&gt;Apache Development Snapshot Repository&lt;/name&gt;&#010;&gt;&gt;&gt;            &lt;url&gt;&#010;&gt;&gt;&gt; https://repository.apache.org/content/repositories/snapshots/&lt;/url&gt;&#010;&gt;&gt;&gt;            &lt;releases&gt;&#010;&gt;&gt;&gt;                &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt;&gt;&gt;            &lt;/releases&gt;&#010;&gt;&gt;&gt;            &lt;snapshots&gt;&#010;&gt;&gt;&gt;                &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt;&gt;&gt;            &lt;/snapshots&gt;&#010;&gt;&gt;&gt;        &lt;/repository&gt;&#010;&gt;&gt;&gt;    &lt;/repositories&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    &lt;dependencies&gt;&#010;&gt;&gt;&gt;        &lt;!-- Apache Flink dependencies --&gt;&#010;&gt;&gt;&gt;        &lt;!-- These dependencies are provided, because they should not be&#010;&gt;&gt;&gt; packaged into the JAR file. --&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-avro&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;1.0&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;2.8.0&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;redis.clients&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;jedis&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;3.3.0&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-hbase_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;1.11-SNAPSHOT&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;2.1.0&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;1.18.12&lt;/version&gt;&#010;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;io.lettuce&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;5.3.1.RELEASE&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;4.13&lt;/version&gt;&#010;&gt;&gt;&gt;            &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;commons-email&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;1.5&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-hive_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${hive.version}&lt;/version&gt;&#010;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;!-- Add logging framework, to produce console output when&#010;&gt;&gt; running&#010;&gt;&gt;&gt; in the IDE. --&gt;&#010;&gt;&gt;&gt;        &lt;!-- These dependencies are excluded from the application JAR by&#010;&gt;&gt;&gt; default. --&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;1.7.7&lt;/version&gt;&#010;&gt;&gt;&gt;            &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;log4j&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;log4j&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;1.2.17&lt;/version&gt;&#010;&gt;&gt;&gt;            &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;1.2.68&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;com.jayway.jsonpath&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;json-path&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;2.4.0&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-jdbc_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;5.1.46&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;vertx-core&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;            &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt;&gt;&gt;            &lt;artifactId&gt;vertx-jdbc-client&lt;/artifactId&gt;&#010;&gt;&gt;&gt;            &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    &lt;/dependencies&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 集群节点flink-1.11.0/lib/:&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root    197597 6月  30 10:28&#010;&gt;&gt; flink-clients_2.11-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root     90782 6月  30 17:46 flink-csv-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root 108349203 6月  30 17:52 flink-dist_2.11-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root     94863 6月  30 17:45 flink-json-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root   7712156 6月  18 10:42&#010;&gt;&gt;&gt; flink-shaded-zookeeper-3.4.14.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root  33325754 6月  30 17:50 flink-table_2.11-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root     47333 6月  30 10:38&#010;&gt;&gt;&gt; flink-table-api-scala-bridge_2.11-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root  37330521 6月  30 17:50&#010;&gt;&gt;&gt; flink-table-blink_2.11-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root    754983 6月  30 12:29&#010;&gt;&gt; flink-table-common-1.11.0.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root     67114 4月  20 20:47 log4j-1.2-api-2.12.1.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root    276771 4月  20 20:47 log4j-api-2.12.1.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root   1674433 4月  20 20:47 log4j-core-2.12.1.jar&#010;&gt;&gt;&gt; -rw-r--r-- 1 root root     23518 4月  20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 把table相关的包都下载下来了，还是报同样的错，好奇怪。。。&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020-07-10 10:24:02，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt; Hi&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 这个看上去是提交到 Yarn 了，具体的原因需要看下 JM log&#010;是啥原因。另外是否是日志没有贴全，这里只看到本地 log，其他的就只有小部分&#010;&gt;&gt;&gt;&gt; jobmanager.err 的 log。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt;&gt; Congxian&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; Zhou Zach &lt;wander669@163.com&gt; 于2020年7月9日周四 下午9:23写道：&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; hi all，&#010;&gt;&gt;&gt;&gt;&gt; 原来用1.10使用per job模式，可以提交的作业，现在用1.11使用应用模式提交失败，看日志，也不清楚原因，&#010;&gt;&gt;&gt;&gt;&gt; yarn log：&#010;&gt;&gt;&gt;&gt;&gt; Log Type: jobmanager.err&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Length: 785&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt;&gt;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; [jar:file:/yarn/nm/usercache/hdfs/appcache/application_1594271580406_0010/filecache/11/data-flow-1.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt;&gt;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt;&gt;&gt;&gt;&gt; explanation.&#010;&gt;&gt;&gt;&gt;&gt; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#010;&gt;&gt;&gt;&gt;&gt; log4j:WARN No appenders could be found for logger&#010;&gt;&gt;&gt;&gt;&gt; (org.apache.flink.runtime.entrypoint.ClusterEntrypoint).&#010;&gt;&gt;&gt;&gt;&gt; log4j:WARN Please initialize the log4j system properly.&#010;&gt;&gt;&gt;&gt;&gt; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig&#010;&gt;&gt;&gt; for&#010;&gt;&gt;&gt;&gt;&gt; more info.&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Type: jobmanager.out&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Length: 0&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Type: prelaunch.err&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Length: 0&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Type: prelaunch.out&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Log Length: 70&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Setting up env variables&#010;&gt;&gt;&gt;&gt;&gt; Setting up job resources&#010;&gt;&gt;&gt;&gt;&gt; Launching container&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 本地log：&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,015 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;&gt;&gt;&gt;&gt;                [] -&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; --------------------------------------------------------------------------------&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt; configuration property: parallelism.default, 1&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,164 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.security.modules.HadoopModule       [] -&#010;&gt;&gt; Hadoop&#010;&gt;&gt;&gt;&gt;&gt; user set to hdfs (auth:SIMPLE)&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,172 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas&#010;&gt;&gt;&gt; file&#010;&gt;&gt;&gt;&gt;&gt; will be created as /tmp/jaas-2213111423022415421.conf.&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,181 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;&gt;&gt;&gt;&gt;                [] - Running 'run-application' command.&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,194 INFO&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer&#010;&gt;&gt;&gt;&gt;&gt; [] - Submitting application in 'Application Mode'.&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,201 WARN&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] - The&#010;&gt;&gt;&gt;&gt;&gt; configuration directory ('/opt/flink-1.11.0/conf') already contains a&#010;&gt;&gt;&gt; LOG4J&#010;&gt;&gt;&gt;&gt;&gt; config file.If you want to use logback, then please delete or rename&#010;&gt;&gt; the&#010;&gt;&gt;&gt;&gt;&gt; log configuration file.&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,537 INFO&#010;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;                [] - No path for the flink jar passed. Using the&#010;&gt;&gt;&gt; location&#010;&gt;&gt;&gt;&gt;&gt; of class org.apache.flink.yarn.YarnClusterDescriptor to locate the jar&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,665 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider [] -&#010;&gt;&gt;&gt;&gt;&gt; Failing over to rm220&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,717 INFO  org.apache.hadoop.conf.Configuration&#010;&gt;&gt;&gt;&gt;&gt;                 [] - resource-types.xml not found&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,718 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.hadoop.yarn.util.resource.ResourceUtils           [] -&#010;&gt;&gt;&gt; Unable to&#010;&gt;&gt;&gt;&gt;&gt; find 'resource-types.xml'.&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,755 INFO&#010;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;                [] - Cluster specification:&#010;&gt;&gt;&gt;&gt;&gt; ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=4096,&#010;&gt;&gt;&gt;&gt;&gt; slotsPerTaskManager=1}&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,723 INFO&#010;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;                [] - Submitting application master&#010;&gt;&gt;&gt;&gt;&gt; application_1594271580406_0010&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt;&gt;&gt;&gt;&gt; org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] -&#010;&gt;&gt;&gt; Submitted&#010;&gt;&gt;&gt;&gt;&gt; application application_1594271580406_0010&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;                [] - Waiting for the cluster to be allocated&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,971 INFO&#010;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;                [] - Deploying cluster, current state ACCEPTED&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:47,619 INFO&#010;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;                [] - YARN application has been deployed successfully.&#010;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:47,620 INFO&#010;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;                [] - Found Web Interface cdh003:38716 of application&#010;&gt;&gt;&gt;&gt;&gt; 'application_1594271580406_0010'&#010;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "5",
        "reply": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>"
    },
    {
        "id": "<fa4c1f5.7dbd.17338062155.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 09:19:52 GMT",
        "subject": "Re:Re: flink1.10升级到flink1.11 提交到yarn失败",
        "content": "Hello，Leonard报的错误是Could not find a suitable table factory for 'org.apache.flink.table.factories.TableSinkFactory'&#010;in the classpath.&#010;&#010;&#010;&#010;&#010;不过，根据你的提示，我下载了flink-connector-jdbc_2.11-1.11.0.jar，放到了/opt/flink-1.11.0/lib/，作业成功运行了！早上跑的第一个作业，也是类似原因，下载了hbase&#010;connector就好了，感谢答疑问！&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-10 11:31:39，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hello，Zach&#010;&gt;&#010;&gt;&gt;&gt;&gt; Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException:&#010;&gt;&gt;&gt;&gt; Could not find a suitable table factory for&#010;&gt;&gt;&gt;&gt; 'org.apache.flink.table.factories.TableSourceFactory' in&#010;&gt;&gt;&gt;&gt; the classpath.&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; Reason: Required context properties mismatch.&#010;&gt;这个错误，一般是SQL 程序缺少了SQL connector 或 format的依赖，你pom里下面的这两个依赖，&#010;&gt;&#010;&gt;      &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;       &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;&#010;&gt;放在一起是会冲突的，flink-sql-connector-kafka_2.11 shaded 了kafka的依赖，&#010;flink-connector-kafka_2.11 是没有shade的。&#010;&gt;你根据你的需要，如果是SQL 程序用第一个， 如果是 dataStream 作业&#010;使用第二个。&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月10日，11:08，Shuiqiang Chen &lt;acqua.csq@gmail.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; Hi，&#010;&gt;&gt; 看样子是kafka table source没有成功创建，也许你需要将&#010;&gt;&gt; &lt;dependency&gt;&#010;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;            &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &lt;/dependency&gt;&#010;&gt;&gt; &#010;&gt;&gt; 这个jar 放到 FLINK_HOME/lib 目录下&#010;&gt;&gt; &#010;&gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月10日周五 上午10:57写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt; Hi&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 从异常看，可能是某个包没有引入导致的，和这个[1]比较像，可能你需要对比一下需要的是哪个包没有引入。&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; PS 从栈那里看到是 csv 相关的，可以优先考虑下 cvs 相关的包&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; ```&#010;&gt;&gt;&gt; The following factories have been considered:&#010;&gt;&gt;&gt; org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;&gt;&gt;&gt; org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;&gt;&gt;&gt; org.apache.flink.table.filesystem.FileSystemTableFactory&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;&gt;&gt;&gt; ... 37 more&#010;&gt;&gt;&gt; ```&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [1] http://apache-flink.147419.n8.nabble.com/flink-1-11-td4471.html&#010;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt; Congxian&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Zhou Zach &lt;wander669@163.com&gt; 于2020年7月10日周五 上午10:39写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 日志贴全了的，这是从yarn ui贴的full log，用yarn logs命令也是这些log，太简短，看不出错误在哪。。。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 我又提交了另外之前用flink1.10跑过的任务，现在用flink1.11跑，报了异常：&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [jar:file:/opt/flink-1.11.0/lib/log4j-slf4j-impl-2.12.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an&#010;&gt;&gt;&gt;&gt; explanation.&#010;&gt;&gt;&gt;&gt; SLF4J: Actual binding is of type&#010;&gt;&gt;&gt;&gt; [org.apache.logging.slf4j.Log4jLoggerFactory]&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; ------------------------------------------------------------&#010;&gt;&gt;&gt;&gt; The program finished with the following exception:&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt;&gt;&gt;&gt; method caused an error: findAndCreateTableSource failed.&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt;&gt;&gt;&gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt;&gt;&gt;&gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt;&gt;&gt;&gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt;&gt;&gt;&gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt;&gt;&gt;&gt; Caused by: org.apache.flink.table.api.TableException:&#010;&gt;&gt;&gt;&gt; findAndCreateTableSource failed.&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:49)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.findAndCreateLegacyTableSource(LegacyCatalogSourceTable.scala:190)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.plan.schema.LegacyCatalogSourceTable.toRel(LegacyCatalogSourceTable.scala:89)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt;&gt;&gt;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:747)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV$.main(FromKafkaSinkJdbcForUserUV.scala:78)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkJdbcForUserUV.main(FromKafkaSinkJdbcForUserUV.scala)&#010;&gt;&gt;&gt;&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt;&gt;&gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt;&gt;&gt; ... 11 more&#010;&gt;&gt;&gt;&gt; Caused by: org.apache.flink.table.api.NoMatchingTableFactoryException:&#010;&gt;&gt;&gt;&gt; Could not find a suitable table factory for&#010;&gt;&gt;&gt;&gt; 'org.apache.flink.table.factories.TableSourceFactory' in&#010;&gt;&gt;&gt;&gt; the classpath.&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; Reason: Required context properties mismatch.&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; The following properties are requested:&#010;&gt;&gt;&gt;&gt; connector.properties.bootstrap.servers=cdh1:9092,cdh2:9092,cdh3:9092&#010;&gt;&gt;&gt;&gt; connector.properties.group.id=user_flink&#010;&gt;&gt;&gt;&gt; connector.properties.zookeeper.connect=cdh1:2181,cdh2:2181,cdh3:2181&#010;&gt;&gt;&gt;&gt; connector.startup-mode=latest-offset&#010;&gt;&gt;&gt;&gt; connector.topic=user&#010;&gt;&gt;&gt;&gt; connector.type=kafka&#010;&gt;&gt;&gt;&gt; connector.version=universal&#010;&gt;&gt;&gt;&gt; format.derive-schema=true&#010;&gt;&gt;&gt;&gt; format.type=json&#010;&gt;&gt;&gt;&gt; schema.0.data-type=VARCHAR(2147483647)&#010;&gt;&gt;&gt;&gt; schema.0.name=uid&#010;&gt;&gt;&gt;&gt; schema.1.data-type=VARCHAR(2147483647)&#010;&gt;&gt;&gt;&gt; schema.1.name=sex&#010;&gt;&gt;&gt;&gt; schema.2.data-type=INT&#010;&gt;&gt;&gt;&gt; schema.2.name=age&#010;&gt;&gt;&gt;&gt; schema.3.data-type=TIMESTAMP(3)&#010;&gt;&gt;&gt;&gt; schema.3.name=created_time&#010;&gt;&gt;&gt;&gt; schema.4.data-type=TIMESTAMP(3) NOT NULL&#010;&gt;&gt;&gt;&gt; schema.4.expr=PROCTIME()&#010;&gt;&gt;&gt;&gt; schema.4.name=proctime&#010;&gt;&gt;&gt;&gt; schema.watermark.0.rowtime=created_time&#010;&gt;&gt;&gt;&gt; schema.watermark.0.strategy.data-type=TIMESTAMP(3)&#010;&gt;&gt;&gt;&gt; schema.watermark.0.strategy.expr=`created_time` - INTERVAL '3' SECOND&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; The following factories have been considered:&#010;&gt;&gt;&gt;&gt; org.apache.flink.table.sources.CsvBatchTableSourceFactory&#010;&gt;&gt;&gt;&gt; org.apache.flink.table.sources.CsvAppendTableSourceFactory&#010;&gt;&gt;&gt;&gt; org.apache.flink.table.filesystem.FileSystemTableFactory&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filterByContext(TableFactoryService.java:322)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.filter(TableFactoryService.java:190)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.findSingleInternal(TableFactoryService.java:143)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryService.find(TableFactoryService.java:96)&#010;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.table.factories.TableFactoryUtil.findAndCreateTableSource(TableFactoryUtil.java:46)&#010;&gt;&gt;&gt;&gt; ... 37 more&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 我把maven依赖的provide范围全部去掉了：&#010;&gt;&gt;&gt;&gt; &lt;properties&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&#010;&gt;&gt;&gt;&gt;        &lt;flink.version&gt;1.11.0&lt;/flink.version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;hive.version&gt;2.1.1&lt;/hive.version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;java.version&gt;1.8&lt;/java.version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;scala.version&gt;2.11.12&lt;/scala.version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;maven.compiler.source&gt;${java.version}&lt;/maven.compiler.source&gt;&#010;&gt;&gt;&gt;&gt;        &lt;maven.compiler.target&gt;${java.version}&lt;/maven.compiler.target&gt;&#010;&gt;&gt;&gt;&gt;    &lt;/properties&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;    &lt;repositories&gt;&#010;&gt;&gt;&gt;&gt;        &lt;repository&gt;&#010;&gt;&gt;&gt;&gt;            &lt;id&gt;maven-net-cn&lt;/id&gt;&#010;&gt;&gt;&gt;&gt;            &lt;name&gt;Maven China Mirror&lt;/name&gt;&#010;&gt;&gt;&gt;&gt;            &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&#010;&gt;&gt;&gt;&gt; &lt;/url&gt;&#010;&gt;&gt;&gt;&gt;            &lt;releases&gt;&#010;&gt;&gt;&gt;&gt;                &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt;&gt;&gt;&gt;            &lt;/releases&gt;&#010;&gt;&gt;&gt;&gt;            &lt;snapshots&gt;&#010;&gt;&gt;&gt;&gt;                &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt;&gt;&gt;&gt;            &lt;/snapshots&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/repository&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;repository&gt;&#010;&gt;&gt;&gt;&gt;            &lt;id&gt;apache.snapshots&lt;/id&gt;&#010;&gt;&gt;&gt;&gt;            &lt;name&gt;Apache Development Snapshot Repository&lt;/name&gt;&#010;&gt;&gt;&gt;&gt;            &lt;url&gt;&#010;&gt;&gt;&gt;&gt; https://repository.apache.org/content/repositories/snapshots/&lt;/url&gt;&#010;&gt;&gt;&gt;&gt;            &lt;releases&gt;&#010;&gt;&gt;&gt;&gt;                &lt;enabled&gt;false&lt;/enabled&gt;&#010;&gt;&gt;&gt;&gt;            &lt;/releases&gt;&#010;&gt;&gt;&gt;&gt;            &lt;snapshots&gt;&#010;&gt;&gt;&gt;&gt;                &lt;enabled&gt;true&lt;/enabled&gt;&#010;&gt;&gt;&gt;&gt;            &lt;/snapshots&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/repository&gt;&#010;&gt;&gt;&gt;&gt;    &lt;/repositories&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;    &lt;dependencies&gt;&#010;&gt;&gt;&gt;&gt;        &lt;!-- Apache Flink dependencies --&gt;&#010;&gt;&gt;&gt;&gt;        &lt;!-- These dependencies are provided, because they should not be&#010;&gt;&gt;&gt;&gt; packaged into the JAR file. --&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt; &lt;!--            &lt;scope&gt;provided&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-sql-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-avro&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;1.0&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;2.8.0&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;redis.clients&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;jedis&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;3.3.0&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-hbase_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;1.11-SNAPSHOT&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;2.1.0&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;1.18.12&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;io.lettuce&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;5.3.1.RELEASE&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;4.13&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;            &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;commons-email&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;1.5&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;3.0.0-cdh6.3.2&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-hive_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${hive.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;            &lt;scope&gt;provided&lt;/scope&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;!-- Add logging framework, to produce console output when&#010;&gt;&gt;&gt; running&#010;&gt;&gt;&gt;&gt; in the IDE. --&gt;&#010;&gt;&gt;&gt;&gt;        &lt;!-- These dependencies are excluded from the application JAR by&#010;&gt;&gt;&gt;&gt; default. --&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;1.7.7&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;            &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;log4j&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;log4j&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;1.2.17&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;            &lt;scope&gt;runtime&lt;/scope&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;com.alibaba&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;fastjson&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;1.2.68&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;com.jayway.jsonpath&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;json-path&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;2.4.0&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;flink-connector-jdbc_2.11&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;5.1.46&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;vertx-core&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;        &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;            &lt;groupId&gt;io.vertx&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;artifactId&gt;vertx-jdbc-client&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;            &lt;version&gt;3.9.1&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;        &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;    &lt;/dependencies&gt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 集群节点flink-1.11.0/lib/:&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root    197597 6月  30 10:28&#010;&gt;&gt;&gt; flink-clients_2.11-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root     90782 6月  30 17:46 flink-csv-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root 108349203 6月  30 17:52 flink-dist_2.11-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root     94863 6月  30 17:45 flink-json-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root   7712156 6月  18 10:42&#010;&gt;&gt;&gt;&gt; flink-shaded-zookeeper-3.4.14.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root  33325754 6月  30 17:50 flink-table_2.11-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root     47333 6月  30 10:38&#010;&gt;&gt;&gt;&gt; flink-table-api-scala-bridge_2.11-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root  37330521 6月  30 17:50&#010;&gt;&gt;&gt;&gt; flink-table-blink_2.11-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root    754983 6月  30 12:29&#010;&gt;&gt;&gt; flink-table-common-1.11.0.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root     67114 4月  20 20:47 log4j-1.2-api-2.12.1.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root    276771 4月  20 20:47 log4j-api-2.12.1.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root   1674433 4月  20 20:47 log4j-core-2.12.1.jar&#010;&gt;&gt;&gt;&gt; -rw-r--r-- 1 root root     23518 4月  20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 把table相关的包都下载下来了，还是报同样的错，好奇怪。。。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 在 2020-07-10 10:24:02，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt;&gt; Hi&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 这个看上去是提交到 Yarn 了，具体的原因需要看下 JM&#010;log 是啥原因。另外是否是日志没有贴全，这里只看到本地 log，其他的就只有小部分&#010;&gt;&gt;&gt;&gt;&gt; jobmanager.err 的 log。&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt;&gt;&gt; Congxian&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Zhou Zach &lt;wander669@163.com&gt; 于2020年7月9日周四 下午9:23写道：&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; hi all，&#010;&gt;&gt;&gt;&gt;&gt;&gt; 原来用1.10使用per job模式，可以提交的作业，现在用1.11使用应用模式提交失败，看日志，也不清楚原因，&#010;&gt;&gt;&gt;&gt;&gt;&gt; yarn log：&#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Type: jobmanager.err&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Length: 785&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; SLF4J: Class path contains multiple SLF4J bindings.&#010;&gt;&gt;&gt;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [jar:file:/yarn/nm/usercache/hdfs/appcache/application_1594271580406_0010/filecache/11/data-flow-1.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt;&gt;&gt;&gt; SLF4J: Found binding in&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [jar:file:/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#010;&gt;&gt;&gt;&gt;&gt;&gt; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for&#010;an&#010;&gt;&gt;&gt;&gt;&gt;&gt; explanation.&#010;&gt;&gt;&gt;&gt;&gt;&gt; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#010;&gt;&gt;&gt;&gt;&gt;&gt; log4j:WARN No appenders could be found for logger&#010;&gt;&gt;&gt;&gt;&gt;&gt; (org.apache.flink.runtime.entrypoint.ClusterEntrypoint).&#010;&gt;&gt;&gt;&gt;&gt;&gt; log4j:WARN Please initialize the log4j system properly.&#010;&gt;&gt;&gt;&gt;&gt;&gt; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig&#010;&gt;&gt;&gt;&gt; for&#010;&gt;&gt;&gt;&gt;&gt;&gt; more info.&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Type: jobmanager.out&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Length: 0&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Type: prelaunch.err&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Length: 0&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Type: prelaunch.out&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Upload Time: Thu Jul 09 21:02:48 +0800 2020&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Log Length: 70&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Setting up env variables&#010;&gt;&gt;&gt;&gt;&gt;&gt; Setting up job resources&#010;&gt;&gt;&gt;&gt;&gt;&gt; Launching container&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 本地log：&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,015 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] -&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; --------------------------------------------------------------------------------&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,020 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration property: parallelism.default, 1&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,021 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt;&gt;&gt;&gt; Loading&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,164 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.security.modules.HadoopModule       [] -&#010;&gt;&gt;&gt; Hadoop&#010;&gt;&gt;&gt;&gt;&gt;&gt; user set to hdfs (auth:SIMPLE)&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,172 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.security.modules.JaasModule         [] -&#010;Jaas&#010;&gt;&gt;&gt;&gt; file&#010;&gt;&gt;&gt;&gt;&gt;&gt; will be created as /tmp/jaas-2213111423022415421.conf.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,181 INFO  org.apache.flink.client.cli.CliFrontend&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - Running 'run-application' command.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,194 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; org.apache.flink.client.deployment.application.cli.ApplicationClusterDeployer&#010;&gt;&gt;&gt;&gt;&gt;&gt; [] - Submitting application in 'Application Mode'.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,201 WARN&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.yarn.configuration.YarnLogConfigUtil        [] -&#010;The&#010;&gt;&gt;&gt;&gt;&gt;&gt; configuration directory ('/opt/flink-1.11.0/conf') already contains&#010;a&#010;&gt;&gt;&gt;&gt; LOG4J&#010;&gt;&gt;&gt;&gt;&gt;&gt; config file.If you want to use logback, then please delete or rename&#010;&gt;&gt;&gt; the&#010;&gt;&gt;&gt;&gt;&gt;&gt; log configuration file.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,537 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - No path for the flink jar passed. Using the&#010;&gt;&gt;&gt;&gt; location&#010;&gt;&gt;&gt;&gt;&gt;&gt; of class org.apache.flink.yarn.YarnClusterDescriptor to locate the&#010;jar&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,665 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider []&#010;-&#010;&gt;&gt;&gt;&gt;&gt;&gt; Failing over to rm220&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,717 INFO  org.apache.hadoop.conf.Configuration&#010;&gt;&gt;&gt;&gt;&gt;&gt;                 [] - resource-types.xml not found&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,718 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.hadoop.yarn.util.resource.ResourceUtils           [] -&#010;&gt;&gt;&gt;&gt; Unable to&#010;&gt;&gt;&gt;&gt;&gt;&gt; find 'resource-types.xml'.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:41,755 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - Cluster specification:&#010;&gt;&gt;&gt;&gt;&gt;&gt; ClusterSpecification{masterMemoryMB=2048, taskManagerMemoryMB=4096,&#010;&gt;&gt;&gt;&gt;&gt;&gt; slotsPerTaskManager=1}&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,723 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - Submitting application master&#010;&gt;&gt;&gt;&gt;&gt;&gt; application_1594271580406_0010&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] -&#010;&gt;&gt;&gt;&gt; Submitted&#010;&gt;&gt;&gt;&gt;&gt;&gt; application application_1594271580406_0010&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,969 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - Waiting for the cluster to be allocated&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:42,971 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - Deploying cluster, current state ACCEPTED&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:47,619 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - YARN application has been deployed successfully.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-07-09 21:02:47,620 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.yarn.YarnClusterDescriptor&#010;&gt;&gt;&gt;&gt;&gt;&gt;                [] - Found Web Interface cdh003:38716 of application&#010;&gt;&gt;&gt;&gt;&gt;&gt; 'application_1594271580406_0010'&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;",
        "depth": "6",
        "reply": "<6d313a5d.b6f7.17333be76c8.Coremail.wander669@163.com>"
    },
    {
        "id": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 00:43:31 GMT",
        "subject": "转发：pyflink1.11.0window",
        "content": "------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"奇怪的不朽琴师\"                          &#010;                                                         &lt;1129656513@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 下午5:08&#013;&#010;收件人:&amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;你好：&#013;&#010;&amp;nbsp; &amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;: org.apache.flink.table.api.ValidationException: A group window expects a time attribute&#010;for grouping in a stream environment.&#013;&#010;&#013;&#010;请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;代码如下&#013;&#010;谢谢&#013;&#010;&#013;&#010;&#013;&#010;def from_kafka_to_kafka_demo():&#013;&#010;&amp;nbsp; &amp;nbsp; s_env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;nbsp; &amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; # use blink table planner&#013;&#010;&amp;nbsp; &amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; # register source and sink&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\"))&#010;\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .group_by(\"w\") \\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\" id,&amp;nbsp; time1 , time1 \")\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .insert_into(\"sink1\")&#013;&#010;&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;def register_rides_source(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; '''&#013;&#010;&amp;nbsp; &amp;nbsp; create table source1(&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; id int,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time1 timestamp,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;type string&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp; &amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092'&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;)&#013;&#010;&amp;nbsp; &amp;nbsp; '''&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;def register_rides_sink(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; sink_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; '''&#013;&#010;&amp;nbsp; &amp;nbsp; create table sink1(&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; id int,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time1 timestamp,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time2 timestamp&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp; &amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp3',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092'&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;)&#013;&#010;&amp;nbsp; &amp;nbsp; '''&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;if __name__ == '__main__':&#013;&#010;&amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp;",
        "depth": "0",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<CABvJ6uXfGw87jZsVROjH7AoMF21D=pww4L7vx3s9WWrPaFZPyA@mail.gmail.com>",
        "from": "Shuiqiang Chen &lt;acqua....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 01:17:19 GMT",
        "subject": "Re: pyflink1.11.0window",
        "content": "琴师你好，&#010;&#010;你的source ddl里有指定time1为 time attribute吗？&#010;create table source1(&#010;        id int,&#010;        time1 timestamp,&#010;        type string,&#010;        WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#010;) with (...)&#010;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&gt; 于2020年7月10日周五 上午8:43写道：&#010;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"奇怪的不朽琴师\"&#010;&gt;                                                                     &lt;&#010;&gt; 1129656513@qq.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月9日(星期四) 下午5:08&#010;&gt; 收件人:&amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;pyflink1.11.0window&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 你好：&#010;&gt; &amp;nbsp; &amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#010;&gt; : org.apache.flink.table.api.ValidationException: A group window expects a&#010;&gt; time attribute for grouping in a stream environment.&#010;&gt;&#010;&gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#010;&gt; 代码如下&#010;&gt; 谢谢&#010;&gt;&#010;&gt;&#010;&gt; def from_kafka_to_kafka_demo():&#010;&gt; &amp;nbsp; &amp;nbsp; s_env =&#010;&gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;nbsp; &amp;nbsp; s_env.set_parallelism(1)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; # use blink table planner&#010;&gt; &amp;nbsp; &amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; # register source and sink&#010;&gt; &amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#010;&gt; &amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.from_path(\"source1\")\\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .group_by(\"w\") \\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\" id,&amp;nbsp; time1 , time1 \")\\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .insert_into(\"sink1\")&#010;&gt; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; def register_rides_source(st_env):&#010;&gt; &amp;nbsp; &amp;nbsp; source_ddl = \\&#010;&gt; &amp;nbsp; &amp;nbsp; '''&#010;&gt; &amp;nbsp; &amp;nbsp; create table source1(&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; id int,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;time1 timestamp,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;type string&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;nbsp; &amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp1',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092'&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;)&#010;&gt; &amp;nbsp; &amp;nbsp; '''&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; def register_rides_sink(st_env):&#010;&gt; &amp;nbsp; &amp;nbsp; sink_ddl = \\&#010;&gt; &amp;nbsp; &amp;nbsp; '''&#010;&gt; &amp;nbsp; &amp;nbsp; create table sink1(&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; id int,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;time1 timestamp,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;time2 timestamp&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;nbsp; &amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp3',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092'&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;)&#010;&gt; &amp;nbsp; &amp;nbsp; '''&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(sink_ddl)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; if __name__ == '__main__':&#010;&gt; &amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_99326339099F76CBDA5B9349672AE0B92208@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:27:15 GMT",
        "subject": "回复： pyflink1.11.0window",
        "content": "你好：&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;Traceback (most recent call last):&#013;&#010;&amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;gt;&#013;&#010;&amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;nbsp; File \"tou.py\", line 21, in from_kafka_to_kafka_demo&#013;&#010;&amp;nbsp; &amp;nbsp; .select(\" id,&amp;nbsp; time1 , time1 \")\\&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line 907,&#010;in select&#013;&#010;&amp;nbsp; &amp;nbsp; return Table(self._j_table.select(fields), self._t_env)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1286,&#010;in __call__&#013;&#010;&amp;nbsp; &amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#010;147, in deco&#013;&#010;&amp;nbsp; &amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value&#013;&#010;&amp;nbsp; &amp;nbsp; format(target_id, \".\", name), value)&#013;&#010;py4j.protocol.Py4JJavaError: An error occurred while calling o26.select.&#013;&#010;: org.apache.flink.table.api.ValidationException: A tumble window expects a size value literal.&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;Method)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;def register_rides_source(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; create table source1(&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;id int,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time1 timestamp,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;type string,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp; &amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;nbsp; &amp;nbsp; 'format.derive-schema' = 'true',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;)&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; s_env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;nbsp; &amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\"))&#010;\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .group_by(\"w\") \\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\" id,&amp;nbsp; time1 , time1 \")\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .insert_into(\"sink1\")&#013;&#010;&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&#013;&#010;&#013;&#010;代码如上&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月10日(星期五) 上午9:17&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;琴师你好，&#013;&#010;&#013;&#010;你的source ddl里有指定time1为 time attribute吗？&#013;&#010;create table source1(&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; id int,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; time1 timestamp,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; type string,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; WATERMARK FOR time1&#010;as time1 - INTERVAL '2' SECOND&#013;&#010;) with (...)&#013;&#010;&#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月10日周五 上午8:43写道：&#013;&#010;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"奇怪的不朽琴师\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; 1129656513@qq.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; : org.apache.flink.table.api.ValidationException: A group window expects a&#013;&#010;&amp;gt; time attribute for grouping in a stream environment.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; 代码如下&#013;&#010;&amp;gt; 谢谢&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # use blink table planner&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # register source and sink&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .select(\" id,&amp;amp;nbsp;&#010;time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; id int,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;type string&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; sink_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table sink1(&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; id int,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time2 timestamp&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp3',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;",
        "depth": "2",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<CABvJ6uVPF30Y_KsQUVs78Mopmh+hxrJMAcqJ9w0BGNWzx_Cf8g@mail.gmail.com>",
        "from": "Shuiqiang Chen &lt;acqua....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:51:48 GMT",
        "subject": "Re: pyflink1.11.0window",
        "content": "琴师你好，&#010;异常栈信息org.apache.flink.table.api.ValidationException: A tumble window&#010;expects a size value literal.&#010;看起来是接下tumble window定义的代码不太正确吧&#010;&#010;Best,&#010;Shuiqiang&#010;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&gt; 于2020年7月15日周三 上午10:27写道：&#010;&#010;&gt; 你好：&#010;&gt; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#010;&gt; Traceback (most recent call last):&#010;&gt; &amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;gt;&#010;&gt; &amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt; &amp;nbsp; File \"tou.py\", line 21, in from_kafka_to_kafka_demo&#010;&gt; &amp;nbsp; &amp;nbsp; .select(\" id,&amp;nbsp; time1 , time1 \")\\&#010;&gt; &amp;nbsp; File&#010;&gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line 907,&#010;&gt; in select&#010;&gt; &amp;nbsp; &amp;nbsp; return Table(self._j_table.select(fields), self._t_env)&#010;&gt; &amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#010;&gt; line 1286, in __call__&#010;&gt; &amp;nbsp; &amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#010;&gt; &amp;nbsp; File&#010;&gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#010;&gt; 147, in deco&#010;&gt; &amp;nbsp; &amp;nbsp; return f(*a, **kw)&#010;&gt; &amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#010;&gt; line 328, in get_return_value&#010;&gt; &amp;nbsp; &amp;nbsp; format(target_id, \".\", name), value)&#010;&gt; py4j.protocol.Py4JJavaError: An error occurred while calling o26.select.&#010;&gt; : org.apache.flink.table.api.ValidationException: A tumble window expects&#010;&gt; a size value literal.&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; def register_rides_source(st_env):&#010;&gt; &amp;nbsp; &amp;nbsp; source_ddl = \\&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; create table source1(&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;id int,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;time1 timestamp,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;type string,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;nbsp; &amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp1',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#010;&gt; &amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#010;&gt; &amp;nbsp; &amp;nbsp; 'format.derive-schema' = 'true',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;)&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; s_env =&#010;&gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;nbsp; &amp;nbsp; s_env.set_parallelism(1)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#010;&gt; &amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.from_path(\"source1\")\\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .group_by(\"w\") \\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\" id,&amp;nbsp; time1 , time1 \")\\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .insert_into(\"sink1\")&#010;&gt; &amp;nbsp; &amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt;&#010;&gt;&#010;&gt; 代码如上&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; acqua.csq@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月10日(星期五) 上午9:17&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: pyflink1.11.0window&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 琴师你好，&#010;&gt;&#010;&gt; 你的source ddl里有指定time1为 time attribute吗？&#010;&gt; create table source1(&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; id int,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; time1 timestamp,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; type string,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; WATERMARK FOR&#010;time1 as time1 -&#010;&gt; INTERVAL '2' SECOND&#010;&gt; ) with (...)&#010;&gt;&#010;&gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月10日周五 上午8:43写道：&#010;&gt;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#010;&gt; &amp;gt; 发件人:&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; \"奇怪的不朽琴师\"&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &lt;&#010;&gt; &amp;gt; 1129656513@qq.com&amp;amp;gt;;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&amp;amp;gt;;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;pyflink1.11.0window&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 你好：&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#010;&gt; &amp;gt; : org.apache.flink.table.api.ValidationException: A group window&#010;&gt; expects a&#010;&gt; &amp;gt; time attribute for grouping in a stream environment.&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#010;&gt; &amp;gt; 代码如下&#010;&gt; &amp;gt; 谢谢&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; def from_kafka_to_kafka_demo():&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env =&#010;&gt; &amp;gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env.set_parallelism(1)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # use blink table planner&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # register source and sink&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_source(st_env)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_sink(st_env)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.from_path(\"source1\")\\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .group_by(\"w\") \\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .select(\" id,&amp;amp;nbsp;&#010;&gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .insert_into(\"sink1\")&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; def register_rides_source(st_env):&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table source1(&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; id int,&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;type string&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp1',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' =&#010;&gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; def register_rides_sink(st_env):&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; sink_ddl = \\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table sink1(&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; id int,&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time2 timestamp&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp3',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' =&#010;&gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(sink_ddl)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; if __name__ == '__main__':&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_9968C692136EEC78B5EF2653CFE8C1A94409@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 03:10:32 GMT",
        "subject": "回复： pyflink1.11.0window",
        "content": "Shuiqiang，你好：&#013;&#010;&amp;nbsp; &amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月15日(星期三) 上午10:51&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;琴师你好，&#013;&#010;异常栈信息org.apache.flink.table.api.ValidationException: A tumble window&#013;&#010;expects a size value literal.&#013;&#010;看起来是接下tumble window定义的代码不太正确吧&#013;&#010;&#013;&#010;Best,&#013;&#010;Shuiqiang&#013;&#010;&#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 上午10:27写道：&#013;&#010;&#013;&#010;&amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;&amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"tou.py\", line 21, in from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; .select(\" id,&amp;amp;nbsp; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line 907,&#013;&#010;&amp;gt; in select&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return Table(self._j_table.select(fields), self._t_env)&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#013;&#010;&amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; format(target_id, \".\", name), value)&#013;&#010;&amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling o26.select.&#013;&#010;&amp;gt; : org.apache.flink.table.api.ValidationException: A tumble window expects&#013;&#010;&amp;gt; a size value literal.&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL&#010;'2' SECOND&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.derive-schema' = 'true',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .select(\" id,&amp;amp;nbsp;&#010;time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 代码如上&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 琴师你好，&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#013;&#010;&amp;gt; create table source1(&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;id int,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;type string,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;WATERMARK FOR time1 as time1 -&#013;&#010;&amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; ) with (...)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月10日周五&#010;上午8:43写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; \"奇怪的不朽琴师\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; 1129656513@qq.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; &amp;amp;gt; : org.apache.flink.table.api.ValidationException: A group window&#013;&#010;&amp;gt; expects a&#013;&#010;&amp;gt; &amp;amp;gt; time attribute for grouping in a stream environment.&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; &amp;amp;gt; 代码如下&#013;&#010;&amp;gt; &amp;amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; # use blink table planner&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; # register source and sink&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;.group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;.select(\" id,&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;.insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;type string&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.properties.bootstrap.servers'&#010;=&#013;&#010;&amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; sink_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; create table sink1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time2 timestamp&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.topic' = 'tp3',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.properties.bootstrap.servers'&#010;=&#013;&#010;&amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;",
        "depth": "4",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<CABvJ6uUHqup=DEHK+FMzbJDekbixNi2hakO7ms_o-U5YUgybzA@mail.gmail.com>",
        "from": "Shuiqiang Chen &lt;acqua....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 03:25:39 GMT",
        "subject": "Re: pyflink1.11.0window",
        "content": "举个sql例子&#010;select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#010;pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#010;rowtime&#010;from payment_msg group by tumble(rt, interval '5' seconds), payPlatform&#010;这个query 对每5s的tumble窗口做统计。&#010;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&gt; 于2020年7月15日周三 上午11:10写道：&#010;&#010;&gt; Shuiqiang，你好：&#010;&gt; &amp;nbsp; &amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; acqua.csq@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月15日(星期三) 上午10:51&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: pyflink1.11.0window&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 琴师你好，&#010;&gt; 异常栈信息org.apache.flink.table.api.ValidationException: A tumble window&#010;&gt; expects a size value literal.&#010;&gt; 看起来是接下tumble window定义的代码不太正确吧&#010;&gt;&#010;&gt; Best,&#010;&gt; Shuiqiang&#010;&gt;&#010;&gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 上午10:27写道：&#010;&gt;&#010;&gt; &amp;gt; 你好：&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#010;&gt; &amp;gt; Traceback (most recent call last):&#010;&gt; &amp;gt; &amp;amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt; &amp;gt; &amp;amp;nbsp; File \"tou.py\", line 21, in from_kafka_to_kafka_demo&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; .select(\" id,&amp;amp;nbsp; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;nbsp; File&#010;&gt; &amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#010;&gt; 907,&#010;&gt; &amp;gt; in select&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return Table(self._j_table.select(fields),&#010;&gt; self._t_env)&#010;&gt; &amp;gt; &amp;amp;nbsp; File&#010;&gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#010;&gt; &amp;gt; line 1286, in __call__&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; answer, self.gateway_client, self.target_id,&#010;&gt; self.name)&#010;&gt; &amp;gt; &amp;amp;nbsp; File&#010;&gt; &amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#010;&gt; line&#010;&gt; &amp;gt; 147, in deco&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return f(*a, **kw)&#010;&gt; &amp;gt; &amp;amp;nbsp; File&#010;&gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#010;&gt; &amp;gt; line 328, in get_return_value&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; format(target_id, \".\", name), value)&#010;&gt; &amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling&#010;&gt; o26.select.&#010;&gt; &amp;gt; : org.apache.flink.table.api.ValidationException: A tumble window&#010;&gt; expects&#010;&gt; &amp;gt; a size value literal.&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; def register_rides_source(st_env):&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table source1(&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;id int,&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;type string,&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;WATERMARK FOR time1 as time1 -&#010;&gt; INTERVAL '2' SECOND&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp1',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' =&#010;&gt; 'localhost:9092',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.zookeeper.connect' =&#010;&gt; 'localhost:2181',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.type' = 'json',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.derive-schema' = 'true',&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.version' = 'universal'&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env =&#010;&gt; &amp;gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; s_env.set_parallelism(1)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_source(st_env)&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_sink(st_env)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.from_path(\"source1\")\\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .group_by(\"w\") \\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .select(\" id,&amp;amp;nbsp;&#010;&gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; .insert_into(\"sink1\")&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 代码如上&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#010;&gt; &amp;gt; 发件人:&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; \"user-zh\"&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &lt;&#010;&gt; &amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 琴师你好，&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#010;&gt; &amp;gt; create table source1(&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;id&#010;&gt; int,&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; time1 timestamp,&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;type&#010;&gt; string,&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; WATERMARK FOR time1 as time1 -&#010;&gt; &amp;gt; INTERVAL '2' SECOND&#010;&gt; &amp;gt; ) with (...)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月10日周五&#010;上午8:43写道：&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#010;&gt; &amp;gt; &amp;amp;gt; 发件人:&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; \"奇怪的不朽琴师\"&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; &lt;&#010;&gt; &amp;gt; &amp;amp;gt; 1129656513@qq.com&amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#010;&gt; &amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&#010;&gt; &amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;pyflink1.11.0window&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 你好：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#010;&gt; &amp;gt; &amp;amp;gt; : org.apache.flink.table.api.ValidationException: A group&#010;&gt; window&#010;&gt; &amp;gt; expects a&#010;&gt; &amp;gt; &amp;amp;gt; time attribute for grouping in a stream environment.&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#010;&gt; &amp;gt; &amp;amp;gt; 代码如下&#010;&gt; &amp;gt; &amp;amp;gt; 谢谢&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; def from_kafka_to_kafka_demo():&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env =&#010;&gt; &amp;gt; &amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env.set_parallelism(1)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; # use blink table planner&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env =&#010;&gt; StreamTableEnvironment.create(s_env)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; # register source and sink&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_source(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_sink(st_env)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; .group_by(\"w\") \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; .select(\" id,&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; .insert_into(\"sink1\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; def register_rides_source(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; source_ddl = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; create table source1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;type string&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;) with&#010;(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.topic' = 'tp1',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; def register_rides_sink(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; sink_ddl = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; create table sink1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time2 timestamp&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;) with&#010;(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.topic' = 'tp3',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.sql_update(sink_ddl)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; if __name__ == '__main__':&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&#010;",
        "depth": "5",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_9E76FCDA14930E6720E0F3227C6389096A08@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 08:40:09 GMT",
        "subject": "回复： pyflink1.11.0window",
        "content": "&amp;nbsp;Shuiqiang，你好：&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#013;&#010;&#013;&#010;&#013;&#010;恳请所有看到此封邮件的大佬！&#013;&#010;&#013;&#010;&#013;&#010;谢谢！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月15日(星期三) 中午11:25&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;举个sql例子&#013;&#010;select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#013;&#010;pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#013;&#010;rowtime&#013;&#010;from payment_msg group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;这个query 对每5s的tumble窗口做统计。&#013;&#010;&#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 上午11:10写道：&#013;&#010;&#013;&#010;&amp;gt; Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 琴师你好，&#013;&#010;&amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A tumble window&#013;&#010;&amp;gt; expects a size value literal.&#013;&#010;&amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Shuiqiang&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三&#010;上午10:27写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;&amp;gt; &amp;amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File \"tou.py\", line 21, in from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; .select(\" id,&amp;amp;amp;nbsp;&#010;time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#013;&#010;&amp;gt; 907,&#013;&#010;&amp;gt; &amp;amp;gt; in select&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; return Table(self._j_table.select(fields),&#013;&#010;&amp;gt; self._t_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; &amp;amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; answer, self.gateway_client, self.target_id,&#013;&#010;&amp;gt; self.name)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#013;&#010;&amp;gt; line&#013;&#010;&amp;gt; &amp;amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; &amp;amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; format(target_id, \".\", name),&#010;value)&#013;&#010;&amp;gt; &amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling&#013;&#010;&amp;gt; o26.select.&#013;&#010;&amp;gt; &amp;amp;gt; : org.apache.flink.table.api.ValidationException: A tumble window&#013;&#010;&amp;gt; expects&#013;&#010;&amp;gt; &amp;amp;gt; a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;at&#013;&#010;&amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;WATERMARK FOR&#010;time1 as time1 -&#013;&#010;&amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.properties.bootstrap.servers'&#010;=&#013;&#010;&amp;gt; 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.properties.zookeeper.connect'&#010;=&#013;&#010;&amp;gt; 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'format.derive-schema' = 'true',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env = StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;.group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;.select(\" id,&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;.insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 代码如上&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#013;&#010;&amp;gt; &amp;amp;gt; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;id&#013;&#010;&amp;gt; int,&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;type&#013;&#010;&amp;gt; string,&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; WATERMARK FOR time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; ) with (...)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月10日周五&#010;上午8:43写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; \"奇怪的不朽琴师\"&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 1129656513@qq.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月9日(星期四)&#010;下午5:08&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; : org.apache.flink.table.api.ValidationException: A&#010;group&#013;&#010;&amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; expects a&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time attribute for grouping in a stream environment.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 代码如下&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env&#010;=&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; # use&#010;blink table planner&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env&#010;=&#013;&#010;&amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; # register&#010;source and sink&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\"))&#010;\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .select(\" id,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; source_ddl&#010;= \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; create&#010;table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;time1&#010;timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;type&#010;string&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#010;with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.type'&#010;= 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'update-mode'&#010;= 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.topic'&#010;= 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; sink_ddl&#010;= \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; create&#010;table sink1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;time1&#010;timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;time2&#010;timestamp&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#010;with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.type'&#010;= 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'update-mode'&#010;= 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.topic'&#010;= 'tp3',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;",
        "depth": "6",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<CABvJ6uWioUKXOk0j7qogsTk1JwgvTPfwccn=Sq-ozXeQ=v2K3g@mail.gmail.com>",
        "from": "Shuiqiang Chen &lt;acqua....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 09:23:09 GMT",
        "subject": "Re: pyflink1.11.0window",
        "content": "下面这个例子从kafka读取json格式的数据， 然后做窗口聚合后写入es，&#010;可以参考下代码结构， 修改相应数据字段。 这份代码你本地应该是不能运行的&#010;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings&#010;from pyflink.table.udf import udf&#010;&#010;&#010;@udf(input_types=[DataTypes.INT()], result_type=DataTypes.STRING())&#010;def platform_code_to_name(code):&#010;    return \"mobile\" if code == 0 else \"pc\"&#010;&#010;&#010;def log_processing():&#010;    env = StreamExecutionEnvironment.get_execution_environment()&#010;    env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;    env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#010;    t_env = StreamTableEnvironment.create(stream_execution_environment=env,&#010;environment_settings=env_settings)&#010;&#010;    source_ddl = \"\"\"&#010;            CREATE TABLE payment_msg(&#010;                createTime VARCHAR,&#010;                rt as TO_TIMESTAMP(createTime),&#010;                orderId BIGINT,&#010;                payAmount DOUBLE,&#010;                payPlatform INT,&#010;                paySource INT,&#010;                WATERMARK FOR rt as rt - INTERVAL '2' SECOND&#010;            ) WITH (&#010;              'connector.type' = 'kafka',&#010;              'connector.version' = 'universal',&#010;              'connector.topic' = 'payment_msg_2',&#010;              'connector.properties.bootstrap.servers' = '0.0.0.0:9092',&#010;              'connector.properties.group.id' = 'test_3',&#010;              'connector.startup-mode' = 'latest-offset',&#010;              'format.type' = 'json'&#010;            )&#010;            \"\"\"&#010;    t_env.sql_update(source_ddl)&#010;&#010;    es_sink_ddl = \"\"\"&#010;            CREATE TABLE es_sink (&#010;            platform VARCHAR,&#010;            pay_amount DOUBLE,&#010;            rowtime TIMESTAMP(3)&#010;            ) with (&#010;                'connector.type' = 'elasticsearch',&#010;                'connector.version' = '7',&#010;                'connector.hosts' = 'http://localhost:9200',&#010;                'connector.index' = 'platform_pay_amount_1',&#010;                'connector.document-type' = 'payment',&#010;                'update-mode' = 'upsert',&#010;                'connector.flush-on-checkpoint' = 'true',&#010;                'connector.key-delimiter' = '$',&#010;                'connector.key-null-literal' = 'n/a',&#010;                'connector.bulk-flush.max-size' = '42mb',&#010;                'connector.bulk-flush.max-actions' = '32',&#010;                'connector.bulk-flush.interval' = '1000',&#010;                'connector.bulk-flush.backoff.delay' = '1000',&#010;                'format.type' = 'json'&#010;            )&#010;    \"\"\"&#010;&#010;    t_env.sql_update(es_sink_ddl)&#010;&#010;    t_env.register_function('platformcodetoname', platform_code_to_name)&#010;&#010;    query = \"\"\"&#010;    select platformcodetoname(payPlatform) as platform, sum(payAmount)&#010;as pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT)&#010;as rowtime&#010;    from payment_msg&#010;    group by tumble(rt, interval '5' seconds), payPlatform&#010;    \"\"\"&#010;&#010;    count_result = t_env.sql_query(query)&#010;&#010;    t_env.create_temporary_view('windowed_values', count_result)&#010;&#010;    query2 = \"\"\"&#010;    select platform, last_value(pay_amount), rowtime from&#010;windowed_values group by platform, rowtime&#010;    \"\"\"&#010;&#010;    final_result = t_env.sql_query(query2)&#010;&#010;    final_result.execute_insert(table_path='es_sink')&#010;&#010;&#010;if __name__ == '__main__':&#010;    log_processing()&#010;&#010;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&gt; 于2020年7月15日周三 下午4:40写道：&#010;&#010;&gt; &amp;nbsp;Shuiqiang，你好：&#010;&gt; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#010;&gt;&#010;&gt;&#010;&gt; 恳请所有看到此封邮件的大佬！&#010;&gt;&#010;&gt;&#010;&gt; 谢谢！&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; acqua.csq@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月15日(星期三) 中午11:25&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: pyflink1.11.0window&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 举个sql例子&#010;&gt; select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#010;&gt; pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#010;&gt; rowtime&#010;&gt; from payment_msg group by tumble(rt, interval '5' seconds), payPlatform&#010;&gt; 这个query 对每5s的tumble窗口做统计。&#010;&gt;&#010;&gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 上午11:10写道：&#010;&gt;&#010;&gt; &amp;gt; Shuiqiang，你好：&#010;&gt; &amp;gt; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#010;&gt; &amp;gt; 发件人:&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; \"user-zh\"&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &lt;&#010;&gt; &amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 琴师你好，&#010;&gt; &amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A tumble window&#010;&gt; &amp;gt; expects a size value literal.&#010;&gt; &amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; Best,&#010;&gt; &amp;gt; Shuiqiang&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三&#010;上午10:27写道：&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 你好：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#010;&gt; &amp;gt; &amp;amp;gt; Traceback (most recent call last):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File \"tou.py\", line 21, in&#010;&gt; from_kafka_to_kafka_demo&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; .select(\" id,&amp;amp;amp;nbsp;&#010;&gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#010;&gt; &amp;gt; 907,&#010;&gt; &amp;gt; &amp;amp;gt; in select&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; return&#010;&gt; Table(self._j_table.select(fields),&#010;&gt; &amp;gt; self._t_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#010;&gt; &amp;gt; &amp;amp;gt; line 1286, in __call__&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; answer, self.gateway_client,&#010;&gt; self.target_id,&#010;&gt; &amp;gt; self.name)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#010;&gt; &amp;gt; line&#010;&gt; &amp;gt; &amp;amp;gt; 147, in deco&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; return f(*a, **kw)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#010;&gt; &amp;gt; &amp;amp;gt; line 328, in get_return_value&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; format(target_id, \".\", name),&#010;&gt; value)&#010;&gt; &amp;gt; &amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling&#010;&gt; &amp;gt; o26.select.&#010;&gt; &amp;gt; &amp;amp;gt; : org.apache.flink.table.api.ValidationException: A tumble&#010;&gt; window&#010;&gt; &amp;gt; expects&#010;&gt; &amp;gt; &amp;amp;gt; a size value literal.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; def register_rides_source(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; source_ddl = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"\"\"&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; create table source1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;type string,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;WATERMARK&#010;FOR&#010;&gt; time1 as time1 -&#010;&gt; &amp;gt; INTERVAL '2' SECOND&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;) with&#010;(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.topic' = 'tp1',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; 'localhost:9092',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; 'connector.properties.zookeeper.connect' =&#010;&gt; &amp;gt; 'localhost:2181',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'format.type' = 'json',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'format.derive-schema' =&#010;&gt; 'true',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 'connector.version' =&#010;&gt; 'universal'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"\"\"&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env =&#010;&gt; &amp;gt; &amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; s_env.set_parallelism(1)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env =&#010;&gt; StreamTableEnvironment.create(s_env)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_source(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; register_rides_sink(st_env)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; .group_by(\"w\") \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; .select(\" id,&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; .insert_into(\"sink1\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 代码如上&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#010;&gt; &amp;gt; &amp;amp;gt; 发件人:&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; \"user-zh\"&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; &lt;&#010;&gt; &amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#010;&gt; &amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#010;&gt; &amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 琴师你好，&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#010;&gt; &amp;gt; &amp;amp;gt; create table source1(&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; id&#010;&gt; &amp;gt; int,&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; type&#010;&gt; &amp;gt; string,&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; WATERMARK FOR time1 as time1 -&#010;&gt; &amp;gt; &amp;amp;gt; INTERVAL '2' SECOND&#010;&gt; &amp;gt; &amp;amp;gt; ) with (...)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月10日周五&#010;&gt; 上午8:43写道：&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; \"奇怪的不朽琴师\"&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &lt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 1129656513@qq.com&amp;amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月9日(星期四)&#010;下午5:08&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"godfrey he\"&lt;&#010;&gt; godfreyhe@gmail.com&#010;&gt; &amp;gt; &amp;amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;pyflink1.11.0window&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你好：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; :&#010;&gt; org.apache.flink.table.api.ValidationException: A group&#010;&gt; &amp;gt; window&#010;&gt; &amp;gt; &amp;amp;gt; expects a&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time attribute for grouping in a stream&#010;&gt; environment.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 代码如下&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 谢谢&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;s_env =&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; s_env.set_parallelism(1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;# use&#010;&gt; blink table planner&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;st_env =&#010;&gt; &amp;gt; StreamTableEnvironment.create(s_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;#&#010;&gt; register source and sink&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; register_rides_source(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; register_rides_sink(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; st_env.from_path(\"source1\")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; .group_by(\"w\") \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; .select(\" id,&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; .insert_into(\"sink1\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_source(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; source_ddl = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;'''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;create&#010;&gt; table source1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;type string&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'connector.topic' = 'tp1',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; &amp;amp;gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;'''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; st_env.sql_update(source_ddl)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_sink(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;sink_ddl&#010;&gt; = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;'''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;create&#010;&gt; table sink1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;time2 timestamp&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'connector.topic' = 'tp3',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; &amp;amp;gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;'''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; st_env.sql_update(sink_ddl)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; if __name__ == '__main__':&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; from_kafka_to_kafka_demo()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&#010;",
        "depth": "7",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_A5B370702511A531FD845E3B496D87C9F207@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 09:30:31 GMT",
        "subject": "回复： pyflink1.11.0window",
        "content": "&amp;nbsp; &amp;nbsp; &amp;nbsp;非常感谢！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月15日(星期三) 下午5:23&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;下面这个例子从kafka读取json格式的数据， 然后做窗口聚合后写入es，&#010;可以参考下代码结构， 修改相应数据字段。 这份代码你本地应该是不能运行的&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings&#013;&#010;from pyflink.table.udf import udf&#013;&#010;&#013;&#010;&#013;&#010;@udf(input_types=[DataTypes.INT()], result_type=DataTypes.STRING())&#013;&#010;def platform_code_to_name(code):&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; return \"mobile\" if code == 0 else \"pc\"&#013;&#010;&#013;&#010;&#013;&#010;def log_processing():&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; t_env = StreamTableEnvironment.create(stream_execution_environment=env,&#013;&#010;environment_settings=env_settings)&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; source_ddl = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;CREATE TABLE payment_msg(&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;createTime VARCHAR,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;rt as TO_TIMESTAMP(createTime),&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;orderId BIGINT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;payAmount DOUBLE,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;payPlatform INT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;paySource INT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;WATERMARK FOR rt as rt - INTERVAL '2' SECOND&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;) WITH (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.type' = 'kafka',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.version' = 'universal',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.topic' = 'payment_msg_2',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.properties.bootstrap.servers' = '0.0.0.0:9092',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.properties.group.id' = 'test_3',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'format.type' = 'json'&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; t_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; es_sink_ddl = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;CREATE TABLE es_sink (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;platform VARCHAR,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;pay_amount DOUBLE,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;rowtime TIMESTAMP(3)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;) with (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.type' = 'elasticsearch',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.version' = '7',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.hosts' = 'http://localhost:9200',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.index' = 'platform_pay_amount_1',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.document-type' = 'payment',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'update-mode' = 'upsert',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.flush-on-checkpoint' = 'true',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.key-delimiter' = '$',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.key-null-literal' = 'n/a',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.bulk-flush.max-size' = '42mb',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.bulk-flush.max-actions' = '32',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.bulk-flush.interval' = '1000',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'connector.bulk-flush.backoff.delay' = '1000',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;'format.type' = 'json'&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; t_env.sql_update(es_sink_ddl)&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; t_env.register_function('platformcodetoname', platform_code_to_name)&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; query = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; select platformcodetoname(payPlatform) as platform, sum(payAmount)&#013;&#010;as pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT)&#013;&#010;as rowtime&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; from payment_msg&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; count_result = t_env.sql_query(query)&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; t_env.create_temporary_view('windowed_values', count_result)&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; select platform, last_value(pay_amount), rowtime from&#013;&#010;windowed_values group by platform, rowtime&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; final_result = t_env.sql_query(query2)&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; final_result.execute_insert(table_path='es_sink')&#013;&#010;&#013;&#010;&#013;&#010;if __name__ == '__main__':&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; log_processing()&#013;&#010;&#013;&#010;&#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 下午4:40写道：&#013;&#010;&#013;&#010;&amp;gt; &amp;amp;nbsp;Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 恳请所有看到此封邮件的大佬！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 谢谢！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 中午11:25&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 举个sql例子&#013;&#010;&amp;gt; select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#013;&#010;&amp;gt; pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#013;&#010;&amp;gt; rowtime&#013;&#010;&amp;gt; from payment_msg group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;gt; 这个query 对每5s的tumble窗口做统计。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三&#010;上午11:10写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A tumble&#010;window&#013;&#010;&amp;gt; &amp;amp;gt; expects a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; Shuiqiang&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月15日周三&#010;上午10:27写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 21, in&#013;&#010;&amp;gt; from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; .select(\"&#010;id,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#013;&#010;&amp;gt; &amp;amp;gt; 907,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; in select&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return&#013;&#010;&amp;gt; Table(self._j_table.select(fields),&#013;&#010;&amp;gt; &amp;amp;gt; self._t_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; answer,&#010;self.gateway_client,&#013;&#010;&amp;gt; self.target_id,&#013;&#010;&amp;gt; &amp;amp;gt; self.name)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#013;&#010;&amp;gt; &amp;amp;gt; line&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return&#010;f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; format(target_id,&#010;\".\", name),&#013;&#010;&amp;gt; value)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred while&#010;calling&#013;&#010;&amp;gt; &amp;amp;gt; o26.select.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; : org.apache.flink.table.api.ValidationException: A&#010;tumble&#013;&#010;&amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; expects&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;Method)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; source_ddl&#010;= \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; create&#010;table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;id&#010;int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;time1&#010;timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;type&#010;string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;WATERMARK&#010;FOR&#013;&#010;&amp;gt; time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#010;with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.type'&#010;= 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'update-mode'&#010;= 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.topic'&#010;= 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.zookeeper.connect' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'format.type'&#010;= 'json',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'format.derive-schema'&#010;=&#013;&#010;&amp;gt; 'true',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.version'&#010;=&#013;&#010;&amp;gt; 'universal'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env&#010;=&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env&#010;=&#013;&#010;&amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\"))&#010;\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .select(\" id,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 代码如上&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月10日(星期五)&#010;上午9:17&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; id&#013;&#010;&amp;gt; &amp;amp;gt; int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; type&#013;&#010;&amp;gt; &amp;amp;gt; string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; WATERMARK FOR time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ) with (...)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;gt;&#010;于2020年7月10日周五&#013;&#010;&amp;gt; 上午8:43写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"奇怪的不朽琴师\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 1129656513@qq.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月9日(星期四)&#010;下午5:08&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"godfrey&#010;he\"&lt;&#013;&#010;&amp;gt; godfreyhe@gmail.com&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; :&#013;&#010;&amp;gt; org.apache.flink.table.api.ValidationException: A group&#013;&#010;&amp;gt; &amp;amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects a&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; time attribute for grouping in&#010;a stream&#013;&#010;&amp;gt; environment.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 代码如下&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;# use&#013;&#010;&amp;gt; blink table planner&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;st_env =&#013;&#010;&amp;gt; &amp;amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;#&#013;&#010;&amp;gt; register source and sink&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;'''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;create&#013;&#010;&amp;gt; table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;type string&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;'''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;sink_ddl&#013;&#010;&amp;gt; = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;'''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;create&#013;&#010;&amp;gt; table sink1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time2 timestamp&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp3',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;'''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;",
        "depth": "8",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_148C061C50C46B8C3C61C12E535B6A32F005@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 08:23:08 GMT",
        "subject": "回复： pyflink1.11.0window",
        "content": "HI ：&#013;&#010;&amp;nbsp; &amp;nbsp; 我现在有一个新的问题，我在此基础上加了一个关联，再写入kafka时报错，如下&#013;&#010;Traceback (most recent call last):&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line 147, in deco&#013;&#010;&amp;nbsp; &amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value&#013;&#010;&amp;nbsp; &amp;nbsp; format(target_id, \".\", name), value)&#013;&#010;py4j.protocol.Py4JJavaError: An error occurred while calling o5.sqlUpdate.&#013;&#010;: org.apache.flink.table.api.TableException: AppendStreamTableSink requires that Table has only insert changes.&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:123)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:48)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:48)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:59)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:685)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:495)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;During handling of the above exception, another exception occurred:&#013;&#010;&#013;&#010;&#013;&#010;Traceback (most recent call last):&#013;&#010;&amp;nbsp; File \"tou.py\", line 99, in &lt;module&amp;gt;&#013;&#010;&amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;nbsp; File \"tou.py\", line 33, in from_kafka_to_kafka_demo&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(\"insert into flink_result select id,type,rowtime from final_result2\")&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/table/table_environment.py\", line 547, in sql_update&#013;&#010;&amp;nbsp; &amp;nbsp; self._j_tenv.sqlUpdate(stmt)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1286, in __call__&#013;&#010;&amp;nbsp; &amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line 154, in deco&#013;&#010;&amp;nbsp; &amp;nbsp; raise exception_mapping[exception](s.split(': ', 1)[1], stack_trace)&#013;&#010;pyflink.util.exceptions.TableException: 'AppendStreamTableSink requires that Table has only insert changes.'&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;这种应该如何实现，需求大概是一个流表（需要分组汇总）关联一个维表。&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings,DataTypes, CsvTableSource, CsvTableSink&#013;&#010;from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;from pyflink.table.window import Tumble&amp;nbsp;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;def from_kafka_to_kafka_demo():&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; # use blink table planner&#013;&#010;&amp;nbsp; &amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;nbsp; &amp;nbsp; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;nbsp; &amp;nbsp; env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;nbsp; &amp;nbsp; st_env = StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; # register source and sink&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;nbsp; &amp;nbsp; register_mysql_source(st_env)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; query = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select&amp;nbsp; cast(sum(t1.id) as int) as id, max(t1.type) as type,cast(tumble_start(t1.time1, interval '4' second) as bigint) as rowtime&#013;&#010;&amp;nbsp; &amp;nbsp; from source1 t1&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; group by tumble(t1.time1, interval '4' second)&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; count_result = st_env.sql_query(query)&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.create_temporary_view('final_result', count_result)&#013;&#010;&amp;nbsp; &amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select&amp;nbsp; t1.id,t2.type,t1.rowtime from final_result t1 left join dim_mysql t2 on t1.type=t2.id&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; count_result2 = st_env.sql_query(query2)&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.create_temporary_view('final_result2', count_result2)&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(\"insert into flink_result select id,type,rowtime from final_result2\")&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;def register_rides_source(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; create table source1(&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;id int,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time2 varchar ,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time1 as TO_TIMESTAMP(time2,'yyyyMMddHHmmss'),&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;type string,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;)&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;&#013;&#010;def register_mysql_source(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; CREATE TABLE dim_mysql (&#013;&#010;&amp;nbsp; &amp;nbsp; id varchar,&amp;nbsp; --&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; type varchar --&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'jdbc',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.table' = 'flink_test',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.driver' = 'com.mysql.jdbc.Driver',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.username' = '****',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.password' = '*****',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.lookup.cache.max-rows' = '5000',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.lookup.cache.ttl' = '10min'&#013;&#010;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;&#013;&#010;def register_rides_sink(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; sink_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; CREATE TABLE flink_result (&#013;&#010;&amp;nbsp; &amp;nbsp; id int,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; type varchar,&#013;&#010;&amp;nbsp; &amp;nbsp; rtime bigint,&#013;&#010;&amp;nbsp; &amp;nbsp; primary key(id)&#013;&#010;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;nbsp; &amp;nbsp; with (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp4',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;)&#013;&#010;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;if __name__ == '__main__':&#013;&#010;&amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                                                                        \"我自己的邮箱\"                                                                                    &lt;1129656513@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月15日(星期三) 下午5:30&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复： pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;非常感谢！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人:                                                                                                                        \"user-zh\"                                                                                    &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月15日(星期三) 下午5:23&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;下面这个例子从kafka读取json格式的数据， 然后做窗口聚合后写入es， 可以参考下代码结构， 修改相应数据字段。 这份代码你本地应该是不能运行的&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings&#013;&#010;from pyflink.table.udf import udf&#013;&#010;&#013;&#010;&#013;&#010;@udf(input_types=[DataTypes.INT()], result_type=DataTypes.STRING())&#013;&#010;def platform_code_to_name(code):&#013;&#010;&amp;nbsp; &amp;nbsp; return \"mobile\" if code == 0 else \"pc\"&#013;&#010;&#013;&#010;&#013;&#010;def log_processing():&#013;&#010;&amp;nbsp; &amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;nbsp; &amp;nbsp; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;nbsp; &amp;nbsp; env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;nbsp; &amp;nbsp; t_env = StreamTableEnvironment.create(stream_execution_environment=env,&#013;&#010;environment_settings=env_settings)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE payment_msg(&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; createTime VARCHAR,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; rt as TO_TIMESTAMP(createTime),&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; orderId BIGINT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; payAmount DOUBLE,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; payPlatform INT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; paySource INT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; WATERMARK FOR rt as rt - INTERVAL '2' SECOND&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.version' = 'universal',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'payment_msg_2',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = '0.0.0.0:9092',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.properties.group.id' = 'test_3',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'format.type' = 'json'&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; t_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; es_sink_ddl = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE es_sink (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; platform VARCHAR,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; pay_amount DOUBLE,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; rowtime TIMESTAMP(3)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) with (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.type' = 'elasticsearch',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.version' = '7',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.hosts' = 'http://localhost:9200',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.index' = 'platform_pay_amount_1',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.document-type' = 'payment',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'update-mode' = 'upsert',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.flush-on-checkpoint' = 'true',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.key-delimiter' = '$',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.key-null-literal' = 'n/a',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.max-size' = '42mb',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.max-actions' = '32',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.interval' = '1000',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.backoff.delay' = '1000',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'format.type' = 'json'&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; t_env.sql_update(es_sink_ddl)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; t_env.register_function('platformcodetoname', platform_code_to_name)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; query = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select platformcodetoname(payPlatform) as platform, sum(payAmount)&#013;&#010;as pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT)&#013;&#010;as rowtime&#013;&#010;&amp;nbsp; &amp;nbsp; from payment_msg&#013;&#010;&amp;nbsp; &amp;nbsp; group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; count_result = t_env.sql_query(query)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; t_env.create_temporary_view('windowed_values', count_result)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select platform, last_value(pay_amount), rowtime from&#013;&#010;windowed_values group by platform, rowtime&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; final_result = t_env.sql_query(query2)&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; final_result.execute_insert(table_path='es_sink')&#013;&#010;&#013;&#010;&#013;&#010;if __name__ == '__main__':&#013;&#010;&amp;nbsp; &amp;nbsp; log_processing()&#013;&#010;&#013;&#010;&#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 下午4:40写道：&#013;&#010;&#013;&#010;&amp;gt; &amp;amp;nbsp;Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 恳请所有看到此封邮件的大佬！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 谢谢！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; &lt;&#013;&#010;&amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 中午11:25&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 举个sql例子&#013;&#010;&amp;gt; select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#013;&#010;&amp;gt; pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#013;&#010;&amp;gt; rowtime&#013;&#010;&amp;gt; from payment_msg group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;gt; 这个query 对每5s的tumble窗口做统计。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三 上午11:10写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A tumble window&#013;&#010;&amp;gt; &amp;amp;gt; expects a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; Shuiqiang&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月15日周三 上午10:27写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 21, in&#013;&#010;&amp;gt; from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; .select(\" id,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#013;&#010;&amp;gt; &amp;amp;gt; 907,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; in select&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return&#013;&#010;&amp;gt; Table(self._j_table.select(fields),&#013;&#010;&amp;gt; &amp;amp;gt; self._t_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; answer, self.gateway_client,&#013;&#010;&amp;gt; self.target_id,&#013;&#010;&amp;gt; &amp;amp;gt; self.name)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#013;&#010;&amp;gt; &amp;amp;gt; line&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; format(target_id, \".\", name),&#013;&#010;&amp;gt; value)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling&#013;&#010;&amp;gt; &amp;amp;gt; o26.select.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; : org.apache.flink.table.api.ValidationException: A tumble&#013;&#010;&amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; expects&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;WATERMARK FOR&#013;&#010;&amp;gt; time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.zookeeper.connect' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'format.derive-schema' =&#013;&#010;&amp;gt; 'true',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.version' =&#013;&#010;&amp;gt; 'universal'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .select(\" id,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 代码如上&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; id&#013;&#010;&amp;gt; &amp;amp;gt; int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; type&#013;&#010;&amp;gt; &amp;amp;gt; string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; WATERMARK FOR time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ) with (...)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;gt; 于2020年7月10日周五&#013;&#010;&amp;gt; 上午8:43写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"奇怪的不朽琴师\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 1129656513@qq.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"godfrey he\"&lt;&#013;&#010;&amp;gt; godfreyhe@gmail.com&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; :&#013;&#010;&amp;gt; org.apache.flink.table.api.ValidationException: A group&#013;&#010;&amp;gt; &amp;amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects a&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; time attribute for grouping in a stream&#013;&#010;&amp;gt; environment.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 代码如下&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; # use&#013;&#010;&amp;gt; blink table planner&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; &amp;amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; #&#013;&#010;&amp;gt; register source and sink&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;type string&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; sink_ddl&#013;&#010;&amp;gt; = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; table sink1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time2 timestamp&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp3',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;",
        "depth": "9",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<CABvJ6uUkRbNP_W1dPC1akQkOhUobc7v48gGR91u0SOgthZS-3A@mail.gmail.com>",
        "from": "Shuiqiang Chen &lt;acqua....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 12:42:57 GMT",
        "subject": "Re: pyflink1.11.0window",
        "content": "看看异常信息， 是不是你的insert mode没配置对。&#010;BTW, 你粘贴的文本带有很多\"&amp;nbsp;\"， 有点影响可读性。&#010;&#010;Best，&#010;Shuiqiang&#010;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&gt; 于2020年7月20日周一 下午4:23写道：&#010;&#010;&gt; HI ：&#010;&gt; &amp;nbsp; &amp;nbsp; 我现在有一个新的问题，我在此基础上加了一个关联，再写入kafka时报错，如下&#010;&gt; Traceback (most recent call last):&#010;&gt; &amp;nbsp; File&#010;&gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#010;&gt; 147, in deco&#010;&gt; &amp;nbsp; &amp;nbsp; return f(*a, **kw)&#010;&gt; &amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#010;&gt; line 328, in get_return_value&#010;&gt; &amp;nbsp; &amp;nbsp; format(target_id, \".\", name), value)&#010;&gt; py4j.protocol.Py4JJavaError: An error occurred while calling o5.sqlUpdate.&#010;&gt; : org.apache.flink.table.api.TableException: AppendStreamTableSink&#010;&gt; requires that Table has only insert changes.&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:123)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:48)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:48)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:59)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:685)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:495)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; During handling of the above exception, another exception occurred:&#010;&gt;&#010;&gt;&#010;&gt; Traceback (most recent call last):&#010;&gt; &amp;nbsp; File \"tou.py\", line 99, in &lt;module&amp;gt;&#010;&gt; &amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt; &amp;nbsp; File \"tou.py\", line 33, in from_kafka_to_kafka_demo&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(\"insert into flink_result select&#010;&gt; id,type,rowtime from final_result2\")&#010;&gt; &amp;nbsp; File&#010;&gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table_environment.py\",&#010;&gt; line 547, in sql_update&#010;&gt; &amp;nbsp; &amp;nbsp; self._j_tenv.sqlUpdate(stmt)&#010;&gt; &amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#010;&gt; line 1286, in __call__&#010;&gt; &amp;nbsp; &amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#010;&gt; &amp;nbsp; File&#010;&gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#010;&gt; 154, in deco&#010;&gt; &amp;nbsp; &amp;nbsp; raise exception_mapping[exception](s.split(': ', 1)[1],&#010;&gt; stack_trace)&#010;&gt; pyflink.util.exceptions.TableException: 'AppendStreamTableSink requires&#010;&gt; that Table has only insert changes.'&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 这种应该如何实现，需求大概是一个流表（需要分组汇总）关联一个维表。&#010;&gt;&#010;&gt;&#010;&gt; from pyflink.datastream import StreamExecutionEnvironment,&#010;&gt; TimeCharacteristic&#010;&gt; from pyflink.table import StreamTableEnvironment, DataTypes,&#010;&gt; EnvironmentSettings,DataTypes, CsvTableSource, CsvTableSink&#010;&gt; from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#010;&gt; from pyflink.table.window import Tumble&amp;nbsp;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; def from_kafka_to_kafka_demo():&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; # use blink table planner&#010;&gt; &amp;nbsp; &amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;nbsp; &amp;nbsp;&#010;&gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;&gt; &amp;nbsp; &amp;nbsp; env_settings =&#010;&gt; EnvironmentSettings.Builder().use_blink_planner().build()&#010;&gt; &amp;nbsp; &amp;nbsp; st_env =&#010;&gt; StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; # register source and sink&#010;&gt; &amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#010;&gt; &amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#010;&gt; &amp;nbsp; &amp;nbsp; register_mysql_source(st_env)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; query = \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; select&amp;nbsp; cast(sum(t1.id) as int) as id, max(t1.type) as&#010;&gt; type,cast(tumble_start(t1.time1, interval '4' second) as bigint) as rowtime&#010;&gt; &amp;nbsp; &amp;nbsp; from source1 t1&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; group by tumble(t1.time1, interval '4' second)&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; count_result = st_env.sql_query(query)&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.create_temporary_view('final_result', count_result)&#010;&gt; &amp;nbsp; &amp;nbsp; query2 = \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; select&amp;nbsp; t1.id,t2.type,t1.rowtime from final_result t1&#010;&gt; left join dim_mysql t2 on t1.type=t2.id&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; count_result2 = st_env.sql_query(query2)&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.create_temporary_view('final_result2', count_result2)&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(\"insert into flink_result select&#010;&gt; id,type,rowtime from final_result2\")&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;nbsp; &amp;nbsp;&amp;nbsp;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; def register_rides_source(st_env):&#010;&gt; &amp;nbsp; &amp;nbsp; source_ddl = \\&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; create table source1(&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;id int,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;time2 varchar ,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;time1 as TO_TIMESTAMP(time2,'yyyyMMddHHmmss'),&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;type string,&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp1',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#010;&gt; &amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;)&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt;&#010;&gt;&#010;&gt; def register_mysql_source(st_env):&#010;&gt; &amp;nbsp; &amp;nbsp; source_ddl = \\&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; CREATE TABLE dim_mysql (&#010;&gt; &amp;nbsp; &amp;nbsp; id varchar,&amp;nbsp; --&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; type varchar --&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; ) WITH (&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.type' = 'jdbc',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.url' = 'jdbc:mysql://localhost:3390/test',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.table' = 'flink_test',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.driver' = 'com.mysql.jdbc.Driver',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.username' = '****',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.password' = '*****',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.lookup.cache.max-rows' = '5000',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.lookup.cache.ttl' = '10min'&#010;&gt; &amp;nbsp; &amp;nbsp; )&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&amp;nbsp; &amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#010;&gt;&#010;&gt;&#010;&gt; def register_rides_sink(st_env):&#010;&gt; &amp;nbsp; &amp;nbsp; sink_ddl = \\&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; CREATE TABLE flink_result (&#010;&gt; &amp;nbsp; &amp;nbsp; id int,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; type varchar,&#010;&gt; &amp;nbsp; &amp;nbsp; rtime bigint,&#010;&gt; &amp;nbsp; &amp;nbsp; primary key(id)&#010;&gt; &amp;nbsp; &amp;nbsp; ) WITH (&#010;&gt; &amp;nbsp; &amp;nbsp; with (&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp4',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#010;&gt; &amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#010;&gt; &amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;)&#010;&gt; &amp;nbsp; &amp;nbsp; )&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; st_env.sql_update(sink_ddl)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; if __name__ == '__main__':&#010;&gt; &amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"我自己的邮箱\"&#010;&gt;                                                                   &lt;&#010;&gt; 1129656513@qq.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月15日(星期三) 下午5:30&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;回复： pyflink1.11.0window&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;非常感谢！&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------ 原始邮件 ------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; acqua.csq@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月15日(星期三) 下午5:23&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: pyflink1.11.0window&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 下面这个例子从kafka读取json格式的数据， 然后做窗口聚合后写入es， 可以参考下代码结构， 修改相应数据字段。 这份代码你本地应该是不能运行的&#010;&gt;&#010;&gt; from pyflink.datastream import StreamExecutionEnvironment,&#010;&gt; TimeCharacteristic&#010;&gt; from pyflink.table import StreamTableEnvironment, DataTypes,&#010;&gt; EnvironmentSettings&#010;&gt; from pyflink.table.udf import udf&#010;&gt;&#010;&gt;&#010;&gt; @udf(input_types=[DataTypes.INT()], result_type=DataTypes.STRING())&#010;&gt; def platform_code_to_name(code):&#010;&gt; &amp;nbsp; &amp;nbsp; return \"mobile\" if code == 0 else \"pc\"&#010;&gt;&#010;&gt;&#010;&gt; def log_processing():&#010;&gt; &amp;nbsp; &amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;nbsp; &amp;nbsp;&#010;&gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;&gt; &amp;nbsp; &amp;nbsp; env_settings =&#010;&gt; EnvironmentSettings.Builder().use_blink_planner().build()&#010;&gt; &amp;nbsp; &amp;nbsp; t_env =&#010;&gt; StreamTableEnvironment.create(stream_execution_environment=env,&#010;&gt; environment_settings=env_settings)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; source_ddl = \"\"\"&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE&#010;&gt; payment_msg(&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; createTime VARCHAR,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; rt as TO_TIMESTAMP(createTime),&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; orderId BIGINT,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; payAmount DOUBLE,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; payPlatform INT,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; paySource INT,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; WATERMARK FOR rt as rt - INTERVAL '2' SECOND&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) WITH (&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;&gt; 'connector.type' = 'kafka',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;&gt; 'connector.version' = 'universal',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;&gt; 'connector.topic' = 'payment_msg_2',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;&gt; 'connector.properties.bootstrap.servers' = '0.0.0.0:9092',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; '&#010;&gt; connector.properties.group.id' = 'test_3',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;&gt; 'connector.startup-mode' = 'latest-offset',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;&gt; 'format.type' = 'json'&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; t_env.sql_update(source_ddl)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; es_sink_ddl = \"\"\"&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE&#010;&gt; es_sink (&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; platform&#010;&gt; VARCHAR,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; pay_amount&#010;&gt; DOUBLE,&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; rowtime&#010;&gt; TIMESTAMP(3)&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) with (&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.type' = 'elasticsearch',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.version' = '7',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.hosts' = 'http://localhost:9200',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.index' = 'platform_pay_amount_1',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.document-type' = 'payment',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'update-mode' = 'upsert',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.flush-on-checkpoint' = 'true',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.key-delimiter' = '$',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.key-null-literal' = 'n/a',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.bulk-flush.max-size' = '42mb',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.bulk-flush.max-actions' = '32',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.bulk-flush.interval' = '1000',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'connector.bulk-flush.backoff.delay' = '1000',&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; 'format.type' = 'json'&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; t_env.sql_update(es_sink_ddl)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; t_env.register_function('platformcodetoname',&#010;&gt; platform_code_to_name)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; query = \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; select platformcodetoname(payPlatform) as platform,&#010;&gt; sum(payAmount)&#010;&gt; as pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT)&#010;&gt; as rowtime&#010;&gt; &amp;nbsp; &amp;nbsp; from payment_msg&#010;&gt; &amp;nbsp; &amp;nbsp; group by tumble(rt, interval '5' seconds), payPlatform&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; count_result = t_env.sql_query(query)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; t_env.create_temporary_view('windowed_values', count_result)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; query2 = \"\"\"&#010;&gt; &amp;nbsp; &amp;nbsp; select platform, last_value(pay_amount), rowtime from&#010;&gt; windowed_values group by platform, rowtime&#010;&gt; &amp;nbsp; &amp;nbsp; \"\"\"&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; final_result = t_env.sql_query(query2)&#010;&gt;&#010;&gt; &amp;nbsp; &amp;nbsp; final_result.execute_insert(table_path='es_sink')&#010;&gt;&#010;&gt;&#010;&gt; if __name__ == '__main__':&#010;&gt; &amp;nbsp; &amp;nbsp; log_processing()&#010;&gt;&#010;&gt;&#010;&gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 下午4:40写道：&#010;&gt;&#010;&gt; &amp;gt; &amp;amp;nbsp;Shuiqiang，你好：&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 恳请所有看到此封邮件的大佬！&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 谢谢！&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#010;&gt; &amp;gt; 发件人:&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; \"user-zh\"&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; &lt;&#010;&gt; &amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 中午11:25&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 举个sql例子&#010;&gt; &amp;gt; select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#010;&gt; &amp;gt; pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#010;&gt; &amp;gt; rowtime&#010;&gt; &amp;gt; from payment_msg group by tumble(rt, interval '5' seconds),&#010;&gt; payPlatform&#010;&gt; &amp;gt; 这个query 对每5s的tumble窗口做统计。&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三 上午11:10写道：&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; Shuiqiang，你好：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#010;&gt; &amp;gt; &amp;amp;gt; 发件人:&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; \"user-zh\"&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; &lt;&#010;&gt; &amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#010;&gt; &amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#010;&gt; &amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 琴师你好，&#010;&gt; &amp;gt; &amp;amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A&#010;&gt; tumble window&#010;&gt; &amp;gt; &amp;amp;gt; expects a size value literal.&#010;&gt; &amp;gt; &amp;amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; Best,&#010;&gt; &amp;gt; &amp;amp;gt; Shuiqiang&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月15日周三&#010;&gt; 上午10:27写道：&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你好：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Traceback (most recent call last):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 71, in&#010;&gt; &lt;module&amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; from_kafka_to_kafka_demo()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 21, in&#010;&gt; &amp;gt; from_kafka_to_kafka_demo&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; .select(\"&#010;&gt; id,&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#010;&gt; &amp;gt; &amp;amp;gt; 907,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; in select&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return&#010;&gt; &amp;gt; Table(self._j_table.select(fields),&#010;&gt; &amp;gt; &amp;amp;gt; self._t_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 1286, in __call__&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; answer,&#010;&gt; self.gateway_client,&#010;&gt; &amp;gt; self.target_id,&#010;&gt; &amp;gt; &amp;amp;gt; self.name)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#010;&gt; &amp;gt; &amp;amp;gt; line&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 147, in deco&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return&#010;&gt; f(*a, **kw)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#010;&gt; &amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 328, in get_return_value&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; format(target_id, \".\", name),&#010;&gt; &amp;gt; value)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred&#010;&gt; while calling&#010;&gt; &amp;gt; &amp;amp;gt; o26.select.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; :&#010;&gt; org.apache.flink.table.api.ValidationException: A tumble&#010;&gt; &amp;gt; window&#010;&gt; &amp;gt; &amp;amp;gt; expects&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; a size value literal.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_source(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; source_ddl = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; create&#010;&gt; table source1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;type string,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;WATERMARK FOR&#010;&gt; &amp;gt; time1 as time1 -&#010;&gt; &amp;gt; &amp;amp;gt; INTERVAL '2' SECOND&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'connector.topic' = 'tp1',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; &amp;amp;gt; 'localhost:9092',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.properties.zookeeper.connect' =&#010;&gt; &amp;gt; &amp;amp;gt; 'localhost:2181',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'format.type' = 'json',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'format.derive-schema' =&#010;&gt; &amp;gt; 'true',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; 'connector.version' =&#010;&gt; &amp;gt; 'universal'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; st_env.sql_update(source_ddl)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env =&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; s_env.set_parallelism(1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env =&#010;&gt; &amp;gt; StreamTableEnvironment.create(s_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; register_rides_source(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; register_rides_sink(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; st_env.from_path(\"source1\")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; .group_by(\"w\") \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; .select(\" id,&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; .insert_into(\"sink1\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 代码如上&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; \"user-zh\"&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &lt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#010;&gt; user-zh@flink.apache.org&#010;&gt; &amp;gt; &amp;amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 琴师你好，&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; create table source1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; id&#010;&gt; &amp;gt; &amp;amp;gt; int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; type&#010;&gt; &amp;gt; &amp;amp;gt; string,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; WATERMARK FOR time1 as time1 -&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INTERVAL '2' SECOND&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ) with (...)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;gt;&#010;&gt; 于2020年7月10日周五&#010;&gt; &amp;gt; 上午8:43写道：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"奇怪的不朽琴师\"&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &lt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 1129656513@qq.com&#010;&gt; &amp;amp;amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"godfrey he\"&lt;&#010;&gt; &amp;gt; godfreyhe@gmail.com&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; 主题:&amp;amp;amp;amp;amp;nbsp;pyflink1.11.0window&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; :&#010;&gt; &amp;gt; org.apache.flink.table.api.ValidationException: A group&#010;&gt; &amp;gt; &amp;amp;gt; window&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects a&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; time attribute for grouping in&#010;&gt; a stream&#010;&gt; &amp;gt; environment.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 代码如下&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 谢谢&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; s_env =&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; StreamExecutionEnvironment.get_execution_environment()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; s_env.set_parallelism(1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; # use&#010;&gt; &amp;gt; blink table planner&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; st_env =&#010;&gt; &amp;gt; &amp;amp;gt; StreamTableEnvironment.create(s_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; #&#010;&gt; &amp;gt; register source and sink&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; register_rides_source(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; register_rides_sink(st_env)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; st_env.from_path(\"source1\")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; .group_by(\"w\") \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 , time1 \")\\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; .insert_into(\"sink1\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def&#010;&gt; register_rides_source(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; source_ddl = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; create&#010;&gt; &amp;gt; table source1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;type string&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.topic' = 'tp1',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; st_env.sql_update(source_ddl)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def&#010;&gt; register_rides_sink(st_env):&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; sink_ddl&#010;&gt; &amp;gt; = \\&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; create&#010;&gt; &amp;gt; table sink1(&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; id int,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;time2 timestamp&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.type' = 'kafka',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'update-mode' = 'append',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; 'connector.topic' = 'tp3',&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp; '''&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; st_env.sql_update(sink_ddl)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; if __name__ == '__main__':&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; from_kafka_to_kafka_demo()&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;amp;amp;nbsp;&#010;&#010;",
        "depth": "10",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_A60E5C5FE46D4C33C1DDAE40BB67D2E48C05@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 02:23:55 GMT",
        "subject": "回复： pyflink1.11.0window",
        "content": "你好：&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;他这个会自动转译空格回车啥的符号，暂时没什么好办法，很难受，大佬这边有pyflink的多表关联的demo么，万分感谢！&#013;&#010;&#013;&#010;&#013;&#010;发送&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                                                                        \"user-zh\"                                                                                    &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月20日(星期一) 晚上8:42&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;看看异常信息， 是不是你的insert mode没配置对。&#013;&#010;BTW, 你粘贴的文本带有很多\"&amp;amp;nbsp;\"， 有点影响可读性。&#013;&#010;&#013;&#010;Best，&#013;&#010;Shuiqiang&#013;&#010;&#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月20日周一 下午4:23写道：&#013;&#010;&#013;&#010;&amp;gt; HI ：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 我现在有一个新的问题，我在此基础上加了一个关联，再写入kafka时报错，如下&#013;&#010;&amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#013;&#010;&amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; format(target_id, \".\", name), value)&#013;&#010;&amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling o5.sqlUpdate.&#013;&#010;&amp;gt; : org.apache.flink.table.api.TableException: AppendStreamTableSink&#013;&#010;&amp;gt; requires that Table has only insert changes.&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:123)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:48)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:48)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:59)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:685)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:495)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; During handling of the above exception, another exception occurred:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"tou.py\", line 99, in &lt;module&amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"tou.py\", line 33, in from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(\"insert into flink_result select&#013;&#010;&amp;gt; id,type,rowtime from final_result2\")&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table_environment.py\",&#013;&#010;&amp;gt; line 547, in sql_update&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; self._j_tenv.sqlUpdate(stmt)&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#013;&#010;&amp;gt; 154, in deco&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; raise exception_mapping[exception](s.split(': ', 1)[1],&#013;&#010;&amp;gt; stack_trace)&#013;&#010;&amp;gt; pyflink.util.exceptions.TableException: 'AppendStreamTableSink requires&#013;&#010;&amp;gt; that Table has only insert changes.'&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 这种应该如何实现，需求大概是一个流表（需要分组汇总）关联一个维表。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; from pyflink.datastream import StreamExecutionEnvironment,&#013;&#010;&amp;gt; TimeCharacteristic&#013;&#010;&amp;gt; from pyflink.table import StreamTableEnvironment, DataTypes,&#013;&#010;&amp;gt; EnvironmentSettings,DataTypes, CsvTableSource, CsvTableSink&#013;&#010;&amp;gt; from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;&amp;gt; from pyflink.table.window import Tumble&amp;amp;nbsp;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # use blink table planner&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env_settings =&#013;&#010;&amp;gt; EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # register source and sink&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_mysql_source(st_env)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select&amp;amp;nbsp; cast(sum(t1.id) as int) as id, max(t1.type) as&#013;&#010;&amp;gt; type,cast(tumble_start(t1.time1, interval '4' second) as bigint) as rowtime&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from source1 t1&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; group by tumble(t1.time1, interval '4' second)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; count_result = st_env.sql_query(query)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.create_temporary_view('final_result', count_result)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select&amp;amp;nbsp; t1.id,t2.type,t1.rowtime from final_result t1&#013;&#010;&amp;gt; left join dim_mysql t2 on t1.type=t2.id&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; count_result2 = st_env.sql_query(query2)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.create_temporary_view('final_result2', count_result2)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(\"insert into flink_result select&#013;&#010;&amp;gt; id,type,rowtime from final_result2\")&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time2 varchar ,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 as TO_TIMESTAMP(time2,'yyyyMMddHHmmss'),&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_mysql_source(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE dim_mysql (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; id varchar,&amp;amp;nbsp; --&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; type varchar --&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'jdbc',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.table' = 'flink_test',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.driver' = 'com.mysql.jdbc.Driver',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.username' = '****',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.password' = '*****',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.lookup.cache.max-rows' = '5000',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.lookup.cache.ttl' = '10min'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; sink_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE flink_result (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; id int,&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; type varchar,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; rtime bigint,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; primary key(id)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; with (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp4',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"我自己的邮箱\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;&#013;&#010;&amp;gt; 1129656513@qq.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 下午5:30&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;回复： pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;非常感谢！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;&#013;&#010;&amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 下午5:23&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 下面这个例子从kafka读取json格式的数据， 然后做窗口聚合后写入es， 可以参考下代码结构， 修改相应数据字段。 这份代码你本地应该是不能运行的&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; from pyflink.datastream import StreamExecutionEnvironment,&#013;&#010;&amp;gt; TimeCharacteristic&#013;&#010;&amp;gt; from pyflink.table import StreamTableEnvironment, DataTypes,&#013;&#010;&amp;gt; EnvironmentSettings&#013;&#010;&amp;gt; from pyflink.table.udf import udf&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; @udf(input_types=[DataTypes.INT()], result_type=DataTypes.STRING())&#013;&#010;&amp;gt; def platform_code_to_name(code):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return \"mobile\" if code == 0 else \"pc\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def log_processing():&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env_settings =&#013;&#010;&amp;gt; EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env =&#013;&#010;&amp;gt; StreamTableEnvironment.create(stream_execution_environment=env,&#013;&#010;&amp;gt; environment_settings=env_settings)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE&#013;&#010;&amp;gt; payment_msg(&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; createTime VARCHAR,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; rt as TO_TIMESTAMP(createTime),&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; orderId BIGINT,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; payAmount DOUBLE,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; payPlatform INT,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; paySource INT,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; WATERMARK FOR rt as rt - INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.version' = 'universal',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'payment_msg_2',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.bootstrap.servers' = '0.0.0.0:9092',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; '&#013;&#010;&amp;gt; connector.properties.group.id' = 'test_3',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'format.type' = 'json'&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; es_sink_ddl = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE&#013;&#010;&amp;gt; es_sink (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; platform&#013;&#010;&amp;gt; VARCHAR,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; pay_amount&#013;&#010;&amp;gt; DOUBLE,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; rowtime&#013;&#010;&amp;gt; TIMESTAMP(3)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ) with (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.type' = 'elasticsearch',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.version' = '7',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.hosts' = 'http://localhost:9200',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.index' = 'platform_pay_amount_1',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.document-type' = 'payment',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'update-mode' = 'upsert',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.flush-on-checkpoint' = 'true',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.key-delimiter' = '$',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.key-null-literal' = 'n/a',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.max-size' = '42mb',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.max-actions' = '32',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.interval' = '1000',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.backoff.delay' = '1000',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'format.type' = 'json'&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.sql_update(es_sink_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.register_function('platformcodetoname',&#013;&#010;&amp;gt; platform_code_to_name)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select platformcodetoname(payPlatform) as platform,&#013;&#010;&amp;gt; sum(payAmount)&#013;&#010;&amp;gt; as pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT)&#013;&#010;&amp;gt; as rowtime&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from payment_msg&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; count_result = t_env.sql_query(query)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.create_temporary_view('windowed_values', count_result)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select platform, last_value(pay_amount), rowtime from&#013;&#010;&amp;gt; windowed_values group by platform, rowtime&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; final_result = t_env.sql_query(query2)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; final_result.execute_insert(table_path='es_sink')&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; log_processing()&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三 下午4:40写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 恳请所有看到此封邮件的大佬！&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 谢谢！&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月15日(星期三) 中午11:25&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 举个sql例子&#013;&#010;&amp;gt; &amp;amp;gt; select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#013;&#010;&amp;gt; &amp;amp;gt; pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#013;&#010;&amp;gt; &amp;amp;gt; rowtime&#013;&#010;&amp;gt; &amp;amp;gt; from payment_msg group by tumble(rt, interval '5' seconds),&#013;&#010;&amp;gt; payPlatform&#013;&#010;&amp;gt; &amp;amp;gt; 这个query 对每5s的tumble窗口做统计。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月15日周三 上午11:10写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A&#013;&#010;&amp;gt; tumble window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Shuiqiang&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;gt; 于2020年7月15日周三&#013;&#010;&amp;gt; 上午10:27写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File \"tou.py\", line 71, in&#013;&#010;&amp;gt; &lt;module&amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File \"tou.py\", line 21, in&#013;&#010;&amp;gt; &amp;amp;gt; from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; .select(\"&#013;&#010;&amp;gt; id,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 907,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; in select&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; return&#013;&#010;&amp;gt; &amp;amp;gt; Table(self._j_table.select(fields),&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; self._t_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; answer,&#013;&#010;&amp;gt; self.gateway_client,&#013;&#010;&amp;gt; &amp;amp;gt; self.target_id,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; self.name)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; return&#013;&#010;&amp;gt; f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; format(target_id, \".\", name),&#013;&#010;&amp;gt; &amp;amp;gt; value)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred&#013;&#010;&amp;gt; while calling&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; o26.select.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; :&#013;&#010;&amp;gt; org.apache.flink.table.api.ValidationException: A tumble&#013;&#010;&amp;gt; &amp;amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;WATERMARK FOR&#013;&#010;&amp;gt; &amp;amp;gt; time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.zookeeper.connect' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'format.derive-schema' =&#013;&#010;&amp;gt; &amp;amp;gt; 'true',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.version' =&#013;&#010;&amp;gt; &amp;amp;gt; 'universal'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; &amp;amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 代码如上&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&amp;gt; user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; type&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; WATERMARK FOR time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; ) with (...)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 于2020年7月10日周五&#013;&#010;&amp;gt; &amp;amp;gt; 上午8:43写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; \"奇怪的不朽琴师\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 1129656513@qq.com&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 发送时间:&amp;amp;amp;amp;amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 收件人:&amp;amp;amp;amp;amp;amp;nbsp;\"godfrey he\"&lt;&#013;&#010;&amp;gt; &amp;amp;gt; godfreyhe@gmail.com&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;amp;amp;amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; :&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.table.api.ValidationException: A group&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; expects a&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; time attribute for grouping in&#013;&#010;&amp;gt; a stream&#013;&#010;&amp;gt; &amp;amp;gt; environment.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 代码如下&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; # use&#013;&#010;&amp;gt; &amp;amp;gt; blink table planner&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; #&#013;&#010;&amp;gt; &amp;amp;gt; register source and sink&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; def&#013;&#010;&amp;gt; register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; &amp;amp;gt; table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;type string&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; def&#013;&#010;&amp;gt; register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; sink_ddl&#013;&#010;&amp;gt; &amp;amp;gt; = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; &amp;amp;gt; table sink1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;time2 timestamp&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.topic' = 'tp3',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;",
        "depth": "11",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_104E4AE02944A5F3F4513DA4F58565756606@qq.com>",
        "from": "&quot;chengyanan1008@foxmail.com&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 02:12:40 GMT",
        "subject": "回复: 回复： pyflink1.11.0window",
        "content": "Hi，因为你的Sink只支持数据的insert插入，请检查insert 语句&#013;&#010;关键报错信息是这一句：&#013;&#010;“AppendStreamTableSink requires that Table has only insert changes.”&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;chengyanan1008@foxmail.com&#013;&#010; &#013;&#010;发件人： 奇怪的不朽琴师&#013;&#010;发送时间： 2020-07-20 16:23&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复： pyflink1.11.0window&#013;&#010;HI ：&#013;&#010;&amp;nbsp; &amp;nbsp; 我现在有一个新的问题，我在此基础上加了一个关联，再写入kafka时报错，如下&#013;&#010;Traceback (most recent call last):&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line 147, in deco&#013;&#010;&amp;nbsp; &amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value&#013;&#010;&amp;nbsp; &amp;nbsp; format(target_id, \".\", name), value)&#013;&#010;py4j.protocol.Py4JJavaError: An error occurred while calling o5.sqlUpdate.&#013;&#010;: org.apache.flink.table.api.TableException: AppendStreamTableSink requires that Table has only insert changes.&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:123)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:48)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:48)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:59)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:685)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:495)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;During handling of the above exception, another exception occurred:&#013;&#010; &#013;&#010; &#013;&#010;Traceback (most recent call last):&#013;&#010;&amp;nbsp; File \"tou.py\", line 99, in &lt;module&amp;gt;&#013;&#010;&amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;nbsp; File \"tou.py\", line 33, in from_kafka_to_kafka_demo&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(\"insert into flink_result select id,type,rowtime from final_result2\")&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/table/table_environment.py\", line 547, in sql_update&#013;&#010;&amp;nbsp; &amp;nbsp; self._j_tenv.sqlUpdate(stmt)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1286, in __call__&#013;&#010;&amp;nbsp; &amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#013;&#010;&amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line 154, in deco&#013;&#010;&amp;nbsp; &amp;nbsp; raise exception_mapping[exception](s.split(': ', 1)[1], stack_trace)&#013;&#010;pyflink.util.exceptions.TableException: 'AppendStreamTableSink requires that Table has only insert changes.'&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;这种应该如何实现，需求大概是一个流表（需要分组汇总）关联一个维表。&#013;&#010; &#013;&#010; &#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings,DataTypes, CsvTableSource, CsvTableSink&#013;&#010;from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;from pyflink.table.window import Tumble&amp;nbsp;&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;def from_kafka_to_kafka_demo():&#013;&#010; &#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; # use blink table planner&#013;&#010;&amp;nbsp; &amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;nbsp; &amp;nbsp; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;nbsp; &amp;nbsp; env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;nbsp; &amp;nbsp; st_env = StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010; &#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; # register source and sink&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;nbsp; &amp;nbsp; register_mysql_source(st_env)&#013;&#010; &#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; query = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select&amp;nbsp; cast(sum(t1.id) as int) as id, max(t1.type) as type,cast(tumble_start(t1.time1, interval '4' second) as bigint) as rowtime&#013;&#010;&amp;nbsp; &amp;nbsp; from source1 t1&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; group by tumble(t1.time1, interval '4' second)&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; count_result = st_env.sql_query(query)&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.create_temporary_view('final_result', count_result)&#013;&#010;&amp;nbsp; &amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select&amp;nbsp; t1.id,t2.type,t1.rowtime from final_result t1 left join dim_mysql t2 on t1.type=t2.id&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; count_result2 = st_env.sql_query(query2)&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.create_temporary_view('final_result2', count_result2)&#013;&#010; &#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(\"insert into flink_result select id,type,rowtime from final_result2\")&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;def register_rides_source(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; create table source1(&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;id int,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time2 varchar ,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;time1 as TO_TIMESTAMP(time2,'yyyyMMddHHmmss'),&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;type string,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;) with (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;)&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010; &#013;&#010; &#013;&#010;def register_mysql_source(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; CREATE TABLE dim_mysql (&#013;&#010;&amp;nbsp; &amp;nbsp; id varchar,&amp;nbsp; --&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; type varchar --&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'jdbc',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.table' = 'flink_test',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.driver' = 'com.mysql.jdbc.Driver',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.username' = '****',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.password' = '*****',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.lookup.cache.max-rows' = '5000',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.lookup.cache.ttl' = '10min'&#013;&#010;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010; &#013;&#010; &#013;&#010;def register_rides_sink(st_env):&#013;&#010;&amp;nbsp; &amp;nbsp; sink_ddl = \\&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; CREATE TABLE flink_result (&#013;&#010;&amp;nbsp; &amp;nbsp; id int,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; type varchar,&#013;&#010;&amp;nbsp; &amp;nbsp; rtime bigint,&#013;&#010;&amp;nbsp; &amp;nbsp; primary key(id)&#013;&#010;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;nbsp; &amp;nbsp; with (&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'tp4',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;nbsp; &amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;nbsp; &amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;)&#013;&#010;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;if __name__ == '__main__':&#013;&#010;&amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                                                                        \"我自己的邮箱\"                                                                                    &lt;1129656513@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月15日(星期三) 下午5:30&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010; &#013;&#010;主题:&amp;nbsp;回复： pyflink1.11.0window&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;非常感谢！&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人:                                                                                                                        \"user-zh\"                                                                                    &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月15日(星期三) 下午5:23&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010; &#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;下面这个例子从kafka读取json格式的数据， 然后做窗口聚合后写入es， 可以参考下代码结构， 修改相应数据字段。 这份代码你本地应该是不能运行的&#013;&#010; &#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings&#013;&#010;from pyflink.table.udf import udf&#013;&#010; &#013;&#010; &#013;&#010;@udf(input_types=[DataTypes.INT()], result_type=DataTypes.STRING())&#013;&#010;def platform_code_to_name(code):&#013;&#010;&amp;nbsp; &amp;nbsp; return \"mobile\" if code == 0 else \"pc\"&#013;&#010; &#013;&#010; &#013;&#010;def log_processing():&#013;&#010;&amp;nbsp; &amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;nbsp; &amp;nbsp; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;nbsp; &amp;nbsp; env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;nbsp; &amp;nbsp; t_env = StreamTableEnvironment.create(stream_execution_environment=env,&#013;&#010;environment_settings=env_settings)&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; source_ddl = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE payment_msg(&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; createTime VARCHAR,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; rt as TO_TIMESTAMP(createTime),&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; orderId BIGINT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; payAmount DOUBLE,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; payPlatform INT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; paySource INT,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; WATERMARK FOR rt as rt - INTERVAL '2' SECOND&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.version' = 'universal',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.topic' = 'payment_msg_2',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.properties.bootstrap.servers' = '0.0.0.0:9092',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.properties.group.id' = 'test_3',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'format.type' = 'json'&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; t_env.sql_update(source_ddl)&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; es_sink_ddl = \"\"\"&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE es_sink (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; platform VARCHAR,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; pay_amount DOUBLE,&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; rowtime TIMESTAMP(3)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) with (&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.type' = 'elasticsearch',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.version' = '7',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.hosts' = 'http://localhost:9200',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.index' = 'platform_pay_amount_1',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.document-type' = 'payment',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'update-mode' = 'upsert',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.flush-on-checkpoint' = 'true',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.key-delimiter' = '$',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.key-null-literal' = 'n/a',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.max-size' = '42mb',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.max-actions' = '32',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.interval' = '1000',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector.bulk-flush.backoff.delay' = '1000',&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'format.type' = 'json'&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; t_env.sql_update(es_sink_ddl)&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; t_env.register_function('platformcodetoname', platform_code_to_name)&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; query = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select platformcodetoname(payPlatform) as platform, sum(payAmount)&#013;&#010;as pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT)&#013;&#010;as rowtime&#013;&#010;&amp;nbsp; &amp;nbsp; from payment_msg&#013;&#010;&amp;nbsp; &amp;nbsp; group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; count_result = t_env.sql_query(query)&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; t_env.create_temporary_view('windowed_values', count_result)&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;nbsp; &amp;nbsp; select platform, last_value(pay_amount), rowtime from&#013;&#010;windowed_values group by platform, rowtime&#013;&#010;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; final_result = t_env.sql_query(query2)&#013;&#010; &#013;&#010;&amp;nbsp; &amp;nbsp; final_result.execute_insert(table_path='es_sink')&#013;&#010; &#013;&#010; &#013;&#010;if __name__ == '__main__':&#013;&#010;&amp;nbsp; &amp;nbsp; log_processing()&#013;&#010; &#013;&#010; &#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月15日周三 下午4:40写道：&#013;&#010; &#013;&#010;&amp;gt; &amp;amp;nbsp;Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 恳请所有看到此封邮件的大佬！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 谢谢！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; &lt;&#013;&#010;&amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 中午11:25&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 举个sql例子&#013;&#010;&amp;gt; select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#013;&#010;&amp;gt; pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#013;&#010;&amp;gt; rowtime&#013;&#010;&amp;gt; from payment_msg group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;gt; 这个query 对每5s的tumble窗口做统计。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三 上午11:10写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A tumble window&#013;&#010;&amp;gt; &amp;amp;gt; expects a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; Shuiqiang&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月15日周三 上午10:27写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 71, in &lt;module&amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File \"tou.py\", line 21, in&#013;&#010;&amp;gt; from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; .select(\" id,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#013;&#010;&amp;gt; &amp;amp;gt; 907,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; in select&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return&#013;&#010;&amp;gt; Table(self._j_table.select(fields),&#013;&#010;&amp;gt; &amp;amp;gt; self._t_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; answer, self.gateway_client,&#013;&#010;&amp;gt; self.target_id,&#013;&#010;&amp;gt; &amp;amp;gt; self.name)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#013;&#010;&amp;gt; &amp;amp;gt; line&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; format(target_id, \".\", name),&#013;&#010;&amp;gt; value)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling&#013;&#010;&amp;gt; &amp;amp;gt; o26.select.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; : org.apache.flink.table.api.ValidationException: A tumble&#013;&#010;&amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; expects&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;WATERMARK FOR&#013;&#010;&amp;gt; time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.zookeeper.connect' =&#013;&#010;&amp;gt; &amp;amp;gt; 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'format.derive-schema' =&#013;&#010;&amp;gt; 'true',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; 'connector.version' =&#013;&#010;&amp;gt; 'universal'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .select(\" id,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 代码如上&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; id&#013;&#010;&amp;gt; &amp;amp;gt; int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; type&#013;&#010;&amp;gt; &amp;amp;gt; string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; WATERMARK FOR time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ) with (...)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;gt; 于2020年7月10日周五&#013;&#010;&amp;gt; 上午8:43写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"奇怪的不朽琴师\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 1129656513@qq.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"godfrey he\"&lt;&#013;&#010;&amp;gt; godfreyhe@gmail.com&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; :&#013;&#010;&amp;gt; org.apache.flink.table.api.ValidationException: A group&#013;&#010;&amp;gt; &amp;amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects a&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; time attribute for grouping in a stream&#013;&#010;&amp;gt; environment.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 代码如下&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; # use&#013;&#010;&amp;gt; blink table planner&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; &amp;amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; #&#013;&#010;&amp;gt; register source and sink&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;type string&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; sink_ddl&#013;&#010;&amp;gt; = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; table sink1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time2 timestamp&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp3',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<tencent_6CC69AC49239D32125FCD4A7D2781AADFE05@qq.com>",
        "from": "&quot;chengyanan1008@foxmail.com&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 08:25:37 GMT",
        "subject": "回复: 回复： pyflink1.11.0window",
        "content": "Hi，&#013;&#010;你这个报错信息主要原因是kafkaTableSink是属于AppendStreamTableSink，而你的上一级数据源final_result2是由前面source1表group by得到的final_result与mysql维表join后而来，经过group by之后的StreamTable 已经变为RetractStreamTableSink，所以这里才会报错&#013;&#010;请查阅有关AppendStreamTableSink、RetractStreamTableSink的资料。&#013;&#010;&#013;&#010;&#013;&#010;关于文中出现 “&amp;nbsp; &amp;nbsp; &amp;nbsp;” ，可以换一个邮箱编辑器。&#013;&#010;&#013;&#010;希望能帮助到你&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;chengyanan1008@foxmail.com&#013;&#010; &#013;&#010;发件人： 奇怪的不朽琴师&#013;&#010;发送时间： 2020-07-21 10:23&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复： pyflink1.11.0window&#013;&#010;你好：&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;他这个会自动转译空格回车啥的符号，暂时没什么好办法，很难受，大佬这边有pyflink的多表关联的demo么，万分感谢！&#013;&#010; &#013;&#010; &#013;&#010;发送&#013;&#010; &#013;&#010; &#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                                                                        \"user-zh\"                                                                                    &lt;acqua.csq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月20日(星期一) 晚上8:42&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010; &#013;&#010;主题:&amp;nbsp;Re: pyflink1.11.0window&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;看看异常信息， 是不是你的insert mode没配置对。&#013;&#010;BTW, 你粘贴的文本带有很多\"&amp;amp;nbsp;\"， 有点影响可读性。&#013;&#010; &#013;&#010;Best，&#013;&#010;Shuiqiang&#013;&#010; &#013;&#010;奇怪的不朽琴师 &lt;1129656513@qq.com&amp;gt; 于2020年7月20日周一 下午4:23写道：&#013;&#010; &#013;&#010;&amp;gt; HI ：&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 我现在有一个新的问题，我在此基础上加了一个关联，再写入kafka时报错，如下&#013;&#010;&amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#013;&#010;&amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; format(target_id, \".\", name), value)&#013;&#010;&amp;gt; py4j.protocol.Py4JJavaError: An error occurred while calling o5.sqlUpdate.&#013;&#010;&amp;gt; : org.apache.flink.table.api.TableException: AppendStreamTableSink&#013;&#010;&amp;gt; requires that Table has only insert changes.&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:123)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.scala:48)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecSink.translateToPlan(StreamExecSink.scala:48)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:60)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:59)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:59)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:153)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:685)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:495)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; During handling of the above exception, another exception occurred:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"tou.py\", line 99, in &lt;module&amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"tou.py\", line 33, in from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(\"insert into flink_result select&#013;&#010;&amp;gt; id,type,rowtime from final_result2\")&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table_environment.py\",&#013;&#010;&amp;gt; line 547, in sql_update&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; self._j_tenv.sqlUpdate(stmt)&#013;&#010;&amp;gt; &amp;amp;nbsp; File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; answer, self.gateway_client, self.target_id, self.name)&#013;&#010;&amp;gt; &amp;amp;nbsp; File&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line&#013;&#010;&amp;gt; 154, in deco&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; raise exception_mapping[exception](s.split(': ', 1)[1],&#013;&#010;&amp;gt; stack_trace)&#013;&#010;&amp;gt; pyflink.util.exceptions.TableException: 'AppendStreamTableSink requires&#013;&#010;&amp;gt; that Table has only insert changes.'&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 这种应该如何实现，需求大概是一个流表（需要分组汇总）关联一个维表。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; from pyflink.datastream import StreamExecutionEnvironment,&#013;&#010;&amp;gt; TimeCharacteristic&#013;&#010;&amp;gt; from pyflink.table import StreamTableEnvironment, DataTypes,&#013;&#010;&amp;gt; EnvironmentSettings,DataTypes, CsvTableSource, CsvTableSink&#013;&#010;&amp;gt; from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;&amp;gt; from pyflink.table.window import Tumble&amp;amp;nbsp;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # use blink table planner&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env_settings =&#013;&#010;&amp;gt; EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; # register source and sink&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; register_mysql_source(st_env)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select&amp;amp;nbsp; cast(sum(t1.id) as int) as id, max(t1.type) as&#013;&#010;&amp;gt; type,cast(tumble_start(t1.time1, interval '4' second) as bigint) as rowtime&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from source1 t1&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; group by tumble(t1.time1, interval '4' second)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; count_result = st_env.sql_query(query)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.create_temporary_view('final_result', count_result)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select&amp;amp;nbsp; t1.id,t2.type,t1.rowtime from final_result t1&#013;&#010;&amp;gt; left join dim_mysql t2 on t1.type=t2.id&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; count_result2 = st_env.sql_query(query2)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.create_temporary_view('final_result2', count_result2)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(\"insert into flink_result select&#013;&#010;&amp;gt; id,type,rowtime from final_result2\")&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; create table source1(&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time2 varchar ,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;time1 as TO_TIMESTAMP(time2,'yyyyMMddHHmmss'),&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;WATERMARK FOR time1 as time1 - INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_mysql_source(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE dim_mysql (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; id varchar,&amp;amp;nbsp; --&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; type varchar --&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'jdbc',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.table' = 'flink_test',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.driver' = 'com.mysql.jdbc.Driver',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.username' = '****',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.password' = '*****',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.lookup.cache.max-rows' = '5000',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.lookup.cache.ttl' = '10min'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&amp;amp;nbsp; &amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; sink_ddl = \\&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE flink_result (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; id int,&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; type varchar,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; rtime bigint,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; primary key(id)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; with (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.topic' = 'tp4',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 'connector.version' = 'universal'&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"我自己的邮箱\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;&#013;&#010;&amp;gt; 1129656513@qq.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 下午5:30&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;回复： pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;非常感谢！&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;&#013;&#010;&amp;gt; acqua.csq@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月15日(星期三) 下午5:23&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 下面这个例子从kafka读取json格式的数据， 然后做窗口聚合后写入es， 可以参考下代码结构， 修改相应数据字段。 这份代码你本地应该是不能运行的&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; from pyflink.datastream import StreamExecutionEnvironment,&#013;&#010;&amp;gt; TimeCharacteristic&#013;&#010;&amp;gt; from pyflink.table import StreamTableEnvironment, DataTypes,&#013;&#010;&amp;gt; EnvironmentSettings&#013;&#010;&amp;gt; from pyflink.table.udf import udf&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; @udf(input_types=[DataTypes.INT()], result_type=DataTypes.STRING())&#013;&#010;&amp;gt; def platform_code_to_name(code):&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; return \"mobile\" if code == 0 else \"pc\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; def log_processing():&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; env_settings =&#013;&#010;&amp;gt; EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env =&#013;&#010;&amp;gt; StreamTableEnvironment.create(stream_execution_environment=env,&#013;&#010;&amp;gt; environment_settings=env_settings)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; source_ddl = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE&#013;&#010;&amp;gt; payment_msg(&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; createTime VARCHAR,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; rt as TO_TIMESTAMP(createTime),&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; orderId BIGINT,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; payAmount DOUBLE,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; payPlatform INT,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; paySource INT,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; WATERMARK FOR rt as rt - INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.version' = 'universal',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'payment_msg_2',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.properties.bootstrap.servers' = '0.0.0.0:9092',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; '&#013;&#010;&amp;gt; connector.properties.group.id' = 'test_3',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.startup-mode' = 'latest-offset',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; 'format.type' = 'json'&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; es_sink_ddl = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; CREATE TABLE&#013;&#010;&amp;gt; es_sink (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; platform&#013;&#010;&amp;gt; VARCHAR,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; pay_amount&#013;&#010;&amp;gt; DOUBLE,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; rowtime&#013;&#010;&amp;gt; TIMESTAMP(3)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ) with (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.type' = 'elasticsearch',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.version' = '7',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.hosts' = 'http://localhost:9200',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.index' = 'platform_pay_amount_1',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.document-type' = 'payment',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'update-mode' = 'upsert',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.flush-on-checkpoint' = 'true',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.key-delimiter' = '$',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.key-null-literal' = 'n/a',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.max-size' = '42mb',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.max-actions' = '32',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.interval' = '1000',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'connector.bulk-flush.backoff.delay' = '1000',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; 'format.type' = 'json'&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; )&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.sql_update(es_sink_ddl)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.register_function('platformcodetoname',&#013;&#010;&amp;gt; platform_code_to_name)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select platformcodetoname(payPlatform) as platform,&#013;&#010;&amp;gt; sum(payAmount)&#013;&#010;&amp;gt; as pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT)&#013;&#010;&amp;gt; as rowtime&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; from payment_msg&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; group by tumble(rt, interval '5' seconds), payPlatform&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; count_result = t_env.sql_query(query)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; t_env.create_temporary_view('windowed_values', count_result)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; query2 = \"\"\"&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; select platform, last_value(pay_amount), rowtime from&#013;&#010;&amp;gt; windowed_values group by platform, rowtime&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; final_result = t_env.sql_query(query2)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; final_result.execute_insert(table_path='es_sink')&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; log_processing()&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;gt; 于2020年7月15日周三 下午4:40写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;hi，能否请求您贡献一下完整的代码的案例，我是初学者，官网的2-from_kafka_to_kafka.py这个没有窗口，我现在想要一个在此基础上有窗口的demo，尝试编了很久也未能解决。我在给这个demo加上窗口功能后总是有各种各样的问题，十分痛苦，如能帮助，感激不尽。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 恳请所有看到此封邮件的大佬！&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 谢谢！&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月15日(星期三) 中午11:25&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 举个sql例子&#013;&#010;&amp;gt; &amp;amp;gt; select platformcodetoname(payPlatform) as platform, sum(payAmount) as&#013;&#010;&amp;gt; &amp;amp;gt; pay_amount, cast(tumble_start(rt, interval '5' seconds) as BIGINT) as&#013;&#010;&amp;gt; &amp;amp;gt; rowtime&#013;&#010;&amp;gt; &amp;amp;gt; from payment_msg group by tumble(rt, interval '5' seconds),&#013;&#010;&amp;gt; payPlatform&#013;&#010;&amp;gt; &amp;amp;gt; 这个query 对每5s的tumble窗口做统计。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;gt; 于2020年7月15日周三 上午11:10写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Shuiqiang，你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp;我的目的是每间隔一段时间做一次汇总统计，比如每两秒做一下汇总，请问这个需求我改如何定义window？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月15日(星期三) 上午10:51&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 异常栈信息org.apache.flink.table.api.ValidationException: A&#013;&#010;&amp;gt; tumble window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 看起来是接下tumble window定义的代码不太正确吧&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Shuiqiang&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;gt; 于2020年7月15日周三&#013;&#010;&amp;gt; 上午10:27写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;我按着你回复的建议改了source但是会报新的错误，请问这个是因为什么？我想调试一个window一直没有成功，请帮帮我，谢谢。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; Traceback (most recent call last):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File \"tou.py\", line 71, in&#013;&#010;&amp;gt; &lt;module&amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File \"tou.py\", line 21, in&#013;&#010;&amp;gt; &amp;amp;gt; from_kafka_to_kafka_demo&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; .select(\"&#013;&#010;&amp;gt; id,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/table/table.py\", line&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 907,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; in select&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; return&#013;&#010;&amp;gt; &amp;amp;gt; Table(self._j_table.select(fields),&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; self._t_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; line 1286, in __call__&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; answer,&#013;&#010;&amp;gt; self.gateway_client,&#013;&#010;&amp;gt; &amp;amp;gt; self.target_id,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; self.name)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; line&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 147, in deco&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; return&#013;&#010;&amp;gt; f(*a, **kw)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; File&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\",&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; line 328, in get_return_value&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; format(target_id, \".\", name),&#013;&#010;&amp;gt; &amp;amp;gt; value)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; py4j.protocol.Py4JJavaError: An error occurred&#013;&#010;&amp;gt; while calling&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; o26.select.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; :&#013;&#010;&amp;gt; org.apache.flink.table.api.ValidationException: A tumble&#013;&#010;&amp;gt; &amp;amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; expects&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; a size value literal.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.getAsValueLiteral(AggregateOperationFactory.java:384)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.validateAndCreateTumbleWindow(AggregateOperationFactory.java:302)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.AggregateOperationFactory.createResolvedWindow(AggregateOperationFactory.java:236)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.operations.utils.OperationTreeBuilder.windowAggregate(OperationTreeBuilder.java:250)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:794)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.table.api.internal.TableImpl$WindowGroupedTableImpl.select(TableImpl.java:781)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;type string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;WATERMARK FOR&#013;&#010;&amp;gt; &amp;amp;gt; time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:9092',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.properties.zookeeper.connect' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'localhost:2181',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'format.type' = 'json',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'format.derive-schema' =&#013;&#010;&amp;gt; &amp;amp;gt; 'true',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector.version' =&#013;&#010;&amp;gt; &amp;amp;gt; 'universal'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; &amp;amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; .window(Tumble.over(\"2.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 代码如上&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; acqua.csq@gmail.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月10日(星期五) 上午9:17&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&amp;gt; user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;Re: pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 琴师你好，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你的source ddl里有指定time1为 time attribute吗？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; create table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; id&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; type&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; string,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; WATERMARK FOR time1 as time1 -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; INTERVAL '2' SECOND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; ) with (...)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 奇怪的不朽琴师 &lt;1129656513@qq.com&amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 于2020年7月10日周五&#013;&#010;&amp;gt; &amp;amp;gt; 上午8:43写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; \"奇怪的不朽琴师\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 1129656513@qq.com&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 发送时间:&amp;amp;amp;amp;amp;amp;nbsp;2020年7月9日(星期四) 下午5:08&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 收件人:&amp;amp;amp;amp;amp;amp;nbsp;\"godfrey he\"&lt;&#013;&#010;&amp;gt; &amp;amp;gt; godfreyhe@gmail.com&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;amp;amp;amp;amp;nbsp;pyflink1.11.0window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 你好：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;我在使用pyflink1.11版本时，window开窗仍会报错&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; :&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.table.api.ValidationException: A group&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; window&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; expects a&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; time attribute for grouping in&#013;&#010;&amp;gt; a stream&#013;&#010;&amp;gt; &amp;amp;gt; environment.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 请问这个问题没有修复么？或者是我使用的方式不对，如果是使用不对，能提供一个正确的案例么？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 代码如下&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; s_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; s_env.set_parallelism(1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; # use&#013;&#010;&amp;gt; &amp;amp;gt; blink table planner&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; st_env =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; StreamTableEnvironment.create(s_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; #&#013;&#010;&amp;gt; &amp;amp;gt; register source and sink&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; register_rides_source(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; register_rides_sink(st_env)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.from_path(\"source1\")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; .window(Tumble.over(\"1.secends\").on(\"time1\").alias(\"w\")) \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .group_by(\"w\") \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .select(\" id,&amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; time1 , time1 \")\\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; .insert_into(\"sink1\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; def&#013;&#010;&amp;gt; register_rides_source(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; source_ddl = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; &amp;amp;gt; table source1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;type string&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.topic' = 'tp1',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; def&#013;&#010;&amp;gt; register_rides_sink(st_env):&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; sink_ddl&#013;&#010;&amp;gt; &amp;amp;gt; = \\&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; create&#013;&#010;&amp;gt; &amp;amp;gt; table sink1(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; id int,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;time1 timestamp,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;time2 timestamp&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;) with (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.type' = 'kafka',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'update-mode' = 'append',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; 'connector.topic' = 'tp3',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'connector.properties.bootstrap.servers' =&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 'localhost:9092'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp; '''&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_4A614C2E41DB6E523B7F52ED8C009C546405@qq.com>"
    },
    {
        "id": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 01:07:23 GMT",
        "subject": "flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "大家好：&#010; 我在用flink 1.11 的sql从kafka消费然后写入hdfs的过程中，发现没法自动提交分区，请问这个是什么原因呢？谢谢&#010;&#010;我的checkpoint设置了间隔10s，对于如下的配置，正常应该是每隔10在hdfs相应的分区下会有_SUCCESS文件，但是实际上过了好久也没有，ORC格式的结果数据是正常写入了。&#010;&#010;public static void main(String[] args) throws Exception{&#010;StreamExecutionEnvironment bsEnv =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;bsEnv.enableCheckpointing(10000);&#010;bsEnv.setParallelism(1);&#010;StreamTableEnvironment tEnv = StreamTableEnvironment.create(bsEnv);&#010;&#010;String sqlSource = \"CREATE TABLE  source_kafka (\\n\" +&#010;                  \"    appName  STRING,\\n\" +&#010;                  \"    appVersion STRING,\\n\" +&#010;                  \"    uploadTime STRING\\n\" +&#010;                  \") WITH (\\n\" +&#010;                  \"  'connector.type' = 'kafka',       \\n\" +&#010;                  \"  'connector.version' = '0.10',\\n\" +&#010;                  \"  'connector.topic' = 'mytest',\\n\" +&#010;                  \"  'connector.properties.zookeeper.connect' =&#010;'localhost:2181',\\n\" +&#010;                  \"  'connector.properties.bootstrap.servers' =&#010;'localhost:9092',\\n\" +&#010;                  \"  'connector.properties.group.id' = 'testGroup',\\n\" +&#010;                  \"  'format.type'='json',\\n\" +&#010;                  \"  'update-mode' = 'append' )\";&#010;&#010;tEnv.executeSql(sqlSource);&#010;&#010;String sql = \"CREATE TABLE fs_table (\\n\" +&#010;            \"    appName  STRING,\\n\" +&#010;            \"    appVersion STRING,\\n\" +&#010;            \"    uploadTime STRING,\\n\" +&#010;            \"  dt STRING,\" +&#010;            \"  h string\" +&#010;            \")  PARTITIONED BY (dt,h)  WITH (\\n\" +&#010;            \"  'connector'='filesystem',\\n\" +&#010;                     \"  'path'='hdfs://localhost/tmp/',\\n\" +&#010;                     \" 'sink.partition-commit.policy.kind' =&#010;'success-file', \" +&#010;                     \"  'format'='orc'\\n\" +&#010;                     \")\";&#010;tEnv.executeSql(sql);&#010;&#010;String insertSql = \"insert into  fs_table SELECT appName&#010;,appVersion,uploadTime, \" +&#010;                  \" DATE_FORMAT(LOCALTIMESTAMP, 'yyyy-MM-dd'),&#010;DATE_FORMAT(LOCALTIMESTAMP, 'HH') FROM source_kafka\";&#010;&#010;tEnv.executeSql(insertSql);&#010;&#010;}&#010;&#010;",
        "depth": "0",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jQt9d9uHN+j4koXfPhxfFd7jJmL4TenuLO=Q5m=QtEsJg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 06:54:15 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "Hi,&#013;&#010;&#013;&#010;默认情况下，对ORC来说，理论上一旦有正式数据文件的生成，就会有对应SUCCESS文件产生，你是怎么确认没有SUCCESS文件的呢？&#013;&#010;我用同样SQL在我的环境是有的。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 10, 2020 at 9:07 AM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;wrote:&#013;&#010;&#013;&#010;&gt; 大家好：&#013;&#010;&gt;  我在用flink 1.11 的sql从kafka消费然后写入hdfs的过程中，发现没法自动提交分区，请问这个是什么原因呢？谢谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我的checkpoint设置了间隔10s，对于如下的配置，正常应该是每隔10在hdfs相应的分区下会有_SUCCESS文件，但是实际上过了好久也没有，ORC格式的结果数据是正常写入了。&#013;&#010;&gt;&#013;&#010;&gt; public static void main(String[] args) throws Exception{&#013;&#010;&gt; StreamExecutionEnvironment bsEnv =&#013;&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; bsEnv.enableCheckpointing(10000);&#013;&#010;&gt; bsEnv.setParallelism(1);&#013;&#010;&gt; StreamTableEnvironment tEnv = StreamTableEnvironment.create(bsEnv);&#013;&#010;&gt;&#013;&#010;&gt; String sqlSource = \"CREATE TABLE  source_kafka (\\n\" +&#013;&#010;&gt;                   \"    appName  STRING,\\n\" +&#013;&#010;&gt;                   \"    appVersion STRING,\\n\" +&#013;&#010;&gt;                   \"    uploadTime STRING\\n\" +&#013;&#010;&gt;                   \") WITH (\\n\" +&#013;&#010;&gt;                   \"  'connector.type' = 'kafka',       \\n\" +&#013;&#010;&gt;                   \"  'connector.version' = '0.10',\\n\" +&#013;&#010;&gt;                   \"  'connector.topic' = 'mytest',\\n\" +&#013;&#010;&gt;                   \"  'connector.properties.zookeeper.connect' =&#013;&#010;&gt; 'localhost:2181',\\n\" +&#013;&#010;&gt;                   \"  'connector.properties.bootstrap.servers' =&#013;&#010;&gt; 'localhost:9092',\\n\" +&#013;&#010;&gt;                   \"  'connector.properties.group.id' = 'testGroup',\\n\" +&#013;&#010;&gt;                   \"  'format.type'='json',\\n\" +&#013;&#010;&gt;                   \"  'update-mode' = 'append' )\";&#013;&#010;&gt;&#013;&#010;&gt; tEnv.executeSql(sqlSource);&#013;&#010;&gt;&#013;&#010;&gt; String sql = \"CREATE TABLE fs_table (\\n\" +&#013;&#010;&gt;             \"    appName  STRING,\\n\" +&#013;&#010;&gt;             \"    appVersion STRING,\\n\" +&#013;&#010;&gt;             \"    uploadTime STRING,\\n\" +&#013;&#010;&gt;             \"  dt STRING,\" +&#013;&#010;&gt;             \"  h string\" +&#013;&#010;&gt;             \")  PARTITIONED BY (dt,h)  WITH (\\n\" +&#013;&#010;&gt;             \"  'connector'='filesystem',\\n\" +&#013;&#010;&gt;                      \"  'path'='hdfs://localhost/tmp/',\\n\" +&#013;&#010;&gt;                      \" 'sink.partition-commit.policy.kind' =&#013;&#010;&gt; 'success-file', \" +&#013;&#010;&gt;                      \"  'format'='orc'\\n\" +&#013;&#010;&gt;                      \")\";&#013;&#010;&gt; tEnv.executeSql(sql);&#013;&#010;&gt;&#013;&#010;&gt; String insertSql = \"insert into  fs_table SELECT appName&#013;&#010;&gt; ,appVersion,uploadTime, \" +&#013;&#010;&gt;                   \" DATE_FORMAT(LOCALTIMESTAMP, 'yyyy-MM-dd'),&#013;&#010;&gt; DATE_FORMAT(LOCALTIMESTAMP, 'HH') FROM source_kafka\";&#013;&#010;&gt;&#013;&#010;&gt; tEnv.executeSql(insertSql);&#013;&#010;&gt;&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LA5ofqcSx23pxCDqAFpebRi3J0fFonMZ1n7CEnBJM-6NA@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 07:09:27 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "hi，jinsong：&#013;&#010;&#013;&#010;在我的测试环境下，对于我贴出来的那个代码。&#013;&#010;1.如果source使用的有界的数据，比如bsEnv.fromElements(...)，这样会有success文件生成，如果是kafka数据，就不行。&#013;&#010;2.如果设置程序的并行度是大于1，那么也会有success生成。&#013;&#010;3.如果我写入的是local file，比如 file:///tmp/aaa ，而不是hdfs，也会有success文件生成。&#013;&#010;&#013;&#010;综上，在并行度设置为1，消费的是kafka的永不停止的数据，写入的是hdfs，我的checkpoint设置是10s，这种情况下，我测试了好多遍，都没有success文件生成。&#013;&#010;&#013;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月10日周五 下午2:54写道：&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 默认情况下，对ORC来说，理论上一旦有正式数据文件的生成，就会有对应SUCCESS文件产生，你是怎么确认没有SUCCESS文件的呢？&#013;&#010;&gt; 我用同样SQL在我的环境是有的。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Fri, Jul 10, 2020 at 9:07 AM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; 大家好：&#013;&#010;&gt; &gt;  我在用flink 1.11 的sql从kafka消费然后写入hdfs的过程中，发现没法自动提交分区，请问这个是什么原因呢？谢谢&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我的checkpoint设置了间隔10s，对于如下的配置，正常应该是每隔10在hdfs相应的分区下会有_SUCCESS文件，但是实际上过了好久也没有，ORC格式的结果数据是正常写入了。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; public static void main(String[] args) throws Exception{&#013;&#010;&gt; &gt; StreamExecutionEnvironment bsEnv =&#013;&#010;&gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; &gt; bsEnv.enableCheckpointing(10000);&#013;&#010;&gt; &gt; bsEnv.setParallelism(1);&#013;&#010;&gt; &gt; StreamTableEnvironment tEnv = StreamTableEnvironment.create(bsEnv);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; String sqlSource = \"CREATE TABLE  source_kafka (\\n\" +&#013;&#010;&gt; &gt;                   \"    appName  STRING,\\n\" +&#013;&#010;&gt; &gt;                   \"    appVersion STRING,\\n\" +&#013;&#010;&gt; &gt;                   \"    uploadTime STRING\\n\" +&#013;&#010;&gt; &gt;                   \") WITH (\\n\" +&#013;&#010;&gt; &gt;                   \"  'connector.type' = 'kafka',       \\n\" +&#013;&#010;&gt; &gt;                   \"  'connector.version' = '0.10',\\n\" +&#013;&#010;&gt; &gt;                   \"  'connector.topic' = 'mytest',\\n\" +&#013;&#010;&gt; &gt;                   \"  'connector.properties.zookeeper.connect' =&#013;&#010;&gt; &gt; 'localhost:2181',\\n\" +&#013;&#010;&gt; &gt;                   \"  'connector.properties.bootstrap.servers' =&#013;&#010;&gt; &gt; 'localhost:9092',\\n\" +&#013;&#010;&gt; &gt;                   \"  'connector.properties.group.id' = 'testGroup',\\n\" +&#013;&#010;&gt; &gt;                   \"  'format.type'='json',\\n\" +&#013;&#010;&gt; &gt;                   \"  'update-mode' = 'append' )\";&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; tEnv.executeSql(sqlSource);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; String sql = \"CREATE TABLE fs_table (\\n\" +&#013;&#010;&gt; &gt;             \"    appName  STRING,\\n\" +&#013;&#010;&gt; &gt;             \"    appVersion STRING,\\n\" +&#013;&#010;&gt; &gt;             \"    uploadTime STRING,\\n\" +&#013;&#010;&gt; &gt;             \"  dt STRING,\" +&#013;&#010;&gt; &gt;             \"  h string\" +&#013;&#010;&gt; &gt;             \")  PARTITIONED BY (dt,h)  WITH (\\n\" +&#013;&#010;&gt; &gt;             \"  'connector'='filesystem',\\n\" +&#013;&#010;&gt; &gt;                      \"  'path'='hdfs://localhost/tmp/',\\n\" +&#013;&#010;&gt; &gt;                      \" 'sink.partition-commit.policy.kind' =&#013;&#010;&gt; &gt; 'success-file', \" +&#013;&#010;&gt; &gt;                      \"  'format'='orc'\\n\" +&#013;&#010;&gt; &gt;                      \")\";&#013;&#010;&gt; &gt; tEnv.executeSql(sql);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; String insertSql = \"insert into  fs_table SELECT appName&#013;&#010;&gt; &gt; ,appVersion,uploadTime, \" +&#013;&#010;&gt; &gt;                   \" DATE_FORMAT(LOCALTIMESTAMP, 'yyyy-MM-dd'),&#013;&#010;&gt; &gt; DATE_FORMAT(LOCALTIMESTAMP, 'HH') FROM source_kafka\";&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; tEnv.executeSql(insertSql);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; }&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jRaLoK20985=Q_k=ruBkPc6zz6=if_oXhu94M4T=F1Ksg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 07:25:32 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "Hi, Jun,&#013;&#010;&#013;&#010;非常感谢详细充分的测试~&#013;&#010;&#013;&#010;接下来我复现排查下~&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 10, 2020 at 3:09 PM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;wrote:&#013;&#010;&#013;&#010;&gt; hi，jinsong：&#013;&#010;&gt;&#013;&#010;&gt; 在我的测试环境下，对于我贴出来的那个代码。&#013;&#010;&gt;&#013;&#010;&gt; 1.如果source使用的有界的数据，比如bsEnv.fromElements(...)，这样会有success文件生成，如果是kafka数据，就不行。&#013;&#010;&gt; 2.如果设置程序的并行度是大于1，那么也会有success生成。&#013;&#010;&gt; 3.如果我写入的是local file，比如 file:///tmp/aaa ，而不是hdfs，也会有success文件生成。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 综上，在并行度设置为1，消费的是kafka的永不停止的数据，写入的是hdfs，我的checkpoint设置是10s，这种情况下，我测试了好多遍，都没有success文件生成。&#013;&#010;&gt;&#013;&#010;&gt; Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月10日周五 下午2:54写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 默认情况下，对ORC来说，理论上一旦有正式数据文件的生成，就会有对应SUCCESS文件产生，你是怎么确认没有SUCCESS文件的呢？&#013;&#010;&gt; &gt; 我用同样SQL在我的环境是有的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Jingsong&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Fri, Jul 10, 2020 at 9:07 AM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;&gt; &gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 大家好：&#013;&#010;&gt; &gt; &gt;  我在用flink 1.11 的sql从kafka消费然后写入hdfs的过程中，发现没法自动提交分区，请问这个是什么原因呢？谢谢&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我的checkpoint设置了间隔10s，对于如下的配置，正常应该是每隔10在hdfs相应的分区下会有_SUCCESS文件，但是实际上过了好久也没有，ORC格式的结果数据是正常写入了。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; public static void main(String[] args) throws Exception{&#013;&#010;&gt; &gt; &gt; StreamExecutionEnvironment bsEnv =&#013;&#010;&gt; &gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; &gt; &gt; bsEnv.enableCheckpointing(10000);&#013;&#010;&gt; &gt; &gt; bsEnv.setParallelism(1);&#013;&#010;&gt; &gt; &gt; StreamTableEnvironment tEnv = StreamTableEnvironment.create(bsEnv);&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; String sqlSource = \"CREATE TABLE  source_kafka (\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"    appName  STRING,\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"    appVersion STRING,\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"    uploadTime STRING\\n\" +&#013;&#010;&gt; &gt; &gt;                   \") WITH (\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'connector.type' = 'kafka',       \\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'connector.version' = '0.10',\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'connector.topic' = 'mytest',\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'connector.properties.zookeeper.connect' =&#013;&#010;&gt; &gt; &gt; 'localhost:2181',\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'connector.properties.bootstrap.servers' =&#013;&#010;&gt; &gt; &gt; 'localhost:9092',\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'connector.properties.group.id' =&#013;&#010;&gt; 'testGroup',\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'format.type'='json',\\n\" +&#013;&#010;&gt; &gt; &gt;                   \"  'update-mode' = 'append' )\";&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; tEnv.executeSql(sqlSource);&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; String sql = \"CREATE TABLE fs_table (\\n\" +&#013;&#010;&gt; &gt; &gt;             \"    appName  STRING,\\n\" +&#013;&#010;&gt; &gt; &gt;             \"    appVersion STRING,\\n\" +&#013;&#010;&gt; &gt; &gt;             \"    uploadTime STRING,\\n\" +&#013;&#010;&gt; &gt; &gt;             \"  dt STRING,\" +&#013;&#010;&gt; &gt; &gt;             \"  h string\" +&#013;&#010;&gt; &gt; &gt;             \")  PARTITIONED BY (dt,h)  WITH (\\n\" +&#013;&#010;&gt; &gt; &gt;             \"  'connector'='filesystem',\\n\" +&#013;&#010;&gt; &gt; &gt;                      \"  'path'='hdfs://localhost/tmp/',\\n\" +&#013;&#010;&gt; &gt; &gt;                      \" 'sink.partition-commit.policy.kind' =&#013;&#010;&gt; &gt; &gt; 'success-file', \" +&#013;&#010;&gt; &gt; &gt;                      \"  'format'='orc'\\n\" +&#013;&#010;&gt; &gt; &gt;                      \")\";&#013;&#010;&gt; &gt; &gt; tEnv.executeSql(sql);&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; String insertSql = \"insert into  fs_table SELECT appName&#013;&#010;&gt; &gt; &gt; ,appVersion,uploadTime, \" +&#013;&#010;&gt; &gt; &gt;                   \" DATE_FORMAT(LOCALTIMESTAMP, 'yyyy-MM-dd'),&#013;&#010;&gt; &gt; &gt; DATE_FORMAT(LOCALTIMESTAMP, 'HH') FROM source_kafka\";&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; tEnv.executeSql(insertSql);&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; }&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best, Jingsong Lee&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "3",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LDqJ_JUwjS52oQsyLcYPCqRHkjPzWVVTezsCOnmaSEgHw@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 07:39:35 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "此外，如果设置并行度是大于1，虽然可以生成success文件，但是貌似不是第一次checkpoint结束的时候就生成了，我反复测试之后，好像是不固定的时间，比如可能是第5次，也可能是第10次checkpoint之后才生成的。&#013;&#010;&#013;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月10日周五 下午3:35写道：&#013;&#010;&#013;&#010;&gt; Hi, Jun,&#013;&#010;&gt;&#013;&#010;&gt; 非常感谢详细充分的测试~&#013;&#010;&gt;&#013;&#010;&gt; 接下来我复现排查下~&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Fri, Jul 10, 2020 at 3:09 PM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi，jinsong：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在我的测试环境下，对于我贴出来的那个代码。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.如果source使用的有界的数据，比如bsEnv.fromElements(...)，这样会有success文件生成，如果是kafka数据，就不行。&#013;&#010;&gt; &gt; 2.如果设置程序的并行度是大于1，那么也会有success生成。&#013;&#010;&gt; &gt; 3.如果我写入的是local file，比如 file:///tmp/aaa ，而不是hdfs，也会有success文件生成。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 综上，在并行度设置为1，消费的是kafka的永不停止的数据，写入的是hdfs，我的checkpoint设置是10s，这种情况下，我测试了好多遍，都没有success文件生成。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月10日周五 下午2:54写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi,&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 默认情况下，对ORC来说，理论上一旦有正式数据文件的生成，就会有对应SUCCESS文件产生，你是怎么确认没有SUCCESS文件的呢？&#013;&#010;&gt; &gt; &gt; 我用同样SQL在我的环境是有的。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Jingsong&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; On Fri, Jul 10, 2020 at 9:07 AM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;&gt; &gt; &gt; wrote:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 大家好：&#013;&#010;&gt; &gt; &gt; &gt;  我在用flink 1.11 的sql从kafka消费然后写入hdfs的过程中，发现没法自动提交分区，请问这个是什么原因呢？谢谢&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我的checkpoint设置了间隔10s，对于如下的配置，正常应该是每隔10在hdfs相应的分区下会有_SUCCESS文件，但是实际上过了好久也没有，ORC格式的结果数据是正常写入了。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; public static void main(String[] args) throws Exception{&#013;&#010;&gt; &gt; &gt; &gt; StreamExecutionEnvironment bsEnv =&#013;&#010;&gt; &gt; &gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; &gt; &gt; &gt; bsEnv.enableCheckpointing(10000);&#013;&#010;&gt; &gt; &gt; &gt; bsEnv.setParallelism(1);&#013;&#010;&gt; &gt; &gt; &gt; StreamTableEnvironment tEnv = StreamTableEnvironment.create(bsEnv);&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; String sqlSource = \"CREATE TABLE  source_kafka (\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"    appName  STRING,\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"    appVersion STRING,\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"    uploadTime STRING\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \") WITH (\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'connector.type' = 'kafka',       \\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'connector.version' = '0.10',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'connector.topic' = 'mytest',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'connector.properties.zookeeper.connect' =&#013;&#010;&gt; &gt; &gt; &gt; 'localhost:2181',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'connector.properties.bootstrap.servers' =&#013;&#010;&gt; &gt; &gt; &gt; 'localhost:9092',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'connector.properties.group.id' =&#013;&#010;&gt; &gt; 'testGroup',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'format.type'='json',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                   \"  'update-mode' = 'append' )\";&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; tEnv.executeSql(sqlSource);&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; String sql = \"CREATE TABLE fs_table (\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;             \"    appName  STRING,\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;             \"    appVersion STRING,\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;             \"    uploadTime STRING,\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;             \"  dt STRING,\" +&#013;&#010;&gt; &gt; &gt; &gt;             \"  h string\" +&#013;&#010;&gt; &gt; &gt; &gt;             \")  PARTITIONED BY (dt,h)  WITH (\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;             \"  'connector'='filesystem',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                      \"  'path'='hdfs://localhost/tmp/',\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                      \" 'sink.partition-commit.policy.kind' =&#013;&#010;&gt; &gt; &gt; &gt; 'success-file', \" +&#013;&#010;&gt; &gt; &gt; &gt;                      \"  'format'='orc'\\n\" +&#013;&#010;&gt; &gt; &gt; &gt;                      \")\";&#013;&#010;&gt; &gt; &gt; &gt; tEnv.executeSql(sql);&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; String insertSql = \"insert into  fs_table SELECT appName&#013;&#010;&gt; &gt; &gt; &gt; ,appVersion,uploadTime, \" +&#013;&#010;&gt; &gt; &gt; &gt;                   \" DATE_FORMAT(LOCALTIMESTAMP, 'yyyy-MM-dd'),&#013;&#010;&gt; &gt; &gt; &gt; DATE_FORMAT(LOCALTIMESTAMP, 'HH') FROM source_kafka\";&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; tEnv.executeSql(insertSql);&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; }&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; --&#013;&#010;&gt; &gt; &gt; Best, Jingsong Lee&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<7e73c95f-a469-4a24-96b4-3fedb81eb25a.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 09:39:16 GMT",
        "subject": "回复：flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "你好,&#010;我这边同样的代码,并没有出现类似的问题&#010;是本地跑么,可以提供下日志信息么?&#010;&#010;",
        "depth": "5",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LAJgraXK=exJ4DpRuejgzRaYuzFth-XMbyNeZxaSfad1Q@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 03:15:23 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "hi，夏帅：&#013;&#010;抱歉，这几天没搞这个，我这个问题是文件是正常写入hdfs了，但是没有自动提交，也没有错误日志，就是如果写入的是文件系统，没有SUCCESS文件，写入hive的话，没有自动更新分区。&#013;&#010;&#013;&#010;你测试没有问题的情况并行度是 1 吗？写入hdfs？&#013;&#010;&#013;&#010;夏帅 &lt;jkillers@dingtalk.com&gt; 于2020年7月10日周五 下午5:39写道：&#013;&#010;&#013;&#010;&gt; 你好,&#013;&#010;&gt; 我这边同样的代码,并没有出现类似的问题&#013;&#010;&gt; 是本地跑么,可以提供下日志信息么?&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LCaeftAGpSSZSOtnnYxHxJGM0O4BsOom8tOGqxEKQgEZg@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 03:34:44 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "hi,jinsong:&#013;&#010;这个问题不知道你后来有没有做过测试，我这里一直不行，就是并发度是1的时候，文件写入是正常的，就是没有生成success文件，如果是hive的话，就没有自动生成分区和更新分区数据。&#013;&#010;&#013;&#010;Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月23日周四 上午11:15写道：&#013;&#010;&#013;&#010;&gt; hi，夏帅：&#013;&#010;&gt;&#013;&#010;&gt; 抱歉，这几天没搞这个，我这个问题是文件是正常写入hdfs了，但是没有自动提交，也没有错误日志，就是如果写入的是文件系统，没有SUCCESS文件，写入hive的话，没有自动更新分区。&#013;&#010;&gt;&#013;&#010;&gt; 你测试没有问题的情况并行度是 1 吗？写入hdfs？&#013;&#010;&gt;&#013;&#010;&gt; 夏帅 &lt;jkillers@dingtalk.com&gt; 于2020年7月10日周五 下午5:39写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 你好,&#013;&#010;&gt;&gt; 我这边同样的代码,并没有出现类似的问题&#013;&#010;&gt;&gt; 是本地跑么,可以提供下日志信息么?&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LDQmc2m2_=_9sjZFFhWQp1EGMJWoFech8K89GAVKRQR0A@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 03:41:37 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "hi,jinsong:&#013;&#010;这个问题不知道你后来有没有做过测试，我这里一直不行，就是并发度是1的时候，文件写入是正常的，就是没有生成success文件，如果是hive的话，就没有自动生成分区和更新分区数据。&#013;&#010;&#013;&#010;Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月23日周四 上午11:34写道：&#013;&#010;&#013;&#010;&gt; hi,jinsong:&#013;&#010;&gt;&#013;&#010;&gt; 这个问题不知道你后来有没有做过测试，我这里一直不行，就是并发度是1的时候，文件写入是正常的，就是没有生成success文件，如果是hive的话，就没有自动生成分区和更新分区数据。&#013;&#010;&gt;&#013;&#010;&gt; Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月23日周四 上午11:15写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; hi，夏帅：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 抱歉，这几天没搞这个，我这个问题是文件是正常写入hdfs了，但是没有自动提交，也没有错误日志，就是如果写入的是文件系统，没有SUCCESS文件，写入hive的话，没有自动更新分区。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 你测试没有问题的情况并行度是 1 吗？写入hdfs？&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 夏帅 &lt;jkillers@dingtalk.com&gt; 于2020年7月10日周五 下午5:39写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 你好,&#013;&#010;&gt;&gt;&gt; 我这边同样的代码,并没有出现类似的问题&#013;&#010;&gt;&gt;&gt; 是本地跑么,可以提供下日志信息么?&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jQ0padpsjk2U2A2yqKe+7vy4soyEkVumR4AhZmbsPZRtg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 03:45:29 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "相同操作我也没有复现。。是可以成功执行的&#013;&#010;&#013;&#010;你的HDFS是什么版本？是否可以考虑换个来测试下&#013;&#010;&#013;&#010;On Thu, Jul 23, 2020 at 11:34 AM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;wrote:&#013;&#010;&#013;&#010;&gt; hi,jinsong:&#013;&#010;&gt;&#013;&#010;&gt; 这个问题不知道你后来有没有做过测试，我这里一直不行，就是并发度是1的时候，文件写入是正常的，就是没有生成success文件，如果是hive的话，就没有自动生成分区和更新分区数据。&#013;&#010;&gt;&#013;&#010;&gt; Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月23日周四 上午11:15写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; hi，夏帅：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 抱歉，这几天没搞这个，我这个问题是文件是正常写入hdfs了，但是没有自动提交，也没有错误日志，就是如果写入的是文件系统，没有SUCCESS文件，写入hive的话，没有自动更新分区。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 你测试没有问题的情况并行度是 1 吗？写入hdfs？&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 夏帅 &lt;jkillers@dingtalk.com&gt; 于2020年7月10日周五 下午5:39写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 你好,&#013;&#010;&gt;&gt;&gt; 我这边同样的代码,并没有出现类似的问题&#013;&#010;&gt;&gt;&gt; 是本地跑么,可以提供下日志信息么?&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "8",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LB+vx5jUCxG2Agb=oyqnd-r1oDOHTpNnvWO_upQRaNNSQ@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 04:47:00 GMT",
        "subject": "Re: flink 1.11 使用sql写入hdfs无法自动提交分区",
        "content": "hi,jinsong&#013;&#010;我们生产环境hdfs是cdh 2.6的，我换了一个hadoop 3 版本的hdfs，还真没问题了，不知道是哪里出问题了。&#013;&#010;&#013;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月23日周四 上午11:45写道：&#013;&#010;&#013;&#010;&gt; 相同操作我也没有复现。。是可以成功执行的&#013;&#010;&gt;&#013;&#010;&gt; 你的HDFS是什么版本？是否可以考虑换个来测试下&#013;&#010;&gt;&#013;&#010;&gt; On Thu, Jul 23, 2020 at 11:34 AM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt;&gt; hi,jinsong:&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 这个问题不知道你后来有没有做过测试，我这里一直不行，就是并发度是1的时候，文件写入是正常的，就是没有生成success文件，如果是hive的话，就没有自动生成分区和更新分区数据。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月23日周四 上午11:15写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&gt; hi，夏帅：&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 抱歉，这几天没搞这个，我这个问题是文件是正常写入hdfs了，但是没有自动提交，也没有错误日志，就是如果写入的是文件系统，没有SUCCESS文件，写入hive的话，没有自动更新分区。&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 你测试没有问题的情况并行度是 1 吗？写入hdfs？&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 夏帅 &lt;jkillers@dingtalk.com&gt; 于2020年7月10日周五 下午5:39写道：&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt;&gt; 你好,&#013;&#010;&gt;&gt;&gt;&gt; 我这边同样的代码,并没有出现类似的问题&#013;&#010;&gt;&gt;&gt;&gt; 是本地跑么,可以提供下日志信息么?&#013;&#010;&gt;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "9",
        "reply": "<CAM2Y1LDrpdxBLUpF-ASkRTbe+EJQGtdth73Mc8-c3_tEwmwHnw@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 01:12:25 GMT",
        "subject": "flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "Hello:&#013;&#010;&#013;&#010;        在用flink sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&#013;&#010;         有什么办法不？我在建表的时候有提示过json_object 类型，但是用了又提示其他类型。&#013;&#010;&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;",
        "depth": "0",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CACaQKu7aoQw0yqaUreqUmRFD++wHCm1EbedNkPKd6=F2+QCyoQ@mail.gmail.com>",
        "from": "LakeShen &lt;shenleifight...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:03:11 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "Hi Peihui,&#013;&#010;&#013;&#010;如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&#013;&#010;{&#013;&#010;    \"a\":\"b\",&#013;&#010;    \"c\":{&#013;&#010;        \"d\":\"e\",&#013;&#010;        \"g\":\"f\"&#013;&#010;    }&#013;&#010;}，&#013;&#010;&#013;&#010;那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&#013;&#010;create table xxx (&#013;&#010;a varchar,&#013;&#010;c row&lt;d varchar , g varchar&gt;&#013;&#010;)&#013;&#010;&#013;&#010;如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&#013;&#010;Best,&#013;&#010;LakeShen&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午9:12写道：&#013;&#010;&#013;&#010;&gt; Hello:&#013;&#010;&gt;&#013;&#010;&gt;         在用flink sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt;&#013;&#010;&gt;          有什么办法不？我在建表的时候有提示过json_object 类型，但是用了又提示其他类型。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpaG7aVAgATsOe9W3Gu_4Sh0YLChybLUS+iqs6ywPAFY2A@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:12:25 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "Hello,&#013;&#010;&#013;&#010;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:03写道：&#013;&#010;&#013;&#010;&gt; Hi Peihui,&#013;&#010;&gt;&#013;&#010;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt;&#013;&#010;&gt; {&#013;&#010;&gt;     \"a\":\"b\",&#013;&#010;&gt;     \"c\":{&#013;&#010;&gt;         \"d\":\"e\",&#013;&#010;&gt;         \"g\":\"f\"&#013;&#010;&gt;     }&#013;&#010;&gt; }，&#013;&#010;&gt;&#013;&#010;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt;&#013;&#010;&gt; create table xxx (&#013;&#010;&gt; a varchar,&#013;&#010;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt; )&#013;&#010;&gt;&#013;&#010;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; LakeShen&#013;&#010;&gt;&#013;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午9:12写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hello:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;         在用flink sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;          有什么办法不？我在建表的时候有提示过json_object 类型，但是用了又提示其他类型。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best wishes.&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpYsCGVU6gVPwmpuPHM7u5Yvu=Mz6tEdPq9tu6mSV_YknA@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:16:08 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "Hello，&#013;&#010;&#013;&#010;       实际上，我也并不太关心这个字段的内容，能按string 保存下来就好了。&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#013;&#010;&#013;&#010;&gt; Hello,&#013;&#010;&gt;&#013;&#010;&gt;        明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:03写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi Peihui,&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; {&#013;&#010;&gt;&gt;     \"a\":\"b\",&#013;&#010;&gt;&gt;     \"c\":{&#013;&#010;&gt;&gt;         \"d\":\"e\",&#013;&#010;&gt;&gt;         \"g\":\"f\"&#013;&#010;&gt;&gt;     }&#013;&#010;&gt;&gt; }，&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; create table xxx (&#013;&#010;&gt;&gt; a varchar,&#013;&#010;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt;&gt; )&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best,&#013;&#010;&gt;&gt; LakeShen&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午9:12写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt; Hello:&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;         在用flink sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;          有什么办法不？我在建表的时候有提示过json_object 类型，但是用了又提示其他类型。&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; Best wishes.&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<5B83F1A9-4BFA-43D6-809A-22C37D6AB00C@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:12:56 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "Hi, Peihui&#010;&#010;我理解你的需求是json中有一些复杂的字段，你不想解析，希望后续用UDTF在来解析，这个应该做不到的，现在的json&#010;format 的解析的底层实现&#010;就是按照json的标准格式解析（jackson）的，没法将一个 jsonObject解析成一个String。另外如果你jsonObject中的内容格式不确定，也不适合在Schema中声明，&#010;因为SQL 是预编译后执行的，不能做到schema里是三个field，执行时又能解析四个field。&#010;&#010;一种做法是定义复杂的jsonObject对应的ROW&lt;a INT, b STRING, …&gt; 将全部可能的字段包含进去，每条记录没有的字段解析出来的会是null，fail-on-missing-field&#010;默认关闭的，&#010;另外一种推荐你把复杂的字段在上游就转义成一个String放到json的一个field中，这样Flink解析出来就是一个String,&#010;然后query里用UDTF处理。&#010;&#010;&#010;祝好&#010;Leonard Xu&#010;&#010;&#010;&#010;&#010;&gt; 在 2020年7月10日，10:16，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Hello，&#010;&gt; &#010;&gt;       实际上，我也并不太关心这个字段的内容，能按string 保存下来就好了。&#010;&gt; &#010;&gt; Best wishes.&#010;&gt; &#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#010;&gt; &#010;&gt;&gt; Hello,&#010;&gt;&gt; &#010;&gt;&gt;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; Best wishes.&#010;&gt;&gt; &#010;&gt;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:03写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt; Hi Peihui,&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; {&#010;&gt;&gt;&gt;    \"a\":\"b\",&#010;&gt;&gt;&gt;    \"c\":{&#010;&gt;&gt;&gt;        \"d\":\"e\",&#010;&gt;&gt;&gt;        \"g\":\"f\"&#010;&gt;&gt;&gt;    }&#010;&gt;&gt;&gt; }，&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; create table xxx (&#010;&gt;&gt;&gt; a varchar,&#010;&gt;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#010;&gt;&gt;&gt; )&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt; LakeShen&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午9:12写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; Hello:&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;        在用flink sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;         有什么办法不？我在建表的时候有提示过json_object&#010;类型，但是用了又提示其他类型。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; Best wishes.&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "4",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CAELO932X01wshpcbzAopD8kPqpUh6zU8VTTrtfekaLRzarfbGg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 04:22:30 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "社区有个 issue 正在解决这个问题，可以关注一下&#013;&#010;https://issues.apache.org/jira/browse/FLINK-18002&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Fri, 10 Jul 2020 at 11:13, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi, Peihui&#013;&#010;&gt;&#013;&#010;&gt; 我理解你的需求是json中有一些复杂的字段，你不想解析，希望后续用UDTF在来解析，这个应该做不到的，现在的json&#010;format 的解析的底层实现&#013;&#010;&gt; 就是按照json的标准格式解析（jackson）的，没法将一个&#013;&#010;&gt; jsonObject解析成一个String。另外如果你jsonObject中的内容格式不确定，也不适合在Schema中声明，&#013;&#010;&gt; 因为SQL 是预编译后执行的，不能做到schema里是三个field，执行时又能解析四个field。&#013;&#010;&gt;&#013;&#010;&gt; 一种做法是定义复杂的jsonObject对应的ROW&lt;a INT, b STRING, …&gt;&#013;&#010;&gt; 将全部可能的字段包含进去，每条记录没有的字段解析出来的会是null，fail-on-missing-field&#010;默认关闭的，&#013;&#010;&gt; 另外一种推荐你把复杂的字段在上游就转义成一个String放到json的一个field中，这样Flink解析出来就是一个String,&#013;&#010;&gt; 然后query里用UDTF处理。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月10日，10:16，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hello，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       实际上，我也并不太关心这个字段的内容，能按string 保存下来就好了。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best wishes.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hello,&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:03写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Hi Peihui,&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; {&#013;&#010;&gt; &gt;&gt;&gt;    \"a\":\"b\",&#013;&#010;&gt; &gt;&gt;&gt;    \"c\":{&#013;&#010;&gt; &gt;&gt;&gt;        \"d\":\"e\",&#013;&#010;&gt; &gt;&gt;&gt;        \"g\":\"f\"&#013;&#010;&gt; &gt;&gt;&gt;    }&#013;&#010;&gt; &gt;&gt;&gt; }，&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; create table xxx (&#013;&#010;&gt; &gt;&gt;&gt; a varchar,&#013;&#010;&gt; &gt;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt; &gt;&gt;&gt; )&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Best,&#013;&#010;&gt; &gt;&gt;&gt; LakeShen&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午9:12写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; Hello:&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;        在用flink&#013;&#010;&gt; sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;         有什么办法不？我在建表的时候有提示过json_object&#010;类型，但是用了又提示其他类型。&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_QXAbKw-yB9TmZUt3QywgU4n7gZW0Uq5dnf_rP9qiUudQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 05:54:11 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "Hi Peihui,&#013;&#010;&#013;&#010;正如Jark所说，FLINK-18002正是想解决这个问题，可以指定任意一个JsonNode为varchar类型。&#013;&#010;&#013;&#010;当然，这个feature不能解决所有问题，比如你有一个字段，内容不太确定，而且也不需要额外处理，&#013;&#010;主要是想保留这个字段，下游输出json的时候仍然还是这个字段。&#013;&#010;如果用FLINK-18002的思路，输出到下游的时候，会把这部分数据整体作为一个json&#010;string，所以&#013;&#010;从结果上来看，*还不能完全做到原封不动的输出到下游*。&#013;&#010;&#013;&#010;不知道后面这个场景是不是你面对的场景。如果是的话，我们目前有两个思路解决这个问题：&#013;&#010;1. 用RAW类型，这个需要json node类型对于flink来讲，都是可以序列化的&#013;&#010;2. 用BINARY类型，因为现在已经有了对BINARY类型的处理，所以还需要额外加一个选项来指定对于BINARY类型&#013;&#010;  的处理模式。我们可以把任意json node转成它的json字符串表达形式，再转成byte[]进行中间的传输和处理；在&#013;&#010;  序列化的时候，再直接通过这个byte[]数据构造一个json node（这样可以保证它跟原来的json&#010;node一模一样）。&#013;&#010;&#013;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月10日周五 下午12:22写道：&#013;&#010;&#013;&#010;&gt; 社区有个 issue 正在解决这个问题，可以关注一下&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18002&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Fri, 10 Jul 2020 at 11:13, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi, Peihui&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我理解你的需求是json中有一些复杂的字段，你不想解析，希望后续用UDTF在来解析，这个应该做不到的，现在的json&#010;format&#013;&#010;&gt; 的解析的底层实现&#013;&#010;&gt; &gt; 就是按照json的标准格式解析（jackson）的，没法将一个&#013;&#010;&gt; &gt; jsonObject解析成一个String。另外如果你jsonObject中的内容格式不确定，也不适合在Schema中声明，&#013;&#010;&gt; &gt; 因为SQL 是预编译后执行的，不能做到schema里是三个field，执行时又能解析四个field。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 一种做法是定义复杂的jsonObject对应的ROW&lt;a INT, b STRING, …&gt;&#013;&#010;&gt; &gt; 将全部可能的字段包含进去，每条记录没有的字段解析出来的会是null，fail-on-missing-field&#010;默认关闭的，&#013;&#010;&gt; &gt; 另外一种推荐你把复杂的字段在上游就转义成一个String放到json的一个field中，这样Flink解析出来就是一个String,&#013;&#010;&gt; &gt; 然后query里用UDTF处理。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 祝好&#013;&#010;&gt; &gt; Leonard Xu&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 在 2020年7月10日，10:16，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Hello，&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;       实际上，我也并不太关心这个字段的内容，能按string&#010;保存下来就好了。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best wishes.&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; Hello,&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:03写道：&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; Hi Peihui,&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; {&#013;&#010;&gt; &gt; &gt;&gt;&gt;    \"a\":\"b\",&#013;&#010;&gt; &gt; &gt;&gt;&gt;    \"c\":{&#013;&#010;&gt; &gt; &gt;&gt;&gt;        \"d\":\"e\",&#013;&#010;&gt; &gt; &gt;&gt;&gt;        \"g\":\"f\"&#013;&#010;&gt; &gt; &gt;&gt;&gt;    }&#013;&#010;&gt; &gt; &gt;&gt;&gt; }，&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; create table xxx (&#013;&#010;&gt; &gt; &gt;&gt;&gt; a varchar,&#013;&#010;&gt; &gt; &gt;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; )&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; Best,&#013;&#010;&gt; &gt; &gt;&gt;&gt; LakeShen&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午9:12写道：&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; Hello:&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;        在用flink&#013;&#010;&gt; &gt; sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;         有什么办法不？我在建表的时候有提示过json_object&#010;类型，但是用了又提示其他类型。&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "6",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LC54ATNA8=OMz_mFzPSAqW6wLS9mV4CmF8cur7ZPTxGuQ@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 06:13:33 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "hi，大家好&#013;&#010;对于json schema的问题，我想问一个其他的问题，&#013;&#010;比如我要做一个实时报警系统，需要消费kafka的json数据来进行实时报警，我的想法是对于每一个报警都生成一个flink任务，主要报警逻辑翻译成一个flink&#013;&#010;sql。&#013;&#010;&#013;&#010;其中kafka里面的json数据，每一个字段都是可以生成报警条件的，比如有一个json格式的header字段，这个字段里面的内容是不固定的，&#013;&#010;某一个用户想用header.aaa字段，另一个用户想用header.bbb字段，比如每分钟header.aaa的count值大于100就报警。&#013;&#010;&#013;&#010;这种情况下，我该如何定义我的schema呢？大家有没有什么想法，谢谢。&#013;&#010;&#013;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月10日周五 下午1:54写道：&#013;&#010;&#013;&#010;&gt; Hi Peihui,&#013;&#010;&gt;&#013;&#010;&gt; 正如Jark所说，FLINK-18002正是想解决这个问题，可以指定任意一个JsonNode为varchar类型。&#013;&#010;&gt;&#013;&#010;&gt; 当然，这个feature不能解决所有问题，比如你有一个字段，内容不太确定，而且也不需要额外处理，&#013;&#010;&gt; 主要是想保留这个字段，下游输出json的时候仍然还是这个字段。&#013;&#010;&gt; 如果用FLINK-18002的思路，输出到下游的时候，会把这部分数据整体作为一个json&#010;string，所以&#013;&#010;&gt; 从结果上来看，*还不能完全做到原封不动的输出到下游*。&#013;&#010;&gt;&#013;&#010;&gt; 不知道后面这个场景是不是你面对的场景。如果是的话，我们目前有两个思路解决这个问题：&#013;&#010;&gt; 1. 用RAW类型，这个需要json node类型对于flink来讲，都是可以序列化的&#013;&#010;&gt; 2. 用BINARY类型，因为现在已经有了对BINARY类型的处理，所以还需要额外加一个选项来指定对于BINARY类型&#013;&#010;&gt;   的处理模式。我们可以把任意json node转成它的json字符串表达形式，再转成byte[]进行中间的传输和处理；在&#013;&#010;&gt;   序列化的时候，再直接通过这个byte[]数据构造一个json node（这样可以保证它跟原来的json&#010;node一模一样）。&#013;&#010;&gt;&#013;&#010;&gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月10日周五 下午12:22写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 社区有个 issue 正在解决这个问题，可以关注一下&#013;&#010;&gt; &gt; https://issues.apache.org/jira/browse/FLINK-18002&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Fri, 10 Jul 2020 at 11:13, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi, Peihui&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 我理解你的需求是json中有一些复杂的字段，你不想解析，希望后续用UDTF在来解析，这个应该做不到的，现在的json&#010;format&#013;&#010;&gt; &gt; 的解析的底层实现&#013;&#010;&gt; &gt; &gt; 就是按照json的标准格式解析（jackson）的，没法将一个&#013;&#010;&gt; &gt; &gt; jsonObject解析成一个String。另外如果你jsonObject中的内容格式不确定，也不适合在Schema中声明，&#013;&#010;&gt; &gt; &gt; 因为SQL 是预编译后执行的，不能做到schema里是三个field，执行时又能解析四个field。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 一种做法是定义复杂的jsonObject对应的ROW&lt;a INT, b STRING, …&gt;&#013;&#010;&gt; &gt; &gt; 将全部可能的字段包含进去，每条记录没有的字段解析出来的会是null，fail-on-missing-field&#010;默认关闭的，&#013;&#010;&gt; &gt; &gt; 另外一种推荐你把复杂的字段在上游就转义成一个String放到json的一个field中，这样Flink解析出来就是一个String,&#013;&#010;&gt; &gt; &gt; 然后query里用UDTF处理。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 祝好&#013;&#010;&gt; &gt; &gt; Leonard Xu&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在 2020年7月10日，10:16，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hello，&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;       实际上，我也并不太关心这个字段的内容，能按string&#010;保存下来就好了。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; Hello,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五&#010;上午10:03写道：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; Hi Peihui,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; {&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;    \"a\":\"b\",&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;    \"c\":{&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;        \"d\":\"e\",&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;        \"g\":\"f\"&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;    }&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; }，&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; create table xxx (&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; a varchar,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; )&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; Best,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; LakeShen&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五&#010;上午9:12写道：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; Hello:&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;        在用flink&#013;&#010;&gt; &gt; &gt; sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;         有什么办法不？我在建表的时候有提示过json_object&#010;类型，但是用了又提示其他类型。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpbnhDNSsw6fDQxKwiEi=pk4GWWdzsma_cgTVK4XTHTWbg@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 01:59:01 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "Hi BenChao,&#013;&#010;&#013;&#010;请问第2个解决思路中 额外加一个选项是指什么呢？&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月10日周五 下午1:54写道：&#013;&#010;&#013;&#010;&gt; Hi Peihui,&#013;&#010;&gt;&#013;&#010;&gt; 正如Jark所说，FLINK-18002正是想解决这个问题，可以指定任意一个JsonNode为varchar类型。&#013;&#010;&gt;&#013;&#010;&gt; 当然，这个feature不能解决所有问题，比如你有一个字段，内容不太确定，而且也不需要额外处理，&#013;&#010;&gt; 主要是想保留这个字段，下游输出json的时候仍然还是这个字段。&#013;&#010;&gt; 如果用FLINK-18002的思路，输出到下游的时候，会把这部分数据整体作为一个json&#010;string，所以&#013;&#010;&gt; 从结果上来看，*还不能完全做到原封不动的输出到下游*。&#013;&#010;&gt;&#013;&#010;&gt; 不知道后面这个场景是不是你面对的场景。如果是的话，我们目前有两个思路解决这个问题：&#013;&#010;&gt; 1. 用RAW类型，这个需要json node类型对于flink来讲，都是可以序列化的&#013;&#010;&gt; 2. 用BINARY类型，因为现在已经有了对BINARY类型的处理，所以还需要额外加一个选项来指定对于BINARY类型&#013;&#010;&gt;   的处理模式。我们可以把任意json node转成它的json字符串表达形式，再转成byte[]进行中间的传输和处理；在&#013;&#010;&gt;   序列化的时候，再直接通过这个byte[]数据构造一个json node（这样可以保证它跟原来的json&#010;node一模一样）。&#013;&#010;&gt;&#013;&#010;&gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月10日周五 下午12:22写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 社区有个 issue 正在解决这个问题，可以关注一下&#013;&#010;&gt; &gt; https://issues.apache.org/jira/browse/FLINK-18002&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Fri, 10 Jul 2020 at 11:13, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi, Peihui&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 我理解你的需求是json中有一些复杂的字段，你不想解析，希望后续用UDTF在来解析，这个应该做不到的，现在的json&#010;format&#013;&#010;&gt; &gt; 的解析的底层实现&#013;&#010;&gt; &gt; &gt; 就是按照json的标准格式解析（jackson）的，没法将一个&#013;&#010;&gt; &gt; &gt; jsonObject解析成一个String。另外如果你jsonObject中的内容格式不确定，也不适合在Schema中声明，&#013;&#010;&gt; &gt; &gt; 因为SQL 是预编译后执行的，不能做到schema里是三个field，执行时又能解析四个field。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 一种做法是定义复杂的jsonObject对应的ROW&lt;a INT, b STRING, …&gt;&#013;&#010;&gt; &gt; &gt; 将全部可能的字段包含进去，每条记录没有的字段解析出来的会是null，fail-on-missing-field&#010;默认关闭的，&#013;&#010;&gt; &gt; &gt; 另外一种推荐你把复杂的字段在上游就转义成一个String放到json的一个field中，这样Flink解析出来就是一个String,&#013;&#010;&gt; &gt; &gt; 然后query里用UDTF处理。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 祝好&#013;&#010;&gt; &gt; &gt; Leonard Xu&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在 2020年7月10日，10:16，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hello，&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;       实际上，我也并不太关心这个字段的内容，能按string&#010;保存下来就好了。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; Hello,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五&#010;上午10:03写道：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; Hi Peihui,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; {&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;    \"a\":\"b\",&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;    \"c\":{&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;        \"d\":\"e\",&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;        \"g\":\"f\"&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;    }&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; }，&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; create table xxx (&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; a varchar,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; )&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; Best,&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; LakeShen&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五&#010;上午9:12写道：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; Hello:&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;        在用flink&#013;&#010;&gt; &gt; &gt; sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;         有什么办法不？我在建表的时候有提示过json_object&#010;类型，但是用了又提示其他类型。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_TDOuM370xu0D5VV9hEBNG_KnuHtUqUcYu0O81+QNbU2Q@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:17:07 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "就是现在Flink json已经有了对于VARBINARY类型的处理逻辑，就是string和byte[]互转，然后还需要有base64编码。&#013;&#010;&#013;&#010;但是我们是想让对于VARBINARY的处理逻辑变成另外一种形式，就是把JsonNode直接toString，获取这个json子树的&#013;&#010;字符串表示，然后再转成byte[]来作为这个字段。输出的时候，也会直接通过这个byte[]数据来构造一个JsonNode树，&#013;&#010;然后放到对应的位置上。也就做到了一个json节点原封不动的保留到了输出里面，不管它是一个什么类型的json节点。&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月15日周三 上午9:59写道：&#013;&#010;&#013;&#010;&gt; Hi BenChao,&#013;&#010;&gt;&#013;&#010;&gt; 请问第2个解决思路中 额外加一个选项是指什么呢？&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月10日周五 下午1:54写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Peihui,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 正如Jark所说，FLINK-18002正是想解决这个问题，可以指定任意一个JsonNode为varchar类型。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 当然，这个feature不能解决所有问题，比如你有一个字段，内容不太确定，而且也不需要额外处理，&#013;&#010;&gt; &gt; 主要是想保留这个字段，下游输出json的时候仍然还是这个字段。&#013;&#010;&gt; &gt; 如果用FLINK-18002的思路，输出到下游的时候，会把这部分数据整体作为一个json&#010;string，所以&#013;&#010;&gt; &gt; 从结果上来看，*还不能完全做到原封不动的输出到下游*。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 不知道后面这个场景是不是你面对的场景。如果是的话，我们目前有两个思路解决这个问题：&#013;&#010;&gt; &gt; 1. 用RAW类型，这个需要json node类型对于flink来讲，都是可以序列化的&#013;&#010;&gt; &gt; 2. 用BINARY类型，因为现在已经有了对BINARY类型的处理，所以还需要额外加一个选项来指定对于BINARY类型&#013;&#010;&gt; &gt;   的处理模式。我们可以把任意json node转成它的json字符串表达形式，再转成byte[]进行中间的传输和处理；在&#013;&#010;&gt; &gt;   序列化的时候，再直接通过这个byte[]数据构造一个json node（这样可以保证它跟原来的json&#010;node一模一样）。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月10日周五 下午12:22写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 社区有个 issue 正在解决这个问题，可以关注一下&#013;&#010;&gt; &gt; &gt; https://issues.apache.org/jira/browse/FLINK-18002&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Jark&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; On Fri, 10 Jul 2020 at 11:13, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hi, Peihui&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 我理解你的需求是json中有一些复杂的字段，你不想解析，希望后续用UDTF在来解析，这个应该做不到的，现在的json&#010;format&#013;&#010;&gt; &gt; &gt; 的解析的底层实现&#013;&#010;&gt; &gt; &gt; &gt; 就是按照json的标准格式解析（jackson）的，没法将一个&#013;&#010;&gt; &gt; &gt; &gt; jsonObject解析成一个String。另外如果你jsonObject中的内容格式不确定，也不适合在Schema中声明，&#013;&#010;&gt; &gt; &gt; &gt; 因为SQL 是预编译后执行的，不能做到schema里是三个field，执行时又能解析四个field。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 一种做法是定义复杂的jsonObject对应的ROW&lt;a INT, b STRING,&#010;…&gt;&#013;&#010;&gt; &gt; &gt; &gt; 将全部可能的字段包含进去，每条记录没有的字段解析出来的会是null，fail-on-missing-field&#010;默认关闭的，&#013;&#010;&gt; &gt; &gt; &gt; 另外一种推荐你把复杂的字段在上游就转义成一个String放到json的一个field中，这样Flink解析出来就是一个String,&#013;&#010;&gt; &gt; &gt; &gt; 然后query里用UDTF处理。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 祝好&#013;&#010;&gt; &gt; &gt; &gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020年7月10日，10:16，Peihui He &lt;peihuihe@gmail.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; Hello，&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;       实际上，我也并不太关心这个字段的内容，能按string&#010;保存下来就好了。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Hello,&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五&#010;上午10:03写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; Hi Peihui,&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; {&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;    \"a\":\"b\",&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;    \"c\":{&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;        \"d\":\"e\",&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;        \"g\":\"f\"&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;    }&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; }，&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; create table xxx (&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; a varchar,&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; )&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; Best,&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; LakeShen&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五&#010;上午9:12写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; Hello:&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;        在用flink&#013;&#010;&gt; &gt; &gt; &gt; sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;         有什么办法不？我在建表的时候有提示过json_object&#010;类型，但是用了又提示其他类型。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Benchao Li&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "8",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpa8YqveQgwckLgwHFUYwfV_rvpEwwgpSPiR-HzC1yQarQ@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 06:26:47 GMT",
        "subject": "Re: flink 1.10 sql kafka format json 定制schema时, 一个字段的数据可以定义为类似json object不？",
        "content": "感谢，已经按后面一种方式做了🤗&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月10日周五 上午11:13写道：&#013;&#010;&#013;&#010;&gt; Hi, Peihui&#013;&#010;&gt;&#013;&#010;&gt; 我理解你的需求是json中有一些复杂的字段，你不想解析，希望后续用UDTF在来解析，这个应该做不到的，现在的json&#010;format 的解析的底层实现&#013;&#010;&gt; 就是按照json的标准格式解析（jackson）的，没法将一个&#013;&#010;&gt; jsonObject解析成一个String。另外如果你jsonObject中的内容格式不确定，也不适合在Schema中声明，&#013;&#010;&gt; 因为SQL 是预编译后执行的，不能做到schema里是三个field，执行时又能解析四个field。&#013;&#010;&gt;&#013;&#010;&gt; 一种做法是定义复杂的jsonObject对应的ROW&lt;a INT, b STRING, …&gt;&#013;&#010;&gt; 将全部可能的字段包含进去，每条记录没有的字段解析出来的会是null，fail-on-missing-field&#010;默认关闭的，&#013;&#010;&gt; 另外一种推荐你把复杂的字段在上游就转义成一个String放到json的一个field中，这样Flink解析出来就是一个String,&#013;&#010;&gt; 然后query里用UDTF处理。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月10日，10:16，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hello，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       实际上，我也并不太关心这个字段的内容，能按string 保存下来就好了。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best wishes.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午10:12写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hello,&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;       明白您的意思。但是当一个字段下的json 字段不确定，类似一个黑盒子一样的化，就不好定义了。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月10日周五 上午10:03写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Hi Peihui,&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 如果消费的 Kafka json 中，json 比较复杂的话，比如存在嵌套，就像下面的格式：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; {&#013;&#010;&gt; &gt;&gt;&gt;    \"a\":\"b\",&#013;&#010;&gt; &gt;&gt;&gt;    \"c\":{&#013;&#010;&gt; &gt;&gt;&gt;        \"d\":\"e\",&#013;&#010;&gt; &gt;&gt;&gt;        \"g\":\"f\"&#013;&#010;&gt; &gt;&gt;&gt;    }&#013;&#010;&gt; &gt;&gt;&gt; }，&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 那么在 kafka table source 可以使用 row 来定义:&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; create table xxx (&#013;&#010;&gt; &gt;&gt;&gt; a varchar,&#013;&#010;&gt; &gt;&gt;&gt; c row&lt;d varchar , g varchar&gt;&#013;&#010;&gt; &gt;&gt;&gt; )&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 如果 还存在嵌套，可以继续再使用 Row 来定义。&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Best,&#013;&#010;&gt; &gt;&gt;&gt; LakeShen&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月10日周五 上午9:12写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; Hello:&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;        在用flink&#013;&#010;&gt; sql从kafka消费数据时，有些json比较复杂，想直接定义为object，在后续通过udf转为string。&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;         有什么办法不？我在建表的时候有提示过json_object&#010;类型，但是用了又提示其他类型。&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAGR9zpYxJmCXyOne7oVtNDpAbowbbM32GPwBM+OLmKJHWJrcFA@mail.gmail.com>"
    },
    {
        "id": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:51:07 GMT",
        "subject": "flink 写入es失败导致数据丢失",
        "content": "hi，&#010;我们现在使用flink消费kafka数据写到es，今天发现在默认设置下，es服务挂掉时，这段时间写入es失败，但是没有积压，而是数据丢失了。这个应该不符合预期。想问下可能是啥原因造成的。",
        "depth": "0",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAA8tFvu0003YvCB7io-qqsePvz19zyFf8HwmEDmGu0ayO9naAw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:00:53 GMT",
        "subject": "Re: flink 写入es失败导致数据丢失",
        "content": "Hi&#013;&#010;&#013;&#010;你 ES Sink 是自己写的，还是用的社区的呢？社区的使用了哪个版本，以及配置是啥样的呢&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月10日周五 上午10:51写道：&#013;&#010;&#013;&#010;&gt; hi，&#013;&#010;&gt;&#013;&#010;&gt; 我们现在使用flink消费kafka数据写到es，今天发现在默认设置下，es服务挂掉时，这段时间写入es失败，但是没有积压，而是数据丢失了。这个应该不符合预期。想问下可能是啥原因造成的。&#013;&#010;",
        "depth": "1",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<143e981b.273f.17336aff488.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:06:07 GMT",
        "subject": "Re:Re: flink 写入es失败导致数据丢失",
        "content": "hi&#010;我使用社区默认的ES，主要配置如下：我使用flink 1.10.1，blink-planner。使用了ES6的sink。&#010;我看了下文档，默认有个参数是 connector.failure-handler,是fail。我也能在TM日志里看到连接es失败的报错，但是整个任务checkpoint并没有失败。数据丢了。&#010;&#010;&#010;WITH (&#010;'connector.type' = 'elasticsearch',&#010;'connector.version' = '&lt;ES_YUNTU.VERSION&gt;',&#010;'connector.hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;'connector.index' = 'realtime_fund_product_all_sell',&#010;'connector.document-type' = '_doc',&#010;'update-mode' = 'upsert',&#010;'connector.key-delimiter' = '$',&#010;'connector.key-null-literal' = 'n/a',&#010;'connector.bulk-flush.interval' = '1000',&#010;'format.type' = 'json'&#010;)&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-10 11:00:53，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;Hi&#010;&gt;&#010;&gt;你 ES Sink 是自己写的，还是用的社区的呢？社区的使用了哪个版本，以及配置是啥样的呢&#010;&gt;&#010;&gt;Best,&#010;&gt;Congxian&#010;&gt;&#010;&gt;&#010;&gt;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月10日周五 上午10:51写道：&#010;&gt;&#010;&gt;&gt; hi，&#010;&gt;&gt;&#010;&gt;&gt; 我们现在使用flink消费kafka数据写到es，今天发现在默认设置下，es服务挂掉时，这段时间写入es失败，但是没有积压，而是数据丢失了。这个应该不符合预期。想问下可能是啥原因造成的。&#010;",
        "depth": "2",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<17B2E8CB-BFAA-46EF-B8D2-B19F6534DF86@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:16:17 GMT",
        "subject": "Re: flink 写入es失败导致数据丢失",
        "content": "Hello, fulin&#013;&#010;&#013;&#010;&gt; es服务挂掉时，这段时间写入es失败，但是没有积压，而是数据丢失了。这个应该不符合预期。想问下可能是啥原因造成的。&#013;&#010;&#013;&#010;es 服务挂掉是指es 集群不可用吗？那这时应该是写入es应该失败，作业也会失败，你说的没有积压是指什么呢？&#013;&#010;&#013;&#010;Best&#013;&#010;Leonard Xu",
        "depth": "1",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<10c27b18.2a49.17336bd5eee.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:20:47 GMT",
        "subject": "Re:Re: flink 写入es失败导致数据丢失",
        "content": "&#010;&#010;&#010;hi，Leonard&#010;是的。es集群服务不可用。我能观察到写入es失败，但是作业确实没失败。等到es集群服务恢复后，作业也正常了，但是故障期间的数据有丢失。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-10 11:16:17，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hello, fulin&#010;&gt;&#010;&gt;&gt; es服务挂掉时，这段时间写入es失败，但是没有积压，而是数据丢失了。这个应该不符合预期。想问下可能是啥原因造成的。&#010;&gt;&#010;&gt;es 服务挂掉是指es 集群不可用吗？那这时应该是写入es应该失败，作业也会失败，你说的没有积压是指什么呢？&#010;&gt;&#010;&gt;Best&#010;&gt;Leonard Xu&#010;",
        "depth": "2",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<E5D9103E-48CA-4674-9F8A-9B32AFEC4C63@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:33:25 GMT",
        "subject": "Re: flink 写入es失败导致数据丢失",
        "content": "Hi，&#013;&#010;我理解作业应该失败才对，你本地可以复现吗？可以复现的话可以在社区建个issue。&#013;&#010;&#013;&#010;Best,&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&gt; 在 2020年7月10日，11:20，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; ，但是作业确实没失败。&#013;&#010;&#013;&#010;",
        "depth": "3",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAA8tFvvWsMstg+OnHAHup5ztsW7KDWmv-Lq8ZF_+pvuP9sKwcA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 03:43:33 GMT",
        "subject": "Re: flink 写入es失败导致数据丢失",
        "content": "Hi&#013;&#010;&#013;&#010;从官方文档的配置[1] 来看，对于 handle failure 来说，默认是 fail，也就是说&#010;request 失败了会导致作业失败的，可以尝试在&#013;&#010;log 中看能否找到这个日志，或者显示的设置成 fail 看看。如果发现 handle&#010;failure 是 fail 的情况下不符合预期，可以想&#013;&#010;Leonard 说的那样建立一个 issue 来追踪这个问题&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#connector-options&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月10日周五 上午11:33写道：&#013;&#010;&#013;&#010;&gt; Hi，&#013;&#010;&gt; 我理解作业应该失败才对，你本地可以复现吗？可以复现的话可以在社区建个issue。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月10日，11:20，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ，但是作业确实没失败。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<382d5517.33e4.17336edcb3e.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 04:13:40 GMT",
        "subject": "Re:Re: flink 写入es失败导致数据丢失",
        "content": "&#010;&#010;&#010;&#010;&#010;&#010;hi,&#010;感谢两位的回复。我先来本地复现下。如果有问题的话，会建个issue。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-10 11:43:33，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;Hi&#010;&gt;&#010;&gt;从官方文档的配置[1] 来看，对于 handle failure 来说，默认是 fail，也就是说&#010;request 失败了会导致作业失败的，可以尝试在&#010;&gt;log 中看能否找到这个日志，或者显示的设置成 fail 看看。如果发现&#010;handle failure 是 fail 的情况下不符合预期，可以想&#010;&gt;Leonard 说的那样建立一个 issue 来追踪这个问题&#010;&gt;&#010;&gt;[1]&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#connector-options&#010;&gt;Best,&#010;&gt;Congxian&#010;&gt;&#010;&gt;&#010;&gt;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月10日周五 上午11:33写道：&#010;&gt;&#010;&gt;&gt; Hi，&#010;&gt;&gt; 我理解作业应该失败才对，你本地可以复现吗？可以复现的话可以在社区建个issue。&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt;&#010;&gt;&gt; &gt; 在 2020年7月10日，11:20，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; ，但是作业确实没失败。&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "5",
        "reply": "<609d2604.23f8.17336a23657.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<202007101427474349285@gmail.com>",
        "from": "&quot;xiaozhennan1995@gmail.com&quot; &lt;xiaozhennan1...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 06:27:51 GMT",
        "subject": "Flink自定义Source 抽取Mysql多张表,并行抽取可以实现吗?",
        "content": "场景: 同时每隔30s 抽取N张Mysql的数据表,根据时间字段抽取&#013;&#010;自定义的Source 能否通过设置Source的并行度  比如有十张表,设置并行度为3&#010;  根据并行度动态分配抽取的表 类似根据并行度和kafka的分区的关系&#013;&#010;在一个source里,所有数据混合一起发往下游&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;xiaozhennan1995@gmail.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007101427474349285@gmail.com>"
    },
    {
        "id": "<1594367317479-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 07:48:37 GMT",
        "subject": "单流的一条数据，需要先sink 至mysql，再sink至kafka，并保证两sink的原子性以及sink顺序，是否可以做到？",
        "content": "请问下，有没有大佬做过类似的事情？&#013;&#010;&#013;&#010;另外，flink side out功能，可以将单流分成多流，但是不是分成多流后，但两条流sink的时候，是不是没法保证sink时候的时序？&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "0",
        "reply": "<1594367317479-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594368752256-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 08:12:32 GMT",
        "subject": "Re: 单流的一条数据，需要先sink 至mysql，再sink至kafka，并保证两sink的原子性以及sink顺序，是否可以做到？",
        "content": "我也有类似的需求。&#013;&#010;期望第一个sink能先执行，然后第二个sink再执行。因为第二个sink要去读第一个sink保存的数据。&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "1",
        "reply": "<1594367317479-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO933iRLwtoQ18Q_1F+UNqPbJWAa8pAtBA=j=o7nCqnECLMw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 02:47:42 GMT",
        "subject": "Re: 单流的一条数据，需要先sink 至mysql，再sink至kafka，并保证两sink的原子性以及sink顺序，是否可以做到？",
        "content": "你可以先用 map 再用 addSink，这样他们的调用被 chain 在一起，可以达到先写入&#010;mysql ，再写入 kafka 的目的。&#013;&#010;&#013;&#010; datastream.map(new MySQLSinkMapFunction()).addSink(new&#013;&#010;FlinkKafkaProducer()).&#013;&#010;&#013;&#010;也就是将 mysql sink 伪装成了一个 MapFunction，里面先做了 写 mysql 的动作，写成功后再将数据输出到下游。&#013;&#010;&#013;&#010;另外，如果要在 SQL 中解决这个需求的话，会比较麻烦，因为标准语法中没有这么个语法支持这个功能。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Fri, 10 Jul 2020 at 16:12, lgs &lt;9925174@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 我也有类似的需求。&#013;&#010;&gt; 期望第一个sink能先执行，然后第二个sink再执行。因为第二个sink要去读第一个sink保存的数据。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;",
        "depth": "2",
        "reply": "<1594367317479-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594625761721-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:36:01 GMT",
        "subject": "Re: 单流的一条数据，需要先sink 至mysql，再sink至kafka，并保证两sink的原子性以及sink顺序，是否可以做到？",
        "content": "&#013;&#010;&#013;&#010;如果可以chain在一起，这个可以保证顺序性，我去试试。&#013;&#010;&#013;&#010;这里再追问下，实际中，如里单流源里的数据也要顺序处理，可以设置并行度为1；&#013;&#010;&#013;&#010;这里可能也要考虑下mysqlSinkFunction里的sink效率问题，需要积累一些数据再sink，这里可以做到跟kafka&#010;sink的batch_size和linger.ms配合联动吗？比如满1000条，或超过1s，就同时sink?&#013;&#010;&#013;&#010;谢谢~&#013;&#010;&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "3",
        "reply": "<1594367317479-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO932+cypdo5N13h8ne=nCkSqcg1osOxa+byWEMRZKYUrzXA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:30:16 GMT",
        "subject": "Re: 单流的一条数据，需要先sink 至mysql，再sink至kafka，并保证两sink的原子性以及sink顺序，是否可以做到？",
        "content": "你可以在 mysqlSinkFunction 中攒 buffer，在 timer trigger 或者 checkpoint 时 flush&#013;&#010;mysql database，以及 output。&#013;&#010;&#013;&#010;On Mon, 13 Jul 2020 at 15:36, jindy_liu &lt;286729788@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 如果可以chain在一起，这个可以保证顺序性，我去试试。&#013;&#010;&gt;&#013;&#010;&gt; 这里再追问下，实际中，如里单流源里的数据也要顺序处理，可以设置并行度为1；&#013;&#010;&gt;&#013;&#010;&gt; 这里可能也要考虑下mysqlSinkFunction里的sink效率问题，需要积累一些数据再sink，这里可以做到跟kafka&#013;&#010;&gt; sink的batch_size和linger.ms配合联动吗？比如满1000条，或超过1s，就同时sink?&#013;&#010;&gt;&#013;&#010;&gt; 谢谢~&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;",
        "depth": "4",
        "reply": "<1594367317479-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594694370508-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:39:30 GMT",
        "subject": "Re: 单流的一条数据，需要先sink 至mysql，再sink至kafka，并保证两sink的原子性以及sink顺序，是否可以做到？",
        "content": "本来还想尽最大可能的复用源码，看了下JdbcOutputFormat的源码实现，batch&#010;size是sql语句的个数据，kafka的batch&#010;size是字节数，两个协调不好，两个各sink各自的时间阈值也同步不了。&#013;&#010;&#013;&#010;我准备按你的说的方式，用RichFlatMapFunction，里面实现实现一个buffer。&#013;&#010;等buffer达阈值或定时时间条件满足时，一次性手动调用JdbcOutputFormat（可以设置更大的buffer值）的writeRecord和flush；不满足的时候，RichFlatMapFunction里不输出元素；&#013;&#010;这样kafka的batch sinka节奏应该就不用管了，两者的batch条件相互独立。&#013;&#010;我自己初步看了下，应该可以？&#013;&#010;初学者，望大佬提点，还有其它的注意事项要注意不？&#013;&#010;&#013;&#010;&#013;&#010;&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "5",
        "reply": "<1594367317479-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAJkeMpj+9dTfkHg2NvkE_zhHU_a0-2mXfPh4BO6rQtb246YunA@mail.gmail.com>",
        "from": "zilong xiao &lt;acidzz...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 07:54:30 GMT",
        "subject": "flink-benchmarks使用求助",
        "content": "如题，最近在新机器上跑flink-benchmarks验证下机器性能，但是不太会对跑出的结果进行分析，不知是否有大神也用过这个，可否指点一二&#013;&#010;",
        "depth": "0",
        "reply": "<CAJkeMpj+9dTfkHg2NvkE_zhHU_a0-2mXfPh4BO6rQtb246YunA@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvsPFDAF-iH++R97Bds7y-qzsPz_iZWp85Usbm1ianLS2g@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 11:18:13 GMT",
        "subject": "Re: flink-benchmarks使用求助",
        "content": "Hi&#013;&#010;你说的 flink-benchmarks 是指 这个仓库[1]的代码吗？ 是这个仓库的代码的话，你按照&#010;readme 能跑出一个结果（csv&#013;&#010;文件，或者终端能看到最终的结果），这个结果就是 JMH 的的结果，具体的可以阅读&#010;JMH 的相关文档[2]&#013;&#010;&#013;&#010;[1] https://github.com/dataArtisans/flink-benchmarks&#013;&#010;[2] http://openjdk.java.net/projects/code-tools/jmh/&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月10日周五 下午3:54写道：&#013;&#010;&#013;&#010;&gt; 如题，最近在新机器上跑flink-benchmarks验证下机器性能，但是不太会对跑出的结果进行分析，不知是否有大神也用过这个，可否指点一二&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAJkeMpj+9dTfkHg2NvkE_zhHU_a0-2mXfPh4BO6rQtb246YunA@mail.gmail.com>"
    },
    {
        "id": "<CAJkeMph-wQhXGweR9aq-s799RQGZo+TnPk1KpkHMXwGd2wTwrA@mail.gmail.com>",
        "from": "zilong xiao &lt;acidzz...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:32:22 GMT",
        "subject": "Re: flink-benchmarks使用求助",
        "content": "是的，用的 flink-benchmarks 代码，在跑的时候，指定参数-t max(最大工程线程)，在运行中会出现异常：&#010;`shutdown&#013;&#010;timeout of 30 seconds expired, forcing forked VM to exit`，前辈有遇到过这种情况吗？&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月10日周五 下午7:18写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt; 你说的 flink-benchmarks 是指 这个仓库[1]的代码吗？ 是这个仓库的代码的话，你按照&#010;readme 能跑出一个结果（csv&#013;&#010;&gt; 文件，或者终端能看到最终的结果），这个结果就是 JMH 的的结果，具体的可以阅读&#010;JMH 的相关文档[2]&#013;&#010;&gt;&#013;&#010;&gt; [1] https://github.com/dataArtisans/flink-benchmarks&#013;&#010;&gt; [2] http://openjdk.java.net/projects/code-tools/jmh/&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月10日周五 下午3:54写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 如题，最近在新机器上跑flink-benchmarks验证下机器性能，但是不太会对跑出的结果进行分析，不知是否有大神也用过这个，可否指点一二&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAJkeMpj+9dTfkHg2NvkE_zhHU_a0-2mXfPh4BO6rQtb246YunA@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvsPtSMiymm3LhqSxZ7UbQkHq=s2WJeS6_FeZU4BDw7xnw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:14:05 GMT",
        "subject": "Re: flink-benchmarks使用求助",
        "content": "Hi&#013;&#010;&#013;&#010;没有遇到过这个错误，这个错误是在指定 `-t max` 之后出现的，还是说其他情况下也会遇到呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月13日周一 下午2:32写道：&#013;&#010;&#013;&#010;&gt; 是的，用的 flink-benchmarks 代码，在跑的时候，指定参数-t max(最大工程线程)，在运行中会出现异常：&#010;`shutdown&#013;&#010;&gt; timeout of 30 seconds expired, forcing forked VM to exit`，前辈有遇到过这种情况吗？&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月10日周五 下午7:18写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt; 你说的 flink-benchmarks 是指 这个仓库[1]的代码吗？ 是这个仓库的代码的话，你按照&#010;readme 能跑出一个结果（csv&#013;&#010;&gt; &gt; 文件，或者终端能看到最终的结果），这个结果就是 JMH 的的结果，具体的可以阅读&#010;JMH 的相关文档[2]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1] https://github.com/dataArtisans/flink-benchmarks&#013;&#010;&gt; &gt; [2] http://openjdk.java.net/projects/code-tools/jmh/&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月10日周五 下午3:54写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 如题，最近在新机器上跑flink-benchmarks验证下机器性能，但是不太会对跑出的结果进行分析，不知是否有大神也用过这个，可否指点一二&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CAJkeMpj+9dTfkHg2NvkE_zhHU_a0-2mXfPh4BO6rQtb246YunA@mail.gmail.com>"
    },
    {
        "id": "<CAJkeMpiwmwkBdbs65m=a24ncjj7xLAee7Gb2=wkU2bW_szYcbw@mail.gmail.com>",
        "from": "zilong xiao &lt;acidzz...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:15:43 GMT",
        "subject": "Re: flink-benchmarks使用求助",
        "content": "`-t max`之后出现的~ 改小并发后貌似没问题&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月13日周一 下午8:14写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 没有遇到过这个错误，这个错误是在指定 `-t max` 之后出现的，还是说其他情况下也会遇到呢？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月13日周一 下午2:32写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 是的，用的 flink-benchmarks 代码，在跑的时候，指定参数-t max(最大工程线程)，在运行中会出现异常：&#010;`shutdown&#013;&#010;&gt; &gt; timeout of 30 seconds expired, forcing forked VM to exit`，前辈有遇到过这种情况吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月10日周五 下午7:18写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt; 你说的 flink-benchmarks 是指 这个仓库[1]的代码吗？ 是这个仓库的代码的话，你按照&#010;readme 能跑出一个结果（csv&#013;&#010;&gt; &gt; &gt; 文件，或者终端能看到最终的结果），这个结果就是 JMH 的的结果，具体的可以阅读&#010;JMH 的相关文档[2]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; [1] https://github.com/dataArtisans/flink-benchmarks&#013;&#010;&gt; &gt; &gt; [2] http://openjdk.java.net/projects/code-tools/jmh/&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月10日周五 下午3:54写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; 如题，最近在新机器上跑flink-benchmarks验证下机器性能，但是不太会对跑出的结果进行分析，不知是否有大神也用过这个，可否指点一二&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAJkeMpj+9dTfkHg2NvkE_zhHU_a0-2mXfPh4BO6rQtb246YunA@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvsNid4+riQLD9ipTYHo715O1-dV8+LwBVF3Sc7QFe-4Gg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 05:56:51 GMT",
        "subject": "Re: flink-benchmarks使用求助",
        "content": "Hi zilong&#013;&#010;&#013;&#010;之前没有使用 `-t max` 跑过，你可以分享一下你使用的全部命令么？我可以本地看看&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月14日周二 上午10:16写道：&#013;&#010;&#013;&#010;&gt; `-t max`之后出现的~ 改小并发后貌似没问题&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月13日周一 下午8:14写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 没有遇到过这个错误，这个错误是在指定 `-t max` 之后出现的，还是说其他情况下也会遇到呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月13日周一 下午2:32写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 是的，用的 flink-benchmarks 代码，在跑的时候，指定参数-t&#010;max(最大工程线程)，在运行中会出现异常： `shutdown&#013;&#010;&gt; &gt; &gt; timeout of 30 seconds expired, forcing forked VM to exit`，前辈有遇到过这种情况吗？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月10日周五 下午7:18写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt; &gt; 你说的 flink-benchmarks 是指 这个仓库[1]的代码吗？ 是这个仓库的代码的话，你按照&#010;readme&#013;&#010;&gt; 能跑出一个结果（csv&#013;&#010;&gt; &gt; &gt; &gt; 文件，或者终端能看到最终的结果），这个结果就是&#010;JMH 的的结果，具体的可以阅读 JMH 的相关文档[2]&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; [1] https://github.com/dataArtisans/flink-benchmarks&#013;&#010;&gt; &gt; &gt; &gt; [2] http://openjdk.java.net/projects/code-tools/jmh/&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月10日周五 下午3:54写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; 如题，最近在新机器上跑flink-benchmarks验证下机器性能，但是不太会对跑出的结果进行分析，不知是否有大神也用过这个，可否指点一二&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAJkeMpj+9dTfkHg2NvkE_zhHU_a0-2mXfPh4BO6rQtb246YunA@mail.gmail.com>"
    },
    {
        "id": "<4c90eaa6.7d0c.173395b54e1.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 15:32:33 GMT",
        "subject": "flink 1.11 local execution oom问题",
        "content": "hi，&#010;我在使用1.11版本在本地idea起一个作业时，并发为1，抛出了如下关于内存的异常。。问题是之前从来没有显示配置过taskmanager的memory参数，这是为何？&#010;感觉由1.10升级到1.11问题还是挺多的。。我尝试增加了JVM参数，增加DirectMemory内存配置，还是没有作用，请教大神帮忙看下。&#010;&#010;&#010;Exception in thread \"main\" java.lang.OutOfMemoryError: Could not allocate enough memory segments&#010;for NetworkBufferPool (required (Mb): 64, allocated (Mb): 63, missing (Mb): 1). Cause: Direct&#010;buffer memory. The direct out-of-memory error has occurred. This can mean two things: either&#010;job(s) require(s) a larger size of JVM direct memory or there is a direct memory leak. The&#010;direct memory can be allocated by user code or some of its dependencies. In this case 'taskmanager.memory.task.off-heap.size'&#010;configuration option should be increased. Flink framework and its dependencies also consume&#010;the direct memory, mostly for network communication. The most of network memory is managed&#010;by Flink and should not result in out-of-memory error. In certain special cases, in particular&#010;for jobs with high parallelism, the framework may require more direct memory which is not&#010;managed by Flink. In this case 'taskmanager.memory.framework.off-heap.size' configuration&#010;option should be increased. If the error persists then there is probably a direct memory leak&#010;in user code or some of its dependencies which has to be investigated and fixed. The task&#010;executor has to be shutdown...",
        "depth": "0",
        "reply": "<4c90eaa6.7d0c.173395b54e1.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAA8tFvsGFw_dCUYGWq=h+BQORjGdkCNofkSFVeepptEzQc7EoQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 07:48:14 GMT",
        "subject": "Re: flink 1.11 local execution oom问题",
        "content": "Hi&#013;&#010;&#013;&#010;这个问题可以看下是否和 releasenote[1] 中 memory configuration&#013;&#010;相关的修改有关，具体到这个错误，你可以按照提示增加一些内存看看&#013;&#010;&#013;&#010;[1]&#013;&#010;https://flink.apache.org/news/2020/07/06/release-1.11.0.html#other-improvements&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月10日周五 下午11:32写道：&#013;&#010;&#013;&#010;&gt; hi，&#013;&#010;&gt;&#013;&#010;&gt; 我在使用1.11版本在本地idea起一个作业时，并发为1，抛出了如下关于内存的异常。。问题是之前从来没有显示配置过taskmanager的memory参数，这是为何？&#013;&#010;&gt; 感觉由1.10升级到1.11问题还是挺多的。。我尝试增加了JVM参数，增加DirectMemory内存配置，还是没有作用，请教大神帮忙看下。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Exception in thread \"main\" java.lang.OutOfMemoryError: Could not allocate&#013;&#010;&gt; enough memory segments for NetworkBufferPool (required (Mb): 64, allocated&#013;&#010;&gt; (Mb): 63, missing (Mb): 1). Cause: Direct buffer memory. The direct&#013;&#010;&gt; out-of-memory error has occurred. This can mean two things: either job(s)&#013;&#010;&gt; require(s) a larger size of JVM direct memory or there is a direct memory&#013;&#010;&gt; leak. The direct memory can be allocated by user code or some of its&#013;&#010;&gt; dependencies. In this case 'taskmanager.memory.task.off-heap.size'&#013;&#010;&gt; configuration option should be increased. Flink framework and its&#013;&#010;&gt; dependencies also consume the direct memory, mostly for network&#013;&#010;&gt; communication. The most of network memory is managed by Flink and should&#013;&#010;&gt; not result in out-of-memory error. In certain special cases, in particular&#013;&#010;&gt; for jobs with high parallelism, the framework may require more direct&#013;&#010;&gt; memory which is not managed by Flink. In this case&#013;&#010;&gt; 'taskmanager.memory.framework.off-heap.size' configuration option should be&#013;&#010;&gt; increased. If the error persists then there is probably a direct memory&#013;&#010;&gt; leak in user code or some of its dependencies which has to be investigated&#013;&#010;&gt; and fixed. The task executor has to be shutdown...&#013;&#010;",
        "depth": "1",
        "reply": "<4c90eaa6.7d0c.173395b54e1.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAHsnkPuXM1GrkVp0gRFc7fhDVvhvSWp8PVZG421YbefiDf9vXw@mail.gmail.com>",
        "from": "Xintong Song &lt;tonysong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 05:51:55 GMT",
        "subject": "Re: flink 1.11 local execution oom问题",
        "content": "Local execution 模式下，Flink 是无法实际控制 JVM 的 Xmx, Xms, MaxDirectMemorySize&#013;&#010;等参数的，这些参数取决于你的 IDE 设置。&#013;&#010;检查一下 idea 的 run configuration 是否有配置过 -XX:MaxDirectMemorySize。&#013;&#010;&#013;&#010;Thank you~&#013;&#010;&#013;&#010;Xintong Song&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;On Sat, Jul 11, 2020 at 3:48 PM Congxian Qiu &lt;qcx978132955@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 这个问题可以看下是否和 releasenote[1] 中 memory configuration&#013;&#010;&gt; 相关的修改有关，具体到这个错误，你可以按照提示增加一些内存看看&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://flink.apache.org/news/2020/07/06/release-1.11.0.html#other-improvements&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月10日周五 下午11:32写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我在使用1.11版本在本地idea起一个作业时，并发为1，抛出了如下关于内存的异常。。问题是之前从来没有显示配置过taskmanager的memory参数，这是为何？&#013;&#010;&gt; &gt; 感觉由1.10升级到1.11问题还是挺多的。。我尝试增加了JVM参数，增加DirectMemory内存配置，还是没有作用，请教大神帮忙看下。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Exception in thread \"main\" java.lang.OutOfMemoryError: Could not allocate&#013;&#010;&gt; &gt; enough memory segments for NetworkBufferPool (required (Mb): 64,&#013;&#010;&gt; allocated&#013;&#010;&gt; &gt; (Mb): 63, missing (Mb): 1). Cause: Direct buffer memory. The direct&#013;&#010;&gt; &gt; out-of-memory error has occurred. This can mean two things: either job(s)&#013;&#010;&gt; &gt; require(s) a larger size of JVM direct memory or there is a direct memory&#013;&#010;&gt; &gt; leak. The direct memory can be allocated by user code or some of its&#013;&#010;&gt; &gt; dependencies. In this case 'taskmanager.memory.task.off-heap.size'&#013;&#010;&gt; &gt; configuration option should be increased. Flink framework and its&#013;&#010;&gt; &gt; dependencies also consume the direct memory, mostly for network&#013;&#010;&gt; &gt; communication. The most of network memory is managed by Flink and should&#013;&#010;&gt; &gt; not result in out-of-memory error. In certain special cases, in&#013;&#010;&gt; particular&#013;&#010;&gt; &gt; for jobs with high parallelism, the framework may require more direct&#013;&#010;&gt; &gt; memory which is not managed by Flink. In this case&#013;&#010;&gt; &gt; 'taskmanager.memory.framework.off-heap.size' configuration option should&#013;&#010;&gt; be&#013;&#010;&gt; &gt; increased. If the error persists then there is probably a direct memory&#013;&#010;&gt; &gt; leak in user code or some of its dependencies which has to be&#013;&#010;&gt; investigated&#013;&#010;&gt; &gt; and fixed. The task executor has to be shutdown...&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<4c90eaa6.7d0c.173395b54e1.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<4076f40f.4534.17346c40193.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:01:57 GMT",
        "subject": "Re:Re: flink 1.11 local execution oom问题",
        "content": "hi，&#010;感谢大神的回复。看了下是因为我执行的SQL作业，有多个sink，在执行TableEnvironment.executeSQL时，Flink提交了多个job，在本地执行时貌似就走到了这个异常。我将多sink修改为TableEnvironment.createStatementSet来提交，就没有这个问题了。谢谢回复。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 13:51:55，\"Xintong Song\" &lt;tonysong820@gmail.com&gt; 写道：&#010;&gt;Local execution 模式下，Flink 是无法实际控制 JVM 的 Xmx, Xms, MaxDirectMemorySize&#010;&gt;等参数的，这些参数取决于你的 IDE 设置。&#010;&gt;检查一下 idea 的 run configuration 是否有配置过 -XX:MaxDirectMemorySize。&#010;&gt;&#010;&gt;Thank you~&#010;&gt;&#010;&gt;Xintong Song&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;On Sat, Jul 11, 2020 at 3:48 PM Congxian Qiu &lt;qcx978132955@gmail.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi&#010;&gt;&gt;&#010;&gt;&gt; 这个问题可以看下是否和 releasenote[1] 中 memory configuration&#010;&gt;&gt; 相关的修改有关，具体到这个错误，你可以按照提示增加一些内存看看&#010;&gt;&gt;&#010;&gt;&gt; [1]&#010;&gt;&gt;&#010;&gt;&gt; https://flink.apache.org/news/2020/07/06/release-1.11.0.html#other-improvements&#010;&gt;&gt; Best,&#010;&gt;&gt; Congxian&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月10日周五 下午11:32写道：&#010;&gt;&gt;&#010;&gt;&gt; &gt; hi，&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; 我在使用1.11版本在本地idea起一个作业时，并发为1，抛出了如下关于内存的异常。。问题是之前从来没有显示配置过taskmanager的memory参数，这是为何？&#010;&gt;&gt; &gt; 感觉由1.10升级到1.11问题还是挺多的。。我尝试增加了JVM参数，增加DirectMemory内存配置，还是没有作用，请教大神帮忙看下。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Exception in thread \"main\" java.lang.OutOfMemoryError: Could not allocate&#010;&gt;&gt; &gt; enough memory segments for NetworkBufferPool (required (Mb): 64,&#010;&gt;&gt; allocated&#010;&gt;&gt; &gt; (Mb): 63, missing (Mb): 1). Cause: Direct buffer memory. The direct&#010;&gt;&gt; &gt; out-of-memory error has occurred. This can mean two things: either job(s)&#010;&gt;&gt; &gt; require(s) a larger size of JVM direct memory or there is a direct memory&#010;&gt;&gt; &gt; leak. The direct memory can be allocated by user code or some of its&#010;&gt;&gt; &gt; dependencies. In this case 'taskmanager.memory.task.off-heap.size'&#010;&gt;&gt; &gt; configuration option should be increased. Flink framework and its&#010;&gt;&gt; &gt; dependencies also consume the direct memory, mostly for network&#010;&gt;&gt; &gt; communication. The most of network memory is managed by Flink and should&#010;&gt;&gt; &gt; not result in out-of-memory error. In certain special cases, in&#010;&gt;&gt; particular&#010;&gt;&gt; &gt; for jobs with high parallelism, the framework may require more direct&#010;&gt;&gt; &gt; memory which is not managed by Flink. In this case&#010;&gt;&gt; &gt; 'taskmanager.memory.framework.off-heap.size' configuration option should&#010;&gt;&gt; be&#010;&gt;&gt; &gt; increased. If the error persists then there is probably a direct memory&#010;&gt;&gt; &gt; leak in user code or some of its dependencies which has to be&#010;&gt;&gt; investigated&#010;&gt;&gt; &gt; and fixed. The task executor has to be shutdown...&#010;&gt;&gt;&#010;",
        "depth": "3",
        "reply": "<4c90eaa6.7d0c.173395b54e1.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<23d6665e.2bfd.1733e43b471.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 14:24:51 GMT",
        "subject": "flink 1.11 sql作业提交JM报错",
        "content": "hi，&#010;我使用flink 1.11提交sql作业，从JM日志中看到有如下异常。我的作业里会通过tEnv.executeSQL执行多个ddl语句，通过tEnv.createStatementSet&#010;add多个dml语句，并执行execute。&#010;如下异常可能原因是啥呢？还有个问题，这个异常虽然抛出来了，但是作业还是正常启动执行了。这又是为何？是不是不推荐在作业里同时使用executeSQL和statementset.execute？&#010;&#010;&#010;Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot have more than one execute()&#010;or executeAsync() call in a single environment.&#010;at org.apache.flink.client.program.StreamContextEnvironment.validateAllowedExecution(StreamContextEnvironment.java:139)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:127)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.table.planner.delegation.ExecutorBase.executeAsync(ExecutorBase.java:57)&#010;~[flink-table-blink_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:699)&#010;~[flink-table_2.12-1.11.0.jar:1.11.0]&#010;... 24 more",
        "depth": "0",
        "reply": "<23d6665e.2bfd.1733e43b471.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<5d3b53fe.2ce9.1733e627745.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 14:58:27 GMT",
        "subject": "Re:flink 1.11 sql作业提交JM报错",
        "content": "&#010;&#010;&#010;hi,&#010;在JM日志中还有如下异常：这个也比较诡异。求大神帮忙解答下。&#010;&#010;&#010;java.lang.IllegalStateException: No operators defined in streaming topology. Cannot execute.&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraphGenerator(StreamExecutionEnvironment.java:1872)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:1863)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getStreamGraph(StreamExecutionEnvironment.java:1848)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1699)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at com.htsc.crm_realtime.fatjob.Jobs.JobEntryBase.run(JobEntryBase.java:67) ~[763f9e05-39d4-4c70-bf9c-e3bea7ef0e0f_FatJob-1.0.jar:?]&#010;at com.htsc.crm_realtime.fatjob.Jobs.sensordata.SensorDataETLTask.main(SensorDataETLTask.java:47)&#010;~[763f9e05-39d4-4c70-bf9c-e3bea7ef0e0f_FatJob-1.0.jar:?]&#010;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_201]&#010;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_201]&#010;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_201]&#010;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_201]&#010;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.client.deployment.application.DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:78)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.client.deployment.application.DetachedApplicationRunner.run(DetachedApplicationRunner.java:67)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler.lambda$handleRequest$0(JarRunHandler.java:99)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) [?:1.8.0_201]&#010;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_201]&#010;at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_201]&#010;at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)&#010;[?:1.8.0_201]&#010;at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)&#010;[?:1.8.0_201]&#010;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_201]&#010;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_201]&#010;at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-11 22:24:51，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;hi，&#010;&gt;我使用flink 1.11提交sql作业，从JM日志中看到有如下异常。我的作业里会通过tEnv.executeSQL执行多个ddl语句，通过tEnv.createStatementSet&#010;add多个dml语句，并执行execute。&#010;&gt;如下异常可能原因是啥呢？还有个问题，这个异常虽然抛出来了，但是作业还是正常启动执行了。这又是为何？是不是不推荐在作业里同时使用executeSQL和statementset.execute？&#010;&gt;&#010;&gt;&#010;&gt;Caused by: org.apache.flink.util.FlinkRuntimeException: Cannot have more than one execute()&#010;or executeAsync() call in a single environment.&#010;&gt;at org.apache.flink.client.program.StreamContextEnvironment.validateAllowedExecution(StreamContextEnvironment.java:139)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;&gt;at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:127)&#010;~[flink-dist_2.12-1.11.0.jar:1.11.0]&#010;&gt;at org.apache.flink.table.planner.delegation.ExecutorBase.executeAsync(ExecutorBase.java:57)&#010;~[flink-table-blink_2.12-1.11.0.jar:1.11.0]&#010;&gt;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:699)&#010;~[flink-table_2.12-1.11.0.jar:1.11.0]&#010;&gt;... 24 more&#010;",
        "depth": "1",
        "reply": "<23d6665e.2bfd.1733e43b471.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<A39BDB89-76CB-4014-8148-16315DBE1718@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 12 Jul 2020 05:47:41 GMT",
        "subject": "Re: flink 1.11 sql作业提交JM报错",
        "content": "HI, fulin&#010;&#010;能大致贴下代码吗？能复现异常即可。简单说下这两个方法，&#010; TableEnvironment.executeSql(String statement)是为了用于执行单条的 sql 语句, SQL语句可以是&#010;DDL/DML/DCL/DQL, DML(如insert)和DQL(如select)的执行是等 Flink job提交后返回该方法的执行结果，DDL(create&#010;table ...) 和 DCL(use database …) 的执行是对应的SQL语句执行完成就返回，理解起来就是需要提交&#010;Flink job 的SQL需要等 job 提交后返回结果，其他是立即执行并返回。&#010;Statementset.execute() 主要用于执行批量的 sql 语句，sql 语句只能是 insert&#010;xx，可以看接口的方法， 这个接口主要是为了 SQL 里有多个query的情况,&#010;（比如multiple sink：insert tableA from xx ；insert tableB from xx), 如果调用 TableEnvironment.executeSql(“insert&#010;tableA from xx”)， TableEnvironment.executeSql(“insert tableA from xx”) 就会起两个&#010;Flink job, 这应该不是用户需要的。&#010;具体使用根据你的需要来使用。&#010;&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月11日，22:24，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt; &#010;&gt; statementset.execute&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<23d6665e.2bfd.1733e43b471.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CADQYLGvGr67qg1ZvsEYmT7ZBH9NHHu8XRe2AApOLeuaMQup4Rg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 12 Jul 2020 14:55:34 GMT",
        "subject": "Re: flink 1.11 sql作业提交JM报错",
        "content": "hi sunfulin，&#013;&#010;&#013;&#010;1.11 对 StreamTableEnvironment.execute()&#013;&#010;和 StreamExecutionEnvironment.execute() 的执行方式有所调整，&#013;&#010;简单概述为：&#013;&#010;1. StreamTableEnvironment.execute() 只能执行 sqlUpdate 和 insertInto 方法执行作业；&#013;&#010;2. Table 转化为 DataStream 后只能通过 StreamExecutionEnvironment.execute() 来执行作业；&#013;&#010;3. 新引入的 TableEnvironment.executeSql() 和 StatementSet.execute() 方法是直接执行sql作业&#013;&#010;(异步提交作业)，不需要再调用 StreamTableEnvironment.execute()&#013;&#010;或 StreamExecutionEnvironment.execute()&#013;&#010;&#013;&#010;详细可以参考 [1] [2]&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;对于 “No operators defined in streaming topology.”，如果使用&#013;&#010;TableEnvironment.executeSql() 或者 StatementSet.execute() 方法提交的作业后再调用&#013;&#010;StreamTableEnvironment.execute() 或 StreamExecutionEnvironment.execute()&#013;&#010;提交作业，就会出现前面的错误。&#013;&#010;&#013;&#010;对于&#013;&#010;“是不是不推荐在作业里同时使用executeSQL和StatementSet.execute？”，这个答案是no。executeSql和StatementSet不会相互干扰。对于出现的错误，能给一个更详细的提交作业的流程描述吗？&#013;&#010;&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C%E6%9F%A5%E8%AF%A2&#013;&#010;[2]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E5%B0%86%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90-datastream-%E6%88%96-dataset&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月12日周日 下午1:48写道：&#013;&#010;&#013;&#010;&gt; HI, fulin&#013;&#010;&gt;&#013;&#010;&gt; 能大致贴下代码吗？能复现异常即可。简单说下这两个方法，&#013;&#010;&gt;  TableEnvironment.executeSql(String statement)是为了用于执行单条的 sql 语句,&#010;SQL语句可以是&#013;&#010;&gt; DDL/DML/DCL/DQL, DML(如insert)和DQL(如select)的执行是等 Flink&#013;&#010;&gt; job提交后返回该方法的执行结果，DDL(create table ...) 和 DCL(use database&#010;…)&#013;&#010;&gt; 的执行是对应的SQL语句执行完成就返回，理解起来就是需要提交&#010;Flink job 的SQL需要等 job 提交后返回结果，其他是立即执行并返回。&#013;&#010;&gt; Statementset.execute() 主要用于执行批量的 sql 语句，sql 语句只能是 insert&#010;xx，可以看接口的方法，&#013;&#010;&gt; 这个接口主要是为了 SQL 里有多个query的情况, （比如multiple sink：insert&#010;tableA from xx ；insert&#013;&#010;&gt; tableB from xx), 如果调用 TableEnvironment.executeSql(“insert tableA from xx”)，&#013;&#010;&gt; TableEnvironment.executeSql(“insert tableA from xx”) 就会起两个 Flink job,&#013;&#010;&gt; 这应该不是用户需要的。&#013;&#010;&gt; 具体使用根据你的需要来使用。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020年7月11日，22:24，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt; statementset.execute&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<23d6665e.2bfd.1733e43b471.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<4f102720.461e.17346c9ea92.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:08:24 GMT",
        "subject": "Re:Re: flink 1.11 sql作业提交JM报错",
        "content": "hi，&#010;感谢详细的解释和回复。那问题就清楚了。之前我们的job提交框架里统一都使用了StreamExecutionEnvironment.execute(jobName)方法，现在基于这个解释就明白了。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-12 22:55:34，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#010;&gt;hi sunfulin，&#010;&gt;&#010;&gt;1.11 对 StreamTableEnvironment.execute()&#010;&gt;和 StreamExecutionEnvironment.execute() 的执行方式有所调整，&#010;&gt;简单概述为：&#010;&gt;1. StreamTableEnvironment.execute() 只能执行 sqlUpdate 和 insertInto 方法执行作业；&#010;&gt;2. Table 转化为 DataStream 后只能通过 StreamExecutionEnvironment.execute() 来执行作业；&#010;&gt;3. 新引入的 TableEnvironment.executeSql() 和 StatementSet.execute() 方法是直接执行sql作业&#010;&gt;(异步提交作业)，不需要再调用 StreamTableEnvironment.execute()&#010;&gt;或 StreamExecutionEnvironment.execute()&#010;&gt;&#010;&gt;详细可以参考 [1] [2]&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;对于 “No operators defined in streaming topology.”，如果使用&#010;&gt;TableEnvironment.executeSql() 或者 StatementSet.execute() 方法提交的作业后再调用&#010;&gt;StreamTableEnvironment.execute() 或 StreamExecutionEnvironment.execute()&#010;&gt;提交作业，就会出现前面的错误。&#010;&gt;&#010;&gt;对于&#010;&gt;“是不是不推荐在作业里同时使用executeSQL和StatementSet.execute？”，这个答案是no。executeSql和StatementSet不会相互干扰。对于出现的错误，能给一个更详细的提交作业的流程描述吗？&#010;&gt;&#010;&gt;&#010;&gt;[1]&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C%E6%9F%A5%E8%AF%A2&#010;&gt;[2]&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E5%B0%86%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90-datastream-%E6%88%96-dataset&#010;&gt;&#010;&gt;Best,&#010;&gt;Godfrey&#010;&gt;&#010;&gt;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月12日周日 下午1:48写道：&#010;&gt;&#010;&gt;&gt; HI, fulin&#010;&gt;&gt;&#010;&gt;&gt; 能大致贴下代码吗？能复现异常即可。简单说下这两个方法，&#010;&gt;&gt;  TableEnvironment.executeSql(String statement)是为了用于执行单条的 sql&#010;语句, SQL语句可以是&#010;&gt;&gt; DDL/DML/DCL/DQL, DML(如insert)和DQL(如select)的执行是等 Flink&#010;&gt;&gt; job提交后返回该方法的执行结果，DDL(create table ...) 和 DCL(use database&#010;…)&#010;&gt;&gt; 的执行是对应的SQL语句执行完成就返回，理解起来就是需要提交&#010;Flink job 的SQL需要等 job 提交后返回结果，其他是立即执行并返回。&#010;&gt;&gt; Statementset.execute() 主要用于执行批量的 sql 语句，sql 语句只能是&#010;insert xx，可以看接口的方法，&#010;&gt;&gt; 这个接口主要是为了 SQL 里有多个query的情况, （比如multiple sink：insert&#010;tableA from xx ；insert&#010;&gt;&gt; tableB from xx), 如果调用 TableEnvironment.executeSql(“insert tableA from xx”)，&#010;&gt;&gt; TableEnvironment.executeSql(“insert tableA from xx”) 就会起两个 Flink job,&#010;&gt;&gt; 这应该不是用户需要的。&#010;&gt;&gt; 具体使用根据你的需要来使用。&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020年7月11日，22:24，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt;&#010;&gt;&gt; statementset.execute&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "3",
        "reply": "<23d6665e.2bfd.1733e43b471.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 15:32:24 GMT",
        "subject": "flink 1.11 es未定义pk的sink问题",
        "content": "hi，&#010;根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#010;不确定是我配置使用的方式不对，还是确实存在bug。。&#010;&#010;&#010;CREATE TABLE ES6_SENSORDATA_OUTPUT (&#010;  event varchar,&#010;  user_id varchar,&#010;  distinct_id varchar,&#010;  _date varchar,&#010;  _event_time varchar,&#010;  recv_time varchar,&#010;  _browser_version varchar,&#010;  path_name varchar,&#010;  _search varchar,&#010;  event_type varchar,&#010;  _current_project varchar,&#010;  message varchar,&#010;  stack varchar,&#010;  component_stack varchar,&#010;  _screen_width varchar,&#010;  _screen_height varchar&#010;) WITH (&#010;'connector' = 'elasticsearch-6',&#010;'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;'index' = 'flink_sensordata_target_event',&#010;'document-type' = 'default',&#010;'document-id.key-delimiter' = '$',&#010;'sink.bulk-flush.interval' = '1000',&#010;'failure-handler' = 'fail',&#010;'format' = 'json'&#010;)&#010;&#010;&#010;&#010;&#010;[1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling",
        "depth": "0",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAFTKPZobr+nYpTxf=+z6=aBUt3GJamnduubUjjF48pfAoK_U4A@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 01:50:07 GMT",
        "subject": "Re: flink 1.11 es未定义pk的sink问题",
        "content": "Hi,&#013;&#010;&#013;&#010;如果没有定义主键，ES connector 会把 _id设为null[1]，这样ES的Java Client会将_id设为一个随机值[2].&#013;&#010;所以应该不会出现您说的这种情况。您那里的ES有没有请求日志之类的，看一下Flink发过来的请求是什么样的。&#013;&#010;&#013;&#010;[1] https://github.com/apache/flink/blob/f0eeaec530e001ab02cb889dfe217e25913660c4/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/RowElasticsearchSinkFunction.java#L102&#013;&#010;[2] https://github.com/elastic/elasticsearch/blob/977230a0ce89a55515dc6ef6452e9f059d9356a2/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L509&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Sat, Jul 11, 2020 at 11:33 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; hi，&#013;&#010;&gt; 根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary&#010;key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#013;&#010;&gt; 不确定是我配置使用的方式不对，还是确实存在bug。。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE ES6_SENSORDATA_OUTPUT (&#013;&#010;&gt;   event varchar,&#013;&#010;&gt;   user_id varchar,&#013;&#010;&gt;   distinct_id varchar,&#013;&#010;&gt;   _date varchar,&#013;&#010;&gt;   _event_time varchar,&#013;&#010;&gt;   recv_time varchar,&#013;&#010;&gt;   _browser_version varchar,&#013;&#010;&gt;   path_name varchar,&#013;&#010;&gt;   _search varchar,&#013;&#010;&gt;   event_type varchar,&#013;&#010;&gt;   _current_project varchar,&#013;&#010;&gt;   message varchar,&#013;&#010;&gt;   stack varchar,&#013;&#010;&gt;   component_stack varchar,&#013;&#010;&gt;   _screen_width varchar,&#013;&#010;&gt;   _screen_height varchar&#013;&#010;&gt; ) WITH (&#013;&#010;&gt; 'connector' = 'elasticsearch-6',&#013;&#010;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#013;&#010;&gt; 'index' = 'flink_sensordata_target_event',&#013;&#010;&gt; 'document-type' = 'default',&#013;&#010;&gt; 'document-id.key-delimiter' = '$',&#013;&#010;&gt; 'sink.bulk-flush.interval' = '1000',&#013;&#010;&gt; 'failure-handler' = 'fail',&#013;&#010;&gt; 'format' = 'json'&#013;&#010;&gt; )&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling&#013;&#010;",
        "depth": "1",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<5037A0D7-6E0E-457F-9B32-6CB0827C24E4@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 05:44:29 GMT",
        "subject": "Re: flink 1.11 es未定义pk的sink问题",
        "content": "Hello, fulin&#010;&#010;这个问题能提供段可以复现的代码吗？&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月13日，09:50，Yangze Guo &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,&#010;&gt; &#010;&gt; 如果没有定义主键，ES connector 会把 _id设为null[1]，这样ES的Java Client会将_id设为一个随机值[2].&#010;&gt; 所以应该不会出现您说的这种情况。您那里的ES有没有请求日志之类的，看一下Flink发过来的请求是什么样的。&#010;&gt; &#010;&gt; [1] https://github.com/apache/flink/blob/f0eeaec530e001ab02cb889dfe217e25913660c4/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/RowElasticsearchSinkFunction.java#L102&#010;&gt; [2] https://github.com/elastic/elasticsearch/blob/977230a0ce89a55515dc6ef6452e9f059d9356a2/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L509&#010;&gt; &#010;&gt; Best,&#010;&gt; Yangze Guo&#010;&gt; &#010;&gt; On Sat, Jul 11, 2020 at 11:33 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&gt; &#010;&gt;&gt; hi，&#010;&gt;&gt; 根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary&#010;key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#010;&gt;&gt; 不确定是我配置使用的方式不对，还是确实存在bug。。&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; CREATE TABLE ES6_SENSORDATA_OUTPUT (&#010;&gt;&gt; event varchar,&#010;&gt;&gt; user_id varchar,&#010;&gt;&gt; distinct_id varchar,&#010;&gt;&gt; _date varchar,&#010;&gt;&gt; _event_time varchar,&#010;&gt;&gt; recv_time varchar,&#010;&gt;&gt; _browser_version varchar,&#010;&gt;&gt; path_name varchar,&#010;&gt;&gt; _search varchar,&#010;&gt;&gt; event_type varchar,&#010;&gt;&gt; _current_project varchar,&#010;&gt;&gt; message varchar,&#010;&gt;&gt; stack varchar,&#010;&gt;&gt; component_stack varchar,&#010;&gt;&gt; _screen_width varchar,&#010;&gt;&gt; _screen_height varchar&#010;&gt;&gt; ) WITH (&#010;&gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt;&gt; 'index' = 'flink_sensordata_target_event',&#010;&gt;&gt; 'document-type' = 'default',&#010;&gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt;&gt; 'failure-handler' = 'fail',&#010;&gt;&gt; 'format' = 'json'&#010;&gt;&gt; )&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; [1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<495d8aa6.44ed.17346c1f5f7.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 05:59:43 GMT",
        "subject": "Re:Re: flink 1.11 es未定义pk的sink问题",
        "content": "&#010;&#010;&#010;hi, Leonard&#010;我定义了一个ddl和一个dml，sql如下。ddl中没有定义PK，我观察到的现象是：这样在sink到es结果中，结果生成的id是index名，导致只有一条记录。&#010;我将DDL更换为之前版本的with参数（声明使用update-mode = ‘upsert’），不使用1.11最新的with参数，观察到sink结果就正常了。不确定是不是我哪边配置的方式不太对，还是说使用方式有问题。&#010;&#010;&#010; @karmagyz@gmail.com  我看了下给的源代码，貌似这个是处理upsert的情况，如果不声明pk的话，是不是会是processInsert？&#010;&#010;&#010;&#010;CREATE TABLE ES6_SENSORDATA_SERVER_API (&#010;  event varchar,&#010;  user_id varchar,&#010;  distinct_id varchar,&#010;  _date varchar,&#010;  _event_time varchar,&#010;  recv_time varchar,&#010;  code varchar,&#010;  _current_project varchar,&#010;  api varchar,&#010;  elapsed int ,&#010;  `start` bigint,&#010;  is_err int&#010;) WITH (&#010;'connector' = 'elasticsearch-6',&#010;'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;'index' = 'flink_sensordata_server_api',&#010;'document-type' = 'default',&#010;'document-id.key-delimiter' = '$',&#010;'sink.bulk-flush.interval' = '1000',&#010;'failure-handler' = 'fail',&#010;'format' = 'json'&#010;)&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;INSERT INTO ES6_SENSORDATA_SERVER_API&#010;&#010;SELECT event,&#010;&#010;       user_id,&#010;&#010;       distinct_id,&#010;&#010;       ts2Date(`time`, 'yyyy-MM-dd') as _date,&#010;&#010;       ts2Date(`time`, 'yyyy-MM-dd HH:mm:ss.SSS') as _event_time,&#010;&#010;       ts2Date(recv_time, false, false) as recv_time,&#010;&#010;       properties.code as code,&#010;&#010;       properties.`project` as _current_project,&#010;&#010;       properties.api as api,&#010;&#010;       properties.elapsed as elapsed,&#010;&#010;       properties.`start` as `start`,&#010;&#010;       case when properties.code = '0' then 0 else 1 end as is_err&#010;&#010;FROM KafkaEventTopic&#010;&#010;where `type` in ('track') and event in ('serverApiReqEvt')&#010;&#010;&#010;在 2020-07-13 13:44:29，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hello, fulin&#010;&gt;&#010;&gt;这个问题能提供段可以复现的代码吗？&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月13日，09:50，Yangze Guo &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; Hi,&#010;&gt;&gt; &#010;&gt;&gt; 如果没有定义主键，ES connector 会把 _id设为null[1]，这样ES的Java&#010;Client会将_id设为一个随机值[2].&#010;&gt;&gt; 所以应该不会出现您说的这种情况。您那里的ES有没有请求日志之类的，看一下Flink发过来的请求是什么样的。&#010;&gt;&gt; &#010;&gt;&gt; [1] https://github.com/apache/flink/blob/f0eeaec530e001ab02cb889dfe217e25913660c4/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/RowElasticsearchSinkFunction.java#L102&#010;&gt;&gt; [2] https://github.com/elastic/elasticsearch/blob/977230a0ce89a55515dc6ef6452e9f059d9356a2/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L509&#010;&gt;&gt; &#010;&gt;&gt; Best,&#010;&gt;&gt; Yangze Guo&#010;&gt;&gt; &#010;&gt;&gt; On Sat, Jul 11, 2020 at 11:33 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; hi，&#010;&gt;&gt;&gt; 根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary&#010;key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#010;&gt;&gt;&gt; 不确定是我配置使用的方式不对，还是确实存在bug。。&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; CREATE TABLE ES6_SENSORDATA_OUTPUT (&#010;&gt;&gt;&gt; event varchar,&#010;&gt;&gt;&gt; user_id varchar,&#010;&gt;&gt;&gt; distinct_id varchar,&#010;&gt;&gt;&gt; _date varchar,&#010;&gt;&gt;&gt; _event_time varchar,&#010;&gt;&gt;&gt; recv_time varchar,&#010;&gt;&gt;&gt; _browser_version varchar,&#010;&gt;&gt;&gt; path_name varchar,&#010;&gt;&gt;&gt; _search varchar,&#010;&gt;&gt;&gt; event_type varchar,&#010;&gt;&gt;&gt; _current_project varchar,&#010;&gt;&gt;&gt; message varchar,&#010;&gt;&gt;&gt; stack varchar,&#010;&gt;&gt;&gt; component_stack varchar,&#010;&gt;&gt;&gt; _screen_width varchar,&#010;&gt;&gt;&gt; _screen_height varchar&#010;&gt;&gt;&gt; ) WITH (&#010;&gt;&gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt;&gt;&gt; 'index' = 'flink_sensordata_target_event',&#010;&gt;&gt;&gt; 'document-type' = 'default',&#010;&gt;&gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt;&gt;&gt; 'failure-handler' = 'fail',&#010;&gt;&gt;&gt; 'format' = 'json'&#010;&gt;&gt;&gt; )&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling&#010;",
        "depth": "3",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAFTKPZrGjfpV51GD63m7TK5LgSd16czk4ynmszJ-RS8KpE4qqA@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:03:21 GMT",
        "subject": "Re: Re: flink 1.11 es未定义pk的sink问题",
        "content": "INSERT走的就是processUpsert这个方法，当不指定PK时，生成的key会是null，然后创建一个IndexRequest。&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Mon, Jul 13, 2020 at 2:00 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi, Leonard&#013;&#010;&gt; 我定义了一个ddl和一个dml，sql如下。ddl中没有定义PK，我观察到的现象是：这样在sink到es结果中，结果生成的id是index名，导致只有一条记录。&#013;&#010;&gt; 我将DDL更换为之前版本的with参数（声明使用update-mode = ‘upsert’），不使用1.11最新的with参数，观察到sink结果就正常了。不确定是不是我哪边配置的方式不太对，还是说使用方式有问题。&#013;&#010;&gt;&#013;&#010;&gt;  @karmagyz@gmail.com  我看了下给的源代码，貌似这个是处理upsert的情况，如果不声明pk的话，是不是会是processInsert？&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE ES6_SENSORDATA_SERVER_API (&#013;&#010;&gt;   event varchar,&#013;&#010;&gt;   user_id varchar,&#013;&#010;&gt;   distinct_id varchar,&#013;&#010;&gt;   _date varchar,&#013;&#010;&gt;   _event_time varchar,&#013;&#010;&gt;   recv_time varchar,&#013;&#010;&gt;   code varchar,&#013;&#010;&gt;   _current_project varchar,&#013;&#010;&gt;   api varchar,&#013;&#010;&gt;   elapsed int ,&#013;&#010;&gt;   `start` bigint,&#013;&#010;&gt;   is_err int&#013;&#010;&gt; ) WITH (&#013;&#010;&gt; 'connector' = 'elasticsearch-6',&#013;&#010;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#013;&#010;&gt; 'index' = 'flink_sensordata_server_api',&#013;&#010;&gt; 'document-type' = 'default',&#013;&#010;&gt; 'document-id.key-delimiter' = '$',&#013;&#010;&gt; 'sink.bulk-flush.interval' = '1000',&#013;&#010;&gt; 'failure-handler' = 'fail',&#013;&#010;&gt; 'format' = 'json'&#013;&#010;&gt; )&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; INSERT INTO ES6_SENSORDATA_SERVER_API&#013;&#010;&gt;&#013;&#010;&gt; SELECT event,&#013;&#010;&gt;&#013;&#010;&gt;        user_id,&#013;&#010;&gt;&#013;&#010;&gt;        distinct_id,&#013;&#010;&gt;&#013;&#010;&gt;        ts2Date(`time`, 'yyyy-MM-dd') as _date,&#013;&#010;&gt;&#013;&#010;&gt;        ts2Date(`time`, 'yyyy-MM-dd HH:mm:ss.SSS') as _event_time,&#013;&#010;&gt;&#013;&#010;&gt;        ts2Date(recv_time, false, false) as recv_time,&#013;&#010;&gt;&#013;&#010;&gt;        properties.code as code,&#013;&#010;&gt;&#013;&#010;&gt;        properties.`project` as _current_project,&#013;&#010;&gt;&#013;&#010;&gt;        properties.api as api,&#013;&#010;&gt;&#013;&#010;&gt;        properties.elapsed as elapsed,&#013;&#010;&gt;&#013;&#010;&gt;        properties.`start` as `start`,&#013;&#010;&gt;&#013;&#010;&gt;        case when properties.code = '0' then 0 else 1 end as is_err&#013;&#010;&gt;&#013;&#010;&gt; FROM KafkaEventTopic&#013;&#010;&gt;&#013;&#010;&gt; where `type` in ('track') and event in ('serverApiReqEvt')&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-13 13:44:29，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;Hello, fulin&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;这个问题能提供段可以复现的代码吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;祝好，&#013;&#010;&gt; &gt;Leonard Xu&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; 在 2020年7月13日，09:50，Yangze Guo &lt;karmagyz@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Hi,&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 如果没有定义主键，ES connector 会把 _id设为null[1]，这样ES的Java&#010;Client会将_id设为一个随机值[2].&#013;&#010;&gt; &gt;&gt; 所以应该不会出现您说的这种情况。您那里的ES有没有请求日志之类的，看一下Flink发过来的请求是什么样的。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; [1] https://github.com/apache/flink/blob/f0eeaec530e001ab02cb889dfe217e25913660c4/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/RowElasticsearchSinkFunction.java#L102&#013;&#010;&gt; &gt;&gt; [2] https://github.com/elastic/elasticsearch/blob/977230a0ce89a55515dc6ef6452e9f059d9356a2/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L509&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best,&#013;&#010;&gt; &gt;&gt; Yangze Guo&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; On Sat, Jul 11, 2020 at 11:33 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; hi，&#013;&#010;&gt; &gt;&gt;&gt; 根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary&#010;key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#013;&#010;&gt; &gt;&gt;&gt; 不确定是我配置使用的方式不对，还是确实存在bug。。&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; CREATE TABLE ES6_SENSORDATA_OUTPUT (&#013;&#010;&gt; &gt;&gt;&gt; event varchar,&#013;&#010;&gt; &gt;&gt;&gt; user_id varchar,&#013;&#010;&gt; &gt;&gt;&gt; distinct_id varchar,&#013;&#010;&gt; &gt;&gt;&gt; _date varchar,&#013;&#010;&gt; &gt;&gt;&gt; _event_time varchar,&#013;&#010;&gt; &gt;&gt;&gt; recv_time varchar,&#013;&#010;&gt; &gt;&gt;&gt; _browser_version varchar,&#013;&#010;&gt; &gt;&gt;&gt; path_name varchar,&#013;&#010;&gt; &gt;&gt;&gt; _search varchar,&#013;&#010;&gt; &gt;&gt;&gt; event_type varchar,&#013;&#010;&gt; &gt;&gt;&gt; _current_project varchar,&#013;&#010;&gt; &gt;&gt;&gt; message varchar,&#013;&#010;&gt; &gt;&gt;&gt; stack varchar,&#013;&#010;&gt; &gt;&gt;&gt; component_stack varchar,&#013;&#010;&gt; &gt;&gt;&gt; _screen_width varchar,&#013;&#010;&gt; &gt;&gt;&gt; _screen_height varchar&#013;&#010;&gt; &gt;&gt;&gt; ) WITH (&#013;&#010;&gt; &gt;&gt;&gt; 'connector' = 'elasticsearch-6',&#013;&#010;&gt; &gt;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#013;&#010;&gt; &gt;&gt;&gt; 'index' = 'flink_sensordata_target_event',&#013;&#010;&gt; &gt;&gt;&gt; 'document-type' = 'default',&#013;&#010;&gt; &gt;&gt;&gt; 'document-id.key-delimiter' = '$',&#013;&#010;&gt; &gt;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#013;&#010;&gt; &gt;&gt;&gt; 'failure-handler' = 'fail',&#013;&#010;&gt; &gt;&gt;&gt; 'format' = 'json'&#013;&#010;&gt; &gt;&gt;&gt; )&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; [1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<29d24188.3d8c.17347214484.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:43:49 GMT",
        "subject": "Re:Re: Re: flink 1.11 es未定义pk的sink问题",
        "content": "hi，YangZe，Leonard，&#010;我增加了一个可以复现问题的测试类，可以执行下看看。可以明显观察到，两个sink在有PK时写入正常，在没有PK时只有一条记录（id是索引名）。&#010;&#010;&#010;&#010;import org.apache.flink.api.common.typeinfo.Types;&#010;&#010;import org.apache.flink.streaming.api.datastream.DataStream;&#010;&#010;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&#010;import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;&#010;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#010;&#010;import org.apache.flink.table.api.StatementSet;&#010;&#010;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;&#010;&#010;import org.apache.flink.types.Row;&#010;&#010;&#010;&#010;&#010;import static org.apache.flink.table.api.Expressions.$;&#010;&#010;&#010;&#010;&#010;public class ESNewJobTest {&#010;&#010;&#010;&#010;&#010;    //构建StreamExecutionEnvironment&#010;&#010;    public static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&#010;&#010;&#010;&#010;    //构建EnvironmentSettings 并指定Blink Planner&#010;&#010;    private static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&#010;&#010;&#010;&#010;    //构建StreamTableEnvironment&#010;&#010;    public static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);&#010;&#010;&#010;&#010;&#010;    //DDL语句&#010;&#010;    public static final String ES_SINK_DDL_NO_PK = \"CREATE TABLE es_sink_test_no_pk (\\n\" +&#010;&#010;            \"  idx integer,\\n\" +&#010;&#010;            \"  firstx varchar\\n\" +&#010;&#010;            \") WITH (\\n\" +&#010;&#010;            \"'connector' = 'elasticsearch-6',\\n\" +&#010;&#010;            \"'hosts' = '168.61.113.171:9200',\\n\" +&#010;&#010;            \"'index' = 'es_sink_test_no_pk',\\n\" +&#010;&#010;            \"'document-type' = 'default',\\n\" +&#010;&#010;            \"'document-id.key-delimiter' = '$',\\n\" +&#010;&#010;            \"'sink.bulk-flush.interval' = '1000',\\n\" +&#010;&#010;            \"'failure-handler' = 'fail',\\n\" +&#010;&#010;            \"'format' = 'json'\\n\" +&#010;&#010;            \")\";&#010;&#010;    public static final String ES_SINK_DDL_WITH_PK = \"CREATE TABLE es_sink_test_with_pk (\\n\"&#010;+&#010;&#010;            \"  idx integer,\\n\" +&#010;&#010;            \"  firstx varchar,\\n\" +&#010;&#010;            \"  primary key (idx, firstx) not enforced\\n\" +&#010;&#010;            \") WITH (\\n\" +&#010;&#010;            \"'connector' = 'elasticsearch-6',\\n\" +&#010;&#010;            \"'hosts' = '168.61.113.171:9200',\\n\" +&#010;&#010;            \"'index' = 'es_sink_test_with_pk',\\n\" +&#010;&#010;            \"'document-type' = 'default',\\n\" +&#010;&#010;            \"'document-id.key-delimiter' = '$',\\n\" +&#010;&#010;            \"'sink.bulk-flush.interval' = '1000',\\n\" +&#010;&#010;            \"'failure-handler' = 'fail',\\n\" +&#010;&#010;            \"'format' = 'json'\\n\" +&#010;&#010;            \")\";&#010;&#010;&#010;&#010;&#010;    public static String getCharAndNumr(int length) {&#010;&#010;        StringBuffer valSb = new StringBuffer();&#010;&#010;        for (int i = 0; i &lt; length; i++) {&#010;&#010;            String charOrNum = Math.round(Math.random()) % 2 == 0 ? \"char\" : \"num\"; // 输出字母还是数字&#010;&#010;            if (\"char\".equalsIgnoreCase(charOrNum)) {&#010;&#010;                // 字符串&#010;&#010;                int choice = Math.round(Math.random()) % 2 == 0 ? 65 : 97;  // 取得大写字母还是小写字母&#010;&#010;                valSb.append((char) (choice + Math.round(Math.random()*25)));&#010;&#010;            } else if (\"num\".equalsIgnoreCase(charOrNum)) {&#010;&#010;                // 数字&#010;&#010;                valSb.append(String.valueOf(Math.round(Math.random()*9)));&#010;&#010;            }&#010;&#010;        }&#010;&#010;        return valSb.toString();&#010;&#010;&#010;&#010;&#010;    }&#010;&#010;&#010;&#010;&#010;    public static void main(String[] args) throws Exception {&#010;&#010;&#010;&#010;&#010;        DataStream&lt;Row&gt; ds = env.addSource(new RichParallelSourceFunction&lt;Row&gt;()&#010;{&#010;&#010;&#010;&#010;&#010;            volatile boolean flag = true;&#010;&#010;&#010;&#010;&#010;            @Override&#010;&#010;            public void run(SourceContext&lt;Row&gt; ctx) throws Exception {&#010;&#010;                while (flag) {&#010;&#010;                    Row row = new Row(2);&#010;&#010;                    row.setField(0, 2207);&#010;&#010;                    row.setField(1, getCharAndNumr(4));&#010;&#010;                    ctx.collect(row);&#010;&#010;                    Thread.sleep(1000);&#010;&#010;                }&#010;&#010;&#010;&#010;&#010;            }&#010;&#010;&#010;&#010;&#010;            @Override&#010;&#010;            public void cancel() {&#010;&#010;                flag = false;&#010;&#010;            }&#010;&#010;        }).setParallelism(1).returns(Types.ROW(Types.INT, Types.STRING));&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;        //ES sink测试ddl&#010;&#010;        tEnv.executeSql(ES_SINK_DDL_NO_PK);&#010;&#010;        tEnv.executeSql(ES_SINK_DDL_WITH_PK);&#010;&#010;&#010;&#010;&#010;        //source注册成表&#010;&#010;        tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"idx\"), $(\"f1\").as(\"firstx\"), $(\"p\").proctime());&#010;&#010;&#010;&#010;&#010;        //sink写入&#010;&#010;        StatementSet ss = tEnv.createStatementSet();&#010;&#010;        ss.addInsertSql(\"insert into es_sink_test_no_pk select idx, firstx from test\");&#010;&#010;        ss.addInsertSql(\"insert into es_sink_test_with_pk select idx, firstx from test\");&#010;&#010;        ss.execute();&#010;&#010;    }&#010;&#010;}&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 14:03:21，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt;INSERT走的就是processUpsert这个方法，当不指定PK时，生成的key会是null，然后创建一个IndexRequest。&#010;&gt;&#010;&gt;Best,&#010;&gt;Yangze Guo&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 2:00 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; hi, Leonard&#010;&gt;&gt; 我定义了一个ddl和一个dml，sql如下。ddl中没有定义PK，我观察到的现象是：这样在sink到es结果中，结果生成的id是index名，导致只有一条记录。&#010;&gt;&gt; 我将DDL更换为之前版本的with参数（声明使用update-mode = ‘upsert’），不使用1.11最新的with参数，观察到sink结果就正常了。不确定是不是我哪边配置的方式不太对，还是说使用方式有问题。&#010;&gt;&gt;&#010;&gt;&gt;  @karmagyz@gmail.com  我看了下给的源代码，貌似这个是处理upsert的情况，如果不声明pk的话，是不是会是processInsert？&#010;&gt;&gt;&#010;&gt;&gt; CREATE TABLE ES6_SENSORDATA_SERVER_API (&#010;&gt;&gt;   event varchar,&#010;&gt;&gt;   user_id varchar,&#010;&gt;&gt;   distinct_id varchar,&#010;&gt;&gt;   _date varchar,&#010;&gt;&gt;   _event_time varchar,&#010;&gt;&gt;   recv_time varchar,&#010;&gt;&gt;   code varchar,&#010;&gt;&gt;   _current_project varchar,&#010;&gt;&gt;   api varchar,&#010;&gt;&gt;   elapsed int ,&#010;&gt;&gt;   `start` bigint,&#010;&gt;&gt;   is_err int&#010;&gt;&gt; ) WITH (&#010;&gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt;&gt; 'index' = 'flink_sensordata_server_api',&#010;&gt;&gt; 'document-type' = 'default',&#010;&gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt;&gt; 'failure-handler' = 'fail',&#010;&gt;&gt; 'format' = 'json'&#010;&gt;&gt; )&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; INSERT INTO ES6_SENSORDATA_SERVER_API&#010;&gt;&gt;&#010;&gt;&gt; SELECT event,&#010;&gt;&gt;&#010;&gt;&gt;        user_id,&#010;&gt;&gt;&#010;&gt;&gt;        distinct_id,&#010;&gt;&gt;&#010;&gt;&gt;        ts2Date(`time`, 'yyyy-MM-dd') as _date,&#010;&gt;&gt;&#010;&gt;&gt;        ts2Date(`time`, 'yyyy-MM-dd HH:mm:ss.SSS') as _event_time,&#010;&gt;&gt;&#010;&gt;&gt;        ts2Date(recv_time, false, false) as recv_time,&#010;&gt;&gt;&#010;&gt;&gt;        properties.code as code,&#010;&gt;&gt;&#010;&gt;&gt;        properties.`project` as _current_project,&#010;&gt;&gt;&#010;&gt;&gt;        properties.api as api,&#010;&gt;&gt;&#010;&gt;&gt;        properties.elapsed as elapsed,&#010;&gt;&gt;&#010;&gt;&gt;        properties.`start` as `start`,&#010;&gt;&gt;&#010;&gt;&gt;        case when properties.code = '0' then 0 else 1 end as is_err&#010;&gt;&gt;&#010;&gt;&gt; FROM KafkaEventTopic&#010;&gt;&gt;&#010;&gt;&gt; where `type` in ('track') and event in ('serverApiReqEvt')&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-13 13:44:29，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;Hello, fulin&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;这个问题能提供段可以复现的代码吗？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;祝好，&#010;&gt;&gt; &gt;Leonard Xu&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; 在 2020年7月13日，09:50，Yangze Guo &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Hi,&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 如果没有定义主键，ES connector 会把 _id设为null[1]，这样ES的Java&#010;Client会将_id设为一个随机值[2].&#010;&gt;&gt; &gt;&gt; 所以应该不会出现您说的这种情况。您那里的ES有没有请求日志之类的，看一下Flink发过来的请求是什么样的。&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; [1] https://github.com/apache/flink/blob/f0eeaec530e001ab02cb889dfe217e25913660c4/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/RowElasticsearchSinkFunction.java#L102&#010;&gt;&gt; &gt;&gt; [2] https://github.com/elastic/elasticsearch/blob/977230a0ce89a55515dc6ef6452e9f059d9356a2/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L509&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Best,&#010;&gt;&gt; &gt;&gt; Yangze Guo&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; On Sat, Jul 11, 2020 at 11:33 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; hi，&#010;&gt;&gt; &gt;&gt;&gt; 根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary&#010;key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#010;&gt;&gt; &gt;&gt;&gt; 不确定是我配置使用的方式不对，还是确实存在bug。。&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; CREATE TABLE ES6_SENSORDATA_OUTPUT (&#010;&gt;&gt; &gt;&gt;&gt; event varchar,&#010;&gt;&gt; &gt;&gt;&gt; user_id varchar,&#010;&gt;&gt; &gt;&gt;&gt; distinct_id varchar,&#010;&gt;&gt; &gt;&gt;&gt; _date varchar,&#010;&gt;&gt; &gt;&gt;&gt; _event_time varchar,&#010;&gt;&gt; &gt;&gt;&gt; recv_time varchar,&#010;&gt;&gt; &gt;&gt;&gt; _browser_version varchar,&#010;&gt;&gt; &gt;&gt;&gt; path_name varchar,&#010;&gt;&gt; &gt;&gt;&gt; _search varchar,&#010;&gt;&gt; &gt;&gt;&gt; event_type varchar,&#010;&gt;&gt; &gt;&gt;&gt; _current_project varchar,&#010;&gt;&gt; &gt;&gt;&gt; message varchar,&#010;&gt;&gt; &gt;&gt;&gt; stack varchar,&#010;&gt;&gt; &gt;&gt;&gt; component_stack varchar,&#010;&gt;&gt; &gt;&gt;&gt; _screen_width varchar,&#010;&gt;&gt; &gt;&gt;&gt; _screen_height varchar&#010;&gt;&gt; &gt;&gt;&gt; ) WITH (&#010;&gt;&gt; &gt;&gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt;&gt; &gt;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt;&gt; &gt;&gt;&gt; 'index' = 'flink_sensordata_target_event',&#010;&gt;&gt; &gt;&gt;&gt; 'document-type' = 'default',&#010;&gt;&gt; &gt;&gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt;&gt; &gt;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt;&gt; &gt;&gt;&gt; 'failure-handler' = 'fail',&#010;&gt;&gt; &gt;&gt;&gt; 'format' = 'json'&#010;&gt;&gt; &gt;&gt;&gt; )&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; [1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "5",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAFTKPZqRj+PSmAqjV=02pU1DvHDosTn7ksVuBjUZG1iqW7ZWYw@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:19:23 GMT",
        "subject": "Re: Re: Re: flink 1.11 es未定义pk的sink问题",
        "content": "验证了一下，这确实是一个bug，原因出在这一行[1]。我会提一个ticket来解决它，争取在1.11.1修复。&#010;&#010;[1] https://github.com/apache/flink/blob/0fbea46ac0271dd84fa8acd7f99f449a9a0d458c/flink-connectors/flink-connector-elasticsearch6/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/Elasticsearch6DynamicSink.java#L285&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Mon, Jul 13, 2020 at 3:44 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&#010;&gt; hi，YangZe，Leonard，&#010;&gt; 我增加了一个可以复现问题的测试类，可以执行下看看。可以明显观察到，两个sink在有PK时写入正常，在没有PK时只有一条记录（id是索引名）。&#010;&gt;&#010;&gt; import org.apache.flink.api.common.typeinfo.Types;&#010;&gt;&#010;&gt; import org.apache.flink.streaming.api.datastream.DataStream;&#010;&gt;&#010;&gt; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt;&#010;&gt; import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;&#010;&gt;&#010;&gt; import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt;&#010;&gt; import org.apache.flink.table.api.StatementSet;&#010;&gt;&#010;&gt; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;&#010;&gt;&#010;&gt; import org.apache.flink.types.Row;&#010;&gt;&#010;&gt;&#010;&gt; import static org.apache.flink.table.api.Expressions.$;&#010;&gt;&#010;&gt;&#010;&gt; public class ESNewJobTest {&#010;&gt;&#010;&gt;&#010;&gt;     //构建StreamExecutionEnvironment&#010;&gt;&#010;&gt;     public static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&#010;&gt;&#010;&gt;     //构建EnvironmentSettings 并指定Blink Planner&#010;&gt;&#010;&gt;     private static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&#010;&gt;&#010;&gt;     //构建StreamTableEnvironment&#010;&gt;&#010;&gt;     public static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#010;bsSettings);&#010;&gt;&#010;&gt;&#010;&gt;     //DDL语句&#010;&gt;&#010;&gt;     public static final String ES_SINK_DDL_NO_PK = \"CREATE TABLE es_sink_test_no_pk (\\n\"&#010;+&#010;&gt;&#010;&gt;             \"  idx integer,\\n\" +&#010;&gt;&#010;&gt;             \"  firstx varchar\\n\" +&#010;&gt;&#010;&gt;             \") WITH (\\n\" +&#010;&gt;&#010;&gt;             \"'connector' = 'elasticsearch-6',\\n\" +&#010;&gt;&#010;&gt;             \"'hosts' = '168.61.113.171:9200',\\n\" +&#010;&gt;&#010;&gt;             \"'index' = 'es_sink_test_no_pk',\\n\" +&#010;&gt;&#010;&gt;             \"'document-type' = 'default',\\n\" +&#010;&gt;&#010;&gt;             \"'document-id.key-delimiter' = '$',\\n\" +&#010;&gt;&#010;&gt;             \"'sink.bulk-flush.interval' = '1000',\\n\" +&#010;&gt;&#010;&gt;             \"'failure-handler' = 'fail',\\n\" +&#010;&gt;&#010;&gt;             \"'format' = 'json'\\n\" +&#010;&gt;&#010;&gt;             \")\";&#010;&gt;&#010;&gt;     public static final String ES_SINK_DDL_WITH_PK = \"CREATE TABLE es_sink_test_with_pk&#010;(\\n\" +&#010;&gt;&#010;&gt;             \"  idx integer,\\n\" +&#010;&gt;&#010;&gt;             \"  firstx varchar,\\n\" +&#010;&gt;&#010;&gt;             \"  primary key (idx, firstx) not enforced\\n\" +&#010;&gt;&#010;&gt;             \") WITH (\\n\" +&#010;&gt;&#010;&gt;             \"'connector' = 'elasticsearch-6',\\n\" +&#010;&gt;&#010;&gt;             \"'hosts' = '168.61.113.171:9200',\\n\" +&#010;&gt;&#010;&gt;             \"'index' = 'es_sink_test_with_pk',\\n\" +&#010;&gt;&#010;&gt;             \"'document-type' = 'default',\\n\" +&#010;&gt;&#010;&gt;             \"'document-id.key-delimiter' = '$',\\n\" +&#010;&gt;&#010;&gt;             \"'sink.bulk-flush.interval' = '1000',\\n\" +&#010;&gt;&#010;&gt;             \"'failure-handler' = 'fail',\\n\" +&#010;&gt;&#010;&gt;             \"'format' = 'json'\\n\" +&#010;&gt;&#010;&gt;             \")\";&#010;&gt;&#010;&gt;&#010;&gt;     public static String getCharAndNumr(int length) {&#010;&gt;&#010;&gt;         StringBuffer valSb = new StringBuffer();&#010;&gt;&#010;&gt;         for (int i = 0; i &lt; length; i++) {&#010;&gt;&#010;&gt;             String charOrNum = Math.round(Math.random()) % 2 == 0 ? \"char\" : \"num\"; //&#010;输出字母还是数字&#010;&gt;&#010;&gt;             if (\"char\".equalsIgnoreCase(charOrNum)) {&#010;&gt;&#010;&gt;                 // 字符串&#010;&gt;&#010;&gt;                 int choice = Math.round(Math.random()) % 2 == 0 ? 65 : 97;  // 取得大写字母还是小写字母&#010;&gt;&#010;&gt;                 valSb.append((char) (choice + Math.round(Math.random()*25)));&#010;&gt;&#010;&gt;             } else if (\"num\".equalsIgnoreCase(charOrNum)) {&#010;&gt;&#010;&gt;                 // 数字&#010;&gt;&#010;&gt;                 valSb.append(String.valueOf(Math.round(Math.random()*9)));&#010;&gt;&#010;&gt;             }&#010;&gt;&#010;&gt;         }&#010;&gt;&#010;&gt;         return valSb.toString();&#010;&gt;&#010;&gt;&#010;&gt;     }&#010;&gt;&#010;&gt;&#010;&gt;     public static void main(String[] args) throws Exception {&#010;&gt;&#010;&gt;&#010;&gt;         DataStream&lt;Row&gt; ds = env.addSource(new RichParallelSourceFunction&lt;Row&gt;()&#010;{&#010;&gt;&#010;&gt;&#010;&gt;             volatile boolean flag = true;&#010;&gt;&#010;&gt;&#010;&gt;             @Override&#010;&gt;&#010;&gt;             public void run(SourceContext&lt;Row&gt; ctx) throws Exception {&#010;&gt;&#010;&gt;                 while (flag) {&#010;&gt;&#010;&gt;                     Row row = new Row(2);&#010;&gt;&#010;&gt;                     row.setField(0, 2207);&#010;&gt;&#010;&gt;                     row.setField(1, getCharAndNumr(4));&#010;&gt;&#010;&gt;                     ctx.collect(row);&#010;&gt;&#010;&gt;                     Thread.sleep(1000);&#010;&gt;&#010;&gt;                 }&#010;&gt;&#010;&gt;&#010;&gt;             }&#010;&gt;&#010;&gt;&#010;&gt;             @Override&#010;&gt;&#010;&gt;             public void cancel() {&#010;&gt;&#010;&gt;                 flag = false;&#010;&gt;&#010;&gt;             }&#010;&gt;&#010;&gt;         }).setParallelism(1).returns(Types.ROW(Types.INT, Types.STRING));&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;         //ES sink测试ddl&#010;&gt;&#010;&gt;         tEnv.executeSql(ES_SINK_DDL_NO_PK);&#010;&gt;&#010;&gt;         tEnv.executeSql(ES_SINK_DDL_WITH_PK);&#010;&gt;&#010;&gt;&#010;&gt;         //source注册成表&#010;&gt;&#010;&gt;         tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"idx\"), $(\"f1\").as(\"firstx\"),&#010;$(\"p\").proctime());&#010;&gt;&#010;&gt;&#010;&gt;         //sink写入&#010;&gt;&#010;&gt;         StatementSet ss = tEnv.createStatementSet();&#010;&gt;&#010;&gt;         ss.addInsertSql(\"insert into es_sink_test_no_pk select idx, firstx from test\");&#010;&gt;&#010;&gt;         ss.addInsertSql(\"insert into es_sink_test_with_pk select idx, firstx from test\");&#010;&gt;&#010;&gt;         ss.execute();&#010;&gt;&#010;&gt;     }&#010;&gt;&#010;&gt; }&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-13 14:03:21，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt; &gt;INSERT走的就是processUpsert这个方法，当不指定PK时，生成的key会是null，然后创建一个IndexRequest。&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Yangze Guo&#010;&gt; &gt;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 2:00 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; hi, Leonard&#010;&gt; &gt;&gt; 我定义了一个ddl和一个dml，sql如下。ddl中没有定义PK，我观察到的现象是：这样在sink到es结果中，结果生成的id是index名，导致只有一条记录。&#010;&gt; &gt;&gt; 我将DDL更换为之前版本的with参数（声明使用update-mode = ‘upsert’），不使用1.11最新的with参数，观察到sink结果就正常了。不确定是不是我哪边配置的方式不太对，还是说使用方式有问题。&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;  @karmagyz@gmail.com  我看了下给的源代码，貌似这个是处理upsert的情况，如果不声明pk的话，是不是会是processInsert？&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; CREATE TABLE ES6_SENSORDATA_SERVER_API (&#010;&gt; &gt;&gt;   event varchar,&#010;&gt; &gt;&gt;   user_id varchar,&#010;&gt; &gt;&gt;   distinct_id varchar,&#010;&gt; &gt;&gt;   _date varchar,&#010;&gt; &gt;&gt;   _event_time varchar,&#010;&gt; &gt;&gt;   recv_time varchar,&#010;&gt; &gt;&gt;   code varchar,&#010;&gt; &gt;&gt;   _current_project varchar,&#010;&gt; &gt;&gt;   api varchar,&#010;&gt; &gt;&gt;   elapsed int ,&#010;&gt; &gt;&gt;   `start` bigint,&#010;&gt; &gt;&gt;   is_err int&#010;&gt; &gt;&gt; ) WITH (&#010;&gt; &gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt; &gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt; &gt;&gt; 'index' = 'flink_sensordata_server_api',&#010;&gt; &gt;&gt; 'document-type' = 'default',&#010;&gt; &gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt; &gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt; &gt;&gt; 'failure-handler' = 'fail',&#010;&gt; &gt;&gt; 'format' = 'json'&#010;&gt; &gt;&gt; )&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; INSERT INTO ES6_SENSORDATA_SERVER_API&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; SELECT event,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        user_id,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        distinct_id,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        ts2Date(`time`, 'yyyy-MM-dd') as _date,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        ts2Date(`time`, 'yyyy-MM-dd HH:mm:ss.SSS') as _event_time,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        ts2Date(recv_time, false, false) as recv_time,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        properties.code as code,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        properties.`project` as _current_project,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        properties.api as api,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        properties.elapsed as elapsed,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        properties.`start` as `start`,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;        case when properties.code = '0' then 0 else 1 end as is_err&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; FROM KafkaEventTopic&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; where `type` in ('track') and event in ('serverApiReqEvt')&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-13 13:44:29，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;Hello, fulin&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;这个问题能提供段可以复现的代码吗？&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;祝好，&#010;&gt; &gt;&gt; &gt;Leonard Xu&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; 在 2020年7月13日，09:50，Yangze Guo &lt;karmagyz@gmail.com&gt;&#010;写道：&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 如果没有定义主键，ES connector 会把 _id设为null[1]，这样ES的Java&#010;Client会将_id设为一个随机值[2].&#010;&gt; &gt;&gt; &gt;&gt; 所以应该不会出现您说的这种情况。您那里的ES有没有请求日志之类的，看一下Flink发过来的请求是什么样的。&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; [1] https://github.com/apache/flink/blob/f0eeaec530e001ab02cb889dfe217e25913660c4/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/RowElasticsearchSinkFunction.java#L102&#010;&gt; &gt;&gt; &gt;&gt; [2] https://github.com/elastic/elasticsearch/blob/977230a0ce89a55515dc6ef6452e9f059d9356a2/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L509&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Best,&#010;&gt; &gt;&gt; &gt;&gt; Yangze Guo&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; On Sat, Jul 11, 2020 at 11:33 PM sunfulin &lt;sunfulin0321@163.com&gt;&#010;wrote:&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; hi，&#010;&gt; &gt;&gt; &gt;&gt;&gt; 根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary&#010;key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#010;&gt; &gt;&gt; &gt;&gt;&gt; 不确定是我配置使用的方式不对，还是确实存在bug。。&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; CREATE TABLE ES6_SENSORDATA_OUTPUT (&#010;&gt; &gt;&gt; &gt;&gt;&gt; event varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; user_id varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; distinct_id varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; _date varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; _event_time varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; recv_time varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; _browser_version varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; path_name varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; _search varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; event_type varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; _current_project varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; message varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; stack varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; component_stack varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; _screen_width varchar,&#010;&gt; &gt;&gt; &gt;&gt;&gt; _screen_height varchar&#010;&gt; &gt;&gt; &gt;&gt;&gt; ) WITH (&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'index' = 'flink_sensordata_target_event',&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'document-type' = 'default',&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'failure-handler' = 'fail',&#010;&gt; &gt;&gt; &gt;&gt;&gt; 'format' = 'json'&#010;&gt; &gt;&gt; &gt;&gt;&gt; )&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; [1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "6",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<4A298C24-D026-41AE-89AF-9E70A0EE5F63@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 10:15:35 GMT",
        "subject": "Re: flink 1.11 es未定义pk的sink问题",
        "content": "HI，fulin&#010;&#010;如 Yangze所说，这是es6 new connector 引入的一个bug,  你可以使用用old connector的语法绕过，就是connector.type=’xx’&#010;，这样代码路径还走之前的代码, 或者使用es7 nconnector。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月13日，17:19，Yangze Guo &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; 验证了一下，这确实是一个bug，原因出在这一行[1]。我会提一个ticket来解决它，争取在1.11.1修复。&#010;&gt; &#010;&gt; [1] https://github.com/apache/flink/blob/0fbea46ac0271dd84fa8acd7f99f449a9a0d458c/flink-connectors/flink-connector-elasticsearch6/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/Elasticsearch6DynamicSink.java#L285&#010;&gt; &#010;&gt; Best,&#010;&gt; Yangze Guo&#010;&gt; &#010;&gt; On Mon, Jul 13, 2020 at 3:44 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&gt; &#010;&gt;&gt; hi，YangZe，Leonard，&#010;&gt;&gt; 我增加了一个可以复现问题的测试类，可以执行下看看。可以明显观察到，两个sink在有PK时写入正常，在没有PK时只有一条记录（id是索引名）。&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.api.common.typeinfo.Types;&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.streaming.api.datastream.DataStream;&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.table.api.StatementSet;&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;&#010;&gt;&gt; &#010;&gt;&gt; import org.apache.flink.types.Row;&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; import static org.apache.flink.table.api.Expressions.$;&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; public class ESNewJobTest {&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    //构建StreamExecutionEnvironment&#010;&gt;&gt; &#010;&gt;&gt;    public static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    //构建EnvironmentSettings 并指定Blink Planner&#010;&gt;&gt; &#010;&gt;&gt;    private static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    //构建StreamTableEnvironment&#010;&gt;&gt; &#010;&gt;&gt;    public static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#010;bsSettings);&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    //DDL语句&#010;&gt;&gt; &#010;&gt;&gt;    public static final String ES_SINK_DDL_NO_PK = \"CREATE TABLE es_sink_test_no_pk&#010;(\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"  idx integer,\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"  firstx varchar\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \") WITH (\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'connector' = 'elasticsearch-6',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'hosts' = '168.61.113.171:9200',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'index' = 'es_sink_test_no_pk',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'document-type' = 'default',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'document-id.key-delimiter' = '$',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'sink.bulk-flush.interval' = '1000',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'failure-handler' = 'fail',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'format' = 'json'\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \")\";&#010;&gt;&gt; &#010;&gt;&gt;    public static final String ES_SINK_DDL_WITH_PK = \"CREATE TABLE es_sink_test_with_pk&#010;(\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"  idx integer,\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"  firstx varchar,\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"  primary key (idx, firstx) not enforced\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \") WITH (\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'connector' = 'elasticsearch-6',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'hosts' = '168.61.113.171:9200',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'index' = 'es_sink_test_with_pk',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'document-type' = 'default',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'document-id.key-delimiter' = '$',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'sink.bulk-flush.interval' = '1000',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'failure-handler' = 'fail',\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \"'format' = 'json'\\n\" +&#010;&gt;&gt; &#010;&gt;&gt;            \")\";&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    public static String getCharAndNumr(int length) {&#010;&gt;&gt; &#010;&gt;&gt;        StringBuffer valSb = new StringBuffer();&#010;&gt;&gt; &#010;&gt;&gt;        for (int i = 0; i &lt; length; i++) {&#010;&gt;&gt; &#010;&gt;&gt;            String charOrNum = Math.round(Math.random()) % 2 == 0 ? \"char\" : \"num\";&#010;// 输出字母还是数字&#010;&gt;&gt; &#010;&gt;&gt;            if (\"char\".equalsIgnoreCase(charOrNum)) {&#010;&gt;&gt; &#010;&gt;&gt;                // 字符串&#010;&gt;&gt; &#010;&gt;&gt;                int choice = Math.round(Math.random()) % 2 == 0 ? 65 : 97;  // 取得大写字母还是小写字母&#010;&gt;&gt; &#010;&gt;&gt;                valSb.append((char) (choice + Math.round(Math.random()*25)));&#010;&gt;&gt; &#010;&gt;&gt;            } else if (\"num\".equalsIgnoreCase(charOrNum)) {&#010;&gt;&gt; &#010;&gt;&gt;                // 数字&#010;&gt;&gt; &#010;&gt;&gt;                valSb.append(String.valueOf(Math.round(Math.random()*9)));&#010;&gt;&gt; &#010;&gt;&gt;            }&#010;&gt;&gt; &#010;&gt;&gt;        }&#010;&gt;&gt; &#010;&gt;&gt;        return valSb.toString();&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    }&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    public static void main(String[] args) throws Exception {&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;        DataStream&lt;Row&gt; ds = env.addSource(new RichParallelSourceFunction&lt;Row&gt;()&#010;{&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;            volatile boolean flag = true;&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;            @Override&#010;&gt;&gt; &#010;&gt;&gt;            public void run(SourceContext&lt;Row&gt; ctx) throws Exception {&#010;&gt;&gt; &#010;&gt;&gt;                while (flag) {&#010;&gt;&gt; &#010;&gt;&gt;                    Row row = new Row(2);&#010;&gt;&gt; &#010;&gt;&gt;                    row.setField(0, 2207);&#010;&gt;&gt; &#010;&gt;&gt;                    row.setField(1, getCharAndNumr(4));&#010;&gt;&gt; &#010;&gt;&gt;                    ctx.collect(row);&#010;&gt;&gt; &#010;&gt;&gt;                    Thread.sleep(1000);&#010;&gt;&gt; &#010;&gt;&gt;                }&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;            }&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;            @Override&#010;&gt;&gt; &#010;&gt;&gt;            public void cancel() {&#010;&gt;&gt; &#010;&gt;&gt;                flag = false;&#010;&gt;&gt; &#010;&gt;&gt;            }&#010;&gt;&gt; &#010;&gt;&gt;        }).setParallelism(1).returns(Types.ROW(Types.INT, Types.STRING));&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;        //ES sink测试ddl&#010;&gt;&gt; &#010;&gt;&gt;        tEnv.executeSql(ES_SINK_DDL_NO_PK);&#010;&gt;&gt; &#010;&gt;&gt;        tEnv.executeSql(ES_SINK_DDL_WITH_PK);&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;        //source注册成表&#010;&gt;&gt; &#010;&gt;&gt;        tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"idx\"), $(\"f1\").as(\"firstx\"),&#010;$(\"p\").proctime());&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;        //sink写入&#010;&gt;&gt; &#010;&gt;&gt;        StatementSet ss = tEnv.createStatementSet();&#010;&gt;&gt; &#010;&gt;&gt;        ss.addInsertSql(\"insert into es_sink_test_no_pk select idx, firstx from test\");&#010;&gt;&gt; &#010;&gt;&gt;        ss.addInsertSql(\"insert into es_sink_test_with_pk select idx, firstx from&#010;test\");&#010;&gt;&gt; &#010;&gt;&gt;        ss.execute();&#010;&gt;&gt; &#010;&gt;&gt;    }&#010;&gt;&gt; &#010;&gt;&gt; }&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 在 2020-07-13 14:03:21，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; INSERT走的就是processUpsert这个方法，当不指定PK时，生成的key会是null，然后创建一个IndexRequest。&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt; Yangze Guo&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; On Mon, Jul 13, 2020 at 2:00 PM sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; hi, Leonard&#010;&gt;&gt;&gt;&gt; 我定义了一个ddl和一个dml，sql如下。ddl中没有定义PK，我观察到的现象是：这样在sink到es结果中，结果生成的id是index名，导致只有一条记录。&#010;&gt;&gt;&gt;&gt; 我将DDL更换为之前版本的with参数（声明使用update-mode = ‘upsert’），不使用1.11最新的with参数，观察到sink结果就正常了。不确定是不是我哪边配置的方式不太对，还是说使用方式有问题。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; @karmagyz@gmail.com  我看了下给的源代码，貌似这个是处理upsert的情况，如果不声明pk的话，是不是会是processInsert？&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; CREATE TABLE ES6_SENSORDATA_SERVER_API (&#010;&gt;&gt;&gt;&gt;  event varchar,&#010;&gt;&gt;&gt;&gt;  user_id varchar,&#010;&gt;&gt;&gt;&gt;  distinct_id varchar,&#010;&gt;&gt;&gt;&gt;  _date varchar,&#010;&gt;&gt;&gt;&gt;  _event_time varchar,&#010;&gt;&gt;&gt;&gt;  recv_time varchar,&#010;&gt;&gt;&gt;&gt;  code varchar,&#010;&gt;&gt;&gt;&gt;  _current_project varchar,&#010;&gt;&gt;&gt;&gt;  api varchar,&#010;&gt;&gt;&gt;&gt;  elapsed int ,&#010;&gt;&gt;&gt;&gt;  `start` bigint,&#010;&gt;&gt;&gt;&gt;  is_err int&#010;&gt;&gt;&gt;&gt; ) WITH (&#010;&gt;&gt;&gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt;&gt;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt;&gt;&gt;&gt; 'index' = 'flink_sensordata_server_api',&#010;&gt;&gt;&gt;&gt; 'document-type' = 'default',&#010;&gt;&gt;&gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt;&gt;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt;&gt;&gt;&gt; 'failure-handler' = 'fail',&#010;&gt;&gt;&gt;&gt; 'format' = 'json'&#010;&gt;&gt;&gt;&gt; )&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; INSERT INTO ES6_SENSORDATA_SERVER_API&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; SELECT event,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       user_id,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       distinct_id,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       ts2Date(`time`, 'yyyy-MM-dd') as _date,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       ts2Date(`time`, 'yyyy-MM-dd HH:mm:ss.SSS') as _event_time,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       ts2Date(recv_time, false, false) as recv_time,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       properties.code as code,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       properties.`project` as _current_project,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       properties.api as api,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       properties.elapsed as elapsed,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       properties.`start` as `start`,&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       case when properties.code = '0' then 0 else 1 end as is_err&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; FROM KafkaEventTopic&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; where `type` in ('track') and event in ('serverApiReqEvt')&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 在 2020-07-13 13:44:29，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt;&gt; Hello, fulin&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 这个问题能提供段可以复现的代码吗？&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 祝好，&#010;&gt;&gt;&gt;&gt;&gt; Leonard Xu&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 在 2020年7月13日，09:50，Yangze Guo &lt;karmagyz@gmail.com&gt;&#010;写道：&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Hi,&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 如果没有定义主键，ES connector 会把 _id设为null[1]，这样ES的Java&#010;Client会将_id设为一个随机值[2].&#010;&gt;&gt;&gt;&gt;&gt;&gt; 所以应该不会出现您说的这种情况。您那里的ES有没有请求日志之类的，看一下Flink发过来的请求是什么样的。&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; [1] https://github.com/apache/flink/blob/f0eeaec530e001ab02cb889dfe217e25913660c4/flink-connectors/flink-connector-elasticsearch-base/src/main/java/org/apache/flink/streaming/connectors/elasticsearch/table/RowElasticsearchSinkFunction.java#L102&#010;&gt;&gt;&gt;&gt;&gt;&gt; [2] https://github.com/elastic/elasticsearch/blob/977230a0ce89a55515dc6ef6452e9f059d9356a2/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L509&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt;&gt;&gt;&gt; Yangze Guo&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; On Sat, Jul 11, 2020 at 11:33 PM sunfulin &lt;sunfulin0321@163.com&gt;&#010;wrote:&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; hi，&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 根据文档[1]的描述，1.11的es sql connector如果在ddl里没有声明primary&#010;key，将会使用append模式sink数据，并使用es本身生成的id作为document_id。但是我在测试时发现，如果我的ddl里没有定义primary&#010;key，写入时没有正确生成document_id，反而是将index作为id生成了。导致只有最新的一条记录。下面是我的ddl定义：&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 不确定是我配置使用的方式不对，还是确实存在bug。。&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; CREATE TABLE ES6_SENSORDATA_OUTPUT (&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; event varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; user_id varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; distinct_id varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _date varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _event_time varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; recv_time varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _browser_version varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; path_name varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _search varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; event_type varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _current_project varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; message varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; stack varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; component_stack varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _screen_width varchar,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; _screen_height varchar&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; ) WITH (&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'connector' = 'elasticsearch-6',&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'hosts' = '&lt;ES_YUNTU.SERVERS&gt;',&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'index' = 'flink_sensordata_target_event',&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'document-type' = 'default',&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'document-id.key-delimiter' = '$',&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'sink.bulk-flush.interval' = '1000',&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'failure-handler' = 'fail',&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 'format' = 'json'&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; )&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; [1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/elasticsearch.html#key-handling&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "7",
        "reply": "<6f72a91a.2dc4.1733e818baf.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<102af46c.1599.17342a6c45c.Coremail.stevenchen01@163.com>",
        "from": "&quot;steven chen&quot; &lt;stevenche...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 12 Jul 2020 10:51:32 GMT",
        "subject": "使用hive 作为元数据 catalag 的问题",
        "content": "hi:&#010;&#010;这种是什么问题？&#010;&#010;",
        "depth": "0",
        "reply": "<102af46c.1599.17342a6c45c.Coremail.stevenchen01@163.com>"
    },
    {
        "id": "<CABi+2jTNYKWrprQ=RqsaNhn6gz_RSTrYOwqfPuciO1YtO7g3DA@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:47:01 GMT",
        "subject": "Re: 使用hive 作为元数据 catalag 的问题",
        "content": "图挂了&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 4:46 PM steven chen &lt;stevenchen01@163.com&gt; wrote:&#010;&#010;&gt; hi:&#010;&gt;&#010;&gt; 这种是什么问题？&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "1",
        "reply": "<102af46c.1599.17342a6c45c.Coremail.stevenchen01@163.com>"
    },
    {
        "id": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>",
        "from": "程龙 &lt;13162790...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 02:49:09 GMT",
        "subject": "flink on yarn日志问题",
        "content": "请问一下两个问题&#010;1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉 无法查看&#010;，除了使用es收集日志的这种方案， 还有没有可以使taskmanager 挂掉，相关日志仍然可以保留。&#010;2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager 却一直存在，&#010;有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后 taskmanager挂掉，jobmanager也挂掉&#010;  ",
        "depth": "0",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAFTKPZqXVuzzxuO8HdkDVW7AmqtnnNCwUsLnKtUoo+eVeXfbLw@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 03:18:46 GMT",
        "subject": "Re: flink on yarn日志问题",
        "content": "Hi,&#013;&#010;&#013;&#010;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&#013;&#010;第二个问题，您可以尝试一下per-job mode [2][3]&#013;&#010;&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;[2] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;[3] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; 请问一下两个问题&#013;&#010;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉 无法查看&#010;，除了使用es收集日志的这种方案， 还有没有可以使taskmanager 挂掉，相关日志仍然可以保留。&#013;&#010;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager 却一直存在，&#010;有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后 taskmanager挂掉，jobmanager也挂掉&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<438a3b65.5a04.17346e6a81b.Coremail.13162790856@163.com>",
        "from": "程龙 &lt;13162790...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:39:48 GMT",
        "subject": "Re:Re: flink on yarn日志问题",
        "content": "不好意思  怪我灭有描述清楚 &#010;1 目前开启日志收集功能&#010;2 目前已是 per-job模式&#010;3 集群使用cdh flink.1.10&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#010;&gt;&#010;&gt;第二个问题，您可以尝试一下per-job mode [2][3]&#010;&gt;&#010;&gt;[1] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#010;&gt;[2] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#010;&gt;[3] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#010;&gt;&#010;&gt;&#010;&gt;Best,&#010;&gt;Yangze Guo&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt; 请问一下两个问题&#010;&gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉&#010;无法查看 ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager&#010;挂掉，相关日志仍然可以保留。&#010;&gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager&#010;却一直存在， 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#010;&gt;&gt;&#010;",
        "depth": "2",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAFTKPZoorSxvwc0GNSt6KMcAhaFXOhu_Axs19n1a2WNDf74Yag@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:03:10 GMT",
        "subject": "Re: Re: flink on yarn日志问题",
        "content": "1. 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#013;&#010;2. 你是否需要调整一下重启策略[1]? 如果开启了ck，默认情况下就会一直尝试重启job&#013;&#010;&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;&#013;&#010;On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; 不好意思  怪我灭有描述清楚&#013;&#010;&gt; 1 目前开启日志收集功能&#013;&#010;&gt; 2 目前已是 per-job模式&#013;&#010;&gt; 3 集群使用cdh flink.1.10&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;Hi,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;第二个问题，您可以尝试一下per-job mode [2][3]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;[1] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;&gt; &gt;[2] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;&gt; &gt;[3] https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Yangze Guo&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 请问一下两个问题&#013;&#010;&gt; &gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉&#010;无法查看 ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager&#010;挂掉，相关日志仍然可以保留。&#013;&#010;&gt; &gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager&#010;却一直存在， 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#013;&#010;&gt; &gt;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CABFgzE4C0L5XQxZSLW=7LF_VSYM1SLYd-Y1hOn-W8x-h+juLOA@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 00:25:56 GMT",
        "subject": "Re: Re: flink on yarn日志问题",
        "content": "我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#013;&#010;&#013;&#010;Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月13日周一 下午5:03写道：&#013;&#010;&#013;&#010;&gt; 1.&#013;&#010;&gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#013;&#010;&gt; 2. 你是否需要调整一下重启策略[1]? 如果开启了ck，默认情况下就会一直尝试重启job&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yangze Guo&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 不好意思  怪我灭有描述清楚&#013;&#010;&gt; &gt; 1 目前开启日志收集功能&#013;&#010;&gt; &gt; 2 目前已是 per-job模式&#013;&#010;&gt; &gt; 3 集群使用cdh flink.1.10&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;Hi,&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;第二个问题，您可以尝试一下per-job mode [2][3]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;[1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;&gt; &gt; &gt;[2]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;&gt; &gt; &gt;[3]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;Best,&#013;&#010;&gt; &gt; &gt;Yangze Guo&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; 请问一下两个问题&#013;&#010;&gt; &gt; &gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉&#010;无法查看&#013;&#010;&gt; ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager 挂掉，相关日志仍然可以保留。&#013;&#010;&gt; &gt; &gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager&#010;却一直存在，&#013;&#010;&gt; 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAFTKPZp=ngoFDwojerfYMJiEHs+hN1QiTT-1WooPdD8LFbjvoQ@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:58:15 GMT",
        "subject": "Re: Re: flink on yarn日志问题",
        "content": "Hi, 王松&#013;&#010;&#013;&#010;我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#013;&#010;&gt;&#013;&#010;&gt; Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月13日周一 下午5:03写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 1.&#013;&#010;&gt; &gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#013;&#010;&gt; &gt; 2. 你是否需要调整一下重启策略[1]? 如果开启了ck，默认情况下就会一直尝试重启job&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Yangze Guo&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 不好意思  怪我灭有描述清楚&#013;&#010;&gt; &gt; &gt; 1 目前开启日志收集功能&#013;&#010;&gt; &gt; &gt; 2 目前已是 per-job模式&#013;&#010;&gt; &gt; &gt; 3 集群使用cdh flink.1.10&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; &gt;Hi,&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;第二个问题，您可以尝试一下per-job mode [2][3]&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;[1]&#013;&#010;&gt; &gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;&gt; &gt; &gt; &gt;[2]&#013;&#010;&gt; &gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;&gt; &gt; &gt; &gt;[3]&#013;&#010;&gt; &gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;Best,&#013;&#010;&gt; &gt; &gt; &gt;Yangze Guo&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; 请问一下两个问题&#013;&#010;&gt; &gt; &gt; &gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉&#010;无法查看&#013;&#010;&gt; &gt; ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager&#010;挂掉，相关日志仍然可以保留。&#013;&#010;&gt; &gt; &gt; &gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager&#010;却一直存在，&#013;&#010;&gt; &gt; 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;",
        "depth": "5",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAMhjQvihdBW9HXv3J4zuxk7is3HJ16_foapT9m68sc-DBtOkcg@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:35:06 GMT",
        "subject": "Re: Re: flink on yarn日志问题",
        "content": "知道 YARN 的 applicationId，应该也可以去 HDFS 找对应的 taskmanager 的日志（可以拼出路径），然后复制到本地去查看&#013;&#010;&#013;&#010;Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月14日周二 上午11:58写道：&#013;&#010;&#013;&#010;&gt; Hi, 王松&#013;&#010;&gt;&#013;&#010;&gt; 我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yangze Guo&#013;&#010;&gt;&#013;&#010;&gt; On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月13日周一 下午5:03写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 1.&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#013;&#010;&gt; &gt; &gt; 2. 你是否需要调整一下重启策略[1]? 如果开启了ck，默认情况下就会一直尝试重启job&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; [1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Yangze Guo&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&gt; wrote:&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 不好意思  怪我灭有描述清楚&#013;&#010;&gt; &gt; &gt; &gt; 1 目前开启日志收集功能&#013;&#010;&gt; &gt; &gt; &gt; 2 目前已是 per-job模式&#013;&#010;&gt; &gt; &gt; &gt; 3 集群使用cdh flink.1.10&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;Hi,&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;第二个问题，您可以尝试一下per-job mode [2][3]&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;[1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;&gt; &gt; &gt; &gt; &gt;[2]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;&gt; &gt; &gt; &gt; &gt;[3]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;Best,&#013;&#010;&gt; &gt; &gt; &gt; &gt;Yangze Guo&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt;&#010;wrote:&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 请问一下两个问题&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉&#010;无法查看&#013;&#010;&gt; &gt; &gt; ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager&#010;挂掉，相关日志仍然可以保留。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager&#010;却一直存在，&#013;&#010;&gt; &gt; &gt; 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<54be3cca.3a0e.1734c8043a5.Coremail.13162790856@163.com>",
        "from": "程龙 &lt;13162790...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:45:41 GMT",
        "subject": "Re:Re: Re: flink on yarn日志问题",
        "content": "运行的日志会越来越多 导致查看日志比较慢 大多采用elk这种方式  除了这个有没有比较好的方案推荐一下&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 12:35:06，\"zhisheng\" &lt;zhisheng2018@gmail.com&gt; 写道：&#010;&gt;知道 YARN 的 applicationId，应该也可以去 HDFS 找对应的 taskmanager 的日志（可以拼出路径），然后复制到本地去查看&#010;&gt;&#010;&gt;Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月14日周二 上午11:58写道：&#010;&gt;&#010;&gt;&gt; Hi, 王松&#010;&gt;&gt;&#010;&gt;&gt; 我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Yangze Guo&#010;&gt;&gt;&#010;&gt;&gt; On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月13日周一 下午5:03写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; &gt; 1.&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#010;&gt;&gt; &gt; &gt; 2. 你是否需要调整一下重启策略[1]? 如果开启了ck，默认情况下就会一直尝试重启job&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; [1]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; Yangze Guo&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&gt; wrote:&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; 不好意思  怪我灭有描述清楚&#010;&gt;&gt; &gt; &gt; &gt; 1 目前开启日志收集功能&#010;&gt;&gt; &gt; &gt; &gt; 2 目前已是 per-job模式&#010;&gt;&gt; &gt; &gt; &gt; 3 集群使用cdh flink.1.10&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; 在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt;&#010;写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt;Hi,&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;第二个问题，您可以尝试一下per-job mode [2][3]&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;[1]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#010;&gt;&gt; &gt; &gt; &gt; &gt;[2]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#010;&gt;&gt; &gt; &gt; &gt; &gt;[3]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;Best,&#010;&gt;&gt; &gt; &gt; &gt; &gt;Yangze Guo&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt;&#010;wrote:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 请问一下两个问题&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉&#010;无法查看&#010;&gt;&gt; &gt; &gt; ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager&#010;挂掉，相关日志仍然可以保留。&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager&#010;却一直存在，&#010;&gt;&gt; &gt; &gt; 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt;&#010;",
        "depth": "7",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<55901288.6799.1734c808998.Coremail.read3210@163.com>",
        "from": "nicygan  &lt;read3...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:45:59 GMT",
        "subject": "Re:Re: Re: flink on yarn日志问题",
        "content": "是有这个毛病，看TM日志不方便。&#010;&#010;而且本地日志过几小时就会被清理，时间一久就看不到了，只剩JM日志。&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 12:35:06，\"zhisheng\" &lt;zhisheng2018@gmail.com&gt; 写道：&#010;&gt;知道 YARN 的 applicationId，应该也可以去 HDFS 找对应的 taskmanager 的日志（可以拼出路径），然后复制到本地去查看&#010;&gt;&#010;&gt;Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月14日周二 上午11:58写道：&#010;&gt;&#010;&gt;&gt; Hi, 王松&#010;&gt;&gt;&#010;&gt;&gt; 我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Yangze Guo&#010;&gt;&gt;&#010;&gt;&gt; On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月13日周一 下午5:03写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; &gt; 1.&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#010;&gt;&gt; &gt; &gt; 2. 你是否需要调整一下重启策略[1]? 如果开启了ck，默认情况下就会一直尝试重启job&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; [1]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; Yangze Guo&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&gt; wrote:&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; 不好意思  怪我灭有描述清楚&#010;&gt;&gt; &gt; &gt; &gt; 1 目前开启日志收集功能&#010;&gt;&gt; &gt; &gt; &gt; 2 目前已是 per-job模式&#010;&gt;&gt; &gt; &gt; &gt; 3 集群使用cdh flink.1.10&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; 在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt;&#010;写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt;Hi,&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;第二个问题，您可以尝试一下per-job mode [2][3]&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;[1]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#010;&gt;&gt; &gt; &gt; &gt; &gt;[2]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#010;&gt;&gt; &gt; &gt; &gt; &gt;[3]&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;Best,&#010;&gt;&gt; &gt; &gt; &gt; &gt;Yangze Guo&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt;&#010;wrote:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 请问一下两个问题&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉&#010;无法查看&#010;&gt;&gt; &gt; &gt; ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager&#010;挂掉，相关日志仍然可以保留。&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager&#010;却一直存在，&#010;&gt;&gt; &gt; &gt; 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt;&#010;",
        "depth": "7",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAFTKPZr=KrTEkbDjh00LYJ=JTWTBY=Mi_0-OHfAkG4Z6mtoGag@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:05:57 GMT",
        "subject": "Re: Re: Re: flink on yarn日志问题",
        "content": "Flink在1.11开始默认使用log4j2, log4j2已经有了很多appender[1]可以用来将日志输出到外部系统或服务。&#013;&#010;&#013;&#010;[1] https://logging.apache.org/log4j/2.x/manual/appenders.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Tue, Jul 14, 2020 at 4:46 PM nicygan &lt;read3210@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; 是有这个毛病，看TM日志不方便。&#013;&#010;&gt;&#013;&#010;&gt; 而且本地日志过几小时就会被清理，时间一久就看不到了，只剩JM日志。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-14 12:35:06，\"zhisheng\" &lt;zhisheng2018@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;知道 YARN 的 applicationId，应该也可以去 HDFS 找对应的 taskmanager&#010;的日志（可以拼出路径），然后复制到本地去查看&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月14日周二 上午11:58写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi, 王松&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best,&#013;&#010;&gt; &gt;&gt; Yangze Guo&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年7月13日周一 下午5:03写道：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; 1.&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#013;&#010;&gt; &gt;&gt; &gt; &gt; 2. 你是否需要调整一下重启策略[1]? 如果开启了ck，默认情况下就会一直尝试重启job&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; [1]&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt;&gt; &gt; &gt; Yangze Guo&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&gt;&#010;wrote:&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 不好意思  怪我灭有描述清楚&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 1 目前开启日志收集功能&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 2 目前已是 per-job模式&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 3 集群使用cdh flink.1.10&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 在 2020-07-13 11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&gt;&#010;写道：&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;Hi,&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;第二个问题，您可以尝试一下per-job mode [2][3]&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;[1]&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;[2]&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;[3]&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;Best,&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;Yangze Guo&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;On Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&gt;&#010;wrote:&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; 请问一下两个问题&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; 1 flink on yarn的时候 taskmanager 挂掉的时候&#010;上面的日志会被删除掉 无法查看&#013;&#010;&gt; &gt;&gt; &gt; &gt; ，除了使用es收集日志的这种方案， 还有没有可以使taskmanager&#010;挂掉，相关日志仍然可以保留。&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; 2 flink on yarn模式 当由于错误导致taskmanager&#010;挂掉，但是jobmanager 却一直存在，&#013;&#010;&gt; &gt;&gt; &gt; &gt; 有没有好的方式或者策略 ，   可以是当task失败 达到重试次数之后&#010;taskmanager挂掉，jobmanager也挂掉&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<tencent_451C792085089639075BA5F6541F64AE8E06@qq.com>",
        "from": "&quot;Cayden chen&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:07:26 GMT",
        "subject": "回复： Re: Re: flink on yarn日志问题",
        "content": "有个问题，如何区分日志是哪个任务的呢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;karmagyz@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月14日(星期二) 下午5:05&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: Re: Re: flink on yarn日志问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Flink在1.11开始默认使用log4j2, log4j2已经有了很多appender[1]可以用来将日志输出到外部系统或服务。&#013;&#010;&#013;&#010;[1] https://logging.apache.org/log4j/2.x/manual/appenders.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Tue, Jul 14, 2020 at 4:46 PM nicygan &lt;read3210@163.com&amp;gt; wrote:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 是有这个毛病，看TM日志不方便。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 而且本地日志过几小时就会被清理，时间一久就看不到了，只剩JM日志。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 在 2020-07-14 12:35:06，\"zhisheng\" &lt;zhisheng2018@gmail.com&amp;gt; 写道：&#013;&#010;&amp;gt; &amp;gt;知道 YARN 的 applicationId，应该也可以去 HDFS 找对应的 taskmanager&#010;的日志（可以拼出路径），然后复制到本地去查看&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;Yangze Guo &lt;karmagyz@gmail.com&amp;gt; 于2020年7月14日周二 上午11:58写道：&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; Hi, 王松&#013;&#010;&amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; 我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#013;&#010;&amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; Best,&#013;&#010;&amp;gt; &amp;gt;&amp;gt; Yangze Guo&#013;&#010;&amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&amp;gt;&#010;wrote:&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; Yangze Guo &lt;karmagyz@gmail.com&amp;gt; 于2020年7月13日周一&#010;下午5:03写道：&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; 1.&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; 2. 你是否需要调整一下重启策略[1]?&#010;如果开启了ck，默认情况下就会一直尝试重启job&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; [1]&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; Best,&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; Yangze Guo&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&amp;gt;&#010;wrote:&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 不好意思&amp;nbsp; 怪我灭有描述清楚&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 1 目前开启日志收集功能&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 2 目前已是 per-job模式&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 3 集群使用cdh flink.1.10&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 在 2020-07-13 11:18:46，\"Yangze Guo\"&#010;&lt;karmagyz@gmail.com&amp;gt; 写道：&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;Hi,&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;第二个问题，您可以尝试一下per-job&#010;mode [2][3]&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;[1]&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;[2]&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;[3]&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;Best,&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;Yangze Guo&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;On Mon, Jul 13, 2020 at 10:49&#010;AM 程龙 &lt;13162790856@163.com&amp;gt; wrote:&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt; 请问一下两个问题&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt; 1 flink on yarn的时候&#010;taskmanager 挂掉的时候 上面的日志会被删除掉 无法查看&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; ，除了使用es收集日志的这种方案，&#010;还有没有可以使taskmanager 挂掉，相关日志仍然可以保留。&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt; 2 flink on yarn模式&#010;当由于错误导致taskmanager 挂掉，但是jobmanager 却一直存在，&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; 有没有好的方式或者策略 ，&amp;nbsp;&amp;nbsp;&#010;可以是当task失败 达到重试次数之后 taskmanager挂掉，jobmanager也挂掉&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;gt;",
        "depth": "9",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<4c6eaca.4b0b.1734caa0e00.Coremail.rjianxu@163.com>",
        "from": "jianxu &lt;rjia...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:31:19 GMT",
        "subject": "回复：   flink on yarn日志问题",
        "content": "&#010;&#010;我们获取运行Yarn日志逻辑大概是这样的：&#010;1. 访问rm-address/ws/v1/cluster/apps/applicationId，拿到amContainerLog中的url即为jm的url.&#010;2. taskmanager日志url通过rm-address/proxy/applicationId/taskmanagers,拿到所有taskmanager的基本信息，替换amContainerLog中的containername和ip。&#010;&#010;&#010;日志比较大时：指定读取的字节开始和结束位置，url?start=0&amp;end=1024&#010;| |&#010;jianxu&#010;|&#010;|&#010;rjianxu@163.com&#010;|&#010;在2020年07月14日 17:07，Cayden chen&lt;1193216154@qq.com&gt; 写道：&#010;有个问题，如何区分日志是哪个任务的呢&#010;&#010;&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;karmagyz@gmail.com&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月14日(星期二) 下午5:05&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Re: Re: Re: flink on yarn日志问题&#010;&#010;&#010;&#010;Flink在1.11开始默认使用log4j2, log4j2已经有了很多appender[1]可以用来将日志输出到外部系统或服务。&#010;&#010;[1] https://logging.apache.org/log4j/2.x/manual/appenders.html&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Tue, Jul 14, 2020 at 4:46 PM nicygan &lt;read3210@163.com&amp;gt; wrote:&#010;&amp;gt;&#010;&amp;gt; 是有这个毛病，看TM日志不方便。&#010;&amp;gt;&#010;&amp;gt; 而且本地日志过几小时就会被清理，时间一久就看不到了，只剩JM日志。&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; 在 2020-07-14 12:35:06，\"zhisheng\" &lt;zhisheng2018@gmail.com&amp;gt; 写道：&#010;&amp;gt; &amp;gt;知道 YARN 的 applicationId，应该也可以去 HDFS 找对应的 taskmanager&#010;的日志（可以拼出路径），然后复制到本地去查看&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;Yangze Guo &lt;karmagyz@gmail.com&amp;gt; 于2020年7月14日周二 上午11:58写道：&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; Hi, 王松&#010;&amp;gt; &amp;gt;&amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; 我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#010;&amp;gt; &amp;gt;&amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; Best,&#010;&amp;gt; &amp;gt;&amp;gt; Yangze Guo&#010;&amp;gt; &amp;gt;&amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&amp;gt;&#010;wrote:&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; Yangze Guo &lt;karmagyz@gmail.com&amp;gt; 于2020年7月13日周一&#010;下午5:03写道：&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; 1.&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; 2. 你是否需要调整一下重启策略[1]?&#010;如果开启了ck，默认情况下就会一直尝试重启job&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; [1]&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; Best,&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; Yangze Guo&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; On Mon, Jul 13, 2020 at 2:40 PM 程龙 &lt;13162790856@163.com&amp;gt;&#010;wrote:&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 不好意思&amp;nbsp; 怪我灭有描述清楚&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 1 目前开启日志收集功能&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 2 目前已是 per-job模式&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 3 集群使用cdh flink.1.10&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; 在 2020-07-13 11:18:46，\"Yangze Guo\"&#010;&lt;karmagyz@gmail.com&amp;gt; 写道：&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;Hi,&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;第二个问题，您可以尝试一下per-job&#010;mode [2][3]&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;[1]&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;[2]&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;[3]&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;Best,&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;Yangze Guo&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;On Mon, Jul 13, 2020 at 10:49&#010;AM 程龙 &lt;13162790856@163.com&amp;gt; wrote:&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt; 请问一下两个问题&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt; 1 flink on yarn的时候&#010;taskmanager 挂掉的时候 上面的日志会被删除掉 无法查看&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; ，除了使用es收集日志的这种方案，&#010;还有没有可以使taskmanager 挂掉，相关日志仍然可以保留。&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt; 2 flink on yarn模式&#010;当由于错误导致taskmanager 挂掉，但是jobmanager 却一直存在，&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; 有没有好的方式或者策略 ，&amp;nbsp;&amp;nbsp;&#010;可以是当task失败 达到重试次数之后 taskmanager挂掉，jobmanager也挂掉&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt; &amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&amp;gt;",
        "depth": "10",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<tencent_AAB6CC75D84EB4D3FAB27215E55C036B4409@qq.com>",
        "from": "&quot;Cayden chen&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 06:56:30 GMT",
        "subject": "回复：   flink on yarn日志问题",
        "content": "我们的获取逻辑是通过定义 logback的appder，appder通过解析当前系统路径(因为flink每个taskmanager会自己定义一个带有applicationId的路径，然后里面会放各种jar包，包括我自定义的appder)，获取之后通过MDC.put(),给日志加一列appId，在appder里面把日志上报到外部的日志系统&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;rjianxu@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月14日(星期二) 下午5:31&#013;&#010;收件人:&amp;nbsp;\"user-zh@flink.apache.org\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复：   flink on yarn日志问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我们获取运行Yarn日志逻辑大概是这样的：&#013;&#010;1. 访问rm-address/ws/v1/cluster/apps/applicationId，拿到amContainerLog中的url即为jm的url.&#013;&#010;2. taskmanager日志url通过rm-address/proxy/applicationId/taskmanagers,拿到所有taskmanager的基本信息，替换amContainerLog中的containername和ip。&#013;&#010;&#013;&#010;&#013;&#010;日志比较大时：指定读取的字节开始和结束位置，url?start=0&amp;amp;end=1024&#013;&#010;| |&#013;&#010;jianxu&#013;&#010;|&#013;&#010;|&#013;&#010;rjianxu@163.com&#013;&#010;|&#013;&#010;在2020年07月14日 17:07，Cayden chen&lt;1193216154@qq.com&amp;gt; 写道：&#013;&#010;有个问题，如何区分日志是哪个任务的呢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;karmagyz@gmail.com&amp;amp;gt;;&#013;&#010;发送时间:&amp;amp;nbsp;2020年7月14日(星期二) 下午5:05&#013;&#010;收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;amp;nbsp;Re: Re: Re: flink on yarn日志问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Flink在1.11开始默认使用log4j2, log4j2已经有了很多appender[1]可以用来将日志输出到外部系统或服务。&#013;&#010;&#013;&#010;[1] https://logging.apache.org/log4j/2.x/manual/appenders.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Tue, Jul 14, 2020 at 4:46 PM nicygan &lt;read3210@163.com&amp;amp;gt; wrote:&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; 是有这个毛病，看TM日志不方便。&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; 而且本地日志过几小时就会被清理，时间一久就看不到了，只剩JM日志。&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; 在 2020-07-14 12:35:06，\"zhisheng\" &lt;zhisheng2018@gmail.com&amp;amp;gt; 写道：&#013;&#010;&amp;amp;gt; &amp;amp;gt;知道 YARN 的 applicationId，应该也可以去 HDFS 找对应的&#010;taskmanager 的日志（可以拼出路径），然后复制到本地去查看&#013;&#010;&amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;Yangze Guo &lt;karmagyz@gmail.com&amp;amp;gt; 于2020年7月14日周二&#010;上午11:58写道：&#013;&#010;&amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; Hi, 王松&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; 我理解拼接url就可以了，不用实际去登陆机器然后进到对应目录。&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; Best,&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; Yangze Guo&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; On Tue, Jul 14, 2020 at 8:26 AM 王松 &lt;sdlcwangsong11@gmail.com&amp;amp;gt;&#010;wrote:&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; 我们也有问题 1，和 Yangze Guo 说的一样，每次都要去对应的tm目录中去找日志，很麻烦，不知道有没有更简单的办法。&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; Yangze Guo &lt;karmagyz@gmail.com&amp;amp;gt;&#010;于2020年7月13日周一 下午5:03写道：&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 1.&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; 我验证了一下，如果开启了日志收集，那tm的日志是会保存的，但是你整个application结束前可能看不到，有一个trick的方法，首先在jm日志中找到tm分配到了哪个NodeManager上，通过拼接url的方式来获取container的日志&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 2. 你是否需要调整一下重启策略[1]?&#010;如果开启了ck，默认情况下就会一直尝试重启job&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; [1]&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/task_failure_recovery.html&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; Yangze Guo&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; On Mon, Jul 13, 2020 at 2:40&#010;PM 程龙 &lt;13162790856@163.com&amp;amp;gt; wrote:&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 不好意思&amp;amp;nbsp;&#010;怪我灭有描述清楚&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 1 目前开启日志收集功能&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 2 目前已是&#010;per-job模式&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 3 集群使用cdh&#010;flink.1.10&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 在 2020-07-13&#010;11:18:46，\"Yangze Guo\" &lt;karmagyz@gmail.com&amp;amp;gt; 写道：&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;Hi,&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;第一个问题，您可以尝试开启Yarn的日志收集功能[1]&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;第二个问题，您可以尝试一下per-job&#010;mode [2][3]&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;[1]&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#log-files&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;[2]&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/#per-job-mode&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;[3]&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/ops/deployment/yarn_setup.html#run-a-single-flink-job-on-yarn&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;Best,&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;Yangze&#010;Guo&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;On&#010;Mon, Jul 13, 2020 at 10:49 AM 程龙 &lt;13162790856@163.com&amp;amp;gt; wrote:&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#010;请问一下两个问题&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#010;1 flink on yarn的时候 taskmanager 挂掉的时候 上面的日志会被删除掉 无法查看&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; ，除了使用es收集日志的这种方案，&#010;还有没有可以使taskmanager 挂掉，相关日志仍然可以保留。&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#010;2 flink on yarn模式 当由于错误导致taskmanager 挂掉，但是jobmanager 却一直存在，&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; 有没有好的方式或者策略&#010;，&amp;amp;nbsp;&amp;amp;nbsp; 可以是当task失败 达到重试次数之后 taskmanager挂掉，jobmanager也挂掉&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt; &amp;amp;gt; &amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;gt;&amp;amp;gt;",
        "depth": "11",
        "reply": "<3323d86a.3008.17346137e40.Coremail.13162790856@163.com>"
    },
    {
        "id": "<1d2eb785.34f1.1734632e568.Coremail.flink_learn@163.com>",
        "from": "flink_learner &lt;flink_le...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 03:23:27 GMT",
        "subject": "flink sql连接HBase报错",
        "content": "Hello：&#010;        在使用如下语句创建Flink SQL任务，执行查询报错，我想问下，是我遗漏了什么配置项导致flink在“/hbase”&#010;node去取元数据，实际集群的hbase配置是在zk的“/hbase-unsecure” node下的&#010;&#010;&#010;Flink 版本是1.10，hbase的t1表有数据&#010;&#010;&#010;create table t1 (&#010;  rowkey string,&#010;  f1 ROW&lt;num BIGINT&gt;&#010;) WITH (&#010;  'connector.type' = 'hbase',&#010;  'connector.version' = '1.4.3',&#010;  'connector.table-name' = 't1',&#010;  'connector.zookeeper.quorum' = '10.101.236.2:2181,10.101.236.3:2181,10.101.236.4:2181',&#010;  'connector.zookeeper.znode.parent' = '/hbase-unsecure',&#010;  'connector.write.buffer-flush.max-size' = '10mb',&#010;  'connector.write.buffer-flush.max-rows' = '1',&#010;  'connector.write.buffer-flush.interval' = '2s'&#010;);&#010;&#010;&#010;执行查询时，报错如下：&#010;Flink SQL&gt; create table t1 (&#010;&gt;   rowkey string,&#010;&gt;   f1 ROW&lt;num BIGINT&gt;&#010;&gt; ) WITH (&#010;&gt;   'connector.type' = 'hbase',&#010;&gt;   'connector.version' = '1.4.3',&#010;&gt;   'connector.table-name' = 't1',&#010;&gt;   'connector.zookeeper.quorum' = '10.101.236.2:2181,10.101.236.3:2181,10.101.236.4:2181',&#010;&gt;   'connector.zookeeper.znode.parent' = '/hbase-unsecure',&#010;&gt;   'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt;   'connector.write.buffer-flush.max-rows' = '1',&#010;&gt;   'connector.write.buffer-flush.interval' = '2s'&#010;&gt; );&#010;[INFO] Table has been created.&#010;&#010;&#010;Flink SQL&gt; select * from t1;&#010;[ERROR] Could not execute SQL statement. Reason:&#010;org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., &lt;Exception&#010;on server side:&#010;org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.&#010;at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336)&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)&#010;at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException:&#010;Could not set up JobManager&#010;at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)&#010;at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)&#010;... 6 more&#010;Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager&#010;at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:152)&#010;at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)&#010;at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379)&#010;at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)&#010;... 7 more&#010;Caused by: org.apache.flink.runtime.JobException: Creating the input splits caused an error:&#010;Can't get the location for replica 0&#010;at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&lt;init&gt;(ExecutionJobVertex.java:271)&#010;at org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph(ExecutionGraph.java:807)&#010;at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:228)&#010;at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:255)&#010;at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:227)&#010;at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:215)&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:120)&#010;at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:105)&#010;at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:278)&#010;at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:266)&#010;at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)&#010;at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)&#010;at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:146)&#010;... 10 more&#010;Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Can't get the location&#010;for replica 0&#010;at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:332)&#010;at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)&#010;at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)&#010;at org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:192)&#010;at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:268)&#010;at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:436)&#010;at org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:311)&#010;at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:596)&#010;at org.apache.hadoop.hbase.MetaTableAccessor.scanMeta(MetaTableAccessor.java:754)&#010;at org.apache.hadoop.hbase.MetaTableAccessor.scanMeta(MetaTableAccessor.java:670)&#010;at org.apache.hadoop.hbase.MetaTableAccessor.scanMetaForTableRegions(MetaTableAccessor.java:665)&#010;at org.apache.hadoop.hbase.client.HRegionLocator.listRegionLocations(HRegionLocator.java:152)&#010;at org.apache.hadoop.hbase.client.HRegionLocator.getStartEndKeys(HRegionLocator.java:118)&#010;at org.apache.flink.addons.hbase.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:202)&#010;at org.apache.flink.addons.hbase.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:44)&#010;at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&lt;init&gt;(ExecutionJobVertex.java:257)&#010;... 22 more&#010;Caused by: java.io.IOException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode&#010;= NoNode for /hbase/meta-region-server&#010;at org.apache.hadoop.hbase.client.ConnectionImplementation.get(ConnectionImplementation.java:2009)&#010;at org.apache.hadoop.hbase.client.ConnectionImplementation.locateMeta(ConnectionImplementation.java:785)&#010;at org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:752)&#010;at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:325)&#010;... 37 more&#010;Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode&#010;for /hbase/meta-region-server&#010;at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)&#010;at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)&#010;at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$ZKTask$1.exec(ReadOnlyZKClient.java:168)&#010;at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.run(ReadOnlyZKClient.java:323)&#010;at java.lang.Thread.run(Thread.java:748)&#010;&#010;&#010;End of exception on server side&gt;]&#010;&#010;",
        "depth": "0",
        "reply": "<1d2eb785.34f1.1734632e568.Coremail.flink_learn@163.com>"
    },
    {
        "id": "<BE9C7A80-5785-463B-AA5B-80DE88750BAE@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 03:55:10 GMT",
        "subject": "Re: flink sql连接HBase报错",
        "content": "hello，&#010;&#010; 这应该是碰到了Hbase connector的bug [1],  用户配置的hbaseconf 相关的参数，如connector.zookeeper.quorum&#010;不会生效，这个 bug 在1.11.0 已经修复，可以升级下版本。&#010; 在1.10.0版本上一种 walkwaround 的方式是把把这些参数放在 hbase-site.xml&#010;的配置文件中，然后将把配置文件添加到 HADOOP_CLASSPATH中，这样Flink程序也可以加载到正确的配置。&#010;&#010;&#010;祝好，&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-17968 &lt;https://issues.apache.org/jira/browse/FLINK-17968&gt;&#010;&#010;&gt; 在 2020年7月13日，11:23，flink_learner &lt;flink_learn@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hello：&#010;&gt;        在使用如下语句创建Flink SQL任务，执行查询报错，我想问下，是我遗漏了什么配置项导致flink在“/hbase”&#010;node去取元数据，实际集群的hbase配置是在zk的“/hbase-unsecure” node下的&#010;&gt; &#010;&gt; &#010;&gt; Flink 版本是1.10，hbase的t1表有数据&#010;&gt; &#010;&gt; &#010;&gt; create table t1 (&#010;&gt;  rowkey string,&#010;&gt;  f1 ROW&lt;num BIGINT&gt;&#010;&gt; ) WITH (&#010;&gt;  'connector.type' = 'hbase',&#010;&gt;  'connector.version' = '1.4.3',&#010;&gt;  'connector.table-name' = 't1',&#010;&gt;  'connector.zookeeper.quorum' = '10.101.236.2:2181,10.101.236.3:2181,10.101.236.4:2181',&#010;&gt;  'connector.zookeeper.znode.parent' = '/hbase-unsecure',&#010;&gt;  'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt;  'connector.write.buffer-flush.max-rows' = '1',&#010;&gt;  'connector.write.buffer-flush.interval' = '2s'&#010;&gt; );&#010;&gt; &#010;&gt; &#010;&gt; 执行查询时，报错如下：&#010;&gt; Flink SQL&gt; create table t1 (&#010;&gt;&gt;  rowkey string,&#010;&gt;&gt;  f1 ROW&lt;num BIGINT&gt;&#010;&gt;&gt; ) WITH (&#010;&gt;&gt;  'connector.type' = 'hbase',&#010;&gt;&gt;  'connector.version' = '1.4.3',&#010;&gt;&gt;  'connector.table-name' = 't1',&#010;&gt;&gt;  'connector.zookeeper.quorum' = '10.101.236.2:2181,10.101.236.3:2181,10.101.236.4:2181',&#010;&gt;&gt;  'connector.zookeeper.znode.parent' = '/hbase-unsecure',&#010;&gt;&gt;  'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt;&gt;  'connector.write.buffer-flush.max-rows' = '1',&#010;&gt;&gt;  'connector.write.buffer-flush.interval' = '2s'&#010;&gt;&gt; );&#010;&gt; [INFO] Table has been created.&#010;&gt; &#010;&gt; &#010;&gt; Flink SQL&gt; select * from t1;&#010;&gt; [ERROR] Could not execute SQL statement. Reason:&#010;&gt; org.apache.flink.runtime.rest.util.RestClientException: [Internal server error., &lt;Exception&#010;on server side:&#010;&gt; org.apache.flink.runtime.client.JobSubmissionException: Failed to submit job.&#010;&gt; at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$internalSubmitJob$3(Dispatcher.java:336)&#010;&gt; at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)&#010;&gt; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&gt; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobExecutionException:&#010;Could not set up JobManager&#010;&gt; at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:36)&#010;&gt; at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)&#010;&gt; ... 6 more&#010;&gt; Caused by: org.apache.flink.runtime.client.JobExecutionException: Could not set up JobManager&#010;&gt; at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:152)&#010;&gt; at org.apache.flink.runtime.dispatcher.DefaultJobManagerRunnerFactory.createJobManagerRunner(DefaultJobManagerRunnerFactory.java:84)&#010;&gt; at org.apache.flink.runtime.dispatcher.Dispatcher.lambda$createJobManagerRunner$6(Dispatcher.java:379)&#010;&gt; at org.apache.flink.util.function.CheckedSupplier.lambda$unchecked$0(CheckedSupplier.java:34)&#010;&gt; ... 7 more&#010;&gt; Caused by: org.apache.flink.runtime.JobException: Creating the input splits caused an&#010;error: Can't get the location for replica 0&#010;&gt; at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&lt;init&gt;(ExecutionJobVertex.java:271)&#010;&gt; at org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph(ExecutionGraph.java:807)&#010;&gt; at org.apache.flink.runtime.executiongraph.ExecutionGraphBuilder.buildGraph(ExecutionGraphBuilder.java:228)&#010;&gt; at org.apache.flink.runtime.scheduler.SchedulerBase.createExecutionGraph(SchedulerBase.java:255)&#010;&gt; at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:227)&#010;&gt; at org.apache.flink.runtime.scheduler.SchedulerBase.&lt;init&gt;(SchedulerBase.java:215)&#010;&gt; at org.apache.flink.runtime.scheduler.DefaultScheduler.&lt;init&gt;(DefaultScheduler.java:120)&#010;&gt; at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:105)&#010;&gt; at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:278)&#010;&gt; at org.apache.flink.runtime.jobmaster.JobMaster.&lt;init&gt;(JobMaster.java:266)&#010;&gt; at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:98)&#010;&gt; at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.createJobMasterService(DefaultJobMasterServiceFactory.java:40)&#010;&gt; at org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl.&lt;init&gt;(JobManagerRunnerImpl.java:146)&#010;&gt; ... 10 more&#010;&gt; Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Can't get the location&#010;for replica 0&#010;&gt; at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:332)&#010;&gt; at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:153)&#010;&gt; at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:58)&#010;&gt; at org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.callWithoutRetries(RpcRetryingCallerImpl.java:192)&#010;&gt; at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:268)&#010;&gt; at org.apache.hadoop.hbase.client.ClientScanner.loadCache(ClientScanner.java:436)&#010;&gt; at org.apache.hadoop.hbase.client.ClientScanner.nextWithSyncCache(ClientScanner.java:311)&#010;&gt; at org.apache.hadoop.hbase.client.ClientScanner.next(ClientScanner.java:596)&#010;&gt; at org.apache.hadoop.hbase.MetaTableAccessor.scanMeta(MetaTableAccessor.java:754)&#010;&gt; at org.apache.hadoop.hbase.MetaTableAccessor.scanMeta(MetaTableAccessor.java:670)&#010;&gt; at org.apache.hadoop.hbase.MetaTableAccessor.scanMetaForTableRegions(MetaTableAccessor.java:665)&#010;&gt; at org.apache.hadoop.hbase.client.HRegionLocator.listRegionLocations(HRegionLocator.java:152)&#010;&gt; at org.apache.hadoop.hbase.client.HRegionLocator.getStartEndKeys(HRegionLocator.java:118)&#010;&gt; at org.apache.flink.addons.hbase.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:202)&#010;&gt; at org.apache.flink.addons.hbase.AbstractTableInputFormat.createInputSplits(AbstractTableInputFormat.java:44)&#010;&gt; at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&lt;init&gt;(ExecutionJobVertex.java:257)&#010;&gt; ... 22 more&#010;&gt; Caused by: java.io.IOException: org.apache.zookeeper.KeeperException$NoNodeException:&#010;KeeperErrorCode = NoNode for /hbase/meta-region-server&#010;&gt; at org.apache.hadoop.hbase.client.ConnectionImplementation.get(ConnectionImplementation.java:2009)&#010;&gt; at org.apache.hadoop.hbase.client.ConnectionImplementation.locateMeta(ConnectionImplementation.java:785)&#010;&gt; at org.apache.hadoop.hbase.client.ConnectionImplementation.locateRegion(ConnectionImplementation.java:752)&#010;&gt; at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:325)&#010;&gt; ... 37 more&#010;&gt; Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode&#010;for /hbase/meta-region-server&#010;&gt; at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)&#010;&gt; at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)&#010;&gt; at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$ZKTask$1.exec(ReadOnlyZKClient.java:168)&#010;&gt; at org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.run(ReadOnlyZKClient.java:323)&#010;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &#010;&gt; &#010;&gt; End of exception on server side&gt;]&#010;&gt; &#010;&#010;&#010;",
        "depth": "1",
        "reply": "<1d2eb785.34f1.1734632e568.Coremail.flink_learn@163.com>"
    },
    {
        "id": "<CAEZk042V0anPb=fHjJSvLQRDHmCcM=TRNYpwtEW19vnkqk2ZTA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 03:30:53 GMT",
        "subject": "flink sql 侧输出",
        "content": "hi、&#013;&#010;大佬们、我们这面主要基于blink sql完成转换计算，但是可能会有延迟数据，现在想把延迟数据通过侧输出保存下来，在table/sql&#013;&#010;api中要怎么操作比较合理一点？或者有没有其他处理延迟数据的方式？&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk042V0anPb=fHjJSvLQRDHmCcM=TRNYpwtEW19vnkqk2ZTA@mail.gmail.com>"
    },
    {
        "id": "<CAELO932SNbr7EALpgH5hk22SFLeUmdab9123ecoWyLOzNKXq3g@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 04:07:27 GMT",
        "subject": "Re: flink sql 侧输出",
        "content": "Hi,&#013;&#010;&#013;&#010;Flink SQL/Table 目前还不支持 side output。不过有一个实验性的功能可以处理延迟数据，&#013;&#010;你可以给你的作业配上:&#013;&#010;&#013;&#010;table.exec.emit.late-fire.enabled = true&#013;&#010;table.exec.emit.late-fire.delay = 1min&#013;&#010;&#013;&#010;同时 TableConfig#setIdleStateRetentionTime 需要配上，表示窗口状态允许保留多久，即&#010;window&#013;&#010;allowLateness 。&#013;&#010;&#013;&#010;具体可以看下 org.apache.flink.table.planner.plan.utils.WindowEmitStrategy 这个类。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Mon, 13 Jul 2020 at 11:31, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi、&#013;&#010;&gt; 大佬们、我们这面主要基于blink sql完成转换计算，但是可能会有延迟数据，现在想把延迟数据通过侧输出保存下来，在table/sql&#013;&#010;&gt; api中要怎么操作比较合理一点？或者有没有其他处理延迟数据的方式？&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk042V0anPb=fHjJSvLQRDHmCcM=TRNYpwtEW19vnkqk2ZTA@mail.gmail.com>"
    },
    {
        "id": "<CAEZk040Hxx_8Be7MDamUGt4EevCBBZJdShfa-vJeNDO0AfR=5Q@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:12:19 GMT",
        "subject": "Re: flink sql 侧输出",
        "content": "hi、&#013;&#010;好的，感谢&#013;&#010;&#013;&#010;On Mon, Jul 13, 2020 at 12:07 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; Flink SQL/Table 目前还不支持 side output。不过有一个实验性的功能可以处理延迟数据，&#013;&#010;&gt; 你可以给你的作业配上:&#013;&#010;&gt;&#013;&#010;&gt; table.exec.emit.late-fire.enabled = true&#013;&#010;&gt; table.exec.emit.late-fire.delay = 1min&#013;&#010;&gt;&#013;&#010;&gt; 同时 TableConfig#setIdleStateRetentionTime 需要配上，表示窗口状态允许保留多久，即&#010;window&#013;&#010;&gt; allowLateness 。&#013;&#010;&gt;&#013;&#010;&gt; 具体可以看下 org.apache.flink.table.planner.plan.utils.WindowEmitStrategy 这个类。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Mon, 13 Jul 2020 at 11:31, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi、&#013;&#010;&gt; &gt; 大佬们、我们这面主要基于blink sql完成转换计算，但是可能会有延迟数据，现在想把延迟数据通过侧输出保存下来，在table/sql&#013;&#010;&gt; &gt; api中要怎么操作比较合理一点？或者有没有其他处理延迟数据的方式？&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk042V0anPb=fHjJSvLQRDHmCcM=TRNYpwtEW19vnkqk2ZTA@mail.gmail.com>"
    },
    {
        "id": "<BYAPR01MB429480CD19D8EBCF63C7DEDBD4600@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 03:46:02 GMT",
        "subject": "flink1.10.1 flink sql消费kafka当parallelism大于1时不产生watermark",
        "content": "Hi, all：&#013;&#010;本人使用的flink版本为flink 1.10.1， flink sql消费kafka， 当parallelism为1时正常运行，但讲parallelism修改为2时，在yarn-session&#010;web页面看不到watermark的指标信息了，也没有计算结果输出，sql如下：&#013;&#010;insert into&#013;&#010;  x.report.bi_report_fence_common_indicators&#013;&#010;select&#013;&#010;  fence_id,&#013;&#010;  'finishedOrderCnt' as indicator_name,&#013;&#010;  TUMBLE_END(dt, INTERVAL '5' MINUTE) as ts,&#013;&#010;  count(1) as indicator_val&#013;&#010;from&#013;&#010;  (&#013;&#010;    select&#013;&#010;      dt,&#013;&#010;      fence_id,&#013;&#010;      fence_coordinates_array,&#013;&#010;      c.driver_location&#013;&#010;    from&#013;&#010;      (&#013;&#010;        select&#013;&#010;          *&#013;&#010;        from&#013;&#010;          (&#013;&#010;            select&#013;&#010;              dt,&#013;&#010;              driver_location,&#013;&#010;              r1.f1.fence_info as fence_info&#013;&#010;            from&#013;&#010;              (&#013;&#010;                select&#013;&#010;                  o.dt,&#013;&#010;                  o.driver_location,&#013;&#010;                  MD5(r.city_code) as k,&#013;&#010;                  PROCTIME() as proctime&#013;&#010;                from&#013;&#010;                  (&#013;&#010;                    select&#013;&#010;                      order_no,&#013;&#010;                      dt,&#013;&#010;                      driver_location,&#013;&#010;                      PROCTIME() as proctime&#013;&#010;                    from&#013;&#010;                      x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner&#013;&#010;                    where&#013;&#010;                      _type = 'insert'&#013;&#010;                      and event_code = 'arriveAndSettlement'&#013;&#010;                  ) o&#013;&#010;                  LEFT JOIN x.dim.saic_trip_create_t_order FOR SYSTEM_TIME AS OF o.proctime&#010;AS r ON r.order_no = o.order_no&#013;&#010;              ) o1&#013;&#010;              LEFT JOIN x.dim.fence_info FOR SYSTEM_TIME AS OF o1.proctime AS r1 ON r1.k =&#010;o1.k&#013;&#010;          ) a&#013;&#010;        where&#013;&#010;          fence_info is not null&#013;&#010;      ) c&#013;&#010;      LEFT JOIN LATERAL TABLE(fence_split(c.fence_info)) as T(fence_id, fence_coordinates_array)&#010;ON TRUE&#013;&#010;  ) as b&#013;&#010;where&#013;&#010;  in_fence(fence_coordinates_array, driver_location)&#013;&#010;group by&#013;&#010;  TUMBLE(dt, INTERVAL '5' MINUTE),&#013;&#010;  fence_id;&#013;&#010;           其中 x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner表中dt为watermark字段，建表语句如下:&#013;&#010;           CREATE TABLE x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner(&#013;&#010;  _type STRING,&#013;&#010;  _old_id BIGINT,&#013;&#010;  id BIGINT,&#013;&#010;  _old_order_no STRING,&#013;&#010;  order_no STRING,&#013;&#010;  _old_event_code STRING,&#013;&#010;  event_code STRING,&#013;&#010;  _old_from_state TINYINT,&#013;&#010;  from_state TINYINT,&#013;&#010;  _old_to_state TINYINT,&#013;&#010;  to_state TINYINT,&#013;&#010;  _old_operator_type TINYINT,&#013;&#010;  operator_type TINYINT,&#013;&#010;  _old_passenger_location STRING,&#013;&#010;  passenger_location STRING,&#013;&#010;  _old_driver_location STRING,&#013;&#010;  driver_location STRING,&#013;&#010;  _old_trans_time STRING,&#013;&#010;  trans_time STRING,&#013;&#010;  _old_create_time STRING,&#013;&#010;  create_time STRING,&#013;&#010;  _old_update_time STRING,&#013;&#010;  update_time STRING,&#013;&#010;  _old_passenger_poi_address STRING,&#013;&#010;  passenger_poi_address STRING,&#013;&#010;  _old_passenger_detail_address STRING,&#013;&#010;  passenger_detail_address STRING,&#013;&#010;  _old_driver_poi_address STRING,&#013;&#010;  driver_poi_address STRING,&#013;&#010;  _old_driver_detail_address STRING,&#013;&#010;  driver_detail_address STRING,&#013;&#010;  _old_operator STRING,&#013;&#010;  operator STRING,&#013;&#010;  _old_partition_index TINYINT,&#013;&#010;  partition_index TINYINT,&#013;&#010;  dt as TO_TIMESTAMP(trans_time),&#013;&#010;  WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#013;&#010;) WITH (&#013;&#010;  'connector.type' = 'kafka',&#013;&#010;  'connector.properties.bootstrap.servers' = '*',&#013;&#010;  'connector.properties.zookeeper.connect' = '*',&#013;&#010;  'connector.version' = 'universal',&#013;&#010;  'format.type' = 'json',&#013;&#010;  'connector.properties.group.id' = 'testGroup',&#013;&#010;  'connector.startup-mode' = 'group-offsets',&#013;&#010;  'connector.topic' = 'xxxxx'&#013;&#010;)&#013;&#010;",
        "depth": "0",
        "reply": "<BYAPR01MB429480CD19D8EBCF63C7DEDBD4600@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CAJkeMphe69-oQZsB3NLrnECrFU14CYg=rw15AtBRLAw1KzqqJg@mail.gmail.com>",
        "from": "zilong xiao &lt;acidzz...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 03:56:28 GMT",
        "subject": "Re: flink1.10.1 flink sql消费kafka当parallelism大于1时不产生watermark",
        "content": "topic是几个分区呢？如果是一个分区，要加一个rebalance参数吧？&#010;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月13日周一 上午11:46写道：&#010;&#010;&gt; Hi, all：&#010;&gt; 本人使用的flink版本为flink 1.10.1， flink sql消费kafka，&#010;&gt; 当parallelism为1时正常运行，但讲parallelism修改为2时，在yarn-session&#010;&gt; web页面看不到watermark的指标信息了，也没有计算结果输出，sql如下：&#010;&gt; insert into&#010;&gt;   x.report.bi_report_fence_common_indicators&#010;&gt; select&#010;&gt;   fence_id,&#010;&gt;   'finishedOrderCnt' as indicator_name,&#010;&gt;   TUMBLE_END(dt, INTERVAL '5' MINUTE) as ts,&#010;&gt;   count(1) as indicator_val&#010;&gt; from&#010;&gt;   (&#010;&gt;     select&#010;&gt;       dt,&#010;&gt;       fence_id,&#010;&gt;       fence_coordinates_array,&#010;&gt;       c.driver_location&#010;&gt;     from&#010;&gt;       (&#010;&gt;         select&#010;&gt;           *&#010;&gt;         from&#010;&gt;           (&#010;&gt;             select&#010;&gt;               dt,&#010;&gt;               driver_location,&#010;&gt;               r1.f1.fence_info as fence_info&#010;&gt;             from&#010;&gt;               (&#010;&gt;                 select&#010;&gt;                   o.dt,&#010;&gt;                   o.driver_location,&#010;&gt;                   MD5(r.city_code) as k,&#010;&gt;                   PROCTIME() as proctime&#010;&gt;                 from&#010;&gt;                   (&#010;&gt;                     select&#010;&gt;                       order_no,&#010;&gt;                       dt,&#010;&gt;                       driver_location,&#010;&gt;                       PROCTIME() as proctime&#010;&gt;                     from&#010;&gt;                       x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner&#010;&gt;                     where&#010;&gt;                       _type = 'insert'&#010;&gt;                       and event_code = 'arriveAndSettlement'&#010;&gt;                   ) o&#010;&gt;                   LEFT JOIN x.dim.saic_trip_create_t_order FOR SYSTEM_TIME&#010;&gt; AS OF o.proctime AS r ON r.order_no = o.order_no&#010;&gt;               ) o1&#010;&gt;               LEFT JOIN x.dim.fence_info FOR SYSTEM_TIME AS OF o1.proctime&#010;&gt; AS r1 ON r1.k = o1.k&#010;&gt;           ) a&#010;&gt;         where&#010;&gt;           fence_info is not null&#010;&gt;       ) c&#010;&gt;       LEFT JOIN LATERAL TABLE(fence_split(c.fence_info)) as T(fence_id,&#010;&gt; fence_coordinates_array) ON TRUE&#010;&gt;   ) as b&#010;&gt; where&#010;&gt;   in_fence(fence_coordinates_array, driver_location)&#010;&gt; group by&#010;&gt;   TUMBLE(dt, INTERVAL '5' MINUTE),&#010;&gt;   fence_id;&#010;&gt;            其中&#010;&gt; x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner表中dt为watermark字段，建表语句如下:&#010;&gt;            CREATE TABLE&#010;&gt; x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner(&#010;&gt;   _type STRING,&#010;&gt;   _old_id BIGINT,&#010;&gt;   id BIGINT,&#010;&gt;   _old_order_no STRING,&#010;&gt;   order_no STRING,&#010;&gt;   _old_event_code STRING,&#010;&gt;   event_code STRING,&#010;&gt;   _old_from_state TINYINT,&#010;&gt;   from_state TINYINT,&#010;&gt;   _old_to_state TINYINT,&#010;&gt;   to_state TINYINT,&#010;&gt;   _old_operator_type TINYINT,&#010;&gt;   operator_type TINYINT,&#010;&gt;   _old_passenger_location STRING,&#010;&gt;   passenger_location STRING,&#010;&gt;   _old_driver_location STRING,&#010;&gt;   driver_location STRING,&#010;&gt;   _old_trans_time STRING,&#010;&gt;   trans_time STRING,&#010;&gt;   _old_create_time STRING,&#010;&gt;   create_time STRING,&#010;&gt;   _old_update_time STRING,&#010;&gt;   update_time STRING,&#010;&gt;   _old_passenger_poi_address STRING,&#010;&gt;   passenger_poi_address STRING,&#010;&gt;   _old_passenger_detail_address STRING,&#010;&gt;   passenger_detail_address STRING,&#010;&gt;   _old_driver_poi_address STRING,&#010;&gt;   driver_poi_address STRING,&#010;&gt;   _old_driver_detail_address STRING,&#010;&gt;   driver_detail_address STRING,&#010;&gt;   _old_operator STRING,&#010;&gt;   operator STRING,&#010;&gt;   _old_partition_index TINYINT,&#010;&gt;   partition_index TINYINT,&#010;&gt;   dt as TO_TIMESTAMP(trans_time),&#010;&gt;   WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#010;&gt; ) WITH (&#010;&gt;   'connector.type' = 'kafka',&#010;&gt;   'connector.properties.bootstrap.servers' = '*',&#010;&gt;   'connector.properties.zookeeper.connect' = '*',&#010;&gt;   'connector.version' = 'universal',&#010;&gt;   'format.type' = 'json',&#010;&gt;   'connector.properties.group.id' = 'testGroup',&#010;&gt;   'connector.startup-mode' = 'group-offsets',&#010;&gt;   'connector.topic' = 'xxxxx'&#010;&gt; )&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB429480CD19D8EBCF63C7DEDBD4600@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<4F345776-A677-4FF4-96DD-AE58C9B7EE5B@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 03:57:26 GMT",
        "subject": "Re: flink1.10.1 flink sql消费kafka当parallelism大于1时不产生watermark",
        "content": "Hi，&#010;&#010;可以先看下 Kakfa topic 对应的partition有几个？是否每个分区都有数据。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月13日，11:46，wind.fly.vip@outlook.com 写道：&#010;&gt; &#010;&gt; Hi, all：&#010;&gt; 本人使用的flink版本为flink 1.10.1， flink sql消费kafka， 当parallelism为1时正常运行，但讲parallelism修改为2时，在yarn-session&#010;web页面看不到watermark的指标信息了，也没有计算结果输出，sql如下：&#010;&gt; insert into&#010;&gt;  x.report.bi_report_fence_common_indicators&#010;&gt; select&#010;&gt;  fence_id,&#010;&gt;  'finishedOrderCnt' as indicator_name,&#010;&gt;  TUMBLE_END(dt, INTERVAL '5' MINUTE) as ts,&#010;&gt;  count(1) as indicator_val&#010;&gt; from&#010;&gt;  (&#010;&gt;    select&#010;&gt;      dt,&#010;&gt;      fence_id,&#010;&gt;      fence_coordinates_array,&#010;&gt;      c.driver_location&#010;&gt;    from&#010;&gt;      (&#010;&gt;        select&#010;&gt;          *&#010;&gt;        from&#010;&gt;          (&#010;&gt;            select&#010;&gt;              dt,&#010;&gt;              driver_location,&#010;&gt;              r1.f1.fence_info as fence_info&#010;&gt;            from&#010;&gt;              (&#010;&gt;                select&#010;&gt;                  o.dt,&#010;&gt;                  o.driver_location,&#010;&gt;                  MD5(r.city_code) as k,&#010;&gt;                  PROCTIME() as proctime&#010;&gt;                from&#010;&gt;                  (&#010;&gt;                    select&#010;&gt;                      order_no,&#010;&gt;                      dt,&#010;&gt;                      driver_location,&#010;&gt;                      PROCTIME() as proctime&#010;&gt;                    from&#010;&gt;                      x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner&#010;&gt;                    where&#010;&gt;                      _type = 'insert'&#010;&gt;                      and event_code = 'arriveAndSettlement'&#010;&gt;                  ) o&#010;&gt;                  LEFT JOIN x.dim.saic_trip_create_t_order FOR SYSTEM_TIME AS OF o.proctime&#010;AS r ON r.order_no = o.order_no&#010;&gt;              ) o1&#010;&gt;              LEFT JOIN x.dim.fence_info FOR SYSTEM_TIME AS OF o1.proctime AS r1 ON r1.k&#010;= o1.k&#010;&gt;          ) a&#010;&gt;        where&#010;&gt;          fence_info is not null&#010;&gt;      ) c&#010;&gt;      LEFT JOIN LATERAL TABLE(fence_split(c.fence_info)) as T(fence_id, fence_coordinates_array)&#010;ON TRUE&#010;&gt;  ) as b&#010;&gt; where&#010;&gt;  in_fence(fence_coordinates_array, driver_location)&#010;&gt; group by&#010;&gt;  TUMBLE(dt, INTERVAL '5' MINUTE),&#010;&gt;  fence_id;&#010;&gt;           其中 x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner表中dt为watermark字段，建表语句如下:&#010;&gt;           CREATE TABLE x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner(&#010;&gt;  _type STRING,&#010;&gt;  _old_id BIGINT,&#010;&gt;  id BIGINT,&#010;&gt;  _old_order_no STRING,&#010;&gt;  order_no STRING,&#010;&gt;  _old_event_code STRING,&#010;&gt;  event_code STRING,&#010;&gt;  _old_from_state TINYINT,&#010;&gt;  from_state TINYINT,&#010;&gt;  _old_to_state TINYINT,&#010;&gt;  to_state TINYINT,&#010;&gt;  _old_operator_type TINYINT,&#010;&gt;  operator_type TINYINT,&#010;&gt;  _old_passenger_location STRING,&#010;&gt;  passenger_location STRING,&#010;&gt;  _old_driver_location STRING,&#010;&gt;  driver_location STRING,&#010;&gt;  _old_trans_time STRING,&#010;&gt;  trans_time STRING,&#010;&gt;  _old_create_time STRING,&#010;&gt;  create_time STRING,&#010;&gt;  _old_update_time STRING,&#010;&gt;  update_time STRING,&#010;&gt;  _old_passenger_poi_address STRING,&#010;&gt;  passenger_poi_address STRING,&#010;&gt;  _old_passenger_detail_address STRING,&#010;&gt;  passenger_detail_address STRING,&#010;&gt;  _old_driver_poi_address STRING,&#010;&gt;  driver_poi_address STRING,&#010;&gt;  _old_driver_detail_address STRING,&#010;&gt;  driver_detail_address STRING,&#010;&gt;  _old_operator STRING,&#010;&gt;  operator STRING,&#010;&gt;  _old_partition_index TINYINT,&#010;&gt;  partition_index TINYINT,&#010;&gt;  dt as TO_TIMESTAMP(trans_time),&#010;&gt;  WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#010;&gt; ) WITH (&#010;&gt;  'connector.type' = 'kafka',&#010;&gt;  'connector.properties.bootstrap.servers' = '*',&#010;&gt;  'connector.properties.zookeeper.connect' = '*',&#010;&gt;  'connector.version' = 'universal',&#010;&gt;  'format.type' = 'json',&#010;&gt;  'connector.properties.group.id' = 'testGroup',&#010;&gt;  'connector.startup-mode' = 'group-offsets',&#010;&gt;  'connector.topic' = 'xxxxx'&#010;&gt; )&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB429480CD19D8EBCF63C7DEDBD4600@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB429434990D8183E41DE47ABFD4600@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:59:19 GMT",
        "subject": "回复: flink1.10.1 flink sql消费kafka当parallelism大于1时不产生watermark",
        "content": "Hi,&#013;&#010;    确实是一共三个分区，只有一个分区有数据，已经解决，谢谢。&#013;&#010;&#013;&#010;Best,&#013;&#010;Junbao Zhang&#013;&#010;________________________________&#013;&#010;发件人: Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#013;&#010;发送时间: 2020年7月13日 11:57&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink1.10.1 flink sql消费kafka当parallelism大于1时不产生watermark&#013;&#010;&#013;&#010;Hi，&#013;&#010;&#013;&#010;可以先看下 Kakfa topic 对应的partition有几个？是否每个分区都有数据。&#013;&#010;&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&gt; 在 2020年7月13日，11:46，wind.fly.vip@outlook.com 写道：&#013;&#010;&gt;&#013;&#010;&gt; Hi, all：&#013;&#010;&gt; 本人使用的flink版本为flink 1.10.1， flink sql消费kafka， 当parallelism为1时正常运行，但讲parallelism修改为2时，在yarn-session&#010;web页面看不到watermark的指标信息了，也没有计算结果输出，sql如下：&#013;&#010;&gt; insert into&#013;&#010;&gt;  x.report.bi_report_fence_common_indicators&#013;&#010;&gt; select&#013;&#010;&gt;  fence_id,&#013;&#010;&gt;  'finishedOrderCnt' as indicator_name,&#013;&#010;&gt;  TUMBLE_END(dt, INTERVAL '5' MINUTE) as ts,&#013;&#010;&gt;  count(1) as indicator_val&#013;&#010;&gt; from&#013;&#010;&gt;  (&#013;&#010;&gt;    select&#013;&#010;&gt;      dt,&#013;&#010;&gt;      fence_id,&#013;&#010;&gt;      fence_coordinates_array,&#013;&#010;&gt;      c.driver_location&#013;&#010;&gt;    from&#013;&#010;&gt;      (&#013;&#010;&gt;        select&#013;&#010;&gt;          *&#013;&#010;&gt;        from&#013;&#010;&gt;          (&#013;&#010;&gt;            select&#013;&#010;&gt;              dt,&#013;&#010;&gt;              driver_location,&#013;&#010;&gt;              r1.f1.fence_info as fence_info&#013;&#010;&gt;            from&#013;&#010;&gt;              (&#013;&#010;&gt;                select&#013;&#010;&gt;                  o.dt,&#013;&#010;&gt;                  o.driver_location,&#013;&#010;&gt;                  MD5(r.city_code) as k,&#013;&#010;&gt;                  PROCTIME() as proctime&#013;&#010;&gt;                from&#013;&#010;&gt;                  (&#013;&#010;&gt;                    select&#013;&#010;&gt;                      order_no,&#013;&#010;&gt;                      dt,&#013;&#010;&gt;                      driver_location,&#013;&#010;&gt;                      PROCTIME() as proctime&#013;&#010;&gt;                    from&#013;&#010;&gt;                      x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner&#013;&#010;&gt;                    where&#013;&#010;&gt;                      _type = 'insert'&#013;&#010;&gt;                      and event_code = 'arriveAndSettlement'&#013;&#010;&gt;                  ) o&#013;&#010;&gt;                  LEFT JOIN x.dim.saic_trip_create_t_order FOR SYSTEM_TIME AS OF o.proctime&#010;AS r ON r.order_no = o.order_no&#013;&#010;&gt;              ) o1&#013;&#010;&gt;              LEFT JOIN x.dim.fence_info FOR SYSTEM_TIME AS OF o1.proctime AS r1 ON r1.k&#010;= o1.k&#013;&#010;&gt;          ) a&#013;&#010;&gt;        where&#013;&#010;&gt;          fence_info is not null&#013;&#010;&gt;      ) c&#013;&#010;&gt;      LEFT JOIN LATERAL TABLE(fence_split(c.fence_info)) as T(fence_id, fence_coordinates_array)&#010;ON TRUE&#013;&#010;&gt;  ) as b&#013;&#010;&gt; where&#013;&#010;&gt;  in_fence(fence_coordinates_array, driver_location)&#013;&#010;&gt; group by&#013;&#010;&gt;  TUMBLE(dt, INTERVAL '5' MINUTE),&#013;&#010;&gt;  fence_id;&#013;&#010;&gt;           其中 x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner表中dt为watermark字段，建表语句如下:&#013;&#010;&gt;           CREATE TABLE x.ods.ods_binlog_saic_trip_order2_t_order_trans_inner(&#013;&#010;&gt;  _type STRING,&#013;&#010;&gt;  _old_id BIGINT,&#013;&#010;&gt;  id BIGINT,&#013;&#010;&gt;  _old_order_no STRING,&#013;&#010;&gt;  order_no STRING,&#013;&#010;&gt;  _old_event_code STRING,&#013;&#010;&gt;  event_code STRING,&#013;&#010;&gt;  _old_from_state TINYINT,&#013;&#010;&gt;  from_state TINYINT,&#013;&#010;&gt;  _old_to_state TINYINT,&#013;&#010;&gt;  to_state TINYINT,&#013;&#010;&gt;  _old_operator_type TINYINT,&#013;&#010;&gt;  operator_type TINYINT,&#013;&#010;&gt;  _old_passenger_location STRING,&#013;&#010;&gt;  passenger_location STRING,&#013;&#010;&gt;  _old_driver_location STRING,&#013;&#010;&gt;  driver_location STRING,&#013;&#010;&gt;  _old_trans_time STRING,&#013;&#010;&gt;  trans_time STRING,&#013;&#010;&gt;  _old_create_time STRING,&#013;&#010;&gt;  create_time STRING,&#013;&#010;&gt;  _old_update_time STRING,&#013;&#010;&gt;  update_time STRING,&#013;&#010;&gt;  _old_passenger_poi_address STRING,&#013;&#010;&gt;  passenger_poi_address STRING,&#013;&#010;&gt;  _old_passenger_detail_address STRING,&#013;&#010;&gt;  passenger_detail_address STRING,&#013;&#010;&gt;  _old_driver_poi_address STRING,&#013;&#010;&gt;  driver_poi_address STRING,&#013;&#010;&gt;  _old_driver_detail_address STRING,&#013;&#010;&gt;  driver_detail_address STRING,&#013;&#010;&gt;  _old_operator STRING,&#013;&#010;&gt;  operator STRING,&#013;&#010;&gt;  _old_partition_index TINYINT,&#013;&#010;&gt;  partition_index TINYINT,&#013;&#010;&gt;  dt as TO_TIMESTAMP(trans_time),&#013;&#010;&gt;  WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#013;&#010;&gt; ) WITH (&#013;&#010;&gt;  'connector.type' = 'kafka',&#013;&#010;&gt;  'connector.properties.bootstrap.servers' = '*',&#013;&#010;&gt;  'connector.properties.zookeeper.connect' = '*',&#013;&#010;&gt;  'connector.version' = 'universal',&#013;&#010;&gt;  'format.type' = 'json',&#013;&#010;&gt;  'connector.properties.group.id' = 'testGroup',&#013;&#010;&gt;  'connector.startup-mode' = 'group-offsets',&#013;&#010;&gt;  'connector.topic' = 'xxxxx'&#013;&#010;&gt; )&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB429480CD19D8EBCF63C7DEDBD4600@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 04:53:57 GMT",
        "subject": "flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "各位好，写了个demo，代码如下，在本地跑没有问题，提交到yarn session上报错：&#010;Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;any factory for identifier 'kafka' that implements&#010;'org.apache.flink.table.factories.DynamicTableSourceFactory' in the&#010;classpath.&#010;请问是什么原因导致的呢？&#010;&#010;代码如下：&#010;&#010;-----------------------------------------------------------------------------------------------------------------------------&#010;        StreamExecutionEnvironment env =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;        EnvironmentSettings settings =&#010;EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;        StreamTableEnvironment tenv = StreamTableEnvironment.create(env,&#010;settings);&#010;&#010;        tenv.executeSql(\"CREATE TABLE test_table (\\n\" +&#010;                \" id INT,\\n\" +&#010;                \" name STRING,\\n\" +&#010;                \" age INT,\\n\" +&#010;                \" create_at TIMESTAMP(3)\\n\" +&#010;                \") WITH (\\n\" +&#010;                \" 'connector' = 'kafka',\\n\" +&#010;                \" 'topic' = 'test_json',\\n\" +&#010;                \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;                \" 'properties.group.id' = 'testGroup',\\n\" +&#010;                \" 'format' = 'json',\\n\" +&#010;                \" 'scan.startup.mode' = 'latest-offset'\\n\" +&#010;                \")\");&#010;        Table table = tenv.sqlQuery(\"select * from test_table\");&#010;        tenv.toRetractStream(table, Row.class).print();&#010;        env.execute(\"flink 1.11.0 demo\");&#010;-----------------------------------------------------------------------------------------------------------------------------&#010;&#010;pom 文件如下：&#010;=============================================&#010;&lt;properties&gt;&#010;        &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;&#010;        &lt;flink.version&gt;1.11.0&lt;/flink.version&gt;&#010;&lt;/properties&gt;&#010;&lt;dependencies&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&#010;&lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&#010;&lt;artifactId&gt;flink-table-runtime-blink_${scala.binary.version}&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&#010;&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&#010;&lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&#010;&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-clients_${scala.binary.version}&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;    &lt;/dependencies&gt;&#010;=============================================&#010;&#010;",
        "depth": "0",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CALL9TYJUQ0iVtwqrCLBcgTuGNiftmBkn8TJBkK+8ZMCDi_jjFA@mail.gmail.com>",
        "from": "tison &lt;wander4...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 05:27:33 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "那就要看下你是什么 Flink 版本，怎么提交到 YARN 上的，以及 YARN 的日志上的&#010;classpath 是啥了&#010;&#010;Best,&#010;tison.&#010;&#010;&#010;王松 &lt;sdlcwangsong11@gmail.com&gt; 于2020年7月13日周一 下午12:54写道：&#010;&#010;&gt; 各位好，写了个demo，代码如下，在本地跑没有问题，提交到yarn session上报错：&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;&gt; any factory for identifier 'kafka' that implements&#010;&gt; 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the&#010;&gt; classpath.&#010;&gt; 请问是什么原因导致的呢？&#010;&gt;&#010;&gt; 代码如下：&#010;&gt;&#010;&gt;&#010;&gt; -----------------------------------------------------------------------------------------------------------------------------&#010;&gt;         StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;         EnvironmentSettings settings =&#010;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;         StreamTableEnvironment tenv = StreamTableEnvironment.create(env,&#010;&gt; settings);&#010;&gt;&#010;&gt;         tenv.executeSql(\"CREATE TABLE test_table (\\n\" +&#010;&gt;                 \" id INT,\\n\" +&#010;&gt;                 \" name STRING,\\n\" +&#010;&gt;                 \" age INT,\\n\" +&#010;&gt;                 \" create_at TIMESTAMP(3)\\n\" +&#010;&gt;                 \") WITH (\\n\" +&#010;&gt;                 \" 'connector' = 'kafka',\\n\" +&#010;&gt;                 \" 'topic' = 'test_json',\\n\" +&#010;&gt;                 \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;&gt;                 \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt;                 \" 'format' = 'json',\\n\" +&#010;&gt;                 \" 'scan.startup.mode' = 'latest-offset'\\n\" +&#010;&gt;                 \")\");&#010;&gt;         Table table = tenv.sqlQuery(\"select * from test_table\");&#010;&gt;         tenv.toRetractStream(table, Row.class).print();&#010;&gt;         env.execute(\"flink 1.11.0 demo\");&#010;&gt;&#010;&gt; -----------------------------------------------------------------------------------------------------------------------------&#010;&gt;&#010;&gt; pom 文件如下：&#010;&gt; =============================================&#010;&gt; &lt;properties&gt;&#010;&gt;         &lt;scala.binary.version&gt;2.11&lt;/scala.binary.version&gt;&#010;&gt;         &lt;flink.version&gt;1.11.0&lt;/flink.version&gt;&#010;&gt; &lt;/properties&gt;&#010;&gt; &lt;dependencies&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&#010;&gt; &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&#010;&gt; &lt;artifactId&gt;flink-table-runtime-blink_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-clients_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;     &lt;/dependencies&gt;&#010;&gt; =============================================&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<61F81BF8-93B2-440B-8503-49C304CE3C12@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 05:39:43 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "Hi, 王松&#010;&#010;这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream  connector 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#010;可以参考官网文档[1], 查看和下载SQL Client Jar. 另外， Kafka SQL connector&#010;和 Kafka datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#010;&#010;&#010;祝好，&#010;Leonard Xu&#010;[1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&gt;&#010;&gt;        &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;        &lt;/dependency&gt;&#010;&gt;        &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;        &lt;/dependency&gt;&#010;&gt;        &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;        &lt;/dependency&gt;&#010;&gt;        &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;            &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;        &lt;/dependency&gt; &#010;&gt; =============================================&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CABFgzE5p2eayKOCgbBJg2bUi3GRMUjyUsdR4b5SeNGcNgXgOBw@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:42:06 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "@Leonard Xu，&#010;非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#010;中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;=============================&#010;          &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&#010;&lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;&#010;         &lt;!--&lt;dependency&gt;--&gt;&#010;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;&#010;&lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#010;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;        &lt;!--&lt;/dependency&gt;--&gt;&#010;        &lt;!--&lt;dependency&gt;--&gt;&#010;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;            &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#010;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;            &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#010;        &lt;!--&lt;/dependency&gt;--&gt;&#010;&#010;        &lt;!--&lt;dependency&gt;--&gt;&#010;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;            &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#010;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;        &lt;!--&lt;/dependency&gt;--&gt;&#010;=============================&#010;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午1:39写道：&#010;&#010;&gt; Hi, 王松&#010;&gt;&#010;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream  connector&#010;&gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#010;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar. 另外， Kafka SQL connector&#010;和 Kafka&#010;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#010;&gt;&#010;&gt;&#010;&gt; 祝好，&#010;&gt; Leonard Xu&#010;&gt; [1]&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt; &lt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt; &gt;&#010;&gt; &gt;        &lt;dependency&gt;&#010;&gt; &gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;&#010;&gt; &gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt; &gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;        &lt;/dependency&gt;&#010;&gt; &gt;        &lt;dependency&gt;&#010;&gt; &gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt; &gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;        &lt;/dependency&gt;&#010;&gt; &gt;        &lt;dependency&gt;&#010;&gt; &gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;&#010;&gt; &gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt; &gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;        &lt;/dependency&gt;&#010;&gt; &gt;        &lt;dependency&gt;&#010;&gt; &gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &gt;            &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#010;&gt; &gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt; &gt;        &lt;/dependency&gt;&#010;&gt; &gt; =============================================&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<0961E6CA-A847-4821-8026-83FFD4636FD6@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:05:40 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "Hi，&#010;flink-connector-kafka_${scala.binary.version 和 flink-sql-connector-kafka_${scala.binary.version&#010;只用加载一个应该就好了，前者的话是dataStream 或者 Table API 程序使用，&#010;后者的话主要是对前者做了shade处理，方便用户在 SQL Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#010;可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#010;&#010;[1] 中的话是有SQL Client JAR 的下载链接，就是 flink-sql-connector-kafka_${scala.binary.version&#010;jar 包的下载链接，你看一看下。 &#010;&#010;祝好&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; @Leonard Xu，&#010;&gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#010;&gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#010;&gt; &#010;&gt; [1]&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt; =============================&#010;&gt;          &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt; &#010;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;        &lt;/dependency&gt;&#010;&gt; &#010;&gt;         &lt;!--&lt;dependency&gt;--&gt;&#010;&gt;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;&gt; &#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#010;&gt;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;&gt;        &lt;!--&lt;/dependency&gt;--&gt;&#010;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#010;&gt;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;&gt;            &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#010;&gt;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;&gt;            &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#010;&gt;        &lt;!--&lt;/dependency&gt;--&gt;&#010;&gt; &#010;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#010;&gt;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;&gt;            &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#010;&gt;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;&gt;        &lt;!--&lt;/dependency&gt;--&gt;&#010;&gt; =============================&#010;&gt; &#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午1:39写道：&#010;&gt; &#010;&gt;&gt; Hi, 王松&#010;&gt;&gt; &#010;&gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream  connector&#010;&gt;&gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#010;&gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar. 另外， Kafka SQL connector&#010;和 Kafka&#010;&gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 祝好，&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt; [1]&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt;&gt; &lt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;       &lt;dependency&gt;&#010;&gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;       &lt;/dependency&gt;&#010;&gt;&gt;&gt;       &lt;dependency&gt;&#010;&gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;       &lt;/dependency&gt;&#010;&gt;&gt;&gt;       &lt;dependency&gt;&#010;&gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;       &lt;/dependency&gt;&#010;&gt;&gt;&gt;       &lt;dependency&gt;&#010;&gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;           &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#010;&gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;       &lt;/dependency&gt;&#010;&gt;&gt;&gt; =============================================&#010;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "3",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CABFgzE4jC27wHwz49Agcr9oN_aErtxn0n-arpN-9G4__KMc1AQ@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:28:16 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "您好，我只加载了flink-sql-connector-kafka，另外 scope没有设置，使用了默认值compile。&#013;&#010;&#013;&#010;我机器上flink/lib下jar包如下：&#013;&#010;-rw-rw-r-- 1 hadoop hadoop    117719 6月  30 12:41 flink-avro-1.11.0.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop     90782 7月   8 10:09 flink-csv-1.11.0.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop 108349203 7月   8 10:09 flink-dist_2.11-1.11.0.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop     94863 7月   8 10:09 flink-json-1.11.0.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop   7712156 7月   8 10:09&#013;&#010;flink-shaded-zookeeper-3.4.14.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop  33325754 7月   8 10:09&#013;&#010;flink-table_2.11-1.11.0.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop  37330521 7月   8 10:09&#013;&#010;flink-table-blink_2.11-1.11.0.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop     67114 7月   8 10:09 log4j-1.2-api-2.12.1.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop    276771 7月   8 10:09 log4j-api-2.12.1.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop   1674433 7月   8 10:09 log4j-core-2.12.1.jar&#013;&#010;-rw-r--r-- 1 hadoop hadoop     23518 7月   8 10:09&#013;&#010;log4j-slf4j-impl-2.12.1.jar&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:05写道：&#013;&#010;&#013;&#010;&gt; Hi，&#013;&#010;&gt; flink-connector-kafka_${scala.binary.version 和&#013;&#010;&gt; flink-sql-connector-kafka_${scala.binary.version&#013;&#010;&gt; 只用加载一个应该就好了，前者的话是dataStream 或者 Table API 程序使用，&#013;&#010;&gt; 后者的话主要是对前者做了shade处理，方便用户在 SQL&#013;&#010;&gt; Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#013;&#010;&gt; 可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#013;&#010;&gt;&#013;&#010;&gt; [1] 中的话是有SQL Client JAR 的下载链接，就是&#013;&#010;&gt; flink-sql-connector-kafka_${scala.binary.version jar 包的下载链接，你看一看下。&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; @Leonard Xu，&#013;&#010;&gt; &gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#013;&#010;&gt; &gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; =============================&#013;&#010;&gt; &gt;          &lt;dependency&gt;&#013;&#010;&gt; &gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;        &lt;/dependency&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;         &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;        &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;        &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#013;&#010;&gt; &gt;        &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;        &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;            &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;        &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; =============================&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午1:39写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi, 王松&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream  connector&#013;&#010;&gt; &gt;&gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#013;&#010;&gt; &gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar. 另外， Kafka&#010;SQL connector 和 Kafka&#013;&#010;&gt; &gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 祝好，&#013;&#010;&gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt; &lt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt; =============================================&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<A43D4F90-59E7-439E-8256-E3B4F89B7C54@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:38:03 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "Hi&#010;你可以试下把 flink-connector-kafka_2.11-1.11.0.jar 的依赖也放lib下试下（pom中删掉），排除是否因为提交作业的方式导致没有正确加载&#010;还是 其他原因。&#010;&#010;祝好&#010;&#010;&gt; 在 2020年7月13日，15:28，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; 您好，我只加载了flink-sql-connector-kafka，另外 scope没有设置，使用了默认值compile。&#010;&gt; &#010;&gt; 我机器上flink/lib下jar包如下：&#010;&gt; -rw-rw-r-- 1 hadoop hadoop    117719 6月  30 12:41 flink-avro-1.11.0.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop     90782 7月   8 10:09 flink-csv-1.11.0.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop 108349203 7月   8 10:09 flink-dist_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop     94863 7月   8 10:09 flink-json-1.11.0.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop   7712156 7月   8 10:09&#010;&gt; flink-shaded-zookeeper-3.4.14.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop  33325754 7月   8 10:09&#010;&gt; flink-table_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop  37330521 7月   8 10:09&#010;&gt; flink-table-blink_2.11-1.11.0.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop     67114 7月   8 10:09 log4j-1.2-api-2.12.1.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop    276771 7月   8 10:09 log4j-api-2.12.1.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop   1674433 7月   8 10:09 log4j-core-2.12.1.jar&#010;&gt; -rw-r--r-- 1 hadoop hadoop     23518 7月   8 10:09&#010;&gt; log4j-slf4j-impl-2.12.1.jar&#010;&gt; &#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:05写道：&#010;&gt; &#010;&gt;&gt; Hi，&#010;&gt;&gt; flink-connector-kafka_${scala.binary.version 和&#010;&gt;&gt; flink-sql-connector-kafka_${scala.binary.version&#010;&gt;&gt; 只用加载一个应该就好了，前者的话是dataStream 或者 Table API 程序使用，&#010;&gt;&gt; 后者的话主要是对前者做了shade处理，方便用户在 SQL&#010;&gt;&gt; Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#010;&gt;&gt; 可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#010;&gt;&gt; &#010;&gt;&gt; [1] 中的话是有SQL Client JAR 的下载链接，就是&#010;&gt;&gt; flink-sql-connector-kafka_${scala.binary.version jar 包的下载链接，你看一看下。&#010;&gt;&gt; &#010;&gt;&gt; 祝好&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; @Leonard Xu，&#010;&gt;&gt;&gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#010;&gt;&gt;&gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [1]&#010;&gt;&gt;&gt; &#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt;&gt;&gt; =============================&#010;&gt;&gt;&gt;         &lt;dependency&gt;&#010;&gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;       &lt;/dependency&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;&gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#010;&gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#010;&gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#010;&gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#010;&gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#010;&gt;&gt;&gt; =============================&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午1:39写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; Hi, 王松&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream  connector&#010;&gt;&gt;&gt;&gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#010;&gt;&gt;&gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar. 另外， Kafka&#010;SQL connector 和 Kafka&#010;&gt;&gt;&gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 祝好，&#010;&gt;&gt;&gt;&gt; Leonard Xu&#010;&gt;&gt;&gt;&gt; [1]&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt;&gt;&gt;&gt; &lt;&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#010;&gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#010;&gt;&gt;&gt;&gt;&gt; =============================================&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "5",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_RtvHaGjJU9m8PGfv9snqP1_Njhu5rSLEGg98kAnN+2ig@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:42:32 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "你的程序打包的时候是不是把依赖都shade进去了呢？像这种connector，一般最好是在用户程序中打进去；&#013;&#010;或者你不打进去的话，也可以在提交作业的时候把这些connector放到classpath里面。&#013;&#010;当然，直接粗暴的放到lib下，也是可以的。&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:38写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt; 你可以试下把 flink-connector-kafka_2.11-1.11.0.jar&#013;&#010;&gt; 的依赖也放lib下试下（pom中删掉），排除是否因为提交作业的方式导致没有正确加载&#010;还是 其他原因。&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月13日，15:28，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 您好，我只加载了flink-sql-connector-kafka，另外 scope没有设置，使用了默认值compile。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我机器上flink/lib下jar包如下：&#013;&#010;&gt; &gt; -rw-rw-r-- 1 hadoop hadoop    117719 6月  30 12:41 flink-avro-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     90782 7月   8 10:09 flink-csv-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop 108349203 7月   8 10:09&#013;&#010;&gt; flink-dist_2.11-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     94863 7月   8 10:09 flink-json-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop   7712156 7月   8 10:09&#013;&#010;&gt; &gt; flink-shaded-zookeeper-3.4.14.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop  33325754 7月   8 10:09&#013;&#010;&gt; &gt; flink-table_2.11-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop  37330521 7月   8 10:09&#013;&#010;&gt; &gt; flink-table-blink_2.11-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     67114 7月   8 10:09&#013;&#010;&gt; log4j-1.2-api-2.12.1.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop    276771 7月   8 10:09 log4j-api-2.12.1.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop   1674433 7月   8 10:09 log4j-core-2.12.1.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     23518 7月   8 10:09&#013;&#010;&gt; &gt; log4j-slf4j-impl-2.12.1.jar&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:05写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt;&gt; flink-connector-kafka_${scala.binary.version 和&#013;&#010;&gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version&#013;&#010;&gt; &gt;&gt; 只用加载一个应该就好了，前者的话是dataStream 或者 Table API&#010;程序使用，&#013;&#010;&gt; &gt;&gt; 后者的话主要是对前者做了shade处理，方便用户在 SQL&#013;&#010;&gt; &gt;&gt; Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#013;&#010;&gt; &gt;&gt; 可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; [1] 中的话是有SQL Client JAR 的下载链接，就是&#013;&#010;&gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version jar 包的下载链接，你看一看下。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 祝好&#013;&#010;&gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; @Leonard Xu，&#013;&#010;&gt; &gt;&gt;&gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#013;&#010;&gt; &gt;&gt;&gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午1:39写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; Hi, 王松&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream&#013;&#010;&gt; connector&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#013;&#010;&gt; &gt;&gt;&gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar. 另外，&#010;Kafka SQL connector 和 Kafka&#013;&#010;&gt; &gt;&gt;&gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; 祝好，&#013;&#010;&gt; &gt;&gt;&gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt;&gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt;&gt;&gt; &lt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt; =============================================&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "6",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CABFgzE6x+MMzAuSPOc7NRacO8s0EnApD3wKx0NPXHs0ZbgbOPg@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:04:20 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "你好本超，&#013;&#010;是的，我尝试解压打包好的jar包，里边是包含我pom中写的依赖的&#013;&#010;&#013;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月13日周一 下午3:42写道：&#013;&#010;&#013;&#010;&gt; 你的程序打包的时候是不是把依赖都shade进去了呢？像这种connector，一般最好是在用户程序中打进去；&#013;&#010;&gt; 或者你不打进去的话，也可以在提交作业的时候把这些connector放到classpath里面。&#013;&#010;&gt; 当然，直接粗暴的放到lib下，也是可以的。&#013;&#010;&gt;&#013;&#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:38写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt; 你可以试下把 flink-connector-kafka_2.11-1.11.0.jar&#013;&#010;&gt; &gt; 的依赖也放lib下试下（pom中删掉），排除是否因为提交作业的方式导致没有正确加载&#010;还是 其他原因。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 祝好&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 在 2020年7月13日，15:28，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 您好，我只加载了flink-sql-connector-kafka，另外 scope没有设置，使用了默认值compile。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 我机器上flink/lib下jar包如下：&#013;&#010;&gt; &gt; &gt; -rw-rw-r-- 1 hadoop hadoop    117719 6月  30 12:41 flink-avro-1.11.0.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     90782 7月   8 10:09 flink-csv-1.11.0.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop 108349203 7月   8 10:09&#013;&#010;&gt; &gt; flink-dist_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     94863 7月   8 10:09 flink-json-1.11.0.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop   7712156 7月   8 10:09&#013;&#010;&gt; &gt; &gt; flink-shaded-zookeeper-3.4.14.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop  33325754 7月   8 10:09&#013;&#010;&gt; &gt; &gt; flink-table_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop  37330521 7月   8 10:09&#013;&#010;&gt; &gt; &gt; flink-table-blink_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     67114 7月   8 10:09&#013;&#010;&gt; &gt; log4j-1.2-api-2.12.1.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop    276771 7月   8 10:09 log4j-api-2.12.1.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop   1674433 7月   8 10:09 log4j-core-2.12.1.jar&#013;&#010;&gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     23518 7月   8 10:09&#013;&#010;&gt; &gt; &gt; log4j-slf4j-impl-2.12.1.jar&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:05写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt; &gt;&gt; flink-connector-kafka_${scala.binary.version 和&#013;&#010;&gt; &gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version&#013;&#010;&gt; &gt; &gt;&gt; 只用加载一个应该就好了，前者的话是dataStream 或者 Table&#010;API 程序使用，&#013;&#010;&gt; &gt; &gt;&gt; 后者的话主要是对前者做了shade处理，方便用户在 SQL&#013;&#010;&gt; &gt; &gt;&gt; Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#013;&#010;&gt; &gt; &gt;&gt; 可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; [1] 中的话是有SQL Client JAR 的下载链接，就是&#013;&#010;&gt; &gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version jar 包的下载链接，你看一看下。&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; 祝好&#013;&#010;&gt; &gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; @Leonard Xu，&#013;&#010;&gt; &gt; &gt;&gt;&gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#013;&#010;&gt; &gt; &gt;&gt;&gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; [1]&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt; &gt;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午1:39写道：&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; Hi, 王松&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream&#013;&#010;&gt; &gt; connector&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar. 另外，&#010;Kafka SQL connector 和 Kafka&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; 祝好，&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; [1]&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt; &lt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&gt; =============================================&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jR4MyvjVpexBV6RVEHf7x5WAnHNwpoYCXqYVzwSOthgMQ@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:35:25 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "Hi,&#013;&#010;&#013;&#010;1.推荐方式：把flink-sql-connector-kafka-0.11_2.11-1.11.0.jar放入lib下。下载链接：[1]&#013;&#010;2.次推荐方式：你的java工程打包时，需要用shade插件把kafka相关类shade到最终的jar中。(不能用jar-with-deps，因为它会覆盖掉java&#013;&#010;spi)&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Mon, Jul 13, 2020 at 4:04 PM 王松 &lt;sdlcwangsong11@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 你好本超，&#013;&#010;&gt; 是的，我尝试解压打包好的jar包，里边是包含我pom中写的依赖的&#013;&#010;&gt;&#013;&#010;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月13日周一 下午3:42写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 你的程序打包的时候是不是把依赖都shade进去了呢？像这种connector，一般最好是在用户程序中打进去；&#013;&#010;&gt; &gt; 或者你不打进去的话，也可以在提交作业的时候把这些connector放到classpath里面。&#013;&#010;&gt; &gt; 当然，直接粗暴的放到lib下，也是可以的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:38写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt; 你可以试下把 flink-connector-kafka_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; 的依赖也放lib下试下（pom中删掉），排除是否因为提交作业的方式导致没有正确加载&#010;还是 其他原因。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 祝好&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在 2020年7月13日，15:28，王松 &lt;sdlcwangsong11@gmail.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 您好，我只加载了flink-sql-connector-kafka，另外 scope没有设置，使用了默认值compile。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 我机器上flink/lib下jar包如下：&#013;&#010;&gt; &gt; &gt; &gt; -rw-rw-r-- 1 hadoop hadoop    117719 6月  30 12:41&#013;&#010;&gt; flink-avro-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     90782 7月   8 10:09&#013;&#010;&gt; flink-csv-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop 108349203 7月   8 10:09&#013;&#010;&gt; &gt; &gt; flink-dist_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     94863 7月   8 10:09&#013;&#010;&gt; flink-json-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop   7712156 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; flink-shaded-zookeeper-3.4.14.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop  33325754 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; flink-table_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop  37330521 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; flink-table-blink_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     67114 7月   8 10:09&#013;&#010;&gt; &gt; &gt; log4j-1.2-api-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop    276771 7月   8 10:09&#013;&#010;&gt; log4j-api-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop   1674433 7月   8 10:09&#013;&#010;&gt; log4j-core-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     23518 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; log4j-slf4j-impl-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:05写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt; &gt; &gt;&gt; flink-connector-kafka_${scala.binary.version 和&#013;&#010;&gt; &gt; &gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version&#013;&#010;&gt; &gt; &gt; &gt;&gt; 只用加载一个应该就好了，前者的话是dataStream 或者&#010;Table API 程序使用，&#013;&#010;&gt; &gt; &gt; &gt;&gt; 后者的话主要是对前者做了shade处理，方便用户在&#010;SQL&#013;&#010;&gt; &gt; &gt; &gt;&gt; Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#013;&#010;&gt; &gt; &gt; &gt;&gt; 可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; [1] 中的话是有SQL Client JAR 的下载链接，就是&#013;&#010;&gt; &gt; &gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version jar 包的下载链接，你看一看下。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; 祝好&#013;&#010;&gt; &gt; &gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; @Leonard Xu，&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; [1]&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt;  &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一&#010;下午1:39写道：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; Hi, 王松&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream&#013;&#010;&gt; &gt; &gt; connector&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar.&#010;另外， Kafka SQL connector 和 Kafka&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; 祝好，&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; [1]&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt; &lt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; =============================================&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Benchao Li&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "8",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CABFgzE6bg_ND8pOKmHRUCDrGtTCnGh6wa3kT8xnqGjaxfnSdkQ@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:56:01 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "感谢大家的热情解答，最后问题解决了。原因正是 Leonard Xu所说的，我应该引入的是&#013;&#010;flink-sql-connector-kafka-${version}_${scala.binary.version}，然后当时改成&#013;&#010;flink-sql-connector-kafka&#013;&#010;后继续报错的原因是：我还在pom文件中引入了flink-table-planner-blink，如下：&#013;&#010;        &lt;dependency&gt;&#013;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&#013;&#010;&lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;        &lt;/dependency&gt;&#013;&#010;添加&lt;scope&gt;provided&lt;/scope&gt;后就没有问题了。&#013;&#010;&#013;&#010;最后附上正确的pom文件 (如 Jingsong&#013;&#010;所说，也可以把flink-sql-connector-kafka、flink-json这些都在pom文件中去掉，直接将jar报放入lib中)：&#013;&#010;&#013;&#010;============================================================&#013;&#010;&lt;dependencies&gt;&#013;&#010;        &lt;dependency&gt;&#013;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&#013;&#010;&lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;            &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;&#013;&#010;        &lt;/dependency&gt;&#013;&#010;        &lt;dependency&gt;&#013;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;            &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#013;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;        &lt;/dependency&gt;&#013;&#010;        &lt;dependency&gt;&#013;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&#013;&#010;&lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;        &lt;/dependency&gt;&#013;&#010;        &lt;dependency&gt;&#013;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;            &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#013;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;        &lt;/dependency&gt;&#013;&#010;        &lt;dependency&gt;&#013;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;            &lt;artifactId&gt;flink-clients_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;        &lt;/dependency&gt;&#013;&#010;    &lt;/dependencies&gt;&#013;&#010;============================================================&#013;&#010;&#013;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月13日周一 下午4:35写道：&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 1.推荐方式：把flink-sql-connector-kafka-0.11_2.11-1.11.0.jar放入lib下。下载链接：[1]&#013;&#010;&gt;&#013;&#010;&gt; 2.次推荐方式：你的java工程打包时，需要用shade插件把kafka相关类shade到最终的jar中。(不能用jar-with-deps，因为它会覆盖掉java&#013;&#010;&gt; spi)&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Mon, Jul 13, 2020 at 4:04 PM 王松 &lt;sdlcwangsong11@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; 你好本超，&#013;&#010;&gt; &gt; 是的，我尝试解压打包好的jar包，里边是包含我pom中写的依赖的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月13日周一 下午3:42写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 你的程序打包的时候是不是把依赖都shade进去了呢？像这种connector，一般最好是在用户程序中打进去；&#013;&#010;&gt; &gt; &gt; 或者你不打进去的话，也可以在提交作业的时候把这些connector放到classpath里面。&#013;&#010;&gt; &gt; &gt; 当然，直接粗暴的放到lib下，也是可以的。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:38写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt; &gt; 你可以试下把 flink-connector-kafka_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; 的依赖也放lib下试下（pom中删掉），排除是否因为提交作业的方式导致没有正确加载&#010;还是 其他原因。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 祝好&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020年7月13日，15:28，王松 &lt;sdlcwangsong11@gmail.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 您好，我只加载了flink-sql-connector-kafka，另外 scope没有设置，使用了默认值compile。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 我机器上flink/lib下jar包如下：&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-rw-r-- 1 hadoop hadoop    117719 6月  30 12:41&#013;&#010;&gt; &gt; flink-avro-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     90782 7月   8 10:09&#013;&#010;&gt; &gt; flink-csv-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop 108349203 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; flink-dist_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     94863 7月   8 10:09&#013;&#010;&gt; &gt; flink-json-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop   7712156 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; &gt; flink-shaded-zookeeper-3.4.14.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop  33325754 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; &gt; flink-table_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop  37330521 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; &gt; flink-table-blink_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     67114 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; log4j-1.2-api-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop    276771 7月   8 10:09&#013;&#010;&gt; &gt; log4j-api-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop   1674433 7月   8 10:09&#013;&#010;&gt; &gt; log4j-core-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt; -rw-r--r-- 1 hadoop hadoop     23518 7月   8 10:09&#013;&#010;&gt; &gt; &gt; &gt; &gt; log4j-slf4j-impl-2.12.1.jar&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:05写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; flink-connector-kafka_${scala.binary.version 和&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 只用加载一个应该就好了，前者的话是dataStream&#010;或者 Table API 程序使用，&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 后者的话主要是对前者做了shade处理，方便用户在&#010;SQL&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; [1] 中的话是有SQL Client JAR 的下载链接，就是&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version jar 包的下载链接，你看一看下。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 祝好&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; @Leonard Xu，&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; [1]&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;  &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一&#010;下午1:39写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; Hi, 王松&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream&#013;&#010;&gt; &gt; &gt; &gt; connector&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client&#010;Jar. 另外， Kafka SQL connector 和&#013;&#010;&gt; Kafka&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; 祝好，&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; [1]&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt; &lt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&gt; =============================================&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; --&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Benchao Li&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "9",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<CABFgzE5YAmtZvDmh6-DXaNGyD-2Qsw+Y3sRhmYciKu1CNTtS5w@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:03:15 GMT",
        "subject": "Re: flink sql报错 Could not find any factory for identifier 'kafka'",
        "content": "这样还是不行，我尝试flink-connector-kafka-0.11_2.11-1.11.0.jar放到lib下，报了另外一个问题：&#013;&#010;Caused by: java.lang.ClassNotFoundException:&#013;&#010;org.apache.flink.streaming.connectors.kafka.KafkaTableSourceSinkFactoryBase&#013;&#010;&#013;&#010;&#013;&#010;另外，我是用 bin/flink run -yid xxx xxx.jar 的方式提交任务的，报错是直接在终端报错，没有提交到flink&#013;&#010;jobmanager上。&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:38写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt; 你可以试下把 flink-connector-kafka_2.11-1.11.0.jar&#013;&#010;&gt; 的依赖也放lib下试下（pom中删掉），排除是否因为提交作业的方式导致没有正确加载&#010;还是 其他原因。&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月13日，15:28，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 您好，我只加载了flink-sql-connector-kafka，另外 scope没有设置，使用了默认值compile。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我机器上flink/lib下jar包如下：&#013;&#010;&gt; &gt; -rw-rw-r-- 1 hadoop hadoop    117719 6月  30 12:41 flink-avro-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     90782 7月   8 10:09 flink-csv-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop 108349203 7月   8 10:09&#013;&#010;&gt; flink-dist_2.11-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     94863 7月   8 10:09 flink-json-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop   7712156 7月   8 10:09&#013;&#010;&gt; &gt; flink-shaded-zookeeper-3.4.14.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop  33325754 7月   8 10:09&#013;&#010;&gt; &gt; flink-table_2.11-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop  37330521 7月   8 10:09&#013;&#010;&gt; &gt; flink-table-blink_2.11-1.11.0.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     67114 7月   8 10:09&#013;&#010;&gt; log4j-1.2-api-2.12.1.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop    276771 7月   8 10:09 log4j-api-2.12.1.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop   1674433 7月   8 10:09 log4j-core-2.12.1.jar&#013;&#010;&gt; &gt; -rw-r--r-- 1 hadoop hadoop     23518 7月   8 10:09&#013;&#010;&gt; &gt; log4j-slf4j-impl-2.12.1.jar&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午3:05写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt;&gt; flink-connector-kafka_${scala.binary.version 和&#013;&#010;&gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version&#013;&#010;&gt; &gt;&gt; 只用加载一个应该就好了，前者的话是dataStream 或者 Table API&#010;程序使用，&#013;&#010;&gt; &gt;&gt; 后者的话主要是对前者做了shade处理，方便用户在 SQL&#013;&#010;&gt; &gt;&gt; Client的环境中使用。理论上两个都应该ok的，还是同样的错误看起来是依赖没有正确的加载，不知道你的依赖的scope是如何制定的，&#013;&#010;&gt; &gt;&gt; 可以检查下yarn集群上Flink对应的lib下是否有对应的依赖了或者依赖的版本是否正确。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; [1] 中的话是有SQL Client JAR 的下载链接，就是&#013;&#010;&gt; &gt;&gt; flink-sql-connector-kafka_${scala.binary.version jar 包的下载链接，你看一看下。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 祝好&#013;&#010;&gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 在 2020年7月13日，14:42，王松 &lt;sdlcwangsong11@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; @Leonard Xu，&#013;&#010;&gt; &gt;&gt;&gt; 非常感谢您的回复，我试了试您说的方式，还是报同样的错误，另外，我在&#010;[1]&#013;&#010;&gt; &gt;&gt;&gt; 中并没有看到关于flink-sql-connecter-kafka相关的信息重新的pom如下：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-sql-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;        &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &lt;!--&lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&amp;lt;!&amp;ndash;&lt;scope&gt;compile&lt;/scope&gt;&amp;ndash;&amp;gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;           &lt;!--&lt;version&gt;${flink.version}&lt;/version&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt;       &lt;!--&lt;/dependency&gt;--&gt;&#013;&#010;&gt; &gt;&gt;&gt; =============================&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午1:39写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; Hi, 王松&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; 这个报错是pom中缺少了 Kafka SQL connector的依赖，你引入的依赖都是Kafka&#010;datastream&#013;&#010;&gt; connector&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; 的依赖，正确的依赖是：flink-sql-connector-kafka-${version}_${scala.binary.version}&#013;&#010;&gt; &gt;&gt;&gt;&gt; 可以参考官网文档[1], 查看和下载SQL Client Jar. 另外，&#010;Kafka SQL connector 和 Kafka&#013;&#010;&gt; &gt;&gt;&gt;&gt; datastream  connector 同时引用是会冲突的，请根据你的需要使用。&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt; 祝好，&#013;&#010;&gt; &gt;&gt;&gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt;&gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt;&gt;&gt; &lt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-connector-kafka_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;artifactId&gt;flink-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;          &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt;      &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&gt; =============================================&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<CABFgzE5y9_vr78ceU2kaGG=r8-S99f0jim3+jB=2dTuboJ4XHA@mail.gmail.com>"
    },
    {
        "id": "<4af98b7c.4fb4.17346c3d411.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:01:45 GMT",
        "subject": "how to set table.sql-dialect in flink1.11 StreamTableEnvironment",
        "content": "hi all，&#010;&#010;&#010;我像下面那种方式尝试，报错了&#010;&#010;&#010;streamTableEnv.executeSql(&#010;\"\"\"&#010;    |&#010;    |&#010;    |SET table.sql-dialect=hive;&#010;    |CREATE TABLE hive_table (&#010;    |  user_id STRING,&#010;    |  age INT&#010;    |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;    |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;    |  'sink.partition-commit.trigger'='partition-time',&#010;    |  'sink.partition-commit.delay'='1 h',&#010;    |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;    |)&#010;    |&#010;    |\"\"\".stripMargin)&#010;&#010;&#010;错误栈：&#010;Exception in thread \"main\" org.apache.flink.table.api.SqlParserException: SQL parse failed.&#010;Encountered \"table\" at line 4, column 5.&#010;Was expecting one of:&#010;    &lt;BRACKET_QUOTED_IDENTIFIER&gt; ...&#010;    &lt;QUOTED_IDENTIFIER&gt; ...&#010;    &lt;BACK_QUOTED_IDENTIFIER&gt; ...&#010;    &lt;IDENTIFIER&gt; ...&#010;    &lt;UNICODE_QUOTED_IDENTIFIER&gt; ...&#010;    &#010;&#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56)&#010;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;&#010;",
        "depth": "0",
        "reply": "<4af98b7c.4fb4.17346c3d411.Coremail.wander669@163.com>"
    },
    {
        "id": "<68b709dd.631b.17346c66c7c.Coremail.17610775726@163.com>",
        "from": "JasonLee  &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:04:35 GMT",
        "subject": "Re:how to set table.sql-dialect in flink1.11 StreamTableEnvironment",
        "content": "&#010;&#010;&#010;&#010;&#010;&#010;hi&#010;把insert into 语句中的table去掉执行.&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;--&#010;&#010;Best&#010;JasonLee&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 14:01:45，\"Zhou Zach\" &lt;wander669@163.com&gt; 写道：&#010;&gt;hi all，&#010;&gt;&#010;&gt;&#010;&gt;我像下面那种方式尝试，报错了&#010;&gt;&#010;&gt;&#010;&gt;streamTableEnv.executeSql(&#010;&gt;\"\"\"&#010;&gt;    |&#010;&gt;    |&#010;&gt;    |SET table.sql-dialect=hive;&#010;&gt;    |CREATE TABLE hive_table (&#010;&gt;    |  user_id STRING,&#010;&gt;    |  age INT&#010;&gt;    |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;&gt;    |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;    |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;    |  'sink.partition-commit.delay'='1 h',&#010;&gt;    |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;    |)&#010;&gt;    |&#010;&gt;    |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt;错误栈：&#010;&gt;Exception in thread \"main\" org.apache.flink.table.api.SqlParserException: SQL parse failed.&#010;Encountered \"table\" at line 4, column 5.&#010;&gt;Was expecting one of:&#010;&gt;    &lt;BRACKET_QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &lt;QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &lt;BACK_QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &lt;IDENTIFIER&gt; ...&#010;&gt;    &lt;UNICODE_QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &#010;&gt;&#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56)&#010;&gt;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76)&#010;&gt;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;&gt;&#010;",
        "depth": "1",
        "reply": "<4af98b7c.4fb4.17346c3d411.Coremail.wander669@163.com>"
    },
    {
        "id": "<12643908.5037.17346c67a47.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:04:39 GMT",
        "subject": "Re:how to set table.sql-dialect in flink1.11 StreamTableEnvironment",
        "content": "找到了：&#010;tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 14:01:45，\"Zhou Zach\" &lt;wander669@163.com&gt; 写道：&#010;&gt;hi all，&#010;&gt;&#010;&gt;&#010;&gt;我像下面那种方式尝试，报错了&#010;&gt;&#010;&gt;&#010;&gt;streamTableEnv.executeSql(&#010;&gt;\"\"\"&#010;&gt;    |&#010;&gt;    |&#010;&gt;    |SET table.sql-dialect=hive;&#010;&gt;    |CREATE TABLE hive_table (&#010;&gt;    |  user_id STRING,&#010;&gt;    |  age INT&#010;&gt;    |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;&gt;    |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;    |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;    |  'sink.partition-commit.delay'='1 h',&#010;&gt;    |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;    |)&#010;&gt;    |&#010;&gt;    |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt;错误栈：&#010;&gt;Exception in thread \"main\" org.apache.flink.table.api.SqlParserException: SQL parse failed.&#010;Encountered \"table\" at line 4, column 5.&#010;&gt;Was expecting one of:&#010;&gt;    &lt;BRACKET_QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &lt;QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &lt;BACK_QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &lt;IDENTIFIER&gt; ...&#010;&gt;    &lt;UNICODE_QUOTED_IDENTIFIER&gt; ...&#010;&gt;    &#010;&gt;&#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56)&#010;&gt;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76)&#010;&gt;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;&gt;&#010;",
        "depth": "1",
        "reply": "<4af98b7c.4fb4.17346c3d411.Coremail.wander669@163.com>"
    },
    {
        "id": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 06:58:21 GMT",
        "subject": "Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;h','sink.partition-commit.policy.kind'='success-file');&#010;也报错误&#010;query:&#010;streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;streamTableEnv.executeSql(&#010;\"\"\"&#010;    |&#010;    |&#010;    |CREATE TABLE hive_table (&#010;    |  user_id STRING,&#010;    |  age INT&#010;    |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;    |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;    |  'sink.partition-commit.trigger'='partition-time',&#010;    |  'sink.partition-commit.delay'='1 h',&#010;    |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;    |)&#010;    |&#010;    |\"\"\".stripMargin)&#010;&#010;streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;streamTableEnv.executeSql(&#010;\"\"\"&#010;    |&#010;    |CREATE TABLE kafka_table (&#010;    |    uid VARCHAR,&#010;    |    -- uid BIGINT,&#010;    |    sex VARCHAR,&#010;    |    age INT,&#010;    |    created_time TIMESTAMP(3),&#010;    |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;    |) WITH (&#010;    |    'connector.type' = 'kafka',&#010;    |    'connector.version' = 'universal',&#010;    |     'connector.topic' = 'user',&#010;    |    -- 'connector.topic' = 'user_long',&#010;    |    'connector.startup-mode' = 'latest-offset',&#010;    |    'connector.properties.zookeeper.connect' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;    |    'connector.properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;    |    'connector.properties.group.id' = 'user_flink',&#010;    |    'format.type' = 'json',&#010;    |    'format.derive-schema' = 'true'&#010;    |)&#010;    |\"\"\".stripMargin)&#010;&#010;&#010;&#010;streamTableEnv.executeSql(&#010;\"\"\"&#010;    |&#010;    |INSERT INTO hive_table&#010;    |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'), DATE_FORMAT(created_time, 'HH')&#010;    |FROM kafka_table&#010;    |&#010;    |\"\"\".stripMargin)&#010;&#010;streamTableEnv.executeSql(&#010;\"\"\"&#010;    |&#010;    |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;    |&#010;    |\"\"\".stripMargin)&#010;.print()&#010;错误栈：&#010;Exception in thread \"main\" org.apache.flink.table.api.ValidationException: Unable to create&#010;a sink for writing table 'default_catalog.default_database.hive_table'.&#010;&#010;Table options are:&#010;&#010;'hive.storage.file-format'='parquet'&#010;'is_generic'='false'&#010;'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;'sink.partition-commit.delay'='1 h'&#010;'sink.partition-commit.policy.kind'='metastore,success-file'&#010;'sink.partition-commit.trigger'='partition-time'&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&#009;at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&#009;at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&#009;at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&#009;at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&#009;at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&#009;at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&#009;at org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&#009;at org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;Caused by: org.apache.flink.table.api.ValidationException: Table options do not contain an&#010;option key 'connector' for discovering a connector.&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&#009;... 19 more&#010;&#010;",
        "depth": "0",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABi+2jTAdEVJAaO9qKo+Tc_3Uhm3gXtj8vr_cArK3w2sTU=k4A@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:01:28 GMT",
        "subject": "Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "Hi，&#010;&#010;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&#010;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&#010;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt; 也报错误&#010;&gt; query:&#010;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; streamTableEnv.executeSql(&#010;&gt; \"\"\"&#010;&gt;     |&#010;&gt;     |&#010;&gt;     |CREATE TABLE hive_table (&#010;&gt;     |  user_id STRING,&#010;&gt;     |  age INT&#010;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; TBLPROPERTIES (&#010;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;     |)&#010;&gt;     |&#010;&gt;     |\"\"\".stripMargin)&#010;&gt;&#010;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; streamTableEnv.executeSql(&#010;&gt; \"\"\"&#010;&gt;     |&#010;&gt;     |CREATE TABLE kafka_table (&#010;&gt;     |    uid VARCHAR,&#010;&gt;     |    -- uid BIGINT,&#010;&gt;     |    sex VARCHAR,&#010;&gt;     |    age INT,&#010;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;     |) WITH (&#010;&gt;     |    'connector.type' = 'kafka',&#010;&gt;     |    'connector.version' = 'universal',&#010;&gt;     |     'connector.topic' = 'user',&#010;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;     |    'format.type' = 'json',&#010;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;     |)&#010;&gt;     |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; streamTableEnv.executeSql(&#010;&gt; \"\"\"&#010;&gt;     |&#010;&gt;     |INSERT INTO hive_table&#010;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;     |FROM kafka_table&#010;&gt;     |&#010;&gt;     |\"\"\".stripMargin)&#010;&gt;&#010;&gt; streamTableEnv.executeSql(&#010;&gt; \"\"\"&#010;&gt;     |&#010;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt;     |&#010;&gt;     |\"\"\".stripMargin)&#010;&gt; .print()&#010;&gt; 错误栈：&#010;&gt; Exception in thread \"main\" org.apache.flink.table.api.ValidationException:&#010;&gt; Unable to create a sink for writing table&#010;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&#010;&gt; Table options are:&#010;&gt;&#010;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; 'is_generic'='false'&#010;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;         at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;         at&#010;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;         at org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table options&#010;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;         ... 19 more&#010;&gt;&#010;&gt;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "1",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<2c930c15.7d9c.173475dc718.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:49:54 GMT",
        "subject": "Re:Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "Hi,&#010;根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;partition到hive表吗，我当前设置了参数&#010;'sink.partition-commit.policy.kind'='metastore'&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt;Hi，&#010;&gt;&#010;&gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt;&#010;&gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt;&gt; 也报错误&#010;&gt;&gt; query:&#010;&gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; \"\"\"&#010;&gt;&gt;     |&#010;&gt;&gt;     |&#010;&gt;&gt;     |CREATE TABLE hive_table (&#010;&gt;&gt;     |  user_id STRING,&#010;&gt;&gt;     |  age INT&#010;&gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt;&gt; TBLPROPERTIES (&#010;&gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt;     |)&#010;&gt;&gt;     |&#010;&gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt;&#010;&gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; \"\"\"&#010;&gt;&gt;     |&#010;&gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt;&gt;     |    uid VARCHAR,&#010;&gt;&gt;     |    -- uid BIGINT,&#010;&gt;&gt;     |    sex VARCHAR,&#010;&gt;&gt;     |    age INT,&#010;&gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;&gt;     |) WITH (&#010;&gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt;&gt;     |    'connector.version' = 'universal',&#010;&gt;&gt;     |     'connector.topic' = 'user',&#010;&gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt;     |    'format.type' = 'json',&#010;&gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;&gt;     |)&#010;&gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; \"\"\"&#010;&gt;&gt;     |&#010;&gt;&gt;     |INSERT INTO hive_table&#010;&gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt;     |FROM kafka_table&#010;&gt;&gt;     |&#010;&gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt;&#010;&gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; \"\"\"&#010;&gt;&gt;     |&#010;&gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt;&gt;     |&#010;&gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; .print()&#010;&gt;&gt; 错误栈：&#010;&gt;&gt; Exception in thread \"main\" org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; Unable to create a sink for writing table&#010;&gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&gt;&#010;&gt;&gt; Table options are:&#010;&gt;&gt;&#010;&gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; 'is_generic'='false'&#010;&gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt;         at&#010;&gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;&gt;         at org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table options&#010;&gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt;         ... 19 more&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "2",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABi+2jT04_fsSzxw8rn5q-XQA_S6GgDy37xqr1GtaaKmGFgncw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:52:16 GMT",
        "subject": "Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "有开checkpoint吧？delay设的多少？&#010;&#010;Add partition 在 checkpoint完成 + delay的时间后&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&#010;&gt; Hi,&#010;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt; partition到hive表吗，我当前设置了参数&#010;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt; &gt;Hi，&#010;&gt; &gt;&#010;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt; &gt;&#010;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt; &gt;&gt;&#010;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt; &gt;&gt; 也报错误&#010;&gt; &gt;&gt; query:&#010;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt; &gt;&gt;     |  age INT&#010;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt; &gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt; &gt;&gt;     |    age INT,&#010;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; SECOND&#010;&gt; &gt;&gt;     |) WITH (&#010;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; .print()&#010;&gt; &gt;&gt; 错误栈：&#010;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt; &gt;&gt;         at&#010;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table options&#010;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt;         ... 19 more&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "3",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<55008171.8390.173476f1e74.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:08:51 GMT",
        "subject": "Re:Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "开了checkpoint，&#010;val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;streamExecutionEnv.enableCheckpointing(5 * 1000, CheckpointingMode.EXACTLY_ONCE)&#010;streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&#010;&#010;&#010;&#010;间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;有开checkpoint吧？delay设的多少？&#010;&gt;&#010;&gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi,&#010;&gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;Hi，&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt;&gt; &gt;&gt; 也报错误&#010;&gt;&gt; &gt;&gt; query:&#010;&gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt;&gt; &gt;&gt;     |  age INT&#010;&gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt;&gt; &gt;&gt;     |    age INT,&#010;&gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt;&gt; SECOND&#010;&gt;&gt; &gt;&gt;     |) WITH (&#010;&gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; .print()&#010;&gt;&gt; &gt;&gt; 错误栈：&#010;&gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table options&#010;&gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt;         ... 19 more&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;--&#010;&gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "4",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABi+2jTZF3Wkm4x0C+NsDRX+jUURxoKrRpL5e34nfzZdas3WpA@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:21:19 GMT",
        "subject": "Re: Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "有没有设置 sink.partition-commit.delay？&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 5:09 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&#010;&gt; 开了checkpoint，&#010;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt; &gt;&#010;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; Hi,&#010;&gt; &gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;Hi，&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table&#010;&gt; options&#010;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "5",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<9363e458-6d10-4b58-8def-641e4ca025a9.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:23:34 GMT",
        "subject": "回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "你好,&#010;你设置了1个小时的&#010;SINK_PARTITION_COMMIT_DELAY&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;发送时间：2020年7月13日(星期一) 17:09&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：Re:Re: Re: Table options do not contain an option key 'connector' for discovering&#010;a connector.&#010;&#010;开了checkpoint，&#010;val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;streamExecutionEnv.enableCheckpointing(5 * 1000, CheckpointingMode.EXACTLY_ONCE)&#010;streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&#010;&#010;&#010;&#010;间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;有开checkpoint吧？delay设的多少？&#010;&gt;&#010;&gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi,&#010;&gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;Hi，&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt;&gt; &gt;&gt; 也报错误&#010;&gt;&gt; &gt;&gt; query:&#010;&gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt;&gt; &gt;&gt;     |  age INT&#010;&gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt;&gt; &gt;&gt;     |    age INT,&#010;&gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt;&gt; SECOND&#010;&gt;&gt; &gt;&gt;     |) WITH (&#010;&gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; .print()&#010;&gt;&gt; &gt;&gt; 错误栈：&#010;&gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table options&#010;&gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt;         ... 19 more&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;--&#010;&gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "5",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<4520dcbf.8d42.17347918242.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:46:25 GMT",
        "subject": "Re:回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "Hi，&#010;&#010;&#010;我现在改成了：&#010;'sink.partition-commit.delay'='0s'&#010;&#010;&#010;checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;hive表还是查不到数据&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&#010;你好,&#010;你设置了1个小时的&#010;SINK_PARTITION_COMMIT_DELAY&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;发送时间：2020年7月13日(星期一) 17:09&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：Re:Re: Re: Table options do not contain an option key 'connector' for discovering&#010;a connector.&#010;&#010;&#010;开了checkpoint，&#010;val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;streamExecutionEnv.enableCheckpointing(5 * 1000, CheckpointingMode.EXACTLY_ONCE)&#010;streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&#010;&#010;&#010;&#010;间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;有开checkpoint吧？delay设的多少？&#010;&gt;&#010;&gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi,&#010;&gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;Hi，&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt;&gt; &gt;&gt; 也报错误&#010;&gt;&gt; &gt;&gt; query:&#010;&gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt;&gt; &gt;&gt;     |  age INT&#010;&gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;&gt; &gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt;&gt; &gt;&gt;     |    age INT,&#010;&gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt;&gt; SECOND&#010;&gt;&gt; &gt;&gt;     |) WITH (&#010;&gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; .print()&#010;&gt;&gt; &gt;&gt; 错误栈：&#010;&gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table options&#010;&gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt;         ... 19 more&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;--&#010;&gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "6",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABi+2jTijeKyhmc-7Ai-QZgpmzrsHUEq+m=PGVFvywGRw5ErNQ@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:52:54 GMT",
        "subject": "Re: 回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "你把完整的程序再贴下呢&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 5:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&#010;&gt; Hi，&#010;&gt;&#010;&gt;&#010;&gt; 我现在改成了：&#010;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&#010;&gt;&#010;&gt; checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;&gt; hive表还是查不到数据&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&gt;&#010;&gt; 你好,&#010;&gt; 你设置了1个小时的&#010;&gt; SINK_PARTITION_COMMIT_DELAY&#010;&gt;&#010;&gt;&#010;&gt; ------------------------------------------------------------------&#010;&gt; 发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; 发送时间：2020年7月13日(星期一) 17:09&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt; 主 题：Re:Re: Re: Table options do not contain an option key 'connector' for&#010;&gt; discovering a connector.&#010;&gt;&#010;&gt;&#010;&gt; 开了checkpoint，&#010;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt; &gt;&#010;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; Hi,&#010;&gt; &gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;Hi，&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table&#010;&gt; options&#010;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "7",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<1d8a0b.965d.17347cc5f45.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 10:50:42 GMT",
        "subject": "Re:Re: 回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "尴尬。。。。&#010;我开了两个项目，改错项目了，现在 已经成功从hive查到数据了，感谢社区的热情回复，@Jingsong&#010;Li,  @夏帅 &#010;这两天刷了Jingsong在群里的那个视频几遍了，由衷感谢!&#010;还有两个问题问下，&#010;问题1:&#010;创建的kafka_table，在hive和Flink SQL客户端都看不到，而且每次重新运行程序，如果不删除hive_table，就会报错，删除hive_table1，就可以执行，但是每次都不需要删除kafka_table，就可以执行程序，所以，是不是创建的kafka_table，是临时表，只有hive_table是存储在metastore&#010;&#010;&#010;&#010;&#010;&#010;&#010;问题2:&#010;刚才有热心社区同学回答，不用hivecatalog，用filesystem connector 也是可以创建hive表，我尝试了一下，报错了：&#010;java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;Could not execute application.&#010;&#009;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_161]&#010;&#009;at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]&#010;&#009;at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;[qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;[qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [qile-data-flow-1.0.jar:?]&#010;Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could&#010;not execute application.&#010;&#009;... 11 more&#010;Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused&#010;an error: Unable to create a sink for writing table 'default_catalog.default_database.hive_table1'.&#010;&#010;Table options are:&#010;&#010;'connector'='filesystem'&#010;'hive.storage.file-format'='parquet'&#010;'is_generic'='false'&#010;'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;'sink.partition-commit.delay'='0s'&#010;'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;Caused by: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing&#010;table 'default_catalog.default_database.hive_table1'.&#010;&#010;Table options are:&#010;&#010;'connector'='filesystem'&#010;'hive.storage.file-format'='parquet'&#010;'is_generic'='false'&#010;'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;'sink.partition-commit.delay'='0s'&#010;'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;Caused by: org.apache.flink.table.api.ValidationException: Cannot discover a connector using&#010;option ''connector'='filesystem''.&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for&#010;identifier 'filesystem' that implements 'org.apache.flink.table.factories.DynamicTableSinkFactory'&#010;in the classpath.&#010;&#010;Available factory identifiers are:&#010;&#010;blackhole&#010;hbase-1.4&#010;jdbc&#010;kafka&#010;print&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[qile-data-flow-1.0.jar:?]&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;&#010;&#010;&#010;&#010;&#010;&#010;query：&#010;&#010;&#010;&#010;&#010;val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;    streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;    streamExecutionEnv.enableCheckpointing(5 * 1000, CheckpointingMode.EXACTLY_ONCE)&#010;    streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&#010;    val blinkEnvSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;    val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv, blinkEnvSettings)&#010;&#010;&#010;&#010;    streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |&#010;        |CREATE TABLE hive_table (&#010;        |  user_id STRING,&#010;        |  age INT&#010;        |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;        |  'connector'='filesystem',&#010;        |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;        |  'sink.partition-commit.delay'='0s',&#010;        |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;        |)&#010;        |&#010;        |\"\"\".stripMargin)&#010;&#010;    streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE kafka_table (&#010;        |    uid VARCHAR,&#010;        |    -- uid BIGINT,&#010;        |    sex VARCHAR,&#010;        |    age INT,&#010;        |    created_time TIMESTAMP(3),&#010;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;        |) WITH (&#010;        |    'connector.type' = 'kafka',&#010;        |    'connector.version' = 'universal',&#010;        |     'connector.topic' = 'user',&#010;        |    -- 'connector.topic' = 'user_long',&#010;        |    'connector.startup-mode' = 'latest-offset',&#010;        |    'connector.properties.zookeeper.connect' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;        |    'connector.properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;        |    'connector.properties.group.id' = 'user_flink',&#010;        |    'format.type' = 'json',&#010;        |    'format.derive-schema' = 'true'&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;    streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |INSERT INTO hive_table&#010;        |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'), DATE_FORMAT(created_time,&#010;'HH')&#010;        |FROM kafka_table&#010;        |&#010;        |\"\"\".stripMargin)&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='18'&#010;        |&#010;        |\"\"\".stripMargin)&#010;      .print()&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 17:52:54，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;你把完整的程序再贴下呢&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 5:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi，&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 我现在改成了：&#010;&gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;&gt;&gt; hive表还是查不到数据&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&gt;&gt;&#010;&gt;&gt; 你好,&#010;&gt;&gt; 你设置了1个小时的&#010;&gt;&gt; SINK_PARTITION_COMMIT_DELAY&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; ------------------------------------------------------------------&#010;&gt;&gt; 发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; 发送时间：2020年7月13日(星期一) 17:09&#010;&gt;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; 主 题：Re:Re: Re: Table options do not contain an option key 'connector' for&#010;&gt;&gt; discovering a connector.&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 开了checkpoint，&#010;&gt;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt;&#010;&gt;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; Hi,&#010;&gt;&gt; &gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;&gt; &gt;Hi，&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt;&#010;wrote:&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL&#010;'3'&#010;&gt;&gt; &gt;&gt; SECOND&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Table&#010;&gt;&gt; options&#010;&gt;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a connector.&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;--&#010;&gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;--&#010;&gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "8",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABi+2jSF=5R6DtP_fOzj9uH7b2=tCQb7NsWe72ngcdJyAsPKyg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:27:44 GMT",
        "subject": "Re: Re: 回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "Hi,&#010;&#010;问题一：&#010;&#010;只要current catalog是HiveCatalog。&#010;理论上Kafka也是存到HiveMetastore里面的，如果不想报错，可以用CREATE TABLE&#010;XXX IF NOT EXISTS.&#010;&#010;明确下，看不见是什么意思？可以单独试试Kafka表，重启后就不见了吗？&#010;&#010;问题二：&#010;&#010;用filesystem创建出来的是filesystem的表，它和hive&#010;metastore是没有关系的，你需要使用创建filesystem表的语法[1]。&#010;&#010;filesystem的表数据是直接写到 文件系统的，它的格式和hive是兼容的，所以写的路径是hive某张表的路径，就可以在hive端查询了。&#010;但是它的partition commit是不支持metastore的，所以不会有自动add&#010;partition到hive的默认实现，你需要自定义partition-commit-policy.&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 6:51 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&#010;&gt; 尴尬。。。。&#010;&gt; 我开了两个项目，改错项目了，现在 已经成功从hive查到数据了，感谢社区的热情回复，@Jingsong&#010;Li,  @夏帅&#010;&gt; 这两天刷了Jingsong在群里的那个视频几遍了，由衷感谢!&#010;&gt; 还有两个问题问下，&#010;&gt; 问题1:&#010;&gt; 创建的kafka_table，在hive和Flink&#010;&gt; SQL客户端都看不到，而且每次重新运行程序，如果不删除hive_table，就会报错，删除hive_table1，就可以执行，但是每次都不需要删除kafka_table，就可以执行程序，所以，是不是创建的kafka_table，是临时表，只有hive_table是存储在metastore&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 问题2:&#010;&gt; 刚才有热心社区同学回答，不用hivecatalog，用filesystem connector 也是可以创建hive表，我尝试了一下，报错了：&#010;&gt; java.util.concurrent.CompletionException:&#010;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt; Could not execute application.&#010;&gt;         at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt; [?:1.8.0_161]&#010;&gt;         at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; [?:1.8.0_161]&#010;&gt;         at&#010;&gt; org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;         at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt; Could not execute application.&#010;&gt;         ... 11 more&#010;&gt; Caused by: org.apache.flink.client.program.ProgramInvocationException: The&#010;&gt; main method caused an error: Unable to create a sink for writing table&#010;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt;&#010;&gt; Table options are:&#010;&gt;&#010;&gt; 'connector'='filesystem'&#010;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; 'is_generic'='false'&#010;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         ... 10 more&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Unable to&#010;&gt; create a sink for writing table&#010;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt;&#010;&gt; Table options are:&#010;&gt;&#010;&gt; 'connector'='filesystem'&#010;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; 'is_generic'='false'&#010;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         ... 10 more&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Cannot discover&#010;&gt; a connector using option ''connector'='filesystem''.&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         ... 10 more&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;&gt; any factory for identifier 'filesystem' that implements&#010;&gt; 'org.apache.flink.table.factories.DynamicTableSinkFactory' in the classpath.&#010;&gt;&#010;&gt; Available factory identifiers are:&#010;&gt;&#010;&gt; blackhole&#010;&gt; hbase-1.4&#010;&gt; jdbc&#010;&gt; kafka&#010;&gt; print&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; ~[?:1.8.0_161]&#010;&gt;         at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         at&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;         ... 10 more&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; query：&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;     streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt;     streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&gt;&#010;&gt;     val blinkEnvSettings =&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;     val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt; blinkEnvSettings)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;     streamTableEnv.executeSql(&#010;&gt;       \"\"\"&#010;&gt;         |&#010;&gt;         |&#010;&gt;         |CREATE TABLE hive_table (&#010;&gt;         |  user_id STRING,&#010;&gt;         |  age INT&#010;&gt;         |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; TBLPROPERTIES (&#010;&gt;         |  'connector'='filesystem',&#010;&gt;         |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;         |  'sink.partition-commit.delay'='0s',&#010;&gt;         |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;         |)&#010;&gt;         |&#010;&gt;         |\"\"\".stripMargin)&#010;&gt;&#010;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;     streamTableEnv.executeSql(&#010;&gt;       \"\"\"&#010;&gt;         |&#010;&gt;         |CREATE TABLE kafka_table (&#010;&gt;         |    uid VARCHAR,&#010;&gt;         |    -- uid BIGINT,&#010;&gt;         |    sex VARCHAR,&#010;&gt;         |    age INT,&#010;&gt;         |    created_time TIMESTAMP(3),&#010;&gt;         |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; SECOND&#010;&gt;         |) WITH (&#010;&gt;         |    'connector.type' = 'kafka',&#010;&gt;         |    'connector.version' = 'universal',&#010;&gt;         |     'connector.topic' = 'user',&#010;&gt;         |    -- 'connector.topic' = 'user_long',&#010;&gt;         |    'connector.startup-mode' = 'latest-offset',&#010;&gt;         |    'connector.properties.zookeeper.connect' =&#010;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;         |    'connector.properties.bootstrap.servers' =&#010;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;         |    'connector.properties.group.id' = 'user_flink',&#010;&gt;         |    'format.type' = 'json',&#010;&gt;         |    'format.derive-schema' = 'true'&#010;&gt;         |)&#010;&gt;         |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&#010;&gt;     streamTableEnv.executeSql(&#010;&gt;       \"\"\"&#010;&gt;         |&#010;&gt;         |INSERT INTO hive_table&#010;&gt;         |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;         |FROM kafka_table&#010;&gt;         |&#010;&gt;         |\"\"\".stripMargin)&#010;&gt;&#010;&gt;     streamTableEnv.executeSql(&#010;&gt;       \"\"\"&#010;&gt;         |&#010;&gt;         |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='18'&#010;&gt;         |&#010;&gt;         |\"\"\".stripMargin)&#010;&gt;       .print()&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-13 17:52:54，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;你把完整的程序再贴下呢&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 5:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; Hi，&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 我现在改成了：&#010;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;&gt; &gt;&gt; hive表还是查不到数据&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 你好,&#010;&gt; &gt;&gt; 你设置了1个小时的&#010;&gt; &gt;&gt; SINK_PARTITION_COMMIT_DELAY&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; ------------------------------------------------------------------&#010;&gt; &gt;&gt; 发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; 发送时间：2020年7月13日(星期一) 17:09&#010;&gt; &gt;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; 主 题：Re:Re: Re: Table options do not contain an option key 'connector'&#010;&gt; for&#010;&gt; &gt;&gt; discovering a connector.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 开了checkpoint，&#010;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt; &gt;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt; &gt;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt; &gt;&gt; &gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt; &gt;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt;&#010;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;Hi，&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt&#010;&gt; $hr:00:00',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time - INTERVAL&#010;'3'&#010;&gt; &gt;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='13'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt; &gt;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException:&#010;Table&#010;&gt; &gt;&gt; options&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a&#010;&gt; connector.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "9",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<4767382d.9b56.17347ff72d1.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:46:30 GMT",
        "subject": "Re:Re: Re: 回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "创建kafka_table的时候，是default Dialect,改成HiveCatalog时，WATERMARK 和with语法都不支持了，&#010;如果是default Dialect创建的表，是不是只是在临时会话有效&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 19:27:44，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;问题一：&#010;&gt;&#010;&gt;只要current catalog是HiveCatalog。&#010;&gt;理论上Kafka也是存到HiveMetastore里面的，如果不想报错，可以用CREATE&#010;TABLE XXX IF NOT EXISTS.&#010;&gt;&#010;&gt;明确下，看不见是什么意思？可以单独试试Kafka表，重启后就不见了吗？&#010;&gt;&#010;&gt;问题二：&#010;&gt;&#010;&gt;用filesystem创建出来的是filesystem的表，它和hive&#010;&gt;metastore是没有关系的，你需要使用创建filesystem表的语法[1]。&#010;&gt;&#010;&gt;filesystem的表数据是直接写到 文件系统的，它的格式和hive是兼容的，所以写的路径是hive某张表的路径，就可以在hive端查询了。&#010;&gt;但是它的partition commit是不支持metastore的，所以不会有自动add&#010;&gt;partition到hive的默认实现，你需要自定义partition-commit-policy.&#010;&gt;&#010;&gt;[1]&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 6:51 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; 尴尬。。。。&#010;&gt;&gt; 我开了两个项目，改错项目了，现在 已经成功从hive查到数据了，感谢社区的热情回复，@Jingsong&#010;Li,  @夏帅&#010;&gt;&gt; 这两天刷了Jingsong在群里的那个视频几遍了，由衷感谢!&#010;&gt;&gt; 还有两个问题问下，&#010;&gt;&gt; 问题1:&#010;&gt;&gt; 创建的kafka_table，在hive和Flink&#010;&gt;&gt; SQL客户端都看不到，而且每次重新运行程序，如果不删除hive_table，就会报错，删除hive_table1，就可以执行，但是每次都不需要删除kafka_table，就可以执行程序，所以，是不是创建的kafka_table，是临时表，只有hive_table是存储在metastore&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 问题2:&#010;&gt;&gt; 刚才有热心社区同学回答，不用hivecatalog，用filesystem connector 也是可以创建hive表，我尝试了一下，报错了：&#010;&gt;&gt; java.util.concurrent.CompletionException:&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt;&gt; Could not execute application.&#010;&gt;&gt;         at&#010;&gt;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt;&gt; [?:1.8.0_161]&#010;&gt;&gt;         at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt;&gt; [?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;&gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; Caused by:&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt;&gt; Could not execute application.&#010;&gt;&gt;         ... 11 more&#010;&gt;&gt; Caused by: org.apache.flink.client.program.ProgramInvocationException: The&#010;&gt;&gt; main method caused an error: Unable to create a sink for writing table&#010;&gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt;&gt;&#010;&gt;&gt; Table options are:&#010;&gt;&gt;&#010;&gt;&gt; 'connector'='filesystem'&#010;&gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; 'is_generic'='false'&#010;&gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         ... 10 more&#010;&gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Unable to&#010;&gt;&gt; create a sink for writing table&#010;&gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt;&gt;&#010;&gt;&gt; Table options are:&#010;&gt;&gt;&#010;&gt;&gt; 'connector'='filesystem'&#010;&gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; 'is_generic'='false'&#010;&gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         ... 10 more&#010;&gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Cannot discover&#010;&gt;&gt; a connector using option ''connector'='filesystem''.&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         ... 10 more&#010;&gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;&gt;&gt; any factory for identifier 'filesystem' that implements&#010;&gt;&gt; 'org.apache.flink.table.factories.DynamicTableSinkFactory' in the classpath.&#010;&gt;&gt;&#010;&gt;&gt; Available factory identifiers are:&#010;&gt;&gt;&#010;&gt;&gt; blackhole&#010;&gt;&gt; hbase-1.4&#010;&gt;&gt; jdbc&#010;&gt;&gt; kafka&#010;&gt;&gt; print&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt;         ... 10 more&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; query：&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt;&#010;&gt;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;&gt;     streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt;&gt;     streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&gt;&gt;&#010;&gt;&gt;     val blinkEnvSettings =&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;&gt;     val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt;&gt; blinkEnvSettings)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt;       \"\"\"&#010;&gt;&gt;         |&#010;&gt;&gt;         |&#010;&gt;&gt;         |CREATE TABLE hive_table (&#010;&gt;&gt;         |  user_id STRING,&#010;&gt;&gt;         |  age INT&#010;&gt;&gt;         |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt;&gt; TBLPROPERTIES (&#010;&gt;&gt;         |  'connector'='filesystem',&#010;&gt;&gt;         |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt;         |  'sink.partition-commit.delay'='0s',&#010;&gt;&gt;         |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt;         |)&#010;&gt;&gt;         |&#010;&gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt;&#010;&gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt;       \"\"\"&#010;&gt;&gt;         |&#010;&gt;&gt;         |CREATE TABLE kafka_table (&#010;&gt;&gt;         |    uid VARCHAR,&#010;&gt;&gt;         |    -- uid BIGINT,&#010;&gt;&gt;         |    sex VARCHAR,&#010;&gt;&gt;         |    age INT,&#010;&gt;&gt;         |    created_time TIMESTAMP(3),&#010;&gt;&gt;         |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt;&gt; SECOND&#010;&gt;&gt;         |) WITH (&#010;&gt;&gt;         |    'connector.type' = 'kafka',&#010;&gt;&gt;         |    'connector.version' = 'universal',&#010;&gt;&gt;         |     'connector.topic' = 'user',&#010;&gt;&gt;         |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt;         |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt;         |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt;         |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt;         |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt;         |    'format.type' = 'json',&#010;&gt;&gt;         |    'format.derive-schema' = 'true'&#010;&gt;&gt;         |)&#010;&gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt;&#010;&gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt;       \"\"\"&#010;&gt;&gt;         |&#010;&gt;&gt;         |INSERT INTO hive_table&#010;&gt;&gt;         |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt;         |FROM kafka_table&#010;&gt;&gt;         |&#010;&gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt;&#010;&gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt;       \"\"\"&#010;&gt;&gt;         |&#010;&gt;&gt;         |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='18'&#010;&gt;&gt;         |&#010;&gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt;       .print()&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-13 17:52:54，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;你把完整的程序再贴下呢&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Mon, Jul 13, 2020 at 5:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; Hi，&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 我现在改成了：&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;&gt;&gt; &gt;&gt; hive表还是查不到数据&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 你好,&#010;&gt;&gt; &gt;&gt; 你设置了1个小时的&#010;&gt;&gt; &gt;&gt; SINK_PARTITION_COMMIT_DELAY&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; ------------------------------------------------------------------&#010;&gt;&gt; &gt;&gt; 发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; &gt;&gt; 发送时间：2020年7月13日(星期一) 17:09&#010;&gt;&gt; &gt;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; &gt;&gt; 主 题：Re:Re: Re: Table options do not contain an option key 'connector'&#010;&gt;&gt; for&#010;&gt;&gt; &gt;&gt; discovering a connector.&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 开了checkpoint，&#010;&gt;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;&gt; &gt;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt;&gt; &gt;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 * 1000)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt;&#010;wrote:&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt;&gt; &gt;&gt; &gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt;&gt; &gt;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt;&#010;&gt;&gt; wrote:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Hi，&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; wrote:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS&#010;parquet&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt&#010;&gt;&gt; $hr:00:00',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time -&#010;INTERVAL '3'&#010;&gt;&gt; &gt;&gt; &gt;&gt; SECOND&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and&#010;hr='13'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException:&#010;Table&#010;&gt;&gt; &gt;&gt; options&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering&#010;a&#010;&gt;&gt; connector.&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;--&#010;&gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;--&#010;&gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "10",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABi+2jT3qR1MCEshjdDca+QYnDd1EY31tV4AU+vk5Govvu4MMg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:49:10 GMT",
        "subject": "Re: Re: Re: 回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "创建kafka_table需要在default dialect下。&#010;&#010;不管什么dialect，都会保存到hive metastore中 (除非使用temporary table的语法)&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 7:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&#010;&gt; 创建kafka_table的时候，是default Dialect,改成HiveCatalog时，WATERMARK 和with语法都不支持了，&#010;&gt; 如果是default Dialect创建的表，是不是只是在临时会话有效&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-13 19:27:44，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;Hi,&#010;&gt; &gt;&#010;&gt; &gt;问题一：&#010;&gt; &gt;&#010;&gt; &gt;只要current catalog是HiveCatalog。&#010;&gt; &gt;理论上Kafka也是存到HiveMetastore里面的，如果不想报错，可以用CREATE&#010;TABLE XXX IF NOT EXISTS.&#010;&gt; &gt;&#010;&gt; &gt;明确下，看不见是什么意思？可以单独试试Kafka表，重启后就不见了吗？&#010;&gt; &gt;&#010;&gt; &gt;问题二：&#010;&gt; &gt;&#010;&gt; &gt;用filesystem创建出来的是filesystem的表，它和hive&#010;&gt; &gt;metastore是没有关系的，你需要使用创建filesystem表的语法[1]。&#010;&gt; &gt;&#010;&gt; &gt;filesystem的表数据是直接写到 文件系统的，它的格式和hive是兼容的，所以写的路径是hive某张表的路径，就可以在hive端查询了。&#010;&gt; &gt;但是它的partition commit是不支持metastore的，所以不会有自动add&#010;&gt; &gt;partition到hive的默认实现，你需要自定义partition-commit-policy.&#010;&gt; &gt;&#010;&gt; &gt;[1]&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 6:51 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; 尴尬。。。。&#010;&gt; &gt;&gt; 我开了两个项目，改错项目了，现在 已经成功从hive查到数据了，感谢社区的热情回复，@Jingsong&#010;Li,  @夏帅&#010;&gt; &gt;&gt; 这两天刷了Jingsong在群里的那个视频几遍了，由衷感谢!&#010;&gt; &gt;&gt; 还有两个问题问下，&#010;&gt; &gt;&gt; 问题1:&#010;&gt; &gt;&gt; 创建的kafka_table，在hive和Flink&#010;&gt; &gt;&gt;&#010;&gt; SQL客户端都看不到，而且每次重新运行程序，如果不删除hive_table，就会报错，删除hive_table1，就可以执行，但是每次都不需要删除kafka_table，就可以执行程序，所以，是不是创建的kafka_table，是临时表，只有hive_table是存储在metastore&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 问题2:&#010;&gt; &gt;&gt; 刚才有热心社区同学回答，不用hivecatalog，用filesystem connector&#010;也是可以创建hive表，我尝试了一下，报错了：&#010;&gt; &gt;&gt; java.util.concurrent.CompletionException:&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt; &gt;&gt; Could not execute application.&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt; &gt;&gt; [?:1.8.0_161]&#010;&gt; &gt;&gt;         at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; &gt;&gt; [?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; Caused by:&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt; &gt;&gt; Could not execute application.&#010;&gt; &gt;&gt;         ... 11 more&#010;&gt; &gt;&gt; Caused by: org.apache.flink.client.program.ProgramInvocationException:&#010;&gt; The&#010;&gt; &gt;&gt; main method caused an error: Unable to create a sink for writing table&#010;&gt; &gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 'connector'='filesystem'&#010;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Unable to&#010;&gt; &gt;&gt; create a sink for writing table&#010;&gt; &gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 'connector'='filesystem'&#010;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Cannot&#010;&gt; discover&#010;&gt; &gt;&gt; a connector using option ''connector'='filesystem''.&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not&#010;&gt; find&#010;&gt; &gt;&gt; any factory for identifier 'filesystem' that implements&#010;&gt; &gt;&gt; 'org.apache.flink.table.factories.DynamicTableSinkFactory' in the&#010;&gt; classpath.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Available factory identifiers are:&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; blackhole&#010;&gt; &gt;&gt; hbase-1.4&#010;&gt; &gt;&gt; jdbc&#010;&gt; &gt;&gt; kafka&#010;&gt; &gt;&gt; print&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; query：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt; &gt;&gt;     streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt; &gt;&gt;     streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 *&#010;&gt; 1000)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     val blinkEnvSettings =&#010;&gt; &gt;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt; &gt;&gt;     val streamTableEnv =&#010;&gt; StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt; &gt;&gt; blinkEnvSettings)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |CREATE TABLE hive_table (&#010;&gt; &gt;&gt;         |  user_id STRING,&#010;&gt; &gt;&gt;         |  age INT&#010;&gt; &gt;&gt;         |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt;         |  'connector'='filesystem',&#010;&gt; &gt;&gt;         |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt; &gt;&gt;         |  'sink.partition-commit.delay'='0s',&#010;&gt; &gt;&gt;         |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt;         |)&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt;         |    uid VARCHAR,&#010;&gt; &gt;&gt;         |    -- uid BIGINT,&#010;&gt; &gt;&gt;         |    sex VARCHAR,&#010;&gt; &gt;&gt;         |    age INT,&#010;&gt; &gt;&gt;         |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt;         |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt;         |) WITH (&#010;&gt; &gt;&gt;         |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt;         |    'connector.version' = 'universal',&#010;&gt; &gt;&gt;         |     'connector.topic' = 'user',&#010;&gt; &gt;&gt;         |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt;         |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt;         |    'connector.properties.zookeeper.connect' =&#010;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt;         |    'connector.properties.bootstrap.servers' =&#010;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt;         |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt;         |    'format.type' = 'json',&#010;&gt; &gt;&gt;         |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt;         |)&#010;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |INSERT INTO hive_table&#010;&gt; &gt;&gt;         |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt;         |FROM kafka_table&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='18'&#010;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt;       .print()&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-13 17:52:54，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;你把完整的程序再贴下呢&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 5:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; Hi，&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 我现在改成了：&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;&gt; &gt;&gt; &gt;&gt; hive表还是查不到数据&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 你好,&#010;&gt; &gt;&gt; &gt;&gt; 你设置了1个小时的&#010;&gt; &gt;&gt; &gt;&gt; SINK_PARTITION_COMMIT_DELAY&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; ------------------------------------------------------------------&#010;&gt; &gt;&gt; &gt;&gt; 发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; &gt;&gt; 发送时间：2020年7月13日(星期一) 17:09&#010;&gt; &gt;&gt; &gt;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; &gt;&gt; 主 题：Re:Re: Re: Table options do not contain an option key 'connector'&#010;&gt; &gt;&gt; for&#010;&gt; &gt;&gt; &gt;&gt; discovering a connector.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 开了checkpoint，&#010;&gt; &gt;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt; &gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt; &gt;&gt; &gt;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; &gt;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt; &gt;&gt; &gt;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 *&#010;&gt; 1000)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt;&#010;写道：&#010;&gt; &gt;&gt; &gt;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt;&#010;&gt; &gt;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Hi，&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED&#010;AS parquet&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt&#010;&gt; &gt;&gt; $hr:00:00',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time&#010;- INTERVAL&#010;&gt; '3'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect'&#010;=&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers'&#010;=&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13'&#010;and hr='13'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt&#010;$hr:00:00'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException:&#010;&gt; Table&#010;&gt; &gt;&gt; &gt;&gt; options&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering&#010;a&#010;&gt; &gt;&gt; connector.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "11",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<65faee3e.9d4f.173481bd63a.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:17:30 GMT",
        "subject": "Re:Re: Re: Re: 回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "好的，感谢答疑&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 19:49:10，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;创建kafka_table需要在default dialect下。&#010;&gt;&#010;&gt;不管什么dialect，都会保存到hive metastore中 (除非使用temporary table的语法)&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Mon, Jul 13, 2020 at 7:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; 创建kafka_table的时候，是default Dialect,改成HiveCatalog时，WATERMARK&#010;和with语法都不支持了，&#010;&gt;&gt; 如果是default Dialect创建的表，是不是只是在临时会话有效&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-13 19:27:44，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;Hi,&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;问题一：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;只要current catalog是HiveCatalog。&#010;&gt;&gt; &gt;理论上Kafka也是存到HiveMetastore里面的，如果不想报错，可以用CREATE&#010;TABLE XXX IF NOT EXISTS.&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;明确下，看不见是什么意思？可以单独试试Kafka表，重启后就不见了吗？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;问题二：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;用filesystem创建出来的是filesystem的表，它和hive&#010;&gt;&gt; &gt;metastore是没有关系的，你需要使用创建filesystem表的语法[1]。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;filesystem的表数据是直接写到 文件系统的，它的格式和hive是兼容的，所以写的路径是hive某张表的路径，就可以在hive端查询了。&#010;&gt;&gt; &gt;但是它的partition commit是不支持metastore的，所以不会有自动add&#010;&gt;&gt; &gt;partition到hive的默认实现，你需要自定义partition-commit-policy.&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;[1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Mon, Jul 13, 2020 at 6:51 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; 尴尬。。。。&#010;&gt;&gt; &gt;&gt; 我开了两个项目，改错项目了，现在 已经成功从hive查到数据了，感谢社区的热情回复，@Jingsong&#010;Li,  @夏帅&#010;&gt;&gt; &gt;&gt; 这两天刷了Jingsong在群里的那个视频几遍了，由衷感谢!&#010;&gt;&gt; &gt;&gt; 还有两个问题问下，&#010;&gt;&gt; &gt;&gt; 问题1:&#010;&gt;&gt; &gt;&gt; 创建的kafka_table，在hive和Flink&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; SQL客户端都看不到，而且每次重新运行程序，如果不删除hive_table，就会报错，删除hive_table1，就可以执行，但是每次都不需要删除kafka_table，就可以执行程序，所以，是不是创建的kafka_table，是临时表，只有hive_table是存储在metastore&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 问题2:&#010;&gt;&gt; &gt;&gt; 刚才有热心社区同学回答，不用hivecatalog，用filesystem connector&#010;也是可以创建hive表，我尝试了一下，报错了：&#010;&gt;&gt; &gt;&gt; java.util.concurrent.CompletionException:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt;&gt; &gt;&gt; Could not execute application.&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt;&gt; &gt;&gt; [?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt;&gt; &gt;&gt; [?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;&gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt; Caused by:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt;&gt; &gt;&gt; Could not execute application.&#010;&gt;&gt; &gt;&gt;         ... 11 more&#010;&gt;&gt; &gt;&gt; Caused by: org.apache.flink.client.program.ProgramInvocationException:&#010;&gt;&gt; The&#010;&gt;&gt; &gt;&gt; main method caused an error: Unable to create a sink for writing table&#010;&gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 'connector'='filesystem'&#010;&gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         ... 10 more&#010;&gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Unable to&#010;&gt;&gt; &gt;&gt; create a sink for writing table&#010;&gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 'connector'='filesystem'&#010;&gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         ... 10 more&#010;&gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Cannot&#010;&gt;&gt; discover&#010;&gt;&gt; &gt;&gt; a connector using option ''connector'='filesystem''.&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         ... 10 more&#010;&gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not&#010;&gt;&gt; find&#010;&gt;&gt; &gt;&gt; any factory for identifier 'filesystem' that implements&#010;&gt;&gt; &gt;&gt; 'org.apache.flink.table.factories.DynamicTableSinkFactory' in the&#010;&gt;&gt; classpath.&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Available factory identifiers are:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; blackhole&#010;&gt;&gt; &gt;&gt; hbase-1.4&#010;&gt;&gt; &gt;&gt; jdbc&#010;&gt;&gt; &gt;&gt; kafka&#010;&gt;&gt; &gt;&gt; print&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;&gt; ~[?:1.8.0_161]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &gt;&gt;         ... 10 more&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; query：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;&gt; &gt;&gt;     streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt;&gt; &gt;&gt;     streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 *&#010;&gt;&gt; 1000)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     val blinkEnvSettings =&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;&gt; &gt;&gt;     val streamTableEnv =&#010;&gt;&gt; StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt;&gt; &gt;&gt; blinkEnvSettings)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt;       \"\"\"&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |CREATE TABLE hive_table (&#010;&gt;&gt; &gt;&gt;         |  user_id STRING,&#010;&gt;&gt; &gt;&gt;         |  age INT&#010;&gt;&gt; &gt;&gt;         |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt;&gt; &gt;&gt;         |  'connector'='filesystem',&#010;&gt;&gt; &gt;&gt;         |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt; &gt;&gt;         |  'sink.partition-commit.delay'='0s',&#010;&gt;&gt; &gt;&gt;         |  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt;         |)&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt;       \"\"\"&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |CREATE TABLE kafka_table (&#010;&gt;&gt; &gt;&gt;         |    uid VARCHAR,&#010;&gt;&gt; &gt;&gt;         |    -- uid BIGINT,&#010;&gt;&gt; &gt;&gt;         |    sex VARCHAR,&#010;&gt;&gt; &gt;&gt;         |    age INT,&#010;&gt;&gt; &gt;&gt;         |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;&gt;         |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt;&gt; &gt;&gt; SECOND&#010;&gt;&gt; &gt;&gt;         |) WITH (&#010;&gt;&gt; &gt;&gt;         |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;&gt;         |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;&gt;         |     'connector.topic' = 'user',&#010;&gt;&gt; &gt;&gt;         |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;&gt;         |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;         |    'connector.properties.zookeeper.connect' =&#010;&gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt; &gt;&gt;         |    'connector.properties.bootstrap.servers' =&#010;&gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt; &gt;&gt;         |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;&gt;         |    'format.type' = 'json',&#010;&gt;&gt; &gt;&gt;         |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;&gt;         |)&#010;&gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt;       \"\"\"&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |INSERT INTO hive_table&#010;&gt;&gt; &gt;&gt;         |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt; &gt;&gt;         |FROM kafka_table&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt;       \"\"\"&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='18'&#010;&gt;&gt; &gt;&gt;         |&#010;&gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt;       .print()&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在 2020-07-13 17:52:54，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&gt; &gt;你把完整的程序再贴下呢&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 5:46 PM Zhou Zach &lt;wander669@163.com&gt;&#010;wrote:&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Hi，&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 我现在改成了：&#010;&gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;&gt;&gt; &gt;&gt; &gt;&gt; hive表还是查不到数据&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt;&#010;写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 你好,&#010;&gt;&gt; &gt;&gt; &gt;&gt; 你设置了1个小时的&#010;&gt;&gt; &gt;&gt; &gt;&gt; SINK_PARTITION_COMMIT_DELAY&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; ------------------------------------------------------------------&#010;&gt;&gt; &gt;&gt; &gt;&gt; 发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 发送时间：2020年7月13日(星期一) 17:09&#010;&gt;&gt; &gt;&gt; &gt;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 主 题：Re:Re: Re: Table options do not contain an option key&#010;'connector'&#010;&gt;&gt; &gt;&gt; for&#010;&gt;&gt; &gt;&gt; &gt;&gt; discovering a connector.&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 开了checkpoint，&#010;&gt;&gt; &gt;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt;&gt; &gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt;&gt; &gt;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt;&gt; &gt;&gt; &gt;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10&#010;*&#010;&gt;&gt; 1000)&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt;&#010;写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; wrote:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt;&#010;&gt;&gt; &gt;&gt; wrote:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Hi，&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; &gt;&gt; wrote:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING)&#010;STORED AS parquet&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt&#010;&gt;&gt; &gt;&gt; $hr:00:00',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time&#010;- INTERVAL&#010;&gt;&gt; '3'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; SECOND&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect'&#010;=&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers'&#010;=&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time,&#010;'yyyy-MM-dd'),&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13'&#010;and hr='13'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt&#010;$hr:00:00'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException:&#010;&gt;&gt; Table&#010;&gt;&gt; &gt;&gt; &gt;&gt; options&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for&#010;discovering a&#010;&gt;&gt; &gt;&gt; connector.&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;--&#010;&gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;--&#010;&gt;&gt; &gt;Best, Jingsong Lee&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "12",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<CAM2Y1LAcv-rL+n03paC1zH4T8saANvW_Hg41z4QD+D0r5Nir2w@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:42:52 GMT",
        "subject": "Re: Re: Re: Re: 回复：Re: Re: Table options do not contain an option key 'connector' for discovering a connector.",
        "content": "hi,Zhou Zach :&#010;问一下，你把你的程序，并行度设置成 1，还能正常读取hive的数据吗？&#010;&#010;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月13日周一 下午8:17写道：&#010;&#010;&gt; 好的，感谢答疑&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-13 19:49:10，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;创建kafka_table需要在default dialect下。&#010;&gt; &gt;&#010;&gt; &gt;不管什么dialect，都会保存到hive metastore中 (除非使用temporary table的语法)&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Mon, Jul 13, 2020 at 7:46 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; 创建kafka_table的时候，是default Dialect,改成HiveCatalog时，WATERMARK 和with语法都不支持了，&#010;&gt; &gt;&gt; 如果是default Dialect创建的表，是不是只是在临时会话有效&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-13 19:27:44，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;Hi,&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;问题一：&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;只要current catalog是HiveCatalog。&#010;&gt; &gt;&gt; &gt;理论上Kafka也是存到HiveMetastore里面的，如果不想报错，可以用CREATE TABLE XXX IF NOT EXISTS.&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;明确下，看不见是什么意思？可以单独试试Kafka表，重启后就不见了吗？&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;问题二：&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;用filesystem创建出来的是filesystem的表，它和hive&#010;&gt; &gt;&gt; &gt;metastore是没有关系的，你需要使用创建filesystem表的语法[1]。&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;filesystem的表数据是直接写到 文件系统的，它的格式和hive是兼容的，所以写的路径是hive某张表的路径，就可以在hive端查询了。&#010;&gt; &gt;&gt; &gt;但是它的partition commit是不支持metastore的，所以不会有自动add&#010;&gt; &gt;&gt; &gt;partition到hive的默认实现，你需要自定义partition-commit-policy.&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;[1]&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 6:51 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; 尴尬。。。。&#010;&gt; &gt;&gt; &gt;&gt; 我开了两个项目，改错项目了，现在 已经成功从hive查到数据了，感谢社区的热情回复，@Jingsong Li,  @夏帅&#010;&gt; &gt;&gt; &gt;&gt; 这两天刷了Jingsong在群里的那个视频几遍了，由衷感谢!&#010;&gt; &gt;&gt; &gt;&gt; 还有两个问题问下，&#010;&gt; &gt;&gt; &gt;&gt; 问题1:&#010;&gt; &gt;&gt; &gt;&gt; 创建的kafka_table，在hive和Flink&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; SQL客户端都看不到，而且每次重新运行程序，如果不删除hive_table，就会报错，删除hive_table1，就可以执行，但是每次都不需要删除kafka_table，就可以执行程序，所以，是不是创建的kafka_table，是临时表，只有hive_table是存储在metastore&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 问题2:&#010;&gt; &gt;&gt; &gt;&gt; 刚才有热心社区同学回答，不用hivecatalog，用filesystem connector&#010;&gt; 也是可以创建hive表，我尝试了一下，报错了：&#010;&gt; &gt;&gt; &gt;&gt; java.util.concurrent.CompletionException:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt; &gt;&gt; &gt;&gt; Could not execute application.&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt; &gt;&gt; &gt;&gt; [?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; &gt;&gt; &gt;&gt; [?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;&gt; &gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&gt; &gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&gt; &gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; &gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; &gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; &gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; &gt;&gt; &gt;&gt; [qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt; Caused by:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;&gt; &gt;&gt; &gt;&gt; Could not execute application.&#010;&gt; &gt;&gt; &gt;&gt;         ... 11 more&#010;&gt; &gt;&gt; &gt;&gt; Caused by:&#010;&gt; org.apache.flink.client.program.ProgramInvocationException:&#010;&gt; &gt;&gt; The&#010;&gt; &gt;&gt; &gt;&gt; main method caused an error: Unable to create a sink for writing&#010;&gt; table&#010;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 'connector'='filesystem'&#010;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Unable to&#010;&gt; &gt;&gt; &gt;&gt; create a sink for writing table&#010;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table1'.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 'connector'='filesystem'&#010;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;&gt; Method)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Cannot&#010;&gt; &gt;&gt; discover&#010;&gt; &gt;&gt; &gt;&gt; a connector using option ''connector'='filesystem''.&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:329)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;&gt; Method)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not&#010;&gt; &gt;&gt; find&#010;&gt; &gt;&gt; &gt;&gt; any factory for identifier 'filesystem' that implements&#010;&gt; &gt;&gt; &gt;&gt; 'org.apache.flink.table.factories.DynamicTableSinkFactory' in the&#010;&gt; &gt;&gt; classpath.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Available factory identifiers are:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; blackhole&#010;&gt; &gt;&gt; &gt;&gt; hbase-1.4&#010;&gt; &gt;&gt; &gt;&gt; jdbc&#010;&gt; &gt;&gt; &gt;&gt; kafka&#010;&gt; &gt;&gt; &gt;&gt; print&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile$.main(FromKafkaSinkHiveByFile.scala:68)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveByFile.main(FromKafkaSinkHiveByFile.scala)&#010;&gt; &gt;&gt; &gt;&gt; ~[qile-data-flow-1.0.jar:?]&#010;&gt; &gt;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;&gt; Method)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt;&gt; ~[?:1.8.0_161]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;&gt; &gt;&gt; &gt;&gt; ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &gt;&gt; &gt;&gt;         ... 10 more&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; query：&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt; &gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt; &gt;&gt; &gt;&gt;     streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; &gt;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt; &gt;&gt; &gt;&gt;     streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 *&#010;&gt; &gt;&gt; 1000)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;     val blinkEnvSettings =&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt; &gt;&gt; &gt;&gt;     val streamTableEnv =&#010;&gt; &gt;&gt; StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt; &gt;&gt; &gt;&gt; blinkEnvSettings)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |CREATE TABLE hive_table (&#010;&gt; &gt;&gt; &gt;&gt;         |  user_id STRING,&#010;&gt; &gt;&gt; &gt;&gt;         |  age INT&#010;&gt; &gt;&gt; &gt;&gt;         |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet&#010;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt; &gt;&gt;         |  'connector'='filesystem',&#010;&gt; &gt;&gt; &gt;&gt;         |  'partition.time-extractor.timestamp-pattern'='$dt&#010;&gt; $hr:00:00',&#010;&gt; &gt;&gt; &gt;&gt;         |  'sink.partition-commit.delay'='0s',&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt;         |)&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt; &gt;&gt;         |    uid VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt;         |    -- uid BIGINT,&#010;&gt; &gt;&gt; &gt;&gt;         |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt;         |    age INT,&#010;&gt; &gt;&gt; &gt;&gt;         |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;&gt;         |    WATERMARK FOR created_time as created_time - INTERVAL&#010;&gt; '3'&#010;&gt; &gt;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;&gt;         |) WITH (&#010;&gt; &gt;&gt; &gt;&gt;         |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;&gt;         |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;&gt;         |     'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;&gt;         |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;&gt;         |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;&gt;         |    'connector.properties.zookeeper.connect' =&#010;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt; &gt;&gt;         |    'connector.properties.bootstrap.servers' =&#010;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt; &gt;&gt;         |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;&gt;         |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;&gt;         |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;&gt;         |)&#010;&gt; &gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;     streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |INSERT INTO hive_table&#010;&gt; &gt;&gt; &gt;&gt;         |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt; &gt;&gt;         |FROM kafka_table&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;     streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt;       \"\"\"&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |SELECT * FROM hive_table WHERE dt='2020-07-13' and hr='18'&#010;&gt; &gt;&gt; &gt;&gt;         |&#010;&gt; &gt;&gt; &gt;&gt;         |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt;       .print()&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 在 2020-07-13 17:52:54，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;&gt; &gt;你把完整的程序再贴下呢&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 5:46 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi，&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 我现在改成了：&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='0s'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; checkpoint完成了20多次，hdfs文件也产生了20多个，&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; hive表还是查不到数据&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 在 2020-07-13 17:23:34，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 你好,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 你设置了1个小时的&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; SINK_PARTITION_COMMIT_DELAY&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; ------------------------------------------------------------------&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 发送时间：2020年7月13日(星期一) 17:09&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 主 题：Re:Re: Re: Table options do not contain an option key&#010;&gt; 'connector'&#010;&gt; &gt;&gt; &gt;&gt; for&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; discovering a connector.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 开了checkpoint，&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt; &gt;&gt; &gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamExecutionEnv.enableCheckpointing(5 * 1000,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; CheckpointingMode.EXACTLY_ONCE)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamExecutionEnv.getCheckpointConfig.setCheckpointTimeout(10 *&#010;&gt; &gt;&gt; 1000)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 间隔5s，超时10s，不过，等了2分多钟，hdfs上写入了10几个文件了，查hive还是没数据&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 在 2020-07-13 16:52:16，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;有开checkpoint吧？delay设的多少？&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Add partition 在 checkpoint完成 + delay的时间后&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 4:50 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 根据你的提示，加上HiveCatalog，已经成功写入数据到hdfs了，不过，为什么，直接通过hue查hive表，没数据，必须手动add&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; partition到hive表吗，我当前设置了参数&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; At 2020-07-13 15:01:28, \"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt;&#010;&gt; &gt;&gt; &gt;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Hi，&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;你用了HiveCatalog了吗？Hive表或Hive方言必须要结合HiveCatalog&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;不然就只能用Filesystem connector，如果你使用filesystem也报错，那就贴下报错信息&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Jingsong&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;On Mon, Jul 13, 2020 at 2:58 PM Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; &gt;&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; flink 1.11 sink hive table的connector设置为什么啊，尝试设置&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; WITH('connector'='filesystem','path'='...','format'='parquet','sink.partition-commit.delay'='1&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; h','sink.partition-commit.policy.kind'='success-file');&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 也报错误&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; query:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE hive_table (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  user_id STRING,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  age INT&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) PARTITIONED BY (dt STRING, hr STRING) STORED AS&#010;&gt; parquet&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; TBLPROPERTIES (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'partition.time-extractor.timestamp-pattern'='$dt&#010;&gt; &gt;&gt; &gt;&gt; $hr:00:00',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.trigger'='partition-time',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |  'sink.partition-commit.delay'='1 h',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |CREATE TABLE kafka_table (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    uid VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- uid BIGINT,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    age INT,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    WATERMARK FOR created_time as created_time -&#010;&gt; INTERVAL&#010;&gt; &gt;&gt; '3'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |) WITH (&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |     'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    -- 'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.zookeeper.connect' =&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.bootstrap.servers' =&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |INSERT INTO hive_table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT uid, age, DATE_FORMAT(created_time,&#010;&gt; 'yyyy-MM-dd'),&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; DATE_FORMAT(created_time, 'HH')&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |FROM kafka_table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; \"\"\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |SELECT * FROM hive_table WHERE dt='2020-07-13' and&#010;&gt; hr='13'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;     |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; .print()&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 错误栈：&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Exception in thread \"main\"&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.apache.flink.table.api.ValidationException:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Unable to create a sink for writing table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'default_catalog.default_database.hive_table'.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Table options are:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'hive.storage.file-format'='parquet'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'is_generic'='false'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.delay'='1 h'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 'sink.partition-commit.trigger'='partition-time'&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:164)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:344)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:204)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:163)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive$.main(FromKafkaSinkHive.scala:65)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; org.rabbit.sql.FromKafkaSinkHive.main(FromKafkaSinkHive.scala)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Caused by: org.apache.flink.table.api.ValidationException:&#010;&gt; &gt;&gt; Table&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; options&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; do not contain an option key 'connector' for discovering a&#010;&gt; &gt;&gt; &gt;&gt; connector.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:321)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSink(FactoryUtil.java:157)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;         ... 19 more&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;--&#010;&gt; &gt;&gt; &gt;Best, Jingsong Lee&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;",
        "depth": "13",
        "reply": "<6629138e.5bb9.17346f7a54f.Coremail.wander669@163.com>"
    },
    {
        "id": "<684d11d0.30cc.1734702bee2.Coremail.lixin58688@163.com>",
        "from": "李宇彬 &lt;lixin58...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:10:29 GMT",
        "subject": "Flink es7 connector认证问题",
        "content": "各位好，&#010;    请教一个问题&#010;    我们生产环境的es7是有用户名密码认证的，使用如下代码启动后会报，这段代码调用了es&#010;rest client api，单独使用是没问题的，不过放到 flink 里就报错了&#010;        org.elasticsearch.client.ResponseException: method [HEAD], host [xxx], URI [/], status&#010;line [HTTP/1.1 401 Unauthorized]&#010;ParameterTool pt = ParameterTool.fromArgs(args);&#010;String confFile = pt.get(\"confFile\");&#010;pt = ParameterTool.fromPropertiesFile(new File(confFile));&#010;provider.setCredentials(AuthScope.ANY,&#010;                new UsernamePasswordCredentials(pt.get(\"es.user.name\"), pt.get(\"es.user.password\")));&#010;&#010;esSinkBuilder.setRestClientFactory(&#010;(RestClientBuilder restClientBuilder) -&gt;&#010;restClientBuilder&#010;.setRequestConfigCallback(requestConfigBuilder -&gt;&#010;requestConfigBuilder.setSocketTimeout(180000)&#010;.setConnectionRequestTimeout(10000)&#010;)&#010;.setHttpClientConfigCallback(httpClientBuilder -&gt;&#010;{&#010;httpClientBuilder.disableAuthCaching(); //禁用 preemptive 身份验证&#010;return httpClientBuilder.setDefaultCredentialsProvider(provider);&#010;}&#010;)&#010;);",
        "depth": "0",
        "reply": "<684d11d0.30cc.1734702bee2.Coremail.lixin58688@163.com>"
    },
    {
        "id": "<CAFTKPZoGBASjJ1hvPBp8dHOxai_RcGvzUFkF4Z0_pAv56JLQHw@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:33:20 GMT",
        "subject": "Re: Flink es7 connector认证问题",
        "content": "Hi,&#013;&#010;&#013;&#010;请问您有检查过pt.get(\"es.user.name\"),&#013;&#010;pt.get(\"es.user.password\")这两个参数读出来是否都是正确的，另外更完整的错误栈方便提供下么？&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Mon, Jul 13, 2020 at 3:10 PM 李宇彬 &lt;lixin58688@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; 各位好，&#013;&#010;&gt;     请教一个问题&#013;&#010;&gt;     我们生产环境的es7是有用户名密码认证的，使用如下代码启动后会报，这段代码调用了es&#010;rest client api，单独使用是没问题的，不过放到 flink 里就报错了&#013;&#010;&gt;         org.elasticsearch.client.ResponseException: method [HEAD], host [xxx], URI [/],&#010;status line [HTTP/1.1 401 Unauthorized]&#013;&#010;&gt; ParameterTool pt = ParameterTool.fromArgs(args);&#013;&#010;&gt; String confFile = pt.get(\"confFile\");&#013;&#010;&gt; pt = ParameterTool.fromPropertiesFile(new File(confFile));&#013;&#010;&gt; provider.setCredentials(AuthScope.ANY,&#013;&#010;&gt;                 new UsernamePasswordCredentials(pt.get(\"es.user.name\"), pt.get(\"es.user.password\")));&#013;&#010;&gt;&#013;&#010;&gt; esSinkBuilder.setRestClientFactory(&#013;&#010;&gt; (RestClientBuilder restClientBuilder) -&gt;&#013;&#010;&gt; restClientBuilder&#013;&#010;&gt; .setRequestConfigCallback(requestConfigBuilder -&gt;&#013;&#010;&gt; requestConfigBuilder.setSocketTimeout(180000)&#013;&#010;&gt; .setConnectionRequestTimeout(10000)&#013;&#010;&gt; )&#013;&#010;&gt; .setHttpClientConfigCallback(httpClientBuilder -&gt;&#013;&#010;&gt; {&#013;&#010;&gt; httpClientBuilder.disableAuthCaching(); //禁用 preemptive 身份验证&#013;&#010;&gt; return httpClientBuilder.setDefaultCredentialsProvider(provider);&#013;&#010;&gt; }&#013;&#010;&gt; )&#013;&#010;&gt; );&#013;&#010;",
        "depth": "1",
        "reply": "<684d11d0.30cc.1734702bee2.Coremail.lixin58688@163.com>"
    },
    {
        "id": "<37877d63.4a0e.17347c6e552.Coremail.lixin58688@163.com>",
        "from": "李宇彬 &lt;lixin58...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 10:44:43 GMT",
        "subject": "回复： Flink es7 connector认证问题",
        "content": "感谢，已找到问题原因，这个provider变量应该放到setHttpClientConfigCallback内部，之前是作为私有成员变量transient声明的，会导致taskmanager无法拿到认证信息&#010;        String user = pt.get(\"es.user.name\");&#010;String password = pt.get(\"es.user.password\");&#010;esSinkBuilder.setRestClientFactory(&#010;                (RestClientBuilder restClientBuilder) -&gt;&#010;                        restClientBuilder&#010;                                .setHttpClientConfigCallback(httpClientBuilder -&gt;&#010;                                        {&#010;                                            CredentialsProvider provider = new BasicCredentialsProvider();&#010;provider.setCredentials(AuthScope.ANY,&#010;                                                    new UsernamePasswordCredentials(user,&#010;password));&#010;httpClientBuilder.disableAuthCaching(); //禁用 preemptive 身份验证&#010;return httpClientBuilder.setDefaultCredentialsProvider(provider);&#010;}&#010;                                )&#010;        );&#010;在2020年7月13日 15:33，Yangze Guo&lt;karmagyz@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;请问您有检查过pt.get(\"es.user.name\"),&#010;pt.get(\"es.user.password\")这两个参数读出来是否都是正确的，另外更完整的错误栈方便提供下么？&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Mon, Jul 13, 2020 at 3:10 PM 李宇彬 &lt;lixin58688@163.com&gt; wrote:&#010;&#010;各位好，&#010;请教一个问题&#010;我们生产环境的es7是有用户名密码认证的，使用如下代码启动后会报，这段代码调用了es&#010;rest client api，单独使用是没问题的，不过放到 flink 里就报错了&#010;org.elasticsearch.client.ResponseException: method [HEAD], host [xxx], URI [/], status line&#010;[HTTP/1.1 401 Unauthorized]&#010;ParameterTool pt = ParameterTool.fromArgs(args);&#010;String confFile = pt.get(\"confFile\");&#010;pt = ParameterTool.fromPropertiesFile(new File(confFile));&#010;provider.setCredentials(AuthScope.ANY,&#010;new UsernamePasswordCredentials(pt.get(\"es.user.name\"), pt.get(\"es.user.password\")));&#010;&#010;esSinkBuilder.setRestClientFactory(&#010;(RestClientBuilder restClientBuilder) -&gt;&#010;restClientBuilder&#010;.setRequestConfigCallback(requestConfigBuilder -&gt;&#010;requestConfigBuilder.setSocketTimeout(180000)&#010;.setConnectionRequestTimeout(10000)&#010;)&#010;.setHttpClientConfigCallback(httpClientBuilder -&gt;&#010;{&#010;httpClientBuilder.disableAuthCaching(); //禁用 preemptive 身份验证&#010;return httpClientBuilder.setDefaultCredentialsProvider(provider);&#010;}&#010;)&#010;);&#010;",
        "depth": "2",
        "reply": "<684d11d0.30cc.1734702bee2.Coremail.lixin58688@163.com>"
    },
    {
        "id": "<1594624236447-0.post@n8.nabble.com>",
        "from": "hieule &lt;hieule1191...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 07:10:36 GMT",
        "subject": "Re: pyflink问题求助",
        "content": " hello Xingbo Huang,&#010;&#010;when I run , I had some error&#010;&#010;`TypeError: Could not found the Java class&#010;'org.apache.flink.api.java.io.jdbc.JDBCAppendTableSinkBuilder'. The Java&#010;dependencies could be specified via command line argument '--jarfile' or the&#010;config option 'pipeline.jars' `&#010;&#010;&#010;how to solve issue ?&#010;&#010;Thank&#010;hieule&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594624236447-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAPxmL=FuP0Rj074D2o7bLbtPUBtB1-c0RWtPqCB-dDFEpFW+8g@mail.gmail.com>",
        "from": "Xingbo Huang &lt;hxbks...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:55:17 GMT",
        "subject": "Re: pyflink问题求助",
        "content": "Hi hieule,&#010;This work around method is used in flink 1.10, in flink 1.11 you can use&#010;ddl directly (blink planner) which you can refer to [1].&#010;For how to use blink planner in PyFlink, you can refer to following code:&#010;&#010;t_env = BatchTableEnvironment.create(&#010;    environment_settings=EnvironmentSettings.new_instance()&#010;    .in_batch_mode().use_blink_planner().build())&#010;&#010;t_env.get_config().get_configuration().set_string(\"taskmanager.memory.task.off-heap.size\",&#010;'80m')&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html&#010;&#010;Best,&#010;Xingbo&#010;&#010;hieule &lt;hieule1191997@gmail.com&gt; 于2020年7月13日周一 下午4:46写道：&#010;&#010;&gt;  hello Xingbo Huang,&#010;&gt;&#010;&gt; when I run , I had some error&#010;&gt;&#010;&gt; `TypeError: Could not found the Java class&#010;&gt; 'org.apache.flink.api.java.io.jdbc.JDBCAppendTableSinkBuilder'. The Java&#010;&gt; dependencies could be specified via command line argument '--jarfile' or&#010;&gt; the&#010;&gt; config option 'pipeline.jars' `&#010;&gt;&#010;&gt;&#010;&gt; how to solve issue ?&#010;&gt;&#010;&gt; Thank&#010;&gt; hieule&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<1594624236447-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594628411664-0.post@n8.nabble.com>",
        "from": "曹武 &lt;14701319...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:20:11 GMT",
        "subject": "flink使用debezium-json format报错",
        "content": "log4j:WARN No appenders could be found for logger&#010;(org.apache.flink.table.module.ModuleManager).&#010;log4j:WARN Please initialize the log4j system properly.&#010;Exception in thread \"main\" org.apache.flink.table.api.TableException:&#010;Provided trait [BEFORE_AND_AFTER] can't satisfy required trait&#010;[ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue. &#010;Current node is TableSourceScan(table=[[default_catalog, default_database,&#010;ddd",
        "depth": "0",
        "reply": "<1594628411664-0.post@n8.nabble.com>"
    },
    {
        "id": "<7113DB98-8E54-4C5E-8BB7-301ECF59BB45@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:56:41 GMT",
        "subject": "Re: flink使用debezium-json format报错",
        "content": "Hi,&#010;这是 changgelog 里的一个bug[1], 在1.11.1和master上已经修复，1.11.1社区已经在准备中了。&#010;&#010;Best,&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18461 &lt;https://issues.apache.org/jira/browse/FLINK-18461&gt;&#010;&#010;&gt; 在 2020年7月13日，16:20，曹武 &lt;14701319164@163.com&gt; 写道：&#010;&gt; &#010;&gt; log4j:WARN No appenders could be found for logger&#010;&gt; (org.apache.flink.table.module.ModuleManager).&#010;&gt; log4j:WARN Please initialize the log4j system properly.&#010;&gt; Exception in thread \"main\" org.apache.flink.table.api.TableException:&#010;&gt; Provided trait [BEFORE_AND_AFTER] can't satisfy required trait&#010;&gt; [ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue. &#010;&gt; Current node is TableSourceScan(table=[[default_catalog, default_database,&#010;&gt; ddd",
        "depth": "1",
        "reply": "<1594628411664-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594630478839-0.post@n8.nabble.com>",
        "from": "hieule &lt;hieule1191...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 08:54:38 GMT",
        "subject": "pyflink connect mysql",
        "content": "Hi , &#010;I has problem when submit job &#010;```&#010;java.lang.AbstractMethodError: Method&#010;org/apache/flink/api/java/io/jdbc/JDBCAppendTableSink.consumeDataSet(Lorg/apache/flink/api/java/DataSet;)Lorg/apache/flink/api/java/operators/DataSink;&#010;is abstract&#010;```&#010;&#010;&#010;My code :&#010;&#010;```&#010;import logging&#010;import os&#010;import shutil&#010;import sys&#010;import tempfile&#010;&#010;from pyflink.dataset import ExecutionEnvironment&#010;from pyflink.table import BatchTableEnvironment, TableConfig,TableSink&#010;from pyflink.table.descriptors import FileSystem, OldCsv, Schema&#010;from pyflink.table.types import DataTypes&#010;from pyflink.java_gateway import get_gateway&#010;from pyflink.table.types import _to_java_type&#010;from pyflink.util import utils&#010;&#010;class JDBCAppendSink(TableSink):&#010;    &#010;    def __init__(self, field_names: list, field_types: list, driver_name:&#010;str, db_url: str,username: str, password: str, query: str):&#010;        gateway = get_gateway()&#010;        j_field_names = utils.to_jarray(gateway.jvm.String, field_names)&#010;        j_field_types = utils.to_jarray(gateway.jvm.TypeInformation,&#010;                                        [_to_java_type(field_type) for&#010;field_type in field_types])&#010;        builder =&#010;gateway.jvm.org.apache.flink.api.java.io.jdbc.JDBCAppendTableSinkBuilder()&#010;        builder.setUsername(username)&#010;        builder.setPassword(password)&#010;        builder.setDrivername(driver_name)&#010;        builder.setDBUrl(db_url)&#010;        builder.setParameterTypes(j_field_types)&#010;        builder.setQuery(query)&#010;        j_jdbc_sink = builder.build()&#010;        j_jdbc_sink = j_jdbc_sink.configure(j_field_names, j_field_types)&#010;        super(JDBCAppendSink, self).__init__(j_jdbc_sink)&#010;&#010;def word_count():&#010;    t_config = TableConfig()&#010;    env = ExecutionEnvironment.get_execution_environment()&#010;    env.set_parallelism(1)&#010;    t_env = BatchTableEnvironment.create(env, t_config)&#010;&#010;    # register Results table in table environment&#010;    tmp_dir = tempfile.gettempdir()&#010;    result_path = tmp_dir + '/result'&#010;    source_path = \"/home/hieulm/code/data/table_orders.csv\"&#010;    if os.path.exists(result_path):&#010;        try:&#010;            if os.path.isfile(result_path):&#010;                os.remove(result_path)&#010;            else:&#010;                shutil.rmtree(result_path)&#010;        except OSError as e:&#010;            logging.error(\"Error removing directory: %s - %s.\", e.filename,&#010;e.strerror)&#010;&#010;    logging.info(\"Read file source CSV: %s\", source_path)&#010;    t_env.connect(FileSystem().path(source_path)) \\&#010;            .with_format(OldCsv()&#010;                        .field_delimiter(',')&#010;                        .field(\"a\", DataTypes.STRING())&#010;                        .field(\"b\", DataTypes.BIGINT())&#010;                        .field(\"c\", DataTypes.BIGINT()) &#010;                        .field(\"rowtime\", DataTypes.STRING())) \\&#010;            .with_schema(Schema()&#010;                        .field(\"a\", DataTypes.STRING())&#010;                        .field(\"b\", DataTypes.BIGINT())&#010;                        .field(\"c\", DataTypes.BIGINT()) &#010;                        .field(\"rowtime\", DataTypes.STRING())) \\&#010;            .create_temporary_table(\"orders\")&#010;    logging.info(\"Results directory: DB bank_age\")&#010;    t_env.register_table_sink(\"bank_age\",&#010;                JDBCAppendSink(&#010;                    [\"age\", \"count_age\"],&#010;                    [&#010;                        DataTypes.BIGINT(),&#010;                        DataTypes.BIGINT()],&#010;                    \"com.mysql.cj.jdbc.Driver\",&#010;                    \"jdbc:mysql://localhost/flink\",&#010;                    \"hieulm\",&#010;                    \"Csda@123\",&#010;                    \"insert into bank_age (age, count_age) values (?, ?)\"&#010;                ))&#010;&#010;    t_env.scan(\"orders\") \\&#010;        .group_by(\"b\")\\&#010;        .select(\"b as age, count(b) as count_age \") \\&#010;        .insert_into(\"bank_age\")&#010;&#010;    t_env.execute(\"bank_count_age\")&#010;&#010;&#010;if __name__ == '__main__':&#010;    logging.basicConfig(stream=sys.stdout, level=logging.INFO,&#010;format=\"%(message)s\")&#010;&#010;    word_count()&#010;```&#010;&#010;How to solve problem ?&#010;&#010;&#010;thank &#010;hieule&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1594630478839-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594632174739-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:22:54 GMT",
        "subject": "flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "使用 flink 1.11 的 tableEnv 的 createTemporaryTable 取注册表，指定 createTemporaryTable&#010;为事件时间，程序包 Field null does not exist 错误，是我用法有问题？&#010;看了下  https://issues.apache.org/jira/browse/FLINK-16160&#010;&lt;https://issues.apache.org/jira/browse/FLINK-16160&gt;   这个 issue 是解决的这个问题吗？&#010;&#010;tableEnv.connect(kafka)&#010;    .withSchema(&#010;      new Schema().field(\"searchTime\",&#010;DataTypes.TIMESTAMP()).rowtime(rowtime);&#010;    )&#010;    .withFormat(&#010;        new Json().failOnMissingField(false)&#010;    )&#010;    .createTemporaryTable(\"tablename\");&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO933yifoXgJquKxe6KxfTozdxMaHdnUX=iWEdfFoNZmjvXg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:26:40 GMT",
        "subject": "Re: flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "能贴下完整的异常栈么？&#013;&#010;&#013;&#010;Btw，TableEnvironment上的 connect API 目前不建议使用，有许多已知的问题和缺失的&#010;feature，建议用&#013;&#010;executeSql(ddl) 来替代。&#013;&#010;社区计划在 1.12 中系统地重构和修复 connect API 。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Mon, 13 Jul 2020 at 17:24, Hito Zhu &lt;qrshi.tao@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 使用 flink 1.11 的 tableEnv 的 createTemporaryTable 取注册表，指定&#013;&#010;&gt; createTemporaryTable&#013;&#010;&gt; 为事件时间，程序包 Field null does not exist 错误，是我用法有问题？&#013;&#010;&gt; 看了下  https://issues.apache.org/jira/browse/FLINK-16160&#013;&#010;&gt; &lt;https://issues.apache.org/jira/browse/FLINK-16160&gt;   这个 issue 是解决的这个问题吗？&#013;&#010;&gt;&#013;&#010;&gt; tableEnv.connect(kafka)&#013;&#010;&gt;     .withSchema(&#013;&#010;&gt;       new Schema().field(\"searchTime\",&#013;&#010;&gt; DataTypes.TIMESTAMP()).rowtime(rowtime);&#013;&#010;&gt;     )&#013;&#010;&gt;     .withFormat(&#013;&#010;&gt;         new Json().failOnMissingField(false)&#013;&#010;&gt;     )&#013;&#010;&gt;     .createTemporaryTable(\"tablename\");&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594644816592-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:53:36 GMT",
        "subject": "Re: flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "Hi Jark 异常信息如下：&#010;Exception in thread \"main\" org.apache.flink.table.api.ValidationException:&#010;Field null does not exist&#010;&#009;at&#010;org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.lambda$mapToResolvedField$4(TimestampExtractorUtils.java:85)&#010;&#009;at java.util.OptionalInt.orElseThrow(OptionalInt.java:189)&#010;&#009;at&#010;org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.mapToResolvedField(TimestampExtractorUtils.java:85)&#010;&#009;at&#010;org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.lambda$getAccessedFields$0(TimestampExtractorUtils.java:58)&#010;&#009;at&#010;java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)&#010;&#009;at&#010;java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)&#010;&#009;at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)&#010;&#009;at&#010;java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)&#010;&#009;at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545)&#010;&#009;at&#010;java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)&#010;&#009;at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438)&#010;&#009;at&#010;org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.getAccessedFields(TimestampExtractorUtils.java:73)&#010;&#009;at&#010;org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.getAccessedFields(TimestampExtractorUtils.java:65)&#010;&#009;at&#010;org.apache.flink.table.planner.sources.TableSourceUtil$.getRowtimeExtractionExpression(TableSourceUtil.scala:244)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan$$anonfun$1.apply(StreamExecLegacyTableSourceScan.scala:119)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan$$anonfun$1.apply(StreamExecLegacyTableSourceScan.scala:118)&#010;&#009;at scala.Option.map(Option.scala:146)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:118)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:63)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlan(StreamExecLegacyTableSourceScan.scala:63)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:127)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&#009;at&#010;org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48)&#010;&#009;at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:67)&#010;&#009;at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66)&#010;&#009;at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&#009;at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&#009;at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&#009;at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&#009;at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&#009;at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&#009;at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&#009;at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&#009;at&#010;org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66)&#010;&#009;at&#010;org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)&#010;&#009;at&#010;org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:321)&#010;&#009;at&#010;org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toRetractStream(StreamTableEnvironmentImpl.java:297)&#010;&#009;at&#010;org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toRetractStream(StreamTableEnvironmentImpl.java:288)&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "3",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO932BCi=45iQKqjq6c50097rtXMpweSZTLy9ivaOv7s4iZw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:11:07 GMT",
        "subject": "Re: flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "你的源码中 new&#010;Schema().field(\"searchTime\",DataTypes.TIMESTAMP()).rowtime(rowtime);&#010;里面的 rowtime 的定义能贴下吗？&#010;&#010;On Mon, 13 Jul 2020 at 20:53, Hito Zhu &lt;qrshi.tao@gmail.com&gt; wrote:&#010;&#010;&gt; Hi Jark 异常信息如下：&#010;&gt; Exception in thread \"main\" org.apache.flink.table.api.ValidationException:&#010;&gt; Field null does not exist&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.lambda$mapToResolvedField$4(TimestampExtractorUtils.java:85)&#010;&gt;         at java.util.OptionalInt.orElseThrow(OptionalInt.java:189)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.mapToResolvedField(TimestampExtractorUtils.java:85)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.lambda$getAccessedFields$0(TimestampExtractorUtils.java:58)&#010;&gt;         at&#010;&gt; java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)&#010;&gt;         at&#010;&gt;&#010;&gt; java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)&#010;&gt;         at&#010;&gt; java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)&#010;&gt;         at&#010;&gt;&#010;&gt; java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)&#010;&gt;         at&#010;&gt; java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545)&#010;&gt;         at&#010;&gt;&#010;&gt; java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260)&#010;&gt;         at&#010;&gt; java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.getAccessedFields(TimestampExtractorUtils.java:73)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.sources.tsextractors.TimestampExtractorUtils.getAccessedFields(TimestampExtractorUtils.java:65)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.sources.TableSourceUtil$.getRowtimeExtractionExpression(TableSourceUtil.scala:244)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan$$anonfun$1.apply(StreamExecLegacyTableSourceScan.scala:119)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan$$anonfun$1.apply(StreamExecLegacyTableSourceScan.scala:118)&#010;&gt;         at scala.Option.map(Option.scala:146)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:118)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlanInternal(StreamExecLegacyTableSourceScan.scala:63)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacyTableSourceScan.translateToPlan(StreamExecLegacyTableSourceScan.scala:63)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:54)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalc.translateToPlanInternal(StreamExecCalc.scala:39)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:127)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:67)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66)&#010;&gt;         at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;         at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;         at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toDataStream(StreamTableEnvironmentImpl.java:321)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toRetractStream(StreamTableEnvironmentImpl.java:297)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.toRetractStream(StreamTableEnvironmentImpl.java:288)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594688091541-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 00:54:51 GMT",
        "subject": "Re: flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "rowtime 定义如下，我发现 SchemaValidator#deriveFieldMapping 方法给移除了。&#010;Rowtime rowtime = new Rowtime()&#010;                .timestampsFromField(\"searchTime\")&#010;                .watermarksPeriodicBounded(5 * 1000);&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "5",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO932s6=6iWPZWBXAVDHnrN6wz8iaCkuYYDj3yXjnR20==rg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:21:31 GMT",
        "subject": "Re: flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "这可能是 connect API  的某个 bug 吧。 建议先用 DDL 。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Tue, 14 Jul 2020 at 08:54, Hito Zhu &lt;qrshi.tao@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; rowtime 定义如下，我发现 SchemaValidator#deriveFieldMapping 方法给移除了。&#013;&#010;&gt; Rowtime rowtime = new Rowtime()&#013;&#010;&gt;                 .timestampsFromField(\"searchTime\")&#013;&#010;&gt;                 .watermarksPeriodicBounded(5 * 1000);&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594694924716-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:48:44 GMT",
        "subject": "Re: flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "好吧，感谢回答&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "7",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594632286922-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:24:46 GMT",
        "subject": "flink 1.11 createTemporaryTable 指定 rowtime 字段报 Field null does not exist 错误",
        "content": "使用 flink 1.11 的 tableEnv 的 createTemporaryTable 取注册表，指定 createTemporaryTable&#010;为事件时间，程序包 Field null does not exist 错误，是我用法有问题？&#010;看了下  https://issues.apache.org/jira/browse/FLINK-16160&#010;&lt;https://issues.apache.org/jira/browse/FLINK-16160&gt;   这个 issue 是解决的这个问题吗？&#010;&#010;tableEnv.connect(kafka)&#010;    .withSchema(&#010;      new Schema().field(\"searchTime\",&#010;DataTypes.TIMESTAMP()).rowtime(rowtime);&#010;    )&#010;    .withFormat(&#010;        new Json().failOnMissingField(false)&#010;    )&#010;    .createTemporaryTable(\"tablename\");&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594632174739-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_DD8A6C937A1D548CFDD45563FC89AB84AC06@qq.com>",
        "from": "&quot;小学霸&quot; &lt;student_x...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:50:22 GMT",
        "subject": "flink 1.11写入mysql问题",
        "content": "各位大佬好，请教一个问题flink从Kafka读数，写入mysql，程序没有报错，但是没有写入mysql任何数据。&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;source=\"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'alarm_test_g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'scan.startup.mode' = 'earliest-offset', &#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:2181',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:9092',&#013;&#010;&amp;nbsp;'format' = 'json'&amp;nbsp; &#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;&#013;&#010;sink=\"\"\"&#013;&#010;CREATE TABLE g_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp; &#013;&#010;&amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = '123456t',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;t_env.execute_sql(source)&#013;&#010;t_env.execute_sql(sink)&#013;&#010;&#013;&#010;&#013;&#010;source = t_env.from_path(\"kafka_source_tab\")\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\"id,alarm_id,trck_id\")&#013;&#010;source.execute_insert(\"g_source_tab\")",
        "depth": "1",
        "reply": "<tencent_DD8A6C937A1D548CFDD45563FC89AB84AC06@qq.com>"
    },
    {
        "id": "<CAELO931hm8Xg9qDA40MdqxP+KkjoU_Ebgy1Z-8zbzuaESj5N5w@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:21:28 GMT",
        "subject": "Re: flink 1.11写入mysql问题",
        "content": "请问你是怎么提交的作业呢？ 是在本地 IDEA 里面执行的，还是打成 jar&#010;包后提交到集群运行的呢？&#010;&#010;On Mon, 13 Jul 2020 at 17:58, 小学霸 &lt;student_xiao@qq.com&gt; wrote:&#010;&#010;&gt; 各位大佬好，请教一个问题flink从Kafka读数，写入mysql，程序没有报错，但是没有写入mysql任何数据。&#010;&gt; from pyflink.datastream import StreamExecutionEnvironment,&#010;&gt; TimeCharacteristic, CheckpointingMode&#010;&gt; from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#010;&gt; source=\"\"\"&#010;&gt; CREATE TABLE kafka_source_tab (&#010;&gt; &amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;trck_id VARCHAR&#010;&gt;&#010;&gt;&#010;&gt; ) WITH (&#010;&gt; &amp;nbsp;'connector' = 'kafka',&#010;&gt; &amp;nbsp;'topic' = 'alarm_test_g',&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;'scan.startup.mode' = 'earliest-offset',&#010;&gt; &amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:2181',&#010;&gt; &amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:9092',&#010;&gt; &amp;nbsp;'format' = 'json'&amp;nbsp;&#010;&gt; )&#010;&gt; \"\"\"&#010;&gt;&#010;&gt; sink=\"\"\"&#010;&gt; CREATE TABLE g_source_tab (&#010;&gt; &amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;trck_id VARCHAR&#010;&gt;&#010;&gt;&#010;&gt; ) WITH (&#010;&gt; &amp;nbsp;'connector' = 'jdbc',&#010;&gt; &amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp;&#010;&gt; &amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;'username' = 'root',&#010;&gt; &amp;nbsp;'password' = '123456t',&#010;&gt; &amp;nbsp;'sink.buffer-flush.interval' = '1s'&#010;&gt; )&#010;&gt; \"\"\"&#010;&gt; env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;&gt; env.set_parallelism(1)&#010;&gt; env_settings =&#010;&gt; EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#010;&gt; t_env = StreamTableEnvironment.create(env,&#010;&gt; environment_settings=env_settings)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; t_env.execute_sql(source)&#010;&gt; t_env.execute_sql(sink)&#010;&gt;&#010;&gt;&#010;&gt; source = t_env.from_path(\"kafka_source_tab\")\\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\"id,alarm_id,trck_id\")&#010;&gt; source.execute_insert(\"g_source_tab\")&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_DD8A6C937A1D548CFDD45563FC89AB84AC06@qq.com>"
    },
    {
        "id": "<tencent_AD6CB96CACBA0E549B7DF2D5493C86AB8A06@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:51:06 GMT",
        "subject": "flink 1.11写入mysql问题",
        "content": "各位大佬好，请教一个问题flink从Kafka读数，写入mysql，程序没有报错，但是没有写入mysql任何数据。&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;source=\"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'alarm_test_g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'scan.startup.mode' = 'earliest-offset', &#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:2181',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:9092',&#013;&#010;&amp;nbsp;'format' = 'json'&amp;nbsp; &#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;&#013;&#010;sink=\"\"\"&#013;&#010;CREATE TABLE g_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp; &#013;&#010;&amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = '123456t',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;t_env.execute_sql(source)&#013;&#010;t_env.execute_sql(sink)&#013;&#010;&#013;&#010;&#013;&#010;source = t_env.from_path(\"kafka_source_tab\")\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\"id,alarm_id,trck_id\")&#013;&#010;source.execute_insert(\"g_source_tab\")",
        "depth": "1",
        "reply": "<tencent_DD8A6C937A1D548CFDD45563FC89AB84AC06@qq.com>"
    },
    {
        "id": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>",
        "from": "苑士旸 &lt;yuanshiy...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:52:42 GMT",
        "subject": "退订",
        "content": "&#010;&#010;&#010;| |&#010;yuanshiyang&#010;|&#010;|&#010;邮箱yuanshiyang@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "1",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<CABi+2jQrEBhQomNiWc7C2SRfQ_1bxZMf=8yyh_okJA_Xwfc5KA@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:55:25 GMT",
        "subject": "Re: 退订",
        "content": "Hi&#013;&#010;&#013;&#010;退订应该发这个邮箱：user-zh-unsubscribe@flink.apache.org&#013;&#010;&#013;&#010;Best&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Mon, Jul 13, 2020 at 5:53 PM 苑士旸 &lt;yuanshiyang@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; yuanshiyang&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱yuanshiyang@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "2",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<3b3aba34.333b.173479b95ee.Coremail.yuanshiyang@163.com>",
        "from": "苑士旸 &lt;yuanshiy...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 09:57:25 GMT",
        "subject": "回复：退订",
        "content": "谢谢，已经找到&#010;&#010;&#010;| |&#010;yuanshiyang&#010;|&#010;|&#010;邮箱yuanshiyang@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月13日 17:55，Jingsong Li 写道：&#010;Hi&#010;&#010;退订应该发这个邮箱：user-zh-unsubscribe@flink.apache.org&#010;&#010;Best&#010;Jingsong&#010;&#010;On Mon, Jul 13, 2020 at 5:53 PM 苑士旸 &lt;yuanshiyang@163.com&gt; wrote:&#010;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; yuanshiyang&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱yuanshiyang@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&#010;&#010;&#010;--&#010;Best, Jingsong Lee&#010;",
        "depth": "3",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<5ecec1d0.bf35.1734ba324fa.Coremail.chq19970719@163.com>",
        "from": "成欢晴 &lt;chq19970...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:44:09 GMT",
        "subject": "退订",
        "content": "退订&#010;&#010;&#010;| |&#010;chq19970719&#010;|&#010;|&#010;邮箱：chq19970719@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master",
        "depth": "1",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<CAA8tFvv+O4ejhzRn--YSenZhJUXiMQhamKNMOM3rc+1tDj3tww@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 05:49:35 GMT",
        "subject": "Re: 退订",
        "content": "Hi&#013;&#010;&#013;&#010;退订需要发送邮件到  user-zh-unsubscribe@flink.apache.org&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;成欢晴 &lt;chq19970719@163.com&gt; 于2020年7月14日周二 下午12:44写道：&#013;&#010;&#013;&#010;&gt; 退订&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; chq19970719&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：chq19970719@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; Signature is customized by Netease Mail Master&#013;&#010;",
        "depth": "2",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<71310a5c.bf47.1734c8920b9.Coremail.gp1907971839@163.com>",
        "from": "李国鹏 &lt;gp1907971...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:55:22 GMT",
        "subject": "退订",
        "content": "退订",
        "depth": "1",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<4651B964-E8E3-49AA-BBD9-DC1714722DD2@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:00:40 GMT",
        "subject": "Re: 退订",
        "content": "你好，&#010;&#010;退订来自user-zh 邮件组的邮件， 请发任意消息到 这个邮箱：user-zh-unsubscribe@flink.apache.org&#010;&lt;mailto:user-zh-unsubscribe@flink.apache.org&gt; 即可退订&#010;&#010;退订其他邮件可以参考[1]&#010;&#010;祝好&#010;&#010;[1] https://flink.apache.org/community.html#mailing-lists &lt;https://flink.apache.org/community.html#mailing-lists&gt;&#010;&#010;&gt; 在 2020年7月14日，16:55，李国鹏 &lt;gp1907971839@163.com&gt; 写道：&#010;&gt; &#010;&gt; 退订&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<CAJkeMpi=dhqVWc=jK4KDf26+rXEmu5b5sKvoniCG3Fy1WC2Ttg@mail.gmail.com>",
        "from": "zilong xiao &lt;acidzz...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:33:17 GMT",
        "subject": "Re: 退订",
        "content": "你好 我想问下，想加入flink dev邮件组应该怎么操作呢？&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月14日周二 下午5:00写道：&#013;&#010;&#013;&#010;&gt; 你好，&#013;&#010;&gt;&#013;&#010;&gt; 退订来自user-zh 邮件组的邮件， 请发任意消息到 这个邮箱：user-zh-unsubscribe@flink.apache.org&#013;&#010;&gt; &lt;mailto:user-zh-unsubscribe@flink.apache.org&gt; 即可退订&#013;&#010;&gt;&#013;&#010;&gt; 退订其他邮件可以参考[1]&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt;&#013;&#010;&gt; [1] https://flink.apache.org/community.html#mailing-lists &lt;&#013;&#010;&gt; https://flink.apache.org/community.html#mailing-lists&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月14日，16:55，李国鹏 &lt;gp1907971839@163.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 退订&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<8D09391F-DE3D-4E39-9C56-3F167C186A20@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:47:08 GMT",
        "subject": "Re: 退订",
        "content": "Hi,&#010;类似的，发送任意邮件到 dev-digest-subscribe@flink.apache.org &lt;mailto:dev-digest-subscribe@flink.apache.org&gt;&#010;即可订阅来自dev &lt;mailto:dev@flink.apache.org&gt;@flink.apache.org &lt;mailto:dev@flink.apache.org&gt;的邮件，&#010;上面的链接【1】里也有订阅方式的。&#010;&#010;祝好&#010;&gt; 在 2020年7月14日，17:33，zilong xiao &lt;acidzz163@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; 你好 我想问下，想加入flink dev邮件组应该怎么操作呢？&#010;&gt; &#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月14日周二 下午5:00写道：&#010;&gt; &#010;&gt;&gt; 你好，&#010;&gt;&gt; &#010;&gt;&gt; 退订来自user-zh 邮件组的邮件， 请发任意消息到 这个邮箱：user-zh-unsubscribe@flink.apache.org&#010;&gt;&gt; &lt;mailto:user-zh-unsubscribe@flink.apache.org&gt; 即可退订&#010;&gt;&gt; &#010;&gt;&gt; 退订其他邮件可以参考[1]&#010;&gt;&gt; &#010;&gt;&gt; 祝好&#010;&gt;&gt; &#010;&gt;&gt; [1] https://flink.apache.org/community.html#mailing-lists &lt;&#010;&gt;&gt; https://flink.apache.org/community.html#mailing-lists&gt;&#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月14日，16:55，李国鹏 &lt;gp1907971839@163.com&gt; 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 退订&#010;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "4",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<942DAAFF-D92A-4866-A216-96C15C3EF7FF@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:49:35 GMT",
        "subject": "Re: 退订",
        "content": "Hi&#010;&#010;打错了，发送任意邮件到 dev-subscribe@flink.apache.org &lt;mailto:dev-subscribe@flink.apache.org&gt;&#010;即可订阅来自dev &lt;mailto:dev@flink.apache.org&gt;@flink.apache.org &lt;mailto:dev@flink.apache.org&gt;的邮件&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; Hi,&#010;&gt; 类似的，发送任意邮件到 dev-subscribe@flink.apache.org &lt;mailto:dev-digest-subscribe@flink.apache.org&gt;&#010;即可订阅来自dev &lt;mailto:dev@flink.apache.org&gt;@flink.apache.org &lt;mailto:dev@flink.apache.org&gt;的邮件，&#010;&gt; 上面的链接【1】里也有订阅方式的。&#010;&gt; &#010;&gt; 祝好&#010;&#010;&#010;&#010;&gt; 在 2020年7月14日，17:47，Leonard Xu &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,&#010;&gt; 类似的，发送任意邮件到 dev-digest-subscribe@flink.apache.org &lt;mailto:dev-digest-subscribe@flink.apache.org&gt;&#010;即可订阅来自dev &lt;mailto:dev@flink.apache.org&gt;@flink.apache.org &lt;mailto:dev@flink.apache.org&gt;的邮件，&#010;&gt; 上面的链接【1】里也有订阅方式的。&#010;&gt; &#010;&gt; 祝好&#010;&gt;&gt; 在 2020年7月14日，17:33，zilong xiao &lt;acidzz163@gmail.com &lt;mailto:acidzz163@gmail.com&gt;&gt;&#010;写道：&#010;&gt;&gt; &#010;&gt;&gt; 你好 我想问下，想加入flink dev邮件组应该怎么操作呢？&#010;&gt;&gt; &#010;&gt;&gt; Leonard Xu &lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&gt;&gt; 于2020年7月14日周二&#010;下午5:00写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt; 你好，&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 退订来自user-zh 邮件组的邮件， 请发任意消息到 这个邮箱：user-zh-unsubscribe@flink.apache.org&#010;&lt;mailto:user-zh-unsubscribe@flink.apache.org&gt;&#010;&gt;&gt;&gt; &lt;mailto:user-zh-unsubscribe@flink.apache.org &lt;mailto:user-zh-unsubscribe@flink.apache.org&gt;&gt;&#010;即可退订&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 退订其他邮件可以参考[1]&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 祝好&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; [1] https://flink.apache.org/community.html#mailing-lists &lt;https://flink.apache.org/community.html#mailing-lists&gt;&#010;&lt;&#010;&gt;&gt;&gt; https://flink.apache.org/community.html#mailing-lists &lt;https://flink.apache.org/community.html#mailing-lists&gt;&gt;&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 在 2020年7月14日，16:55，李国鹏 &lt;gp1907971839@163.com &lt;mailto:gp1907971839@163.com&gt;&gt;&#010;写道：&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 退订&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt; &#010;&#010;&#010;",
        "depth": "5",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<202007141746229012261@qq.com>",
        "from": "&quot;zhaoheng.zhaoheng@qq.com&quot; &lt;zhaoheng.zhaoh...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:47:04 GMT",
        "subject": "Re: Re: 退订",
        "content": "你好,   &#013;&#010;&#013;&#010;  可以发送订阅到dev@flink.apache.org&#013;&#010;&#013;&#010;参见 https://flink.apache.org/community.html&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;zhaoheng.zhaoheng@qq.com&#013;&#010; &#013;&#010;发件人： zilong xiao&#013;&#010;发送时间： 2020-07-14 17:33&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: 退订&#013;&#010;你好 我想问下，想加入flink dev邮件组应该怎么操作呢？&#013;&#010; &#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月14日周二 下午5:00写道：&#013;&#010; &#013;&#010;&gt; 你好，&#013;&#010;&gt;&#013;&#010;&gt; 退订来自user-zh 邮件组的邮件， 请发任意消息到 这个邮箱：user-zh-unsubscribe@flink.apache.org&#013;&#010;&gt; &lt;mailto:user-zh-unsubscribe@flink.apache.org&gt; 即可退订&#013;&#010;&gt;&#013;&#010;&gt; 退订其他邮件可以参考[1]&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt;&#013;&#010;&gt; [1] https://flink.apache.org/community.html#mailing-lists &lt;&#013;&#010;&gt; https://flink.apache.org/community.html#mailing-lists&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月14日，16:55，李国鹏 &lt;gp1907971839@163.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 退订&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<5fe3b9a.eb87.173578f9cf8.Coremail.gp1907971839@163.com>",
        "from": "李国鹏 &lt;gp1907971...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:18:16 GMT",
        "subject": "退订",
        "content": "退订",
        "depth": "1",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<DFB725F6-DAB5-4EEC-B868-CF899DB595FE@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:19:31 GMT",
        "subject": "Re: 退订",
        "content": "Hi,&#010;是指取消订阅邮件吗？&#010;可以发送任意内容的邮件到  user-zh-unsubscribe@flink.apache.org  取消订阅来自&#010;user-zh@flink.apache.org 邮件组的邮件&#010;&#010;邮件组的订阅管理，可以参考[1]&#010;&#010;祝好，&#010;Leonard Xu&#010;https://flink.apache.org/community.html#how-to-subscribe-to-a-mailing-list &lt;https://flink.apache.org/community.html#how-to-subscribe-to-a-mailing-list&gt;&#010;&#010;&#010;&gt; 在 2020年7月16日，20:18，李国鹏 &lt;gp1907971839@163.com&gt; 写道：&#010;&gt; &#010;&gt; 退订&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<647c430e.32de.173479744c8.Coremail.yuanshiyang@163.com>"
    },
    {
        "id": "<CA+QRB_+2hHip3B-W3iPA4ngX-ZhOkndbD00+SOC9wbQJqN=TKw@mail.gmail.com>",
        "from": "Zhefu PENG &lt;pengzf0...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:50:59 GMT",
        "subject": "Re: flink任务checkpoint无法完成snapshot，且报kafka异常",
        "content": "Hi all,&#010;&#010;这封邮件最开始发出已经一个月了，这一个月里尝试了很多朋友或者各位大佬的建议，目前经过一周末加上两个工作日的查看，问题看来是解决了。&#010;&#010;问题的根本原因：Kafka集群的性能不足（怀疑是CPU负荷过大）。问题出现的时候线上kakfa集群只有七台机器，在排除所有别的原因以及能进行到的尝试方案后，决定进行扩容。扩到15台机器。目前来看，平稳运行，没有再报出类似错误。&#010;&#010;反馈一下，如果有朋友遇到类似的问题，可以参考，给这个问题做一个闭环。谢谢各位的关注和帮忙。&#010;&#010;Best,&#010;Zhefu&#010;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年6月12日周五 上午9:49写道：&#010;&#010;&gt; Hi ZheFu,&#010;&gt;&#010;&gt; 可以把你的 Flink 版本说一下，我大致理解是这样的，每次 sink 端&#010;在 snapshotState 的时候，会检查该次 Sink&#010;&gt; 的数据是否都已经 Sink 到了 kafka.&#010;&gt;&#010;&gt; 也就是说，你这次 Checkpoint 的时候，由于你的 Checkpoint 间隔较短，Kafka&#010;那边给回的消息记录 Ack&#010;&gt; 还没有弄完，所以有这个问题。建议 Checkpoint 间隔弄长点。&#010;&gt;&#010;&gt; 具体代码查看：FlinkKafkaProducerBase.snapshotState 这个方法。&#010;&gt;&#010;&gt; Best,&#010;&gt; LakeShen&#010;&gt;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年6月11日周四 上午9:50写道：&#010;&gt;&#010;&gt; &gt; Hi&#010;&gt; &gt;&#010;&gt; &gt; 从错误栈看是因为 task 端 snapshot 出问题了，原因是 “Caused by:&#010;&gt; &gt; java.lang.IllegalStateException: Pending record count must be zero at&#010;&gt; this&#010;&gt; &gt; point: 5”，需要看一下为什么会走到这里&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Congxian&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 李奇 &lt;359502980@qq.com&gt; 于2020年6月10日周三 下午5:57写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 哈喽，根据我自己遇到checkpoint失败，一般是因为你数据有问题，导致算子失败，有可能是数据格式，或者字段类型不匹配，字段数量等相关的原因造成，我看你补充的内容，好像是你kafka数据有问题样，你可以往这个方向看看数据是否正常。解析是否正确。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 在 2020年6月10日，下午1:24，Zhefu PENG &lt;pengzf0802@gmail.com&gt;&#010;写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; ﻿补充一下，在TaskManager发现了如下错误日志：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 2020-06-10 12:44:40,688 ERROR&#010;&gt; &gt; &gt; &gt; org.apache.flink.streaming.runtime.tasks.StreamTask           - Error&#010;&gt; &gt; &gt; &gt; during disposal of stream operator.&#010;&gt; &gt; &gt; &gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#010;&gt; Failed&#010;&gt; &gt; &gt; to&#010;&gt; &gt; &gt; &gt; send data to Kafka: Pending record count must be zero at this point:&#010;&gt; 5&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1218)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:861)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:668)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:579)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:481)&#010;&gt; &gt; &gt; &gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &gt; &gt; &gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &gt; &gt; &gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt; &gt; &gt; Caused by: java.lang.IllegalStateException: Pending record count must&#010;&gt; &gt; be&#010;&gt; &gt; &gt; &gt; zero at this point: 5&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:969)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:834)&#010;&gt; &gt; &gt; &gt; ... 8 more&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 希望得到帮助，感谢！&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Zhefu PENG &lt;pengzf0802@gmail.com&gt; 于2020年6月10日周三 下午1:03写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&gt; Hi all,&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; 现在有一个简单的flink任务，大概chain在一起后的执行图为：&#010;&gt; &gt; &gt; &gt;&gt; Source: Custom Source -&gt; Map -&gt; Source_Map -&gt; Empty_Filer&#010;-&gt;&#010;&gt; &gt; &gt; Field_Filter&#010;&gt; &gt; &gt; &gt;&gt; -&gt; Type_Filter -&gt; Value_Filter -&gt; Map -&gt; Map -&gt; Map&#010;-&gt; Sink: Unnamed&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 但是在上线一段时间后，开始报错，日志中有说到无法完成checkpoint，还提到有kafka的网络和连接异常。但还有别的flink任务在相同的broker上进行数据的读写，并且没有报错。我们暂时定位在，有可能每个checkpoint的完成时间比较长，需要几百毫秒，我们设的时间间隔又比较短，只有一秒，可能是这部分影响到了任务的性能。但是这只是一个不太靠谱的猜想，现在也没有什么排查的切入点，想看看大家有没有一些看法或者建议意见，非常感谢。&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; 部分报错信息如下：&#010;&gt; &gt; &gt; &gt;&gt; 2020-06-10 12:02:49,083 INFO&#010;&gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     -&#010;&gt; &gt; &gt; Triggering&#010;&gt; &gt; &gt; &gt;&gt; checkpoint 1 @ 1591761769060 for job&#010;&gt; c41f4811262db1c4c270b136571c8201.&#010;&gt; &gt; &gt; &gt;&gt; 2020-06-10 12:04:47,898 INFO&#010;&gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     -&#010;&gt; &gt; Decline&#010;&gt; &gt; &gt; &gt;&gt; checkpoint 1 by task 0cb03590fdf18027206ef628b3ef5863 of job&#010;&gt; &gt; &gt; &gt;&gt; c41f4811262db1c4c270b136571c8201 at&#010;&gt; &gt; &gt; &gt;&gt; container_e27_1591466310139_21670_01_000006 @&#010;&gt; &gt; &gt; &gt;&gt; hdp1-hadoop-datanode-4.novalocal (dataPort=44778).&#010;&gt; &gt; &gt; &gt;&gt; 2020-06-10 12:04:47,899 INFO&#010;&gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     -&#010;&gt; &gt; &gt; Discarding&#010;&gt; &gt; &gt; &gt;&gt; checkpoint 1 of job c41f4811262db1c4c270b136571c8201.&#010;&gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointException: Could not&#010;&gt; &gt; &gt; &gt;&gt; complete snapshot 1 for operator Source: Custom Source -&gt; Map -&gt;&#010;&gt; &gt; &gt; Source_Map&#010;&gt; &gt; &gt; &gt;&gt; -&gt; Empty_Filer -&gt; Field_Filter -&gt; Type_Filter -&gt; Value_Filter&#010;-&gt; Map&#010;&gt; &gt; -&gt;&#010;&gt; &gt; &gt; Map&#010;&gt; &gt; &gt; &gt;&gt; -&gt; Map -&gt; Sink: Unnamed (7/12). Failure reason: Checkpoint was&#010;&gt; &gt; declined.&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:434)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1420)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1354)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:991)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:887)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:860)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:793)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$3(StreamTask.java:777)&#010;&gt; &gt; &gt; &gt;&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:261)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&gt; &gt; &gt; &gt;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &gt; &gt; &gt;&gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &gt; &gt; &gt;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt; &gt; &gt;&gt; Caused by:&#010;&gt; &gt; &gt; &gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#010;&gt; &gt; Failed&#010;&gt; &gt; &gt; to&#010;&gt; &gt; &gt; &gt;&gt; send data to Kafka: The server disconnected before a response was&#010;&gt; &gt; &gt; received.&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1218)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:973)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.preCommit(FlinkKafkaProducer.java:892)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.preCommit(FlinkKafkaProducer.java:98)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.snapshotState(TwoPhaseCommitSinkFunction.java:317)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.snapshotState(FlinkKafkaProducer.java:978)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:90)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotStat(AbstractStreamOperator.java:402)&#010;&gt; &gt; &gt; &gt;&gt; ... 18 more&#010;&gt; &gt; &gt; &gt;&gt; Caused by: org.apache.kafka.common.errors.NetworkException: The&#010;&gt; server&#010;&gt; &gt; &gt; &gt;&gt; disconnected before a response was received.&#010;&gt; &gt; &gt; &gt;&gt; 2020-06-10 12:04:47,913 INFO&#010;&gt; &gt; &gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt; &gt; &gt;&gt;                 - Trying to recover from a global failure.&#010;&gt; &gt; &gt; &gt;&gt; org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint&#010;&gt; &gt; &gt; tolerable&#010;&gt; &gt; &gt; &gt;&gt; failure threshold.&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:87)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.failPendingCheckpointDueToTaskFailure(CheckpointCoordinator.java:1467)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.discardCheckpoint(CheckpointCoordinator.java:1377)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:719)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.lambda$declineCheckpoint$5(SchedulerBase.java:807)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt; &gt; &gt; &gt;&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#010;&gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#010;&gt; &gt; &gt; &gt;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; 期望收到各位的回复和帮助。&#010;&gt; &gt; &gt; &gt;&gt; Best,&#010;&gt; &gt; &gt; &gt;&gt; Zhefu&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CA+QRB_+2hHip3B-W3iPA4ngX-ZhOkndbD00+SOC9wbQJqN=TKw@mail.gmail.com>"
    },
    {
        "id": "<94ADA7C9-5A86-4161-B215-95306C8404B2@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:01:38 GMT",
        "subject": "Re: flink任务checkpoint无法完成snapshot，且报kafka异常",
        "content": "&#010;&gt; 反馈一下，如果有朋友遇到类似的问题，可以参考，给这个问题做一个闭环。谢谢各位的关注和帮忙。&#010;&gt; &#010;&gt; Best,&#010;&gt; Zhefu&#010;&#010;谢谢 zhefu,  给你大大点赞，很社区的方式，相信这样的积累越多，小伙伴们都能学习到更多。&#010;&#010;祝好，&#010;Leonard Xu&#010; &#010;&#010;&#010;&gt; &#010;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年6月12日周五 上午9:49写道：&#010;&gt; &#010;&gt;&gt; Hi ZheFu,&#010;&gt;&gt; &#010;&gt;&gt; 可以把你的 Flink 版本说一下，我大致理解是这样的，每次 sink&#010;端 在 snapshotState 的时候，会检查该次 Sink&#010;&gt;&gt; 的数据是否都已经 Sink 到了 kafka.&#010;&gt;&gt; &#010;&gt;&gt; 也就是说，你这次 Checkpoint 的时候，由于你的 Checkpoint 间隔较短，Kafka&#010;那边给回的消息记录 Ack&#010;&gt;&gt; 还没有弄完，所以有这个问题。建议 Checkpoint 间隔弄长点。&#010;&gt;&gt; &#010;&gt;&gt; 具体代码查看：FlinkKafkaProducerBase.snapshotState 这个方法。&#010;&gt;&gt; &#010;&gt;&gt; Best,&#010;&gt;&gt; LakeShen&#010;&gt;&gt; &#010;&gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年6月11日周四 上午9:50写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt; Hi&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 从错误栈看是因为 task 端 snapshot 出问题了，原因是 “Caused&#010;by:&#010;&gt;&gt;&gt; java.lang.IllegalStateException: Pending record count must be zero at&#010;&gt;&gt; this&#010;&gt;&gt;&gt; point: 5”，需要看一下为什么会走到这里&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt; Congxian&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 李奇 &lt;359502980@qq.com&gt; 于2020年6月10日周三 下午5:57写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; 哈喽，根据我自己遇到checkpoint失败，一般是因为你数据有问题，导致算子失败，有可能是数据格式，或者字段类型不匹配，字段数量等相关的原因造成，我看你补充的内容，好像是你kafka数据有问题样，你可以往这个方向看看数据是否正常。解析是否正确。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 在 2020年6月10日，下午1:24，Zhefu PENG &lt;pengzf0802@gmail.com&gt;&#010;写道：&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; ﻿补充一下，在TaskManager发现了如下错误日志：&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 2020-06-10 12:44:40,688 ERROR&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask           - Error&#010;&gt;&gt;&gt;&gt;&gt; during disposal of stream operator.&#010;&gt;&gt;&gt;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#010;&gt;&gt; Failed&#010;&gt;&gt;&gt;&gt; to&#010;&gt;&gt;&gt;&gt;&gt; send data to Kafka: Pending record count must be zero at this point:&#010;&gt;&gt; 5&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1218)&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:861)&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:668)&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:579)&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:481)&#010;&gt;&gt;&gt;&gt;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;&gt;&gt;&gt;&gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;&gt;&gt;&gt;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&gt;&gt;&gt;&gt; Caused by: java.lang.IllegalStateException: Pending record count must&#010;&gt;&gt;&gt; be&#010;&gt;&gt;&gt;&gt;&gt; zero at this point: 5&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:969)&#010;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:834)&#010;&gt;&gt;&gt;&gt;&gt; ... 8 more&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 希望得到帮助，感谢！&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Zhefu PENG &lt;pengzf0802@gmail.com&gt; 于2020年6月10日周三 下午1:03写道：&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Hi all,&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 现在有一个简单的flink任务，大概chain在一起后的执行图为：&#010;&gt;&gt;&gt;&gt;&gt;&gt; Source: Custom Source -&gt; Map -&gt; Source_Map -&gt; Empty_Filer&#010;-&gt;&#010;&gt;&gt;&gt;&gt; Field_Filter&#010;&gt;&gt;&gt;&gt;&gt;&gt; -&gt; Type_Filter -&gt; Value_Filter -&gt; Map -&gt; Map -&gt; Map&#010;-&gt; Sink: Unnamed&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; 但是在上线一段时间后，开始报错，日志中有说到无法完成checkpoint，还提到有kafka的网络和连接异常。但还有别的flink任务在相同的broker上进行数据的读写，并且没有报错。我们暂时定位在，有可能每个checkpoint的完成时间比较长，需要几百毫秒，我们设的时间间隔又比较短，只有一秒，可能是这部分影响到了任务的性能。但是这只是一个不太靠谱的猜想，现在也没有什么排查的切入点，想看看大家有没有一些看法或者建议意见，非常感谢。&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 部分报错信息如下：&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-06-10 12:02:49,083 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     -&#010;&gt;&gt;&gt;&gt; Triggering&#010;&gt;&gt;&gt;&gt;&gt;&gt; checkpoint 1 @ 1591761769060 for job&#010;&gt;&gt; c41f4811262db1c4c270b136571c8201.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-06-10 12:04:47,898 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     -&#010;&gt;&gt;&gt; Decline&#010;&gt;&gt;&gt;&gt;&gt;&gt; checkpoint 1 by task 0cb03590fdf18027206ef628b3ef5863 of job&#010;&gt;&gt;&gt;&gt;&gt;&gt; c41f4811262db1c4c270b136571c8201 at&#010;&gt;&gt;&gt;&gt;&gt;&gt; container_e27_1591466310139_21670_01_000006 @&#010;&gt;&gt;&gt;&gt;&gt;&gt; hdp1-hadoop-datanode-4.novalocal (dataPort=44778).&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-06-10 12:04:47,899 INFO&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     -&#010;&gt;&gt;&gt;&gt; Discarding&#010;&gt;&gt;&gt;&gt;&gt;&gt; checkpoint 1 of job c41f4811262db1c4c270b136571c8201.&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointException: Could not&#010;&gt;&gt;&gt;&gt;&gt;&gt; complete snapshot 1 for operator Source: Custom Source -&gt; Map&#010;-&gt;&#010;&gt;&gt;&gt;&gt; Source_Map&#010;&gt;&gt;&gt;&gt;&gt;&gt; -&gt; Empty_Filer -&gt; Field_Filter -&gt; Type_Filter -&gt; Value_Filter&#010;-&gt; Map&#010;&gt;&gt;&gt; -&gt;&#010;&gt;&gt;&gt;&gt; Map&#010;&gt;&gt;&gt;&gt;&gt;&gt; -&gt; Map -&gt; Sink: Unnamed (7/12). Failure reason: Checkpoint&#010;was&#010;&gt;&gt;&gt; declined.&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:434)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1420)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1354)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:991)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:887)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:860)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:793)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$3(StreamTask.java:777)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:261)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&gt;&gt;&gt;&gt;&gt; Caused by:&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#010;&gt;&gt;&gt; Failed&#010;&gt;&gt;&gt;&gt; to&#010;&gt;&gt;&gt;&gt;&gt;&gt; send data to Kafka: The server disconnected before a response was&#010;&gt;&gt;&gt;&gt; received.&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1218)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:973)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.preCommit(FlinkKafkaProducer.java:892)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.preCommit(FlinkKafkaProducer.java:98)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.snapshotState(TwoPhaseCommitSinkFunction.java:317)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.snapshotState(FlinkKafkaProducer.java:978)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:90)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotStat(AbstractStreamOperator.java:402)&#010;&gt;&gt;&gt;&gt;&gt;&gt; ... 18 more&#010;&gt;&gt;&gt;&gt;&gt;&gt; Caused by: org.apache.kafka.common.errors.NetworkException: The&#010;&gt;&gt; server&#010;&gt;&gt;&gt;&gt;&gt;&gt; disconnected before a response was received.&#010;&gt;&gt;&gt;&gt;&gt;&gt; 2020-06-10 12:04:47,913 INFO&#010;&gt;&gt;&gt;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;&gt;&gt;&gt;&gt;&gt;                - Trying to recover from a global failure.&#010;&gt;&gt;&gt;&gt;&gt;&gt; org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint&#010;&gt;&gt;&gt;&gt; tolerable&#010;&gt;&gt;&gt;&gt;&gt;&gt; failure threshold.&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:87)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.failPendingCheckpointDueToTaskFailure(CheckpointCoordinator.java:1467)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.discardCheckpoint(CheckpointCoordinator.java:1377)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:719)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.lambda$declineCheckpoint$5(SchedulerBase.java:807)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#010;&gt;&gt;&gt;&gt;&gt;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 期望收到各位的回复和帮助。&#010;&gt;&gt;&gt;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt;&gt;&gt;&gt; Zhefu&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "2",
        "reply": "<CA+QRB_+2hHip3B-W3iPA4ngX-ZhOkndbD00+SOC9wbQJqN=TKw@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvsD=CsDfLAb02rWUHY72Fqs5BkZSfFp7cT7b7=xbzzhKA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:01:44 GMT",
        "subject": "Re: flink任务checkpoint无法完成snapshot，且报kafka异常",
        "content": "Hi Zhefu&#010;&#010;感谢你在邮件列表分享你的解决方法，这样其他人遇到类似问题也有一个参考。&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Zhefu PENG &lt;pengzf0802@gmail.com&gt; 于2020年7月13日周一 下午7:51写道：&#010;&#010;&gt; Hi all,&#010;&gt;&#010;&gt; 这封邮件最开始发出已经一个月了，这一个月里尝试了很多朋友或者各位大佬的建议，目前经过一周末加上两个工作日的查看，问题看来是解决了。&#010;&gt;&#010;&gt;&#010;&gt; 问题的根本原因：Kafka集群的性能不足（怀疑是CPU负荷过大）。问题出现的时候线上kakfa集群只有七台机器，在排除所有别的原因以及能进行到的尝试方案后，决定进行扩容。扩到15台机器。目前来看，平稳运行，没有再报出类似错误。&#010;&gt;&#010;&gt; 反馈一下，如果有朋友遇到类似的问题，可以参考，给这个问题做一个闭环。谢谢各位的关注和帮忙。&#010;&gt;&#010;&gt; Best,&#010;&gt; Zhefu&#010;&gt;&#010;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年6月12日周五 上午9:49写道：&#010;&gt;&#010;&gt; &gt; Hi ZheFu,&#010;&gt; &gt;&#010;&gt; &gt; 可以把你的 Flink 版本说一下，我大致理解是这样的，每次 sink&#010;端 在 snapshotState 的时候，会检查该次 Sink&#010;&gt; &gt; 的数据是否都已经 Sink 到了 kafka.&#010;&gt; &gt;&#010;&gt; &gt; 也就是说，你这次 Checkpoint 的时候，由于你的 Checkpoint 间隔较短，Kafka&#010;那边给回的消息记录 Ack&#010;&gt; &gt; 还没有弄完，所以有这个问题。建议 Checkpoint 间隔弄长点。&#010;&gt; &gt;&#010;&gt; &gt; 具体代码查看：FlinkKafkaProducerBase.snapshotState 这个方法。&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; LakeShen&#010;&gt; &gt;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年6月11日周四 上午9:50写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; Hi&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 从错误栈看是因为 task 端 snapshot 出问题了，原因是 “Caused&#010;by:&#010;&gt; &gt; &gt; java.lang.IllegalStateException: Pending record count must be zero at&#010;&gt; &gt; this&#010;&gt; &gt; &gt; point: 5”，需要看一下为什么会走到这里&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 李奇 &lt;359502980@qq.com&gt; 于2020年6月10日周三 下午5:57写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 哈喽，根据我自己遇到checkpoint失败，一般是因为你数据有问题，导致算子失败，有可能是数据格式，或者字段类型不匹配，字段数量等相关的原因造成，我看你补充的内容，好像是你kafka数据有问题样，你可以往这个方向看看数据是否正常。解析是否正确。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020年6月10日，下午1:24，Zhefu PENG &lt;pengzf0802@gmail.com&gt;&#010;写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; ﻿补充一下，在TaskManager发现了如下错误日志：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 2020-06-10 12:44:40,688 ERROR&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.streaming.runtime.tasks.StreamTask           -&#010;&gt; Error&#010;&gt; &gt; &gt; &gt; &gt; during disposal of stream operator.&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#010;&gt; &gt; Failed&#010;&gt; &gt; &gt; &gt; to&#010;&gt; &gt; &gt; &gt; &gt; send data to Kafka: Pending record count must be zero at this&#010;&gt; point:&#010;&gt; &gt; 5&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1218)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:861)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:43)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:668)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:579)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:481)&#010;&gt; &gt; &gt; &gt; &gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &gt; &gt; &gt; &gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &gt; &gt; &gt; &gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt; &gt; &gt; &gt; Caused by: java.lang.IllegalStateException: Pending record count&#010;&gt; must&#010;&gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; zero at this point: 5&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:969)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:834)&#010;&gt; &gt; &gt; &gt; &gt; ... 8 more&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 希望得到帮助，感谢！&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Zhefu PENG &lt;pengzf0802@gmail.com&gt; 于2020年6月10日周三&#010;下午1:03写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Hi all,&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 现在有一个简单的flink任务，大概chain在一起后的执行图为：&#010;&gt; &gt; &gt; &gt; &gt;&gt; Source: Custom Source -&gt; Map -&gt; Source_Map -&gt; Empty_Filer&#010;-&gt;&#010;&gt; &gt; &gt; &gt; Field_Filter&#010;&gt; &gt; &gt; &gt; &gt;&gt; -&gt; Type_Filter -&gt; Value_Filter -&gt; Map -&gt; Map -&gt;&#010;Map -&gt; Sink:&#010;&gt; Unnamed&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 但是在上线一段时间后，开始报错，日志中有说到无法完成checkpoint，还提到有kafka的网络和连接异常。但还有别的flink任务在相同的broker上进行数据的读写，并且没有报错。我们暂时定位在，有可能每个checkpoint的完成时间比较长，需要几百毫秒，我们设的时间间隔又比较短，只有一秒，可能是这部分影响到了任务的性能。但是这只是一个不太靠谱的猜想，现在也没有什么排查的切入点，想看看大家有没有一些看法或者建议意见，非常感谢。&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 部分报错信息如下：&#010;&gt; &gt; &gt; &gt; &gt;&gt; 2020-06-10 12:02:49,083 INFO&#010;&gt; &gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator   &#010; -&#010;&gt; &gt; &gt; &gt; Triggering&#010;&gt; &gt; &gt; &gt; &gt;&gt; checkpoint 1 @ 1591761769060 for job&#010;&gt; &gt; c41f4811262db1c4c270b136571c8201.&#010;&gt; &gt; &gt; &gt; &gt;&gt; 2020-06-10 12:04:47,898 INFO&#010;&gt; &gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator   &#010; -&#010;&gt; &gt; &gt; Decline&#010;&gt; &gt; &gt; &gt; &gt;&gt; checkpoint 1 by task 0cb03590fdf18027206ef628b3ef5863 of job&#010;&gt; &gt; &gt; &gt; &gt;&gt; c41f4811262db1c4c270b136571c8201 at&#010;&gt; &gt; &gt; &gt; &gt;&gt; container_e27_1591466310139_21670_01_000006 @&#010;&gt; &gt; &gt; &gt; &gt;&gt; hdp1-hadoop-datanode-4.novalocal (dataPort=44778).&#010;&gt; &gt; &gt; &gt; &gt;&gt; 2020-06-10 12:04:47,899 INFO&#010;&gt; &gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator   &#010; -&#010;&gt; &gt; &gt; &gt; Discarding&#010;&gt; &gt; &gt; &gt; &gt;&gt; checkpoint 1 of job c41f4811262db1c4c270b136571c8201.&#010;&gt; &gt; &gt; &gt; &gt;&gt; org.apache.flink.runtime.checkpoint.CheckpointException: Could&#010;not&#010;&gt; &gt; &gt; &gt; &gt;&gt; complete snapshot 1 for operator Source: Custom Source -&gt;&#010;Map -&gt;&#010;&gt; &gt; &gt; &gt; Source_Map&#010;&gt; &gt; &gt; &gt; &gt;&gt; -&gt; Empty_Filer -&gt; Field_Filter -&gt; Type_Filter -&gt;&#010;Value_Filter -&gt;&#010;&gt; Map&#010;&gt; &gt; &gt; -&gt;&#010;&gt; &gt; &gt; &gt; Map&#010;&gt; &gt; &gt; &gt; &gt;&gt; -&gt; Map -&gt; Sink: Unnamed (7/12). Failure reason: Checkpoint&#010;was&#010;&gt; &gt; &gt; declined.&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:434)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1420)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1354)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:991)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:887)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:860)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpoint(StreamTask.java:793)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$triggerCheckpointAsync$3(StreamTask.java:777)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:261)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt; &gt; &gt; &gt;&gt; Caused by:&#010;&gt; &gt; &gt; &gt; &gt;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaException:&#010;&gt; &gt; &gt; Failed&#010;&gt; &gt; &gt; &gt; to&#010;&gt; &gt; &gt; &gt; &gt;&gt; send data to Kafka: The server disconnected before a response&#010;was&#010;&gt; &gt; &gt; &gt; received.&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1218)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:973)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.preCommit(FlinkKafkaProducer.java:892)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.preCommit(FlinkKafkaProducer.java:98)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.snapshotState(TwoPhaseCommitSinkFunction.java:317)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.snapshotState(FlinkKafkaProducer.java:978)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:90)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotStat(AbstractStreamOperator.java:402)&#010;&gt; &gt; &gt; &gt; &gt;&gt; ... 18 more&#010;&gt; &gt; &gt; &gt; &gt;&gt; Caused by: org.apache.kafka.common.errors.NetworkException: The&#010;&gt; &gt; server&#010;&gt; &gt; &gt; &gt; &gt;&gt; disconnected before a response was received.&#010;&gt; &gt; &gt; &gt; &gt;&gt; 2020-06-10 12:04:47,913 INFO&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt; &gt; &gt; &gt;&gt;                 - Trying to recover from a global failure.&#010;&gt; &gt; &gt; &gt; &gt;&gt; org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint&#010;&gt; &gt; &gt; &gt; tolerable&#010;&gt; &gt; &gt; &gt; &gt;&gt; failure threshold.&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleTaskLevelCheckpointException(CheckpointFailureManager.java:87)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.failPendingCheckpointDueToTaskFailure(CheckpointCoordinator.java:1467)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.discardCheckpoint(CheckpointCoordinator.java:1377)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:719)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.lambda$declineCheckpoint$5(SchedulerBase.java:807)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#010;&gt; &gt; &gt; &gt; &gt;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 期望收到各位的回复和帮助。&#010;&gt; &gt; &gt; &gt; &gt;&gt; Best,&#010;&gt; &gt; &gt; &gt; &gt;&gt; Zhefu&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CA+QRB_+2hHip3B-W3iPA4ngX-ZhOkndbD00+SOC9wbQJqN=TKw@mail.gmail.com>"
    },
    {
        "id": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>",
        "from": "Yvette zhai &lt;yvettez...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:59:34 GMT",
        "subject": "flink 1.11 on k8s native session cluster模式报找不到configmap",
        "content": "大佬们，请教个问题，我是在k8s上部署flink1.11 natvie session模式。&#013;&#010;下载的flink-1.11.0-bin-scala_2.11.tgz&#013;&#010;&#013;&#010;执行命令是&#013;&#010;./bin/kubernetes-session.sh \\&#013;&#010;    -Dkubernetes.cluster-id=k8s-session-1 \\&#013;&#010;    -Dtaskmanager.memory.process.size=4096m \\&#013;&#010;    -Dkubernetes.taskmanager.cpu=2 \\&#013;&#010;    -Dtaskmanager.numberOfTaskSlots=4 \\&#013;&#010;    -Dresourcemanager.taskmanager-timeout=3600000 \\&#013;&#010;    -Dkubernetes.container.image=flink:1.11.0-scala_2.11&#013;&#010;&#013;&#010;但是会报错，找不到configmap&#013;&#010;[image: image.png]&#013;&#010;&#013;&#010;我看是执行上述命令是会生成configmap的，为什么还会报找不到。&#013;&#010;",
        "depth": "0",
        "reply": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>"
    },
    {
        "id": "<BD39FE7E-29B0-43C4-9C53-C1FF19D9E464@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:03:10 GMT",
        "subject": "Re: flink 1.11 on k8s native session cluster模式报找不到configmap",
        "content": "Hi, zhai&#010;&#010;图挂了。。可以整个图床工具贴出来，如果是异常直接贴文本也可以的。&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月13日，19:59，Yvette zhai &lt;yvettezhai@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; 大佬们，请教个问题，我是在k8s上部署flink1.11 natvie session模式。&#010;&gt; 下载的flink-1.11.0-bin-scala_2.11.tgz&#010;&gt; &#010;&gt; 执行命令是&#010;&gt; ./bin/kubernetes-session.sh \\&#010;&gt;     -Dkubernetes.cluster-id=k8s-session-1 \\&#010;&gt;     -Dtaskmanager.memory.process.size=4096m \\&#010;&gt;     -Dkubernetes.taskmanager.cpu=2 \\&#010;&gt;     -Dtaskmanager.numberOfTaskSlots=4 \\&#010;&gt;     -Dresourcemanager.taskmanager-timeout=3600000 \\&#010;&gt;     -Dkubernetes.container.image=flink:1.11.0-scala_2.11&#010;&gt; &#010;&gt; 但是会报错，找不到configmap&#010;&gt; &#010;&gt; &#010;&gt; 我看是执行上述命令是会生成configmap的，为什么还会报找不到。&#010;&gt; &#010;&#010;&#010;",
        "depth": "1",
        "reply": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>"
    },
    {
        "id": "<CACpxqXZsY0+msJ9ZndW9nMjUG2-7JLQUpY8vY8NHtRieuCsNsA@mail.gmail.com>",
        "from": "Yvette zhai &lt;yvettez...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:11:48 GMT",
        "subject": "Re: flink 1.11 on k8s native session cluster模式报找不到configmap",
        "content": "报错是MountVolume.SetUp failed for volume \"flink-config-volume\" : configmap&#013;&#010;\"flink-config-k8s-session-1\" not found&#013;&#010;&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:03写道：&#013;&#010;&#013;&#010;&gt; Hi, zhai&#013;&#010;&gt;&#013;&#010;&gt; 图挂了。。可以整个图床工具贴出来，如果是异常直接贴文本也可以的。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月13日，19:59，Yvette zhai &lt;yvettezhai@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 大佬们，请教个问题，我是在k8s上部署flink1.11 natvie session模式。&#013;&#010;&gt; &gt; 下载的flink-1.11.0-bin-scala_2.11.tgz&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 执行命令是&#013;&#010;&gt; &gt; ./bin/kubernetes-session.sh \\&#013;&#010;&gt; &gt;     -Dkubernetes.cluster-id=k8s-session-1 \\&#013;&#010;&gt; &gt;     -Dtaskmanager.memory.process.size=4096m \\&#013;&#010;&gt; &gt;     -Dkubernetes.taskmanager.cpu=2 \\&#013;&#010;&gt; &gt;     -Dtaskmanager.numberOfTaskSlots=4 \\&#013;&#010;&gt; &gt;     -Dresourcemanager.taskmanager-timeout=3600000 \\&#013;&#010;&gt; &gt;     -Dkubernetes.container.image=flink:1.11.0-scala_2.11&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 但是会报错，找不到configmap&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我看是执行上述命令是会生成configmap的，为什么还会报找不到。&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>"
    },
    {
        "id": "<36EC1E58-77B5-4501-9E0E-BD83A819AC94@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:40:45 GMT",
        "subject": "Re: flink 1.11 on k8s native session cluster模式报找不到configmap",
        "content": "Hi, zhai&#010;&#010;可以贴详细点吗？我帮你 CC 了熟悉这块的大佬 Yun Gao&#010;&#010;祝好&#010;&#010;&gt; 在 2020年7月13日，20:11，Yvette zhai &lt;yvettezhai@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; 报错是MountVolume.SetUp failed for volume \"flink-config-volume\" : configmap&#010;&gt; \"flink-config-k8s-session-1\" not found&#010;&gt; &#010;&gt; &#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:03写道：&#010;&gt; &#010;&gt;&gt; Hi, zhai&#010;&gt;&gt; &#010;&gt;&gt; 图挂了。。可以整个图床工具贴出来，如果是异常直接贴文本也可以的。&#010;&gt;&gt; &#010;&gt;&gt; Best,&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月13日，19:59，Yvette zhai &lt;yvettezhai@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 大佬们，请教个问题，我是在k8s上部署flink1.11 natvie session模式。&#010;&gt;&gt;&gt; 下载的flink-1.11.0-bin-scala_2.11.tgz&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 执行命令是&#010;&gt;&gt;&gt; ./bin/kubernetes-session.sh \\&#010;&gt;&gt;&gt;    -Dkubernetes.cluster-id=k8s-session-1 \\&#010;&gt;&gt;&gt;    -Dtaskmanager.memory.process.size=4096m \\&#010;&gt;&gt;&gt;    -Dkubernetes.taskmanager.cpu=2 \\&#010;&gt;&gt;&gt;    -Dtaskmanager.numberOfTaskSlots=4 \\&#010;&gt;&gt;&gt;    -Dresourcemanager.taskmanager-timeout=3600000 \\&#010;&gt;&gt;&gt;    -Dkubernetes.container.image=flink:1.11.0-scala_2.11&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 但是会报错，找不到configmap&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 我看是执行上述命令是会生成configmap的，为什么还会报找不到。&#010;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "3",
        "reply": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>"
    },
    {
        "id": "<CACpxqXY7_UV9GKL85+EAPdFTKGUybmrNgWkAKV0LEFhCuOPNgA@mail.gmail.com>",
        "from": "Yvette zhai &lt;yvettez...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:10:03 GMT",
        "subject": "Re: flink 1.11 on k8s native session cluster模式报找不到configmap",
        "content": "1. 执行的脚本，产生的日志是：&#010;2020-07-13 21:00:25,248 INFO&#010; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.rpc.address, localhost&#010;2020-07-13 21:00:25,251 INFO&#010; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.rpc.port, 6123&#010;2020-07-13 21:00:25,251 INFO&#010; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.memory.process.size, 1600m&#010;2020-07-13 21:00:25,251 INFO&#010; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: taskmanager.memory.process.size, 1728m&#010;2020-07-13 21:00:25,251 INFO&#010; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: taskmanager.numberOfTaskSlots, 1&#010;2020-07-13 21:00:25,251 INFO&#010; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: parallelism.default, 1&#010;2020-07-13 21:00:25,252 INFO&#010; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.execution.failover-strategy, region&#010;2020-07-13 21:00:25,344 INFO&#010; org.apache.flink.client.deployment.DefaultClusterClientServiceLoader [] -&#010;Could not load factory due to missing dependencies.&#010;2020-07-13 21:00:26,136 INFO&#010; org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is&#010;less than its min value 192.000mb (201326592 bytes), min value will be used&#010;instead&#010;2020-07-13 21:00:26,154 INFO&#010; org.apache.flink.kubernetes.utils.KubernetesUtils            [] -&#010;Kubernetes deployment requires a fixed port. Configuration blob.server.port&#010;will be set to 6124&#010;2020-07-13 21:00:26,154 INFO&#010; org.apache.flink.kubernetes.utils.KubernetesUtils            [] -&#010;Kubernetes deployment requires a fixed port. Configuration&#010;taskmanager.rpc.port will be set to 6122&#010;2020-07-13 21:00:26,204 INFO&#010; org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is&#010;less than its min value 192.000mb (201326592 bytes), min value will be used&#010;instead&#010;2020-07-13 21:00:26,220 WARN&#010; org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator&#010;[] - Found 0 files in directory null/etc/hadoop, skip to mount the Hadoop&#010;Configuration ConfigMap.&#010;2020-07-13 21:00:26,220 WARN&#010; org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator&#010;[] - Found 0 files in directory null/etc/hadoop, skip to create the Hadoop&#010;Configuration ConfigMap.&#010;2020-07-13 21:00:26,958 INFO&#010; org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Create&#010;flink session cluster k8s-session-1 successfully, JobManager Web Interface:&#010;http://172.16.5.175:8081&#010;&#010;2. 查看 desrcibe 日志是：&#010;MountVolume.SetUp failed for volume \"flink-config-volume\" : configmap&#010;\"flink-config-k8s-session-1\" not found&#010;&#010;3. logs 日志是：&#010;&#010;Start command : /bin/bash -c $JAVA_HOME/bin/java -classpath&#010;$FLINK_CLASSPATH -Xmx1073741824 -Xms1073741824&#010;-XX:MaxMetaspaceSize=268435456 -Dlog.file=/opt/flink/log/jobmanager.log&#010;-Dlogback.configurationFile=file:/opt/flink/conf/logback.xml&#010;-Dlog4j.configurationFile=file:/opt/flink/conf/log4j.properties&#010;org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint&#010;1&gt; /opt/flink/log/jobmanager.out 2&gt; /opt/flink/log/jobmanager.err&#010;&#010;4. kubectl get cm 可以看到&#010;NAME                         DATA   AGE&#010;flink-config-k8s-session-1   3      5m45s&#010;&#010;麻烦大佬帮忙看看~是不是我的语句有问题还是缺什么文件~&#010;我是直接官网下的包，没有改任何文件~&#010;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:41写道：&#010;&#010;&gt; Hi, zhai&#010;&gt;&#010;&gt; 可以贴详细点吗？我帮你 CC 了熟悉这块的大佬 Yun Gao&#010;&gt;&#010;&gt; 祝好&#010;&gt;&#010;&gt; &gt; 在 2020年7月13日，20:11，Yvette zhai &lt;yvettezhai@gmail.com&gt; 写道：&#010;&gt; &gt;&#010;&gt; &gt; 报错是MountVolume.SetUp failed for volume \"flink-config-volume\" : configmap&#010;&gt; &gt; \"flink-config-k8s-session-1\" not found&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:03写道：&#010;&gt; &gt;&#010;&gt; &gt;&gt; Hi, zhai&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 图挂了。。可以整个图床工具贴出来，如果是异常直接贴文本也可以的。&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Best,&#010;&gt; &gt;&gt; Leonard Xu&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&gt; 在 2020年7月13日，19:59，Yvette zhai &lt;yvettezhai@gmail.com&gt;&#010;写道：&#010;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt;&gt; 大佬们，请教个问题，我是在k8s上部署flink1.11 natvie session模式。&#010;&gt; &gt;&gt;&gt; 下载的flink-1.11.0-bin-scala_2.11.tgz&#010;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt;&gt; 执行命令是&#010;&gt; &gt;&gt;&gt; ./bin/kubernetes-session.sh \\&#010;&gt; &gt;&gt;&gt;    -Dkubernetes.cluster-id=k8s-session-1 \\&#010;&gt; &gt;&gt;&gt;    -Dtaskmanager.memory.process.size=4096m \\&#010;&gt; &gt;&gt;&gt;    -Dkubernetes.taskmanager.cpu=2 \\&#010;&gt; &gt;&gt;&gt;    -Dtaskmanager.numberOfTaskSlots=4 \\&#010;&gt; &gt;&gt;&gt;    -Dresourcemanager.taskmanager-timeout=3600000 \\&#010;&gt; &gt;&gt;&gt;    -Dkubernetes.container.image=flink:1.11.0-scala_2.11&#010;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt;&gt; 但是会报错，找不到configmap&#010;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt;&gt; 我看是执行上述命令是会生成configmap的，为什么还会报找不到。&#010;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>"
    },
    {
        "id": "<CACpxqXbYcn73HPgMzqF15i-Fry=K+4XRhUb5czG_N_5fyGdEtQ@mail.gmail.com>",
        "from": "Yvette zhai &lt;yvettez...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:20:07 GMT",
        "subject": "Re: flink 1.11 on k8s native session cluster模式报找不到configmap",
        "content": "补充一下，kubernetes版本是1.18&#010;Yvette zhai &lt;yvettezhai@gmail.com&gt; 于2020年7月13日周一 下午9:10写道：&#010;&#010;&gt; 1. 执行的脚本，产生的日志是：&#010;&gt; 2020-07-13 21:00:25,248 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; 2020-07-13 21:00:25,251 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; 2020-07-13 21:00:25,251 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt; 2020-07-13 21:00:25,251 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; 2020-07-13 21:00:25,251 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; 2020-07-13 21:00:25,251 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: parallelism.default, 1&#010;&gt; 2020-07-13 21:00:25,252 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; 2020-07-13 21:00:25,344 INFO&#010;&gt;  org.apache.flink.client.deployment.DefaultClusterClientServiceLoader [] -&#010;&gt; Could not load factory due to missing dependencies.&#010;&gt; 2020-07-13 21:00:26,136 INFO&#010;&gt;  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;&gt; derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is&#010;&gt; less than its min value 192.000mb (201326592 bytes), min value will be used&#010;&gt; instead&#010;&gt; 2020-07-13 21:00:26,154 INFO&#010;&gt;  org.apache.flink.kubernetes.utils.KubernetesUtils            [] -&#010;&gt; Kubernetes deployment requires a fixed port. Configuration blob.server.port&#010;&gt; will be set to 6124&#010;&gt; 2020-07-13 21:00:26,154 INFO&#010;&gt;  org.apache.flink.kubernetes.utils.KubernetesUtils            [] -&#010;&gt; Kubernetes deployment requires a fixed port. Configuration&#010;&gt; taskmanager.rpc.port will be set to 6122&#010;&gt; 2020-07-13 21:00:26,204 INFO&#010;&gt;  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;&gt; derived from fraction jvm overhead memory (160.000mb (167772162 bytes)) is&#010;&gt; less than its min value 192.000mb (201326592 bytes), min value will be used&#010;&gt; instead&#010;&gt; 2020-07-13 21:00:26,220 WARN&#010;&gt;  org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator&#010;&gt; [] - Found 0 files in directory null/etc/hadoop, skip to mount the Hadoop&#010;&gt; Configuration ConfigMap.&#010;&gt; 2020-07-13 21:00:26,220 WARN&#010;&gt;  org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator&#010;&gt; [] - Found 0 files in directory null/etc/hadoop, skip to create the Hadoop&#010;&gt; Configuration ConfigMap.&#010;&gt; 2020-07-13 21:00:26,958 INFO&#010;&gt;  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Create&#010;&gt; flink session cluster k8s-session-1 successfully, JobManager Web Interface:&#010;&gt; http://172.16.5.175:8081&#010;&gt;&#010;&gt; 2. 查看 desrcibe 日志是：&#010;&gt; MountVolume.SetUp failed for volume \"flink-config-volume\" : configmap&#010;&gt; \"flink-config-k8s-session-1\" not found&#010;&gt;&#010;&gt; 3. logs 日志是：&#010;&gt;&#010;&gt; Start command : /bin/bash -c $JAVA_HOME/bin/java -classpath&#010;&gt; $FLINK_CLASSPATH -Xmx1073741824 -Xms1073741824&#010;&gt; -XX:MaxMetaspaceSize=268435456 -Dlog.file=/opt/flink/log/jobmanager.log&#010;&gt; -Dlogback.configurationFile=file:/opt/flink/conf/logback.xml&#010;&gt; -Dlog4j.configurationFile=file:/opt/flink/conf/log4j.properties&#010;&gt; org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint&#010;&gt; 1&gt; /opt/flink/log/jobmanager.out 2&gt; /opt/flink/log/jobmanager.err&#010;&gt;&#010;&gt; 4. kubectl get cm 可以看到&#010;&gt; NAME                         DATA   AGE&#010;&gt; flink-config-k8s-session-1   3      5m45s&#010;&gt;&#010;&gt; 麻烦大佬帮忙看看~是不是我的语句有问题还是缺什么文件~&#010;&gt; 我是直接官网下的包，没有改任何文件~&#010;&gt;&#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:41写道：&#010;&gt;&#010;&gt;&gt; Hi, zhai&#010;&gt;&gt;&#010;&gt;&gt; 可以贴详细点吗？我帮你 CC 了熟悉这块的大佬 Yun Gao&#010;&gt;&gt;&#010;&gt;&gt; 祝好&#010;&gt;&gt;&#010;&gt;&gt; &gt; 在 2020年7月13日，20:11，Yvette zhai &lt;yvettezhai@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 报错是MountVolume.SetUp failed for volume \"flink-config-volume\" : configmap&#010;&gt;&gt; &gt; \"flink-config-k8s-session-1\" not found&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:03写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; Hi, zhai&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 图挂了。。可以整个图床工具贴出来，如果是异常直接贴文本也可以的。&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Best,&#010;&gt;&gt; &gt;&gt; Leonard Xu&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; 在 2020年7月13日，19:59，Yvette zhai &lt;yvettezhai@gmail.com&gt;&#010;写道：&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; 大佬们，请教个问题，我是在k8s上部署flink1.11 natvie&#010;session模式。&#010;&gt;&gt; &gt;&gt;&gt; 下载的flink-1.11.0-bin-scala_2.11.tgz&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; 执行命令是&#010;&gt;&gt; &gt;&gt;&gt; ./bin/kubernetes-session.sh \\&#010;&gt;&gt; &gt;&gt;&gt;    -Dkubernetes.cluster-id=k8s-session-1 \\&#010;&gt;&gt; &gt;&gt;&gt;    -Dtaskmanager.memory.process.size=4096m \\&#010;&gt;&gt; &gt;&gt;&gt;    -Dkubernetes.taskmanager.cpu=2 \\&#010;&gt;&gt; &gt;&gt;&gt;    -Dtaskmanager.numberOfTaskSlots=4 \\&#010;&gt;&gt; &gt;&gt;&gt;    -Dresourcemanager.taskmanager-timeout=3600000 \\&#010;&gt;&gt; &gt;&gt;&gt;    -Dkubernetes.container.image=flink:1.11.0-scala_2.11&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; 但是会报错，找不到configmap&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; 我看是执行上述命令是会生成configmap的，为什么还会报找不到。&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>"
    },
    {
        "id": "<CAP+gf36W5ZEpcUvy4S9XQqHEYuonxPZJjCen1JUw5eJH7aGWpw@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 10:47:58 GMT",
        "subject": "Re: flink 1.11 on k8s native session cluster模式报找不到configmap",
        "content": "configmap \"flink-config-k8s-session-1\" not found的报错是正常的&#010;&#010;因为目前的实现是先创建JobManager Deployment，然后再创建ConfigMap并设置owner&#010;reference到deployment&#010;所以你才会看到创建Pod的时候报ConfigMap还没有创建出来，这个是正常的信息，K8s会自动重试创建Pod&#010;&#010;你现在是任务起不来吗，还是有什么其他的问题？&#010;&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;Yvette zhai &lt;yvettezhai@gmail.com&gt; 于2020年7月14日周二 上午10:20写道：&#010;&#010;&gt; 补充一下，kubernetes版本是1.18&#010;&gt; Yvette zhai &lt;yvettezhai@gmail.com&gt; 于2020年7月13日周一 下午9:10写道：&#010;&gt;&#010;&gt; &gt; 1. 执行的脚本，产生的日志是：&#010;&gt; &gt; 2020-07-13 21:00:25,248 INFO&#010;&gt; &gt;  org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; &gt; 2020-07-13 21:00:25,251 INFO&#010;&gt; &gt;  org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; &gt; 2020-07-13 21:00:25,251 INFO&#010;&gt; &gt;  org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt; configuration property: jobmanager.memory.process.size, 1600m&#010;&gt; &gt; 2020-07-13 21:00:25,251 INFO&#010;&gt; &gt;  org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; &gt; 2020-07-13 21:00:25,251 INFO&#010;&gt; &gt;  org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; &gt; 2020-07-13 21:00:25,251 INFO&#010;&gt; &gt;  org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt; configuration property: parallelism.default, 1&#010;&gt; &gt; 2020-07-13 21:00:25,252 INFO&#010;&gt; &gt;  org.apache.flink.configuration.GlobalConfiguration           [] -&#010;&gt; Loading&#010;&gt; &gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; &gt; 2020-07-13 21:00:25,344 INFO&#010;&gt; &gt;  org.apache.flink.client.deployment.DefaultClusterClientServiceLoader []&#010;&gt; -&#010;&gt; &gt; Could not load factory due to missing dependencies.&#010;&gt; &gt; 2020-07-13 21:00:26,136 INFO&#010;&gt; &gt;  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;&gt; &gt; derived from fraction jvm overhead memory (160.000mb (167772162 bytes))&#010;&gt; is&#010;&gt; &gt; less than its min value 192.000mb (201326592 bytes), min value will be&#010;&gt; used&#010;&gt; &gt; instead&#010;&gt; &gt; 2020-07-13 21:00:26,154 INFO&#010;&gt; &gt;  org.apache.flink.kubernetes.utils.KubernetesUtils            [] -&#010;&gt; &gt; Kubernetes deployment requires a fixed port. Configuration&#010;&gt; blob.server.port&#010;&gt; &gt; will be set to 6124&#010;&gt; &gt; 2020-07-13 21:00:26,154 INFO&#010;&gt; &gt;  org.apache.flink.kubernetes.utils.KubernetesUtils            [] -&#010;&gt; &gt; Kubernetes deployment requires a fixed port. Configuration&#010;&gt; &gt; taskmanager.rpc.port will be set to 6122&#010;&gt; &gt; 2020-07-13 21:00:26,204 INFO&#010;&gt; &gt;  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;&gt; &gt; derived from fraction jvm overhead memory (160.000mb (167772162 bytes))&#010;&gt; is&#010;&gt; &gt; less than its min value 192.000mb (201326592 bytes), min value will be&#010;&gt; used&#010;&gt; &gt; instead&#010;&gt; &gt; 2020-07-13 21:00:26,220 WARN&#010;&gt; &gt;&#010;&gt; org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator&#010;&gt; &gt; [] - Found 0 files in directory null/etc/hadoop, skip to mount the Hadoop&#010;&gt; &gt; Configuration ConfigMap.&#010;&gt; &gt; 2020-07-13 21:00:26,220 WARN&#010;&gt; &gt;&#010;&gt; org.apache.flink.kubernetes.kubeclient.decorators.HadoopConfMountDecorator&#010;&gt; &gt; [] - Found 0 files in directory null/etc/hadoop, skip to create the&#010;&gt; Hadoop&#010;&gt; &gt; Configuration ConfigMap.&#010;&gt; &gt; 2020-07-13 21:00:26,958 INFO&#010;&gt; &gt;  org.apache.flink.kubernetes.KubernetesClusterDescriptor      [] - Create&#010;&gt; &gt; flink session cluster k8s-session-1 successfully, JobManager Web&#010;&gt; Interface:&#010;&gt; &gt; http://172.16.5.175:8081&#010;&gt; &gt;&#010;&gt; &gt; 2. 查看 desrcibe 日志是：&#010;&gt; &gt; MountVolume.SetUp failed for volume \"flink-config-volume\" : configmap&#010;&gt; &gt; \"flink-config-k8s-session-1\" not found&#010;&gt; &gt;&#010;&gt; &gt; 3. logs 日志是：&#010;&gt; &gt;&#010;&gt; &gt; Start command : /bin/bash -c $JAVA_HOME/bin/java -classpath&#010;&gt; &gt; $FLINK_CLASSPATH -Xmx1073741824 -Xms1073741824&#010;&gt; &gt; -XX:MaxMetaspaceSize=268435456 -Dlog.file=/opt/flink/log/jobmanager.log&#010;&gt; &gt; -Dlogback.configurationFile=file:/opt/flink/conf/logback.xml&#010;&gt; &gt; -Dlog4j.configurationFile=file:/opt/flink/conf/log4j.properties&#010;&gt; &gt; org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint&#010;&gt; &gt; 1&gt; /opt/flink/log/jobmanager.out 2&gt; /opt/flink/log/jobmanager.err&#010;&gt; &gt;&#010;&gt; &gt; 4. kubectl get cm 可以看到&#010;&gt; &gt; NAME                         DATA   AGE&#010;&gt; &gt; flink-config-k8s-session-1   3      5m45s&#010;&gt; &gt;&#010;&gt; &gt; 麻烦大佬帮忙看看~是不是我的语句有问题还是缺什么文件~&#010;&gt; &gt; 我是直接官网下的包，没有改任何文件~&#010;&gt; &gt;&#010;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:41写道：&#010;&gt; &gt;&#010;&gt; &gt;&gt; Hi, zhai&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 可以贴详细点吗？我帮你 CC 了熟悉这块的大佬 Yun Gao&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 祝好&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; 在 2020年7月13日，20:11，Yvette zhai &lt;yvettezhai@gmail.com&gt;&#010;写道：&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; 报错是MountVolume.SetUp failed for volume \"flink-config-volume\" :&#010;&gt; configmap&#010;&gt; &gt;&gt; &gt; \"flink-config-k8s-session-1\" not found&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月13日周一 下午8:03写道：&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; Hi, zhai&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 图挂了。。可以整个图床工具贴出来，如果是异常直接贴文本也可以的。&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Best,&#010;&gt; &gt;&gt; &gt;&gt; Leonard Xu&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 在 2020年7月13日，19:59，Yvette zhai &lt;yvettezhai@gmail.com&gt;&#010;写道：&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 大佬们，请教个问题，我是在k8s上部署flink1.11 natvie&#010;session模式。&#010;&gt; &gt;&gt; &gt;&gt;&gt; 下载的flink-1.11.0-bin-scala_2.11.tgz&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 执行命令是&#010;&gt; &gt;&gt; &gt;&gt;&gt; ./bin/kubernetes-session.sh \\&#010;&gt; &gt;&gt; &gt;&gt;&gt;    -Dkubernetes.cluster-id=k8s-session-1 \\&#010;&gt; &gt;&gt; &gt;&gt;&gt;    -Dtaskmanager.memory.process.size=4096m \\&#010;&gt; &gt;&gt; &gt;&gt;&gt;    -Dkubernetes.taskmanager.cpu=2 \\&#010;&gt; &gt;&gt; &gt;&gt;&gt;    -Dtaskmanager.numberOfTaskSlots=4 \\&#010;&gt; &gt;&gt; &gt;&gt;&gt;    -Dresourcemanager.taskmanager-timeout=3600000 \\&#010;&gt; &gt;&gt; &gt;&gt;&gt;    -Dkubernetes.container.image=flink:1.11.0-scala_2.11&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 但是会报错，找不到configmap&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 我看是执行上述命令是会生成configmap的，为什么还会报找不到。&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt;&#010;&#010;",
        "depth": "6",
        "reply": "<CACpxqXadN6CgSB77nqqBZU1+x4S2-A+8m=y2yujsppMB6OdywQ@mail.gmail.com>"
    },
    {
        "id": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:06:35 GMT",
        "subject": "flink 1.11运算结果存mysql出错",
        "content": "各位大佬好，请教一个问题flink从Kafka读数，写入mysql，程序没有报错，但是没有写入mysql任何数据。代码如下，是在linux下，直接python&#010;*.py执行的。完整代码如下&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;source=\"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'alarm_test_g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'scan.startup.mode' = 'earliest-offset', &#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:2181',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:9092',&#013;&#010;&amp;nbsp;'format' = 'json'&amp;nbsp; &#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;&#013;&#010;sink=\"\"\"&#013;&#010;CREATE TABLE g_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp; &#013;&#010;&amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = '123456t',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;t_env.execute_sql(source)&#013;&#010;t_env.execute_sql(sink)&#013;&#010;&#013;&#010;&#013;&#010;source = t_env.from_path(\"kafka_source_tab\")\\&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\"id,alarm_id,trck_id\")&#013;&#010;source.execute_insert(\"g_source_tab\")",
        "depth": "0",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<E92C3EAD-2498-484D-821E-5B70F96C2488@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:38:56 GMT",
        "subject": "Re: flink 1.11运算结果存mysql出错",
        "content": "Hi,&#010;&#010;简单看了下代码应该没啥问题，alarm_test_g 这个kafka topic里有数据吗？可以检查下是否有脏数据，直接用./bin/kafka-console-consumer.sh&#010;检查下？我有点怀疑这点&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月13日，20:06，小学生 &lt;201782053@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 各位大佬好，请教一个问题flink从Kafka读数，写入mysql，程序没有报错，但是没有写入mysql任何数据。代码如下，是在linux下，直接python&#010;*.py执行的。完整代码如下&#010;&gt; &#010;&gt; &#010;&gt; from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#010;&gt; from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#010;&gt; source=\"\"\"&#010;&gt; CREATE TABLE kafka_source_tab (&#010;&gt; &amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;trck_id VARCHAR&#010;&gt; &#010;&gt; &#010;&gt; ) WITH (&#010;&gt; &amp;nbsp;'connector' = 'kafka',&#010;&gt; &amp;nbsp;'topic' = 'alarm_test_g',&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;'scan.startup.mode' = 'earliest-offset', &#010;&gt; &amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:2181',&#010;&gt; &amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:9092',&#010;&gt; &amp;nbsp;'format' = 'json'&amp;nbsp; &#010;&gt; )&#010;&gt; \"\"\"&#010;&gt; &#010;&gt; sink=\"\"\"&#010;&gt; CREATE TABLE g_source_tab (&#010;&gt; &amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp; &#010;&gt; &amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;trck_id VARCHAR&#010;&gt; &#010;&gt; &#010;&gt; ) WITH (&#010;&gt; &amp;nbsp;'connector' = 'jdbc',&#010;&gt; &amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp; &#010;&gt; &amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;'username' = 'root',&#010;&gt; &amp;nbsp;'password' = '123456t',&#010;&gt; &amp;nbsp;'sink.buffer-flush.interval' = '1s'&#010;&gt; )&#010;&gt; \"\"\"&#010;&gt; env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;&gt; env.set_parallelism(1)&#010;&gt; env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#010;&gt; t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; t_env.execute_sql(source)&#010;&gt; t_env.execute_sql(sink)&#010;&gt; &#010;&gt; &#010;&gt; source = t_env.from_path(\"kafka_source_tab\")\\&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\"id,alarm_id,trck_id\")&#010;&gt; source.execute_insert(\"g_source_tab\")&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<tencent_66D09549BCB96BABAC84ACB53A3BD48CFD06@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:42:55 GMT",
        "subject": "Re: flink 1.11运算结果存mysql出错",
        "content": "检查过了，有topic数据的,这个在flink1.10版本代码下使用insert_into方式结果就是正常的，所以很奇怪。",
        "depth": "2",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<1f89d891.2ada.173483c9e35.Coremail.13122260573@163.com>",
        "from": "&quot;Zhonghan Tang&quot; &lt;13122260...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:53:19 GMT",
        "subject": "回复： flink 1.11运算结果存mysql出错",
        "content": "有新数据进来吗，看起来和这个jira很像&#010;https://issues.apache.org/jira/browse/FLINK-15262&#010;&#010;&#010;&#010;&#010;在2020年07月13日 20:38，Leonard Xu&lt;xbjtdcq@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;简单看了下代码应该没啥问题，alarm_test_g 这个kafka topic里有数据吗？可以检查下是否有脏数据，直接用./bin/kafka-console-consumer.sh&#010;检查下？我有点怀疑这点&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;在 2020年7月13日，20:06，小学生 &lt;201782053@qq.com&gt; 写道：&#010;&#010;各位大佬好，请教一个问题flink从Kafka读数，写入mysql，程序没有报错，但是没有写入mysql任何数据。代码如下，是在linux下，直接python&#010;*.py执行的。完整代码如下&#010;&#010;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#010;source=\"\"\"&#010;CREATE TABLE kafka_source_tab (&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;trck_id VARCHAR&#010;&#010;&#010;) WITH (&#010;&amp;nbsp;'connector' = 'kafka',&#010;&amp;nbsp;'topic' = 'alarm_test_g',&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;'scan.startup.mode' = 'earliest-offset',&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:2181',&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:9092',&#010;&amp;nbsp;'format' = 'json'&amp;nbsp;&#010;)&#010;\"\"\"&#010;&#010;sink=\"\"\"&#010;CREATE TABLE g_source_tab (&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;trck_id VARCHAR&#010;&#010;&#010;) WITH (&#010;&amp;nbsp;'connector' = 'jdbc',&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp;&#010;&amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;'username' = 'root',&#010;&amp;nbsp;'password' = '123456t',&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#010;)&#010;\"\"\"&#010;env = StreamExecutionEnvironment.get_execution_environment()&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;env.set_parallelism(1)&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#010;&#010;&#010;&#010;t_env.execute_sql(source)&#010;t_env.execute_sql(sink)&#010;&#010;&#010;source = t_env.from_path(\"kafka_source_tab\")\\&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .select(\"id,alarm_id,trck_id\")&#010;source.execute_insert(\"g_source_tab\")&#010;",
        "depth": "2",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<tencent_49E3055C8964D3644FD4DBA0F6EC29AE0009@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:57:28 GMT",
        "subject": "Re: flink 1.11运算结果存mysql出错",
        "content": "不像吧，这个是1.10版的，我执行这个程序很快就结束了，不会挂着。",
        "depth": "3",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<861D06CF-EAD5-4436-B63D-A0EA49781EC7@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:07:17 GMT",
        "subject": "Re: flink 1.11运算结果存mysql出错",
        "content": "Hi，知道了&#010;source.execute_insert(\"g_source_tab”) 返回的结果是一个TableResult对象，如果不显示地等待任务的执行，这个任务会直接返回，你试下这个&#010;&#010;result.execute_insert(\"g_source_tab\") \\&#010;    .get_job_client() \\&#010;    .get_job_execution_result() \\&#010;    .result()&#010;&#010;这是Flip-84引入的一个改动，为了更好地处理table程序的返回值。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月13日，20:57，小学生 &lt;201782053@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 不像吧，这个是1.10版的，我执行这个程序很快就结束了，不会挂着。&#010;&#010;&#010;",
        "depth": "4",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<tencent_1368FBDE2BA5F834D771D93FD9557517A807@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:12:46 GMT",
        "subject": "Re: flink 1.11运算结果存mysql出错",
        "content": "嗯嗯，尝试了，这下没问题了，想问下这个TableResult对象，设计的目的是啥呢，不是特别懂呢，谢谢！",
        "depth": "5",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<CADQYLGu7diPisVfiWp+nZdHkHWsx5KBuHm1OvgrF=Qo+P=TxTg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:36:11 GMT",
        "subject": "Re: flink 1.11运算结果存mysql出错",
        "content": "1.11 对 StreamTableEnvironment.execute()&#013;&#010;和 StreamExecutionEnvironment.execute() 的执行方式有所调整，&#013;&#010;简单概述为：&#013;&#010;1. StreamTableEnvironment.execute() 只能执行 sqlUpdate 和 insertInto 方法执行作业；&#013;&#010;2. Table 转化为 DataStream 后只能通过 StreamExecutionEnvironment.execute() 来执行作业；&#013;&#010;3. 新引入的 TableEnvironment.executeSql() 和 StatementSet.execute() 方法是直接执行sql作业&#013;&#010;(异步提交作业)，不需要再调用 StreamTableEnvironment.execute()&#013;&#010;或 StreamExecutionEnvironment.execute()&#013;&#010;&#013;&#010;TableEnvironment.executeSql() 和 StatementSet.execute()&#013;&#010;提交的作业都是异步的，如果是在本地测试的话，不会等有最终结果才会推出。针对这个问题，1.12里准备引入&#010;await 方法&#013;&#010;[3]，代码还在review中。&#013;&#010;&#013;&#010;TableResult是用来描述一个statement执行的结果。对于SELECT和INSERT，TableResult中还包含了JobClient&#013;&#010;[4]&#013;&#010;用来操作对应的job，例如获取job状态，cancel作业，等待作业结束等。TableResult还可以collect方法拿到statement执行的schema和结果数据，例如&#013;&#010;select/show的结果。&#013;&#010;&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C%E6%9F%A5%E8%AF%A2&#013;&#010;[2]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E5%B0%86%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90-datastream-%E6%88%96-dataset&#013;&#010;[3] https://issues.apache.org/jira/browse/FLINK-18337&#013;&#010;[4]&#013;&#010;https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;&#013;&#010;小学生 &lt;201782053@qq.com&gt; 于2020年7月13日周一 下午9:12写道：&#013;&#010;&#013;&#010;&gt; 嗯嗯，尝试了，这下没问题了，想问下这个TableResult对象，设计的目的是啥呢，不是特别懂呢，谢谢！&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<tencent_48F2F82DD732597C5969F51828147BD22B07@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:58:04 GMT",
        "subject": "Re: flink 1.11运算结果存mysql出错",
        "content": "懂了，谢谢",
        "depth": "7",
        "reply": "<tencent_DB22CACAC936AACB4275DBDB5D2DC5D3B80A@qq.com>"
    },
    {
        "id": "<52e6df6d.65c4.173482b475c.Coremail.yxx_cmhd@163.com>",
        "from": "叶贤勋 &lt;yxx_c...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:34:22 GMT",
        "subject": "使用Flink Array Field Type ",
        "content": "Flink 1.10.0&#010;问题描述：source表中有个test_array_string ARRAY&lt;VARCHAR&gt;字段，在DML语句用test_array_string[0]获取数组中的值会报数组越界异常。另外测试过Array&lt;Varchar&gt;也是相同错误，Array&lt;int&gt;,Array&lt;bigint&gt;等类型也会报数组越界问题。&#010;请问这是Flink1.10的bug吗？&#010;&#010;&#010;SQL:&#010;CREATETABLE source (&#010;……&#010;test_array_string ARRAY&lt;VARCHAR&gt;&#010;) WITH (&#010;'connector.type'='kafka',&#010;'update-mode'='append',&#010;'format.type'='json'&#010;  ……&#010;);&#010;&#010;&#010;CREATETABLE sink(&#010;v_string string&#010;) WITH (&#010;  ……&#010;);&#010;&#010;&#010;INSERTINTO&#010;sink&#010;SELECT&#010;test_array_string[0] as v_string&#010;from&#010;source;&#010;&#010;&#010;kafka样例数据：{\"id\":1,\"test_array_string\":[\"ff”]}&#010;&#010;&#010;Flink 执行的时候报以下错误：&#010;java.lang.ArrayIndexOutOfBoundsException: 33554432&#010;    at org.apache.flink.table.runtime.util.SegmentsUtil.getByteMultiSegments(SegmentsUtil.java:598)&#010;    at org.apache.flink.table.runtime.util.SegmentsUtil.getByte(SegmentsUtil.java:590)&#010;    at org.apache.flink.table.runtime.util.SegmentsUtil.bitGet(SegmentsUtil.java:534)&#010;    at org.apache.flink.table.dataformat.BinaryArray.isNullAt(BinaryArray.java:117)&#010;    at StreamExecCalc$9.processElement(UnknownSource)&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;    at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:730)&#010;    at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:708)&#010;    at SourceConversion$1.processElement(UnknownSource)&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:641)&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:616)&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:596)&#010;    at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:730)&#010;    at org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:708)&#010;    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollectWithTimestamp(StreamSourceContexts.java:310)&#010;    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collectWithTimestamp(StreamSourceContexts.java:409)&#010;    at org.apache.flink.streaming.connectors.kafka.internals.AbstractFetcher.emitRecordWithTimestamp(AbstractFetcher.java:408)&#010;    at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.emitRecord(KafkaFetcher.java:185)&#010;    at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:150)&#010;    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:715)&#010;    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#010;    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#010;    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:196)&#010;&#010;&#010;| |&#010;叶贤勋&#010;|&#010;|&#010;yxx_cmhd@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;&#010;",
        "depth": "0",
        "reply": "<52e6df6d.65c4.173482b475c.Coremail.yxx_cmhd@163.com>"
    },
    {
        "id": "<23FBE84F-A054-441C-AA35-ECA0BE98FD46@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:50:33 GMT",
        "subject": "Re: 使用Flink Array Field Type ",
        "content": "Hi, &#010;&#010;SQL 中数据下标是从1开始的，不是从0，所以会有数组越界问题。建议使用数组时通过&#010;select arr[5] from T where CARDINALITY(arr) &gt;= 5 这种方式防止数组访问越界。&#010;&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月13日，20:34，叶贤勋 &lt;yxx_cmhd@163.com&gt; 写道：&#010;&gt; &#010;&gt; test_array_string[0]&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<52e6df6d.65c4.173482b475c.Coremail.yxx_cmhd@163.com>"
    },
    {
        "id": "<42892ad.6849.1734852eaa2.Coremail.yxx_cmhd@163.com>",
        "from": "叶贤勋 &lt;yxx_c...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:17:40 GMT",
        "subject": "回复： 使用Flink Array Field Type ",
        "content": "谢谢 Leonard的解答。刚刚也看到了这个jira单[1]&#010;&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-17847&#010;| |&#010;叶贤勋&#010;|&#010;|&#010;yxx_cmhd@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;&#010;&#010;在2020年07月13日 20:50，Leonard Xu&lt;xbjtdcq@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;SQL 中数据下标是从1开始的，不是从0，所以会有数组越界问题。建议使用数组时通过&#010;select arr[5] from T where CARDINALITY(arr) &gt;= 5 这种方式防止数组访问越界。&#010;&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;在 2020年7月13日，20:34，叶贤勋 &lt;yxx_cmhd@163.com&gt; 写道：&#010;&#010;test_array_string[0]&#010;&#010;",
        "depth": "2",
        "reply": "<52e6df6d.65c4.173482b475c.Coremail.yxx_cmhd@163.com>"
    },
    {
        "id": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:09:03 GMT",
        "subject": "flink 同时sink hbase和hive，hbase少记录",
        "content": "&#010;&#010;flink订阅kafka消息，同时sink到hbase和hive中，&#010;当向kafka发送42条记录，然后停止producer发消息，去hive中查可以精准地查到42条，但是在hbase中却只查到30条&#010;&#010;&#010;query:&#010;streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE hbase_table (&#010;        |    rowkey VARCHAR,&#010;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;        |) WITH (&#010;        |    'connector.type' = 'hbase',&#010;        |    'connector.version' = '2.1.0',&#010;        |    'connector.table-name' = 'ods:user_hbase6',&#010;        |    'connector.zookeeper.quorum' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;        |    'connector.write.buffer-flush.max-size' = '1mb',&#010;        |    'connector.write.buffer-flush.max-rows' = '1',&#010;        |    'connector.write.buffer-flush.interval' = '0s'&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;    val statementSet = streamTableEnv.createStatementSet()&#010;    val insertHbase =&#010;      \"\"\"&#010;        |insert into hbase_table&#010;        |SELECT&#010;        |   CONCAT(SUBSTRING(MD5(CAST(uid AS VARCHAR)), 0, 6), cast(CEILING(UNIX_TIMESTAMP(created_time)/60)&#010;as string), sex) as uid,&#010;        |   ROW(sex, age, created_time ) as cf&#010;        |FROM  (select uid,sex,age, cast(created_time as VARCHAR) as created_time from kafka_table)&#010;        |&#010;        |\"\"\".stripMargin&#010;&#010;    statementSet.addInsertSql(insertHbase)&#010;&#010;    val insertHive =&#010;      \"\"\"&#010;        |&#010;        |INSERT INTO odsCatalog.ods.hive_table&#010;        |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'), DATE_FORMAT(created_time,&#010;'HH')&#010;        |FROM kafka_table&#010;        |&#010;        |\"\"\".stripMargin&#010;    statementSet.addInsertSql(insertHive)&#010;&#010;&#010;    statementSet.execute()&#010;&#010;&#010;是因为参数'connector.write.buffer-flush.max-size' = '1mb'吗？我尝试设置‘0’，‘10b','1kb',都失败了，报错如下：&#010;Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes) value but&#010;was: 1kb&#010;Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes) value but&#010;was: 10b&#010;Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes) value but&#010;was: 1&#010;&#010;&#010;&#010;&#010;&#010;&#010;并且，按照官网文档&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/hbase.html&#010;&#010;&#010;设置参数也不识别，报错：&#010;Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for&#010;identifier 'hbase-2.1.0' that implements 'org.apache.flink.table.factories.DynamicTableSinkFactory'&#010;in the classpath.&#010;&#010;&#010;看了一下源码，&#010;org.apache.flink.table.descriptors.HBaseValidator&#010;public static final String CONNECTOR_TYPE_VALUE_HBASE = \"hbase\";&#010;    public static final String CONNECTOR_VERSION_VALUE_143 = \"2.1.0\";&#010;    public static final String CONNECTOR_TABLE_NAME = \"connector.table-name\";&#010;    public static final String CONNECTOR_ZK_QUORUM = \"connector.zookeeper.quorum\";&#010;    public static final String CONNECTOR_ZK_NODE_PARENT = \"connector.zookeeper.znode.parent\";&#010;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_MAX_SIZE = \"connector.write.buffer-flush.max-size\";&#010;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_MAX_ROWS = \"connector.write.buffer-flush.max-rows\";&#010;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_INTERVAL = \"connector.write.buffer-flush.interval\";&#010;参数还是老参数",
        "depth": "0",
        "reply": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>"
    },
    {
        "id": "<1E265848-73F5-4768-A2D3-994CAAEF7244@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:50:54 GMT",
        "subject": "Re: flink 同时sink hbase和hive，hbase少记录",
        "content": "Hi, Zhou&#010;&#010;&#010;&gt;       'connector.write.buffer-flush.max-size' = '1mb',&#010;&gt;       'connector.write.buffer-flush.interval' = ‘0s'&#010;&#010;(1) connector.write.buffer-flush.max-size这个配置项支持的单位只有mb，其他不支持，所以会报对应的错。这个参数用于&#010;BufferredMutator 做buffer优化的参数，表示buffer存多大的size就触发写，flush.interval参数是按照多长的时间轮询写入，两个参数根据需要配合使用。当connector.write.buffer-flush.interval&#010;设置为 0s 时，表示不会轮询，所以只会等connector.write.buffer-flush.max-size到最大size再写入。你把connector.write.buffer-flush.interval&#010;设置成 1s 应该就能看到数据了。&#010;&#010;(2) Hbase connector 1.11.0 之前的版本只支持1.4.3，所以你填2.1.0会报错，在1.11.0开始支持为1.4.x，&#010;所以1.11.0新的connector里支持的参数为’connector’ = ‘hbase-1.4’, 因为hbase&#010;1.4.x版本API是兼容的，另外社区也在讨论支持HBase 2.x[1]&#010;&#010;&#010;Best,&#010;Leonard Xu&#010;[1] http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Upgrade-HBase-connector-to-2-2-x-tc42657.html#a42674&#010;&lt;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Upgrade-HBase-connector-to-2-2-x-tc42657.html#a42674&gt;&#010;&#010;&#010;&gt; 在 2020年7月13日，21:09，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; flink订阅kafka消息，同时sink到hbase和hive中，&#010;&gt; 当向kafka发送42条记录，然后停止producer发消息，去hive中查可以精准地查到42条，但是在hbase中却只查到30条&#010;&gt; &#010;&gt; &#010;&gt; query:&#010;&gt; streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE hbase_table (&#010;&gt;        |    rowkey VARCHAR,&#010;&gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt;        |) WITH (&#010;&gt;        |    'connector.type' = 'hbase',&#010;&gt;        |    'connector.version' = '2.1.0',&#010;&gt;        |    'connector.table-name' = 'ods:user_hbase6',&#010;&gt;        |    'connector.zookeeper.quorum' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt;        |    'connector.write.buffer-flush.max-size' = '1mb',&#010;&gt;        |    'connector.write.buffer-flush.max-rows' = '1',&#010;&gt;        |    'connector.write.buffer-flush.interval' = '0s'&#010;&gt;        |)&#010;&gt;        |\"\"\".stripMargin)&#010;&gt; &#010;&gt;    val statementSet = streamTableEnv.createStatementSet()&#010;&gt;    val insertHbase =&#010;&gt;      \"\"\"&#010;&gt;        |insert into hbase_table&#010;&gt;        |SELECT&#010;&gt;        |   CONCAT(SUBSTRING(MD5(CAST(uid AS VARCHAR)), 0, 6), cast(CEILING(UNIX_TIMESTAMP(created_time)/60)&#010;as string), sex) as uid,&#010;&gt;        |   ROW(sex, age, created_time ) as cf&#010;&gt;        |FROM  (select uid,sex,age, cast(created_time as VARCHAR) as created_time from&#010;kafka_table)&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin&#010;&gt; &#010;&gt;    statementSet.addInsertSql(insertHbase)&#010;&gt; &#010;&gt;    val insertHive =&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |INSERT INTO odsCatalog.ods.hive_table&#010;&gt;        |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'), DATE_FORMAT(created_time,&#010;'HH')&#010;&gt;        |FROM kafka_table&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin&#010;&gt;    statementSet.addInsertSql(insertHive)&#010;&gt; &#010;&gt; &#010;&gt;    statementSet.execute()&#010;&gt; &#010;&gt; &#010;&gt; 是因为参数'connector.write.buffer-flush.max-size' = '1mb'吗？我尝试设置‘0’，‘10b','1kb',都失败了，报错如下：&#010;&gt; Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes) value&#010;but was: 1kb&#010;&gt; Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes) value&#010;but was: 10b&#010;&gt; Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes) value&#010;but was: 1&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 并且，按照官网文档&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/hbase.html&#010;&gt; &#010;&gt; &#010;&gt; 设置参数也不识别，报错：&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory&#010;for identifier 'hbase-2.1.0' that implements 'org.apache.flink.table.factories.DynamicTableSinkFactory'&#010;in the classpath.&#010;&gt; &#010;&gt; &#010;&gt; 看了一下源码，&#010;&gt; org.apache.flink.table.descriptors.HBaseValidator&#010;&gt; public static final String CONNECTOR_TYPE_VALUE_HBASE = \"hbase\";&#010;&gt;    public static final String CONNECTOR_VERSION_VALUE_143 = \"2.1.0\";&#010;&gt;    public static final String CONNECTOR_TABLE_NAME = \"connector.table-name\";&#010;&gt;    public static final String CONNECTOR_ZK_QUORUM = \"connector.zookeeper.quorum\";&#010;&gt;    public static final String CONNECTOR_ZK_NODE_PARENT = \"connector.zookeeper.znode.parent\";&#010;&gt;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_MAX_SIZE = \"connector.write.buffer-flush.max-size\";&#010;&gt;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_MAX_ROWS = \"connector.write.buffer-flush.max-rows\";&#010;&gt;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_INTERVAL = \"connector.write.buffer-flush.interval\";&#010;&gt; 参数还是老参数&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>"
    },
    {
        "id": "<43dfd5ba.1d39.1734b05c069.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 01:52:14 GMT",
        "subject": "Re:Re: flink 同时sink hbase和hive，hbase少记录",
        "content": "&#010;&#010;&#010;&#010;&#010;&#010;Hi, Leonard&#010;我设置了 'connector.write.buffer-flush.interval' = ‘1s'，然后重启运行程序，&#010;再消息发送刚开始，比如说发送了4条，hive和hbase接收的消息都是4条，再消息发送48条的时候，我停止了producer，&#010;再去查结果hbase是19条，hive是48条，如果说每1s钟flink查一下sink hbase buffer是不是到1mb，到了就sink，没到就不sink，但是这解释不了，为啥刚开始，hbase和hive接收到到数据是同步的，奇怪&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 21:50:54，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hi, Zhou&#010;&gt;&#010;&gt;&#010;&gt;&gt;       'connector.write.buffer-flush.max-size' = '1mb',&#010;&gt;&gt;       'connector.write.buffer-flush.interval' = ‘0s'&#010;&gt;&#010;&gt;(1) connector.write.buffer-flush.max-size这个配置项支持的单位只有mb，其他不支持，所以会报对应的错。这个参数用于&#010;BufferredMutator 做buffer优化的参数，表示buffer存多大的size就触发写，flush.interval参数是按照多长的时间轮询写入，两个参数根据需要配合使用。当connector.write.buffer-flush.interval&#010;设置为 0s 时，表示不会轮询，所以只会等connector.write.buffer-flush.max-size到最大size再写入。你把connector.write.buffer-flush.interval&#010;设置成 1s 应该就能看到数据了。&#010;&gt;&#010;&gt;(2) Hbase connector 1.11.0 之前的版本只支持1.4.3，所以你填2.1.0会报错，在1.11.0开始支持为1.4.x，&#010;所以1.11.0新的connector里支持的参数为’connector’ = ‘hbase-1.4’, 因为hbase&#010;1.4.x版本API是兼容的，另外社区也在讨论支持HBase 2.x[1]&#010;&gt;&#010;&gt;&#010;&gt;Best,&#010;&gt;Leonard Xu&#010;&gt;[1] http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Upgrade-HBase-connector-to-2-2-x-tc42657.html#a42674&#010;&lt;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Upgrade-HBase-connector-to-2-2-x-tc42657.html#a42674&gt;&#010;&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月13日，21:09，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; flink订阅kafka消息，同时sink到hbase和hive中，&#010;&gt;&gt; 当向kafka发送42条记录，然后停止producer发消息，去hive中查可以精准地查到42条，但是在hbase中却只查到30条&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; query:&#010;&gt;&gt; streamTableEnv.executeSql(&#010;&gt;&gt;      \"\"\"&#010;&gt;&gt;        |&#010;&gt;&gt;        |CREATE TABLE hbase_table (&#010;&gt;&gt;        |    rowkey VARCHAR,&#010;&gt;&gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt;&gt;        |) WITH (&#010;&gt;&gt;        |    'connector.type' = 'hbase',&#010;&gt;&gt;        |    'connector.version' = '2.1.0',&#010;&gt;&gt;        |    'connector.table-name' = 'ods:user_hbase6',&#010;&gt;&gt;        |    'connector.zookeeper.quorum' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt;&gt;        |    'connector.write.buffer-flush.max-size' = '1mb',&#010;&gt;&gt;        |    'connector.write.buffer-flush.max-rows' = '1',&#010;&gt;&gt;        |    'connector.write.buffer-flush.interval' = '0s'&#010;&gt;&gt;        |)&#010;&gt;&gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &#010;&gt;&gt;    val statementSet = streamTableEnv.createStatementSet()&#010;&gt;&gt;    val insertHbase =&#010;&gt;&gt;      \"\"\"&#010;&gt;&gt;        |insert into hbase_table&#010;&gt;&gt;        |SELECT&#010;&gt;&gt;        |   CONCAT(SUBSTRING(MD5(CAST(uid AS VARCHAR)), 0, 6), cast(CEILING(UNIX_TIMESTAMP(created_time)/60)&#010;as string), sex) as uid,&#010;&gt;&gt;        |   ROW(sex, age, created_time ) as cf&#010;&gt;&gt;        |FROM  (select uid,sex,age, cast(created_time as VARCHAR) as created_time&#010;from kafka_table)&#010;&gt;&gt;        |&#010;&gt;&gt;        |\"\"\".stripMargin&#010;&gt;&gt; &#010;&gt;&gt;    statementSet.addInsertSql(insertHbase)&#010;&gt;&gt; &#010;&gt;&gt;    val insertHive =&#010;&gt;&gt;      \"\"\"&#010;&gt;&gt;        |&#010;&gt;&gt;        |INSERT INTO odsCatalog.ods.hive_table&#010;&gt;&gt;        |SELECT uid, age, DATE_FORMAT(created_time, 'yyyy-MM-dd'), DATE_FORMAT(created_time,&#010;'HH')&#010;&gt;&gt;        |FROM kafka_table&#010;&gt;&gt;        |&#010;&gt;&gt;        |\"\"\".stripMargin&#010;&gt;&gt;    statementSet.addInsertSql(insertHive)&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;    statementSet.execute()&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 是因为参数'connector.write.buffer-flush.max-size' = '1mb'吗？我尝试设置‘0’，‘10b','1kb',都失败了，报错如下：&#010;&gt;&gt; Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes)&#010;value but was: 1kb&#010;&gt;&gt; Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes)&#010;value but was: 10b&#010;&gt;&gt; Property 'connector.write.buffer-flush.max-size' must be a memory size (in bytes)&#010;value but was: 1&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 并且，按照官网文档&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/hbase.html&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 设置参数也不识别，报错：&#010;&gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory&#010;for identifier 'hbase-2.1.0' that implements 'org.apache.flink.table.factories.DynamicTableSinkFactory'&#010;in the classpath.&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 看了一下源码，&#010;&gt;&gt; org.apache.flink.table.descriptors.HBaseValidator&#010;&gt;&gt; public static final String CONNECTOR_TYPE_VALUE_HBASE = \"hbase\";&#010;&gt;&gt;    public static final String CONNECTOR_VERSION_VALUE_143 = \"2.1.0\";&#010;&gt;&gt;    public static final String CONNECTOR_TABLE_NAME = \"connector.table-name\";&#010;&gt;&gt;    public static final String CONNECTOR_ZK_QUORUM = \"connector.zookeeper.quorum\";&#010;&gt;&gt;    public static final String CONNECTOR_ZK_NODE_PARENT = \"connector.zookeeper.znode.parent\";&#010;&gt;&gt;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_MAX_SIZE = \"connector.write.buffer-flush.max-size\";&#010;&gt;&gt;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_MAX_ROWS = \"connector.write.buffer-flush.max-rows\";&#010;&gt;&gt;    public static final String CONNECTOR_WRITE_BUFFER_FLUSH_INTERVAL = \"connector.write.buffer-flush.interval\";&#010;&gt;&gt; 参数还是老参数&#010;&gt;&#010;",
        "depth": "2",
        "reply": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>"
    },
    {
        "id": "<DA5F40BA-AF1B-44A9-B690-865FE71634B8@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 01:56:00 GMT",
        "subject": "Re: flink 同时sink hbase和hive，hbase少记录",
        "content": "Hi,&#010;&#010;&gt; 在 2020年7月14日，09:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt;&gt;&gt;       |   CONCAT(SUBSTRING(MD5(CAST(uid AS VARCHAR)), 0, 6), cast(CEILING(UNIX_TIMESTAMP(created_time)/60)&#010;as string), sex) as uid,&#010;&#010;看下这个抽取出来的rowkey是否有重复的呢？&#010;&#010;祝好，&#010;Leonard Xu&#010;",
        "depth": "3",
        "reply": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>"
    },
    {
        "id": "<183dfbfc.2ebf.1734b403a24.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:56:07 GMT",
        "subject": "Re:Re: flink 同时sink hbase和hive，hbase少记录",
        "content": "&#010;&#010;&#010;Hi Leonard,&#010;原来是有重复key，hbase做了upsert，请问Hive Streaming Writing是不是目前只支持append模式，不支持upsert模式&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 09:56:00，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;&gt; 在 2020年7月14日，09:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       |   CONCAT(SUBSTRING(MD5(CAST(uid AS VARCHAR)), 0, 6), cast(CEILING(UNIX_TIMESTAMP(created_time)/60)&#010;as string), sex) as uid,&#010;&gt;&#010;&gt;看下这个抽取出来的rowkey是否有重复的呢？&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;",
        "depth": "4",
        "reply": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>"
    },
    {
        "id": "<af078c9f-5a93-4e7f-95ab-e6aeff496438.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:00:18 GMT",
        "subject": "回复：Re: flink 同时sink hbase和hive，hbase少记录",
        "content": "你好,&#010;本质还是StreamingFileSink,所以目前只能append&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;发送时间：2020年7月14日(星期二) 10:56&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：Re:Re: flink 同时sink hbase和hive，hbase少记录&#010;&#010;&#010;&#010;&#010;Hi Leonard,&#010;原来是有重复key，hbase做了upsert，请问Hive Streaming Writing是不是目前只支持append模式，不支持upsert模式&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 09:56:00，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;&gt; 在 2020年7月14日，09:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt;&gt;       |   CONCAT(SUBSTRING(MD5(CAST(uid AS VARCHAR)), 0, 6), cast(CEILING(UNIX_TIMESTAMP(created_time)/60)&#010;as string), sex) as uid,&#010;&gt;&#010;&gt;看下这个抽取出来的rowkey是否有重复的呢？&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;",
        "depth": "4",
        "reply": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>"
    },
    {
        "id": "<4b8c0780.3127.1734b473cb9.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:03:46 GMT",
        "subject": "Re:回复：Re: flink 同时sink hbase和hive，hbase少记录",
        "content": "Hi,&#010;感谢社区热心答疑！&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 11:00:18，\"夏帅\" &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt;你好,&#010;&gt;本质还是StreamingFileSink,所以目前只能append&#010;&gt;&#010;&gt;&#010;&gt;------------------------------------------------------------------&#010;&gt;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;发送时间：2020年7月14日(星期二) 10:56&#010;&gt;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt;主　题：Re:Re: flink 同时sink hbase和hive，hbase少记录&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;Hi Leonard,&#010;&gt;原来是有重复key，hbase做了upsert，请问Hive Streaming Writing是不是目前只支持append模式，不支持upsert模式&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;在 2020-07-14 09:56:00，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;&gt;Hi,&#010;&gt;&gt;&#010;&gt;&gt;&gt; 在 2020年7月14日，09:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;       |   CONCAT(SUBSTRING(MD5(CAST(uid AS VARCHAR)), 0, 6), cast(CEILING(UNIX_TIMESTAMP(created_time)/60)&#010;as string), sex) as uid,&#010;&gt;&gt;&#010;&gt;&gt;看下这个抽取出来的rowkey是否有重复的呢？&#010;&gt;&gt;&#010;&gt;&gt;祝好，&#010;&gt;&gt;Leonard Xu&#010;",
        "depth": "5",
        "reply": "<e81e801.a096.173484b0679.Coremail.wander669@163.com>"
    },
    {
        "id": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>",
        "from": "&quot;Robert.Zhang&quot; &lt;173603...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 13:49:55 GMT",
        "subject": "flink state",
        "content": "Hello,all&#013;&#010;目前stream中遇到一个问题，&#013;&#010;想使用一个全局的state 在所有的keyed stream中使用，或者global parameter，主要的需求在于是这个state是可变的，需要对其进行修改并且对所有stream&#010;operator可见，大家有遇到过类似场景或者可以提供相关思路么，感激不尽&#013;&#010;&#013;&#010;&#013;&#010;Best regards",
        "depth": "0",
        "reply": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>"
    },
    {
        "id": "<CAA8tFvvkZVTrC8uRZhfvRf_yjwQ40kRAfvsWfDFdM2USH9e6Pg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:03:29 GMT",
        "subject": "Re: flink state",
        "content": "Hi Robert&#013;&#010;&#013;&#010;Boardcast state[1] 是否满足你的需求呢？另外也可以看下这篇文章[2]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/stream/state/broadcast_state.html&#013;&#010;[2] https://cloud.tencent.com/developer/article/1509789&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Robert.Zhang &lt;173603082@qq.com&gt; 于2020年7月13日周一 下午9:50写道：&#013;&#010;&#013;&#010;&gt; Hello,all&#013;&#010;&gt; 目前stream中遇到一个问题，&#013;&#010;&gt; 想使用一个全局的state 在所有的keyed stream中使用，或者global&#013;&#010;&gt; parameter，主要的需求在于是这个state是可变的，需要对其进行修改并且对所有stream&#013;&#010;&gt; operator可见，大家有遇到过类似场景或者可以提供相关思路么，感激不尽&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best regards&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>"
    },
    {
        "id": "<BYAPR08MB5525ABB0EB9082D2DE89F1FAF6610@BYAPR08MB5525.namprd08.prod.outlook.com>",
        "from": "zhao liang &lt;zhao...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:09:51 GMT",
        "subject": "答复: flink state",
        "content": "我这边有个类似的实现，需要根据维表数据改变stream的处理，自定义了一个source（从MySQL中定时刷维表数据），kafka的stream&#010;union这个维表数据流，&#013;&#010;额外增加一个数据类型（维表类型或者事实数据）进行数据的处理，后续算子将这个维表进行不同的处理并存到对应算子的state中。&#013;&#010;&#013;&#010;发件人: Congxian Qiu &lt;qcx978132955@gmail.com&gt;&#013;&#010;日期: 星期二, 2020年7月14日 14:03&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink state&#013;&#010;Hi Robert&#013;&#010;&#013;&#010;Boardcast state[1] 是否满足你的需求呢？另外也可以看下这篇文章[2]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/stream/state/broadcast_state.html&#013;&#010;[2] https://cloud.tencent.com/developer/article/1509789&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Robert.Zhang &lt;173603082@qq.com&gt; 于2020年7月13日周一 下午9:50写道：&#013;&#010;&#013;&#010;&gt; Hello,all&#013;&#010;&gt; 目前stream中遇到一个问题，&#013;&#010;&gt; 想使用一个全局的state 在所有的keyed stream中使用，或者global&#013;&#010;&gt; parameter，主要的需求在于是这个state是可变的，需要对其进行修改并且对所有stream&#013;&#010;&gt; operator可见，大家有遇到过类似场景或者可以提供相关思路么，感激不尽&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best regards&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>"
    },
    {
        "id": "<tencent_533E099B69950852B3B80F6A550F2EE0A50A@qq.com>",
        "from": "&quot;Robert.Zhang&quot; &lt;173603...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 07:21:38 GMT",
        "subject": "回复：答复: flink state",
        "content": "是这样的，问题在于我需要使用keyed state 来修改broadcast state，比如根据keyed&#010;state把某些满足条件的key存入这个broadcast state，并在其他算子计算的时候使用这个broadcast&#010;state，比如需要这些key来做&#013;&#010;文档中提到的nonbroadcast side是无法修改broadcast state的，是read-only,&#013;&#010;似乎无法直接去实现&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人: \"zhao liang\"&lt;zhaoawd@gmail.com&amp;gt;; &#013;&#010;发送时间: 2020年7月14日(星期二) 下午4:09&#013;&#010;收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;; &#013;&#010;主题: 答复: flink state&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我这边有个类似的实现，需要根据维表数据改变stream的处理，自定义了一个source（从MySQL中定时刷维表数据），kafka的stream&#010;union这个维表数据流，&#013;&#010;额外增加一个数据类型（维表类型或者事实数据）进行数据的处理，后续算子将这个维表进行不同的处理并存到对应算子的state中。&#013;&#010;&#013;&#010;发件人: Congxian Qiu &lt;qcx978132955@gmail.com&amp;gt;&#013;&#010;日期: 星期二, 2020年7月14日 14:03&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: Re: flink state&#013;&#010;Hi Robert&#013;&#010;&#013;&#010;Boardcast state[1] 是否满足你的需求呢？另外也可以看下这篇文章[2]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/stream/state/broadcast_state.html&#013;&#010;[2] https://cloud.tencent.com/developer/article/1509789&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Robert.Zhang &lt;173603082@qq.com&amp;gt; 于2020年7月13日周一 下午9:50写道：&#013;&#010;&#013;&#010;&amp;gt; Hello,all&#013;&#010;&amp;gt; 目前stream中遇到一个问题，&#013;&#010;&amp;gt; 想使用一个全局的state 在所有的keyed stream中使用，或者global&#013;&#010;&amp;gt; parameter，主要的需求在于是这个state是可变的，需要对其进行修改并且对所有stream&#013;&#010;&amp;gt; operator可见，大家有遇到过类似场景或者可以提供相关思路么，感激不尽&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best regards",
        "depth": "3",
        "reply": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>"
    },
    {
        "id": "<BYAPR08MB5525F1383E535899760BE878F67E0@BYAPR08MB5525.namprd08.prod.outlook.com>",
        "from": "zhao liang &lt;zhao...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 10:04:59 GMT",
        "subject": "答复: 回复：答复: flink state",
        "content": "Broadcast state是无法满足你的要求的，估计你只能像我这样把涉及的state数据融入到数据流中，在算子中针对不同的类型数据做区分了，等于人工维持这个broadcast的流的变化。&#013;&#010;&#013;&#010;发件人: Robert.Zhang &lt;173603082@qq.com&gt;&#013;&#010;日期: 星期三, 2020年7月15日 15:22&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;, user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: 回复：答复: flink state&#013;&#010;是这样的，问题在于我需要使用keyed state 来修改broadcast state，比如根据keyed&#010;state把某些满足条件的key存入这个broadcast state，并在其他算子计算的时候使用这个broadcast&#010;state，比如需要这些key来做&#013;&#010;文档中提到的nonbroadcast side是无法修改broadcast state的，是read-only,&#013;&#010;似乎无法直接去实现&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人: \"zhao liang\"&lt;zhaoawd@gmail.com&amp;gt;;&#013;&#010;发送时间: 2020年7月14日(星期二) 下午4:09&#013;&#010;收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;主题: 答复: flink state&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我这边有个类似的实现，需要根据维表数据改变stream的处理，自定义了一个source（从MySQL中定时刷维表数据），kafka的stream&#010;union这个维表数据流，&#013;&#010;额外增加一个数据类型（维表类型或者事实数据）进行数据的处理，后续算子将这个维表进行不同的处理并存到对应算子的state中。&#013;&#010;&#013;&#010;发件人: Congxian Qiu &lt;qcx978132955@gmail.com&amp;gt;&#013;&#010;日期: 星期二, 2020年7月14日 14:03&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: Re: flink state&#013;&#010;Hi Robert&#013;&#010;&#013;&#010;Boardcast state[1] 是否满足你的需求呢？另外也可以看下这篇文章[2]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/stream/state/broadcast_state.html&#013;&#010;[2] https://cloud.tencent.com/developer/article/1509789&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Robert.Zhang &lt;173603082@qq.com&amp;gt; 于2020年7月13日周一 下午9:50写道：&#013;&#010;&#013;&#010;&amp;gt; Hello,all&#013;&#010;&amp;gt; 目前stream中遇到一个问题，&#013;&#010;&amp;gt; 想使用一个全局的state 在所有的keyed stream中使用，或者global&#013;&#010;&amp;gt; parameter，主要的需求在于是这个state是可变的，需要对其进行修改并且对所有stream&#013;&#010;&amp;gt; operator可见，大家有遇到过类似场景或者可以提供相关思路么，感激不尽&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best regards&#013;&#010;",
        "depth": "4",
        "reply": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>"
    },
    {
        "id": "<CAA8tFvu9haR6nWmZCZavf=Xry5diDxqE1qNPaFit9ZeOS2nswQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 05:01:44 GMT",
        "subject": "Re: 回复：答复: flink state",
        "content": "Hi&#013;&#010;    broadcast state 是无法修改的，如果你还希望进行修改的话，可以使用&#010;zhao liang 的方法，另外如果这个全局 state&#013;&#010;不需要维护一致性等的话，同样可以考虑放到外存中（Redis，HBase 等）&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zhao liang &lt;zhaoawd@gmail.com&gt; 于2020年7月15日周三 下午6:05写道：&#013;&#010;&#013;&#010;&gt; Broadcast&#013;&#010;&gt; state是无法满足你的要求的，估计你只能像我这样把涉及的state数据融入到数据流中，在算子中针对不同的类型数据做区分了，等于人工维持这个broadcast的流的变化。&#013;&#010;&gt;&#013;&#010;&gt; 发件人: Robert.Zhang &lt;173603082@qq.com&gt;&#013;&#010;&gt; 日期: 星期三, 2020年7月15日 15:22&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;, user-zh@flink.apache.org &lt;&#013;&#010;&gt; user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: 回复：答复: flink state&#013;&#010;&gt; 是这样的，问题在于我需要使用keyed state 来修改broadcast state，比如根据keyed&#013;&#010;&gt; state把某些满足条件的key存入这个broadcast state，并在其他算子计算的时候使用这个broadcast&#013;&#010;&gt; state，比如需要这些key来做&#013;&#010;&gt; 文档中提到的nonbroadcast side是无法修改broadcast state的，是read-only,&#013;&#010;&gt; 似乎无法直接去实现&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人: \"zhao liang\"&lt;zhaoawd@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间: 2020年7月14日(星期二) 下午4:09&#013;&#010;&gt; 收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt; 主题: 答复: flink state&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我这边有个类似的实现，需要根据维表数据改变stream的处理，自定义了一个source（从MySQL中定时刷维表数据），kafka的stream&#013;&#010;&gt; union这个维表数据流，&#013;&#010;&gt; 额外增加一个数据类型（维表类型或者事实数据）进行数据的处理，后续算子将这个维表进行不同的处理并存到对应算子的state中。&#013;&#010;&gt;&#013;&#010;&gt; 发件人: Congxian Qiu &lt;qcx978132955@gmail.com&amp;gt;&#013;&#010;&gt; 日期: 星期二, 2020年7月14日 14:03&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;&gt; 主题: Re: flink state&#013;&#010;&gt; Hi Robert&#013;&#010;&gt;&#013;&#010;&gt; Boardcast state[1] 是否满足你的需求呢？另外也可以看下这篇文章[2]&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/stream/state/broadcast_state.html&#013;&#010;&gt; [2] https://cloud.tencent.com/developer/article/1509789&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Robert.Zhang &lt;173603082@qq.com&amp;gt; 于2020年7月13日周一 下午9:50写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; Hello,all&#013;&#010;&gt; &amp;gt; 目前stream中遇到一个问题，&#013;&#010;&gt; &amp;gt; 想使用一个全局的state 在所有的keyed stream中使用，或者global&#013;&#010;&gt; &amp;gt; parameter，主要的需求在于是这个state是可变的，需要对其进行修改并且对所有stream&#013;&#010;&gt; &amp;gt; operator可见，大家有遇到过类似场景或者可以提供相关思路么，感激不尽&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; Best regards&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>"
    },
    {
        "id": "<tencent_CB2C3A837CD5A70BF2A938C21B5A3482D608@qq.com>",
        "from": "&quot;Robert.Zhang&quot; &lt;173603...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 06:29:01 GMT",
        "subject": "回复： 回复：答复: flink state",
        "content": "感谢各位答疑，可能确实要借助其他方式来间接实现了，thanku all&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人: \"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;; &#013;&#010;发送时间: 2020年7月16日(星期四) 中午1:01&#013;&#010;收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;; &#013;&#010;主题: Re: 回复：答复: flink state&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; broadcast state 是无法修改的，如果你还希望进行修改的话，可以使用&#010;zhao liang 的方法，另外如果这个全局 state&#013;&#010;不需要维护一致性等的话，同样可以考虑放到外存中（Redis，HBase 等）&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zhao liang &lt;zhaoawd@gmail.com&amp;gt; 于2020年7月15日周三 下午6:05写道：&#013;&#010;&#013;&#010;&amp;gt; Broadcast&#013;&#010;&amp;gt; state是无法满足你的要求的，估计你只能像我这样把涉及的state数据融入到数据流中，在算子中针对不同的类型数据做区分了，等于人工维持这个broadcast的流的变化。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 发件人: Robert.Zhang &lt;173603082@qq.com&amp;gt;&#013;&#010;&amp;gt; 日期: 星期三, 2020年7月15日 15:22&#013;&#010;&amp;gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;, user-zh@flink.apache.org&#010;&lt;&#013;&#010;&amp;gt; user-zh@flink.apache.org&amp;gt;&#013;&#010;&amp;gt; 主题: 回复：答复: flink state&#013;&#010;&amp;gt; 是这样的，问题在于我需要使用keyed state 来修改broadcast state，比如根据keyed&#013;&#010;&amp;gt; state把某些满足条件的key存入这个broadcast state，并在其他算子计算的时候使用这个broadcast&#013;&#010;&amp;gt; state，比如需要这些key来做&#013;&#010;&amp;gt; 文档中提到的nonbroadcast side是无法修改broadcast state的，是read-only,&#013;&#010;&amp;gt; 似乎无法直接去实现&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人: \"zhao liang\"&lt;zhaoawd@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间: 2020年7月14日(星期二) 下午4:09&#013;&#010;&amp;gt; 收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt; 主题: 答复: flink state&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我这边有个类似的实现，需要根据维表数据改变stream的处理，自定义了一个source（从MySQL中定时刷维表数据），kafka的stream&#013;&#010;&amp;gt; union这个维表数据流，&#013;&#010;&amp;gt; 额外增加一个数据类型（维表类型或者事实数据）进行数据的处理，后续算子将这个维表进行不同的处理并存到对应算子的state中。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 发件人: Congxian Qiu &lt;qcx978132955@gmail.com&amp;amp;gt;&#013;&#010;&amp;gt; 日期: 星期二, 2020年7月14日 14:03&#013;&#010;&amp;gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;amp;gt;&#013;&#010;&amp;gt; 主题: Re: flink state&#013;&#010;&amp;gt; Hi Robert&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Boardcast state[1] 是否满足你的需求呢？另外也可以看下这篇文章[2]&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; [1]&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/stream/state/broadcast_state.html&#013;&#010;&amp;gt; [2] https://cloud.tencent.com/developer/article/1509789&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Congxian&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Robert.Zhang &lt;173603082@qq.com&amp;amp;gt; 于2020年7月13日周一 下午9:50写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Hello,all&#013;&#010;&amp;gt; &amp;amp;gt; 目前stream中遇到一个问题，&#013;&#010;&amp;gt; &amp;amp;gt; 想使用一个全局的state 在所有的keyed stream中使用，或者global&#013;&#010;&amp;gt; &amp;amp;gt; parameter，主要的需求在于是这个state是可变的，需要对其进行修改并且对所有stream&#013;&#010;&amp;gt; &amp;amp;gt; operator可见，大家有遇到过类似场景或者可以提供相关思路么，感激不尽&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best regards&#013;&#010;&amp;gt;",
        "depth": "6",
        "reply": "<tencent_83A69208C368590962B0580F9A54172C8C0A@qq.com>"
    },
    {
        "id": "<372ad87b.2046.1734b294298.Coremail.read3210@163.com>",
        "from": "nicygan &lt;read3...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:31:02 GMT",
        "subject": "flink1.9.1-消费kafka落pg库任务出错",
        "content": "dear all：&#010;      我有一个消费kafka数据写到pg库的任务，任务发生过重启，yarn日志显示jobmanager发生oom，但找不到具体原因，因为数据量非常小，按道理不该发生oom。&#010;      详细如下：&#010;&#010;&#010;1、部署方式：&#010;flink on yarn ，pre-job，每个container 1024 M&#010;jobmanager的jvmoption（默认的）  -Xms424m    -Xmx424m&#010;&#010;&#010;2、数据情况：&#010;kafka数据，约1分钟1条，文本数据，每条数据都非常小。&#010;&#010;&#010;3、任务情况：&#010;很简单，消费kafka然后直接写到pg库，中间没有任何处理，没有自定义的状态。&#010;消费采用 FlinkKafkaConsumer&#010;写库采用 JDBCAppendTableSink&#010;并行度 1&#010;checkpoint 2分钟一次，每次checkpoint约100ms&#010;statebackend rocksdb&#010;&#010;&#010;4、报错情况：&#010;2020-07-10 11:51:54,237 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  &#010;  - Triggering checkpoint 555 @ 1594353114226 for job cd5ceeedeb35e8e094991edf09233483.&#010;2020-07-10 11:51:54,421 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  &#010;  - Completed checkpoint 555 for job cd5ceeedeb35e8e094991edf09233483 (1238 bytes in 77 ms).&#010;2020-07-10 11:53:54,253 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  &#010;  - Triggering checkpoint 556 @ 1594353234226 for job cd5ceeedeb35e8e094991edf09233483.&#010;2020-07-10 11:53:54,457 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  &#010;  - Completed checkpoint 556 for job cd5ceeedeb35e8e094991edf09233483 (1238 bytes in 124 ms).&#010;2020-07-10 11:55:54,246 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  &#010;  - Triggering checkpoint 557 @ 1594353354226 for job cd5ceeedeb35e8e094991edf09233483.&#010;2020-07-10 11:55:54,402 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  &#010;  - Completed checkpoint 557 for job cd5ceeedeb35e8e094991edf09233483 (1238 bytes in 115 ms).&#010;2020-07-10 11:56:34,155 ERROR org.apache.flink.runtime.util.FatalExitExceptionHandler    &#010;  - FATAL: Thread 'flink-akka.actor.default-dispatcher-4673' produced an uncaught exception.&#010;Stopping the process...&#010;java.lang.OutOfMemoryError: unable to create new native thread&#010;&#009;at java.lang.Thread.start0(Native Method)&#010;&#009;at java.lang.Thread.start(Thread.java:717)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.tryAddWorker(ForkJoinPool.java:1672)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.signalWork(ForkJoinPool.java:1966)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.fullExternalPush(ForkJoinPool.java:1905)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.externalPush(ForkJoinPool.java:1834)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.execute(ForkJoinPool.java:2955)&#010;&#009;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool.execute(ForkJoinExecutorConfigurator.scala:30)&#010;&#009;at akka.dispatch.ExecutorServiceDelegate.execute(ThreadPoolBuilder.scala:211)&#010;&#009;at akka.dispatch.ExecutorServiceDelegate.execute$(ThreadPoolBuilder.scala:211)&#010;&#009;at akka.dispatch.Dispatcher$LazyExecutorServiceDelegate.execute(Dispatcher.scala:39)&#010;&#009;at akka.dispatch.Dispatcher.registerForExecution(Dispatcher.scala:115)&#010;&#009;at akka.dispatch.Dispatcher.dispatch(Dispatcher.scala:55)&#010;&#009;at akka.actor.dungeon.Dispatch.sendMessage(Dispatch.scala:142)&#010;&#009;at akka.actor.dungeon.Dispatch.sendMessage$(Dispatch.scala:136)&#010;&#009;at akka.actor.ActorCell.sendMessage(ActorCell.scala:429)&#010;&#009;at akka.actor.Cell.sendMessage(ActorCell.scala:350)&#010;&#009;at akka.actor.Cell.sendMessage$(ActorCell.scala:349)&#010;&#009;at akka.actor.ActorCell.sendMessage(ActorCell.scala:429)&#010;&#009;at akka.actor.RepointableActorRef.$bang(RepointableActorRef.scala:173)&#010;&#009;at akka.actor.Scheduler$$anon$3.run(Scheduler.scala:171)&#010;&#009;at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&#009;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&#009;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&#010;thanks all / by nicygan&#010;",
        "depth": "0",
        "reply": "<372ad87b.2046.1734b294298.Coremail.read3210@163.com>"
    },
    {
        "id": "<CAOMLN=ZDch5n4y+ffu7+i7_Qzke9UJP=iGf40XC07DD7hXhEnQ@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:07:12 GMT",
        "subject": "Re: flink1.9.1-消费kafka落pg库任务出错",
        "content": "Hi nicygan,&#010;&#010;unable to create new native thread指的是无法创建checkpoint线程，并不是内存占用过大。&#010;这种情况一般有3种可能的原因：&#010;1.flink应用开启太多线程&#010;2.机器上句柄设置太小&#010;3.机器上的其他应用开启太多线程&#010;&#010;建议排查一下机器上的ulimit设置（文件句柄会影响应用能开启的线程数），和flink&#010;metrics里监控到的线程数变化。&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;&#010;nicygan &lt;read3210@163.com&gt; 于2020年7月14日周二 上午10:31写道：&#010;&#010;&gt; dear all：&#010;&gt;&#010;&gt; 我有一个消费kafka数据写到pg库的任务，任务发生过重启，yarn日志显示jobmanager发生oom，但找不到具体原因，因为数据量非常小，按道理不该发生oom。&#010;&gt;       详细如下：&#010;&gt;&#010;&gt;&#010;&gt; 1、部署方式：&#010;&gt; flink on yarn ，pre-job，每个container 1024 M&#010;&gt; jobmanager的jvmoption（默认的）  -Xms424m    -Xmx424m&#010;&gt;&#010;&gt;&#010;&gt; 2、数据情况：&#010;&gt; kafka数据，约1分钟1条，文本数据，每条数据都非常小。&#010;&gt;&#010;&gt;&#010;&gt; 3、任务情况：&#010;&gt; 很简单，消费kafka然后直接写到pg库，中间没有任何处理，没有自定义的状态。&#010;&gt; 消费采用 FlinkKafkaConsumer&#010;&gt; 写库采用 JDBCAppendTableSink&#010;&gt; 并行度 1&#010;&gt; checkpoint 2分钟一次，每次checkpoint约100ms&#010;&gt; statebackend rocksdb&#010;&gt;&#010;&gt;&#010;&gt; 4、报错情况：&#010;&gt; 2020-07-10 11:51:54,237 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering&#010;&gt; checkpoint 555 @ 1594353114226 for job cd5ceeedeb35e8e094991edf09233483.&#010;&gt; 2020-07-10 11:51:54,421 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed&#010;&gt; checkpoint 555 for job cd5ceeedeb35e8e094991edf09233483 (1238 bytes in 77&#010;&gt; ms).&#010;&gt; 2020-07-10 11:53:54,253 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering&#010;&gt; checkpoint 556 @ 1594353234226 for job cd5ceeedeb35e8e094991edf09233483.&#010;&gt; 2020-07-10 11:53:54,457 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed&#010;&gt; checkpoint 556 for job cd5ceeedeb35e8e094991edf09233483 (1238 bytes in 124&#010;&gt; ms).&#010;&gt; 2020-07-10 11:55:54,246 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Triggering&#010;&gt; checkpoint 557 @ 1594353354226 for job cd5ceeedeb35e8e094991edf09233483.&#010;&gt; 2020-07-10 11:55:54,402 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator     - Completed&#010;&gt; checkpoint 557 for job cd5ceeedeb35e8e094991edf09233483 (1238 bytes in 115&#010;&gt; ms).&#010;&gt; 2020-07-10 11:56:34,155 ERROR&#010;&gt; org.apache.flink.runtime.util.FatalExitExceptionHandler       - FATAL:&#010;&gt; Thread 'flink-akka.actor.default-dispatcher-4673' produced an uncaught&#010;&gt; exception. Stopping the process...&#010;&gt; java.lang.OutOfMemoryError: unable to create new native thread&#010;&gt;         at java.lang.Thread.start0(Native Method)&#010;&gt;         at java.lang.Thread.start(Thread.java:717)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.tryAddWorker(ForkJoinPool.java:1672)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.signalWork(ForkJoinPool.java:1966)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.fullExternalPush(ForkJoinPool.java:1905)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.externalPush(ForkJoinPool.java:1834)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.execute(ForkJoinPool.java:2955)&#010;&gt;         at&#010;&gt; akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool.execute(ForkJoinExecutorConfigurator.scala:30)&#010;&gt;         at&#010;&gt; akka.dispatch.ExecutorServiceDelegate.execute(ThreadPoolBuilder.scala:211)&#010;&gt;         at&#010;&gt; akka.dispatch.ExecutorServiceDelegate.execute$(ThreadPoolBuilder.scala:211)&#010;&gt;         at&#010;&gt; akka.dispatch.Dispatcher$LazyExecutorServiceDelegate.execute(Dispatcher.scala:39)&#010;&gt;         at&#010;&gt; akka.dispatch.Dispatcher.registerForExecution(Dispatcher.scala:115)&#010;&gt;         at akka.dispatch.Dispatcher.dispatch(Dispatcher.scala:55)&#010;&gt;         at akka.actor.dungeon.Dispatch.sendMessage(Dispatch.scala:142)&#010;&gt;         at akka.actor.dungeon.Dispatch.sendMessage$(Dispatch.scala:136)&#010;&gt;         at akka.actor.ActorCell.sendMessage(ActorCell.scala:429)&#010;&gt;         at akka.actor.Cell.sendMessage(ActorCell.scala:350)&#010;&gt;         at akka.actor.Cell.sendMessage$(ActorCell.scala:349)&#010;&gt;         at akka.actor.ActorCell.sendMessage(ActorCell.scala:429)&#010;&gt;         at&#010;&gt; akka.actor.RepointableActorRef.$bang(RepointableActorRef.scala:173)&#010;&gt;         at akka.actor.Scheduler$$anon$3.run(Scheduler.scala:171)&#010;&gt;         at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)&#010;&gt;         at&#010;&gt; akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt;         at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt;&#010;&gt; thanks all / by nicygan&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<372ad87b.2046.1734b294298.Coremail.read3210@163.com>"
    }
]