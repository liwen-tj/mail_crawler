[
    {
        "id": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>",
        "from": "张浩 &lt;13669299...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 04:48:52 GMT",
        "subject": "滑动窗口数据存储多份问题",
        "content": "Hi,all!&#010;由于第一次咨询，我不确定上一份邮件大家是否收到。&#010;想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#010;份？&#010;&#010;&#010;| |&#010;张浩&#010;|&#010;|&#010;13669299054@163.com&#010;|&#010;签名由网易邮箱大师定制",
        "depth": "0",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<CAA8tFvtPsQUMrEisrU+QFgpz=VwM_HDVGehWztxJ7muk6+h-=g@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 05:46:03 GMT",
        "subject": "Re: 滑动窗口数据存储多份问题",
        "content": "Hi&#013;&#010;现在的实现是这样的，每条数据会在每个窗口中存一份&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#013;&#010;&#013;&#010;&gt; Hi,all!&#013;&#010;&gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#013;&#010;&gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#013;&#010;&gt; 份？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 张浩&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 13669299054@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;",
        "depth": "1",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<CAA8tFvu-3BgGa2SG6uyOTySyT88rgOw-BMdRV4Wg4+cwu0=ahw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 12:56:05 GMT",
        "subject": "Re: 滑动窗口数据存储多份问题",
        "content": "Hi&#013;&#010;&#013;&#010;我理解，如果只存取一份的话，state 的管理会变得麻烦一些（所有需要这份数据的窗口都需要去某个地方取，&#010;state 什么时候清理逻辑也会变得麻烦一些）&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月6日周一 下午1:57写道：&#013;&#010;&#013;&#010;&gt; 你好，我的思考是便于在状态信息中清除或者提取每一个窗口的数据信息。&#013;&#010;&gt; 不知道，我这样理解的对吗？&#013;&#010;&gt; 另外，为什么我们不能只存储一份数据呢？&#013;&#010;&gt; 非常感谢与您交流！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩&#013;&#010;&gt; 邮箱：zhanghao_waxm@163.com&#013;&#010;&gt;&#013;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=%E5%BC%A0%E6%B5%A9&amp;uid=13669299054%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Azhanghao_waxm%40163.com%22%5D&gt;&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt;&#010;定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月06日 13:46，Congxian Qiu &lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt; 现在的实现是这样的，每条数据会在每个窗口中存一份&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi,all!&#013;&#010;&gt; &gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#013;&#010;&gt; &gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#013;&#010;&gt; &gt; 份？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; 张浩&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 13669299054@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 签名由网易邮箱大师定制&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<CAA8tFvu8W9H76Vp_aTgQXOEHhV0OMkdaHp0V8HcZT25f8rFG4A@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 08:23:41 GMT",
        "subject": "Re: 滑动窗口数据存储多份问题",
        "content": "Hi&#013;&#010;&#013;&#010;每个窗口都是一个单独的 state，至于你认为的不同 state 仅保持引用是不对的。这个你可以使用&#010;RocksDBStateBackend&#013;&#010;来考虑，RocksDBStateBackend 中会把 state 序列化成 bytes，然后写到 RocksDB&#010;中，就是每个 State&#013;&#010;中都会有一份。&#013;&#010;&#013;&#010;PS：回复邮件的时候可以选择「全部回复」这样就能够加上 \"user-zh@flink.apache.org\"），这样我们的邮件所有人都能看到了&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月7日周二 上午10:34写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; Hi,我通过看源码发现每条数据到达时，是分配给了所有的窗口，但是我理解这单条数据是不是只是传递给了每个窗口，其实在内存中只有一份，窗口状态保持对它的引用，触发一次窗口就删掉对这些数据的引用？&#013;&#010;&gt; 很高兴与您探讨！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩&#013;&#010;&gt; 邮箱：zhanghao_waxm@163.com&#013;&#010;&gt;&#013;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=%E5%BC%A0%E6%B5%A9&amp;uid=13669299054%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Azhanghao_waxm%40163.com%22%5D&gt;&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt;&#010;定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月06日 20:56，Congxian Qiu &lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 我理解，如果只存取一份的话，state 的管理会变得麻烦一些（所有需要这份数据的窗口都需要去某个地方取，&#010;state&#013;&#010;&gt; 什么时候清理逻辑也会变得麻烦一些）&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月6日周一 下午1:57写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 你好，我的思考是便于在状态信息中清除或者提取每一个窗口的数据信息。&#013;&#010;&gt;&gt; 不知道，我这样理解的对吗？&#013;&#010;&gt;&gt; 另外，为什么我们不能只存储一份数据呢？&#013;&#010;&gt;&gt; 非常感谢与您交流！&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 张浩&#013;&#010;&gt;&gt; 邮箱：zhanghao_waxm@163.com&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=%E5%BC%A0%E6%B5%A9&amp;uid=13669299054%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Azhanghao_waxm%40163.com%22%5D&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt;&#010;定制&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 在2020年07月06日 13:46，Congxian Qiu &lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt;&gt; Hi&#013;&#010;&gt;&gt; 现在的实现是这样的，每条数据会在每个窗口中存一份&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best,&#013;&#010;&gt;&gt; Congxian&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt; Hi,all!&#013;&#010;&gt;&gt; &gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#013;&#010;&gt;&gt; &gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#013;&#010;&gt;&gt; &gt; 份？&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; | |&#013;&#010;&gt;&gt; &gt; 张浩&#013;&#010;&gt;&gt; &gt; |&#013;&#010;&gt;&gt; &gt; |&#013;&#010;&gt;&gt; &gt; 13669299054@163.com&#013;&#010;&gt;&gt; &gt; |&#013;&#010;&gt;&gt; &gt; 签名由网易邮箱大师定制&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<35217f05.7f84.1733d10ddde.Coremail.13669299054@163.com>",
        "from": "&quot;Jimmy Zhang&quot; &lt;13669299...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 08:49:42 GMT",
        "subject": "回复：滑动窗口数据存储多份问题",
        "content": "嗯嗯，之前没有选择回复全部，不好意思。&#010;我看源码关于RocksDB这块确实是需要序列化的，所以肯定是多份保存，如果状态后端是heap呢，也是一样的吗？从我测试内存来看，感觉也是多份，只是heapliststate那个类给了我一些困惑😦&#010;&#010;&#010;在2020年07月11日 16:23，Congxian Qiu 写道：&#010;Hi&#010;&#010;&#010;每个窗口都是一个单独的 state，至于你认为的不同 state 仅保持引用是不对的。这个你可以使用&#010;RocksDBStateBackend 来考虑，RocksDBStateBackend 中会把 state 序列化成 bytes，然后写到&#010;RocksDB 中，就是每个 State 中都会有一份。&#010;&#010;&#010;PS：回复邮件的时候可以选择「全部回复」这样就能够加上 \"user-zh@flink.apache.org\"），这样我们的邮件所有人都能看到了&#010;&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;&#010;&#010;张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月7日周二 上午10:34写道：&#010;&#010;Hi,我通过看源码发现每条数据到达时，是分配给了所有的窗口，但是我理解这单条数据是不是只是传递给了每个窗口，其实在内存中只有一份，窗口状态保持对它的引用，触发一次窗口就删掉对这些数据的引用？&#010;很高兴与您探讨！&#010;&#010;&#010;&#010;&#010;| |&#010;张浩&#010;|&#010;|&#010;邮箱：zhanghao_waxm@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月06日 20:56，Congxian Qiu 写道：&#010;Hi&#010;&#010;&#010;我理解，如果只存取一份的话，state 的管理会变得麻烦一些（所有需要这份数据的窗口都需要去某个地方取，&#010;state 什么时候清理逻辑也会变得麻烦一些）&#010;&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;&#010;&#010;张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月6日周一 下午1:57写道：&#010;&#010;你好，我的思考是便于在状态信息中清除或者提取每一个窗口的数据信息。&#010;不知道，我这样理解的对吗？&#010;另外，为什么我们不能只存储一份数据呢？&#010;非常感谢与您交流！&#010;&#010;&#010;&#010;&#010;| |&#010;张浩&#010;|&#010;|&#010;邮箱：zhanghao_waxm@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月06日 13:46，Congxian Qiu 写道：&#010;Hi&#010;现在的实现是这样的，每条数据会在每个窗口中存一份&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#010;&#010;&gt; Hi,all!&#010;&gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#010;&gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#010;&gt; 份？&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; 张浩&#010;&gt; |&#010;&gt; |&#010;&gt; 13669299054@163.com&#010;&gt; |&#010;&gt; 签名由网易邮箱大师定制&#010;",
        "depth": "4",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<CAA8tFvtaV3vw5J_vEVOdVF7DFvu66cg+ifVJO8JCgrX2wP7tkg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 13:02:33 GMT",
        "subject": "Re: 滑动窗口数据存储多份问题",
        "content": "Hi&#013;&#010;   你说的 HeapListState 的困惑具体是什么呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Jimmy Zhang &lt;13669299054@163.com&gt; 于2020年7月11日周六 下午4:50写道：&#013;&#010;&#013;&#010;&gt; 嗯嗯，之前没有选择回复全部，不好意思。&#013;&#010;&gt;&#013;&#010;&gt; 我看源码关于RocksDB这块确实是需要序列化的，所以肯定是多份保存，如果状态后端是heap呢，也是一样的吗？从我测试内存来看，感觉也是多份，只是heapliststate那个类给了我一些困惑😦&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月11日 16:23，Congxian Qiu 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 每个窗口都是一个单独的 state，至于你认为的不同 state 仅保持引用是不对的。这个你可以使用&#010;RocksDBStateBackend&#013;&#010;&gt; 来考虑，RocksDBStateBackend 中会把 state 序列化成 bytes，然后写到 RocksDB&#010;中，就是每个 State&#013;&#010;&gt; 中都会有一份。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; PS：回复邮件的时候可以选择「全部回复」这样就能够加上 \"user-zh@flink.apache.org\"），这样我们的邮件所有人都能看到了&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月7日周二 上午10:34写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi,我通过看源码发现每条数据到达时，是分配给了所有的窗口，但是我理解这单条数据是不是只是传递给了每个窗口，其实在内存中只有一份，窗口状态保持对它的引用，触发一次窗口就删掉对这些数据的引用？&#013;&#010;&gt; 很高兴与您探讨！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 张浩&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：zhanghao_waxm@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月06日 20:56，Congxian Qiu 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我理解，如果只存取一份的话，state 的管理会变得麻烦一些（所有需要这份数据的窗口都需要去某个地方取，&#010;state&#013;&#010;&gt; 什么时候清理逻辑也会变得麻烦一些）&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月6日周一 下午1:57写道：&#013;&#010;&gt;&#013;&#010;&gt; 你好，我的思考是便于在状态信息中清除或者提取每一个窗口的数据信息。&#013;&#010;&gt; 不知道，我这样理解的对吗？&#013;&#010;&gt; 另外，为什么我们不能只存储一份数据呢？&#013;&#010;&gt; 非常感谢与您交流！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 张浩&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：zhanghao_waxm@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月06日 13:46，Congxian Qiu 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt; 现在的实现是这样的，每条数据会在每个窗口中存一份&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi,all!&#013;&#010;&gt; &gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#013;&#010;&gt; &gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#013;&#010;&gt; &gt; 份？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; 张浩&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 13669299054@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 签名由网易邮箱大师定制&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<13bbe3a0.9246.173405ebc81.Coremail.13669299054@163.com>",
        "from": "&quot;Jimmy Zhang&quot; &lt;13669299...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 12 Jul 2020 00:13:37 GMT",
        "subject": "回复： 滑动窗口数据存储多份问题",
        "content": "Hi,all！&#010;从WindowOperator.java的processElement方法跟进去，使用windowState.add(element.getValue());添加数据，这里面找到add方法的HeapListState类的实现，&#010;&#010;&#010;@Override&#010; public void add(V value) {&#010;  Preconditions.checkNotNull(value, \"You cannot add null to a ListState.\");&#010;  final N namespace = currentNamespace;&#010;  final StateTable&lt;K, N, List&lt;V&gt;&gt; map = stateTable;&#010;  List&lt;V&gt; list = map.get(namespace);&#010;  if (list == null) {&#010;   list = new ArrayList&lt;&gt;();&#010;   map.put(namespace, list);&#010;  }&#010;  list.add(value);&#010; }&#010;就是这个方法，让我产生了 “此value只真实存在一份”的困惑！&#010;|&#010;Best,&#010;Jimmy&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月11日 21:02，Congxian Qiu&lt;qcx978132955@gmail.com&gt; 写道：&#010;Hi&#010;你说的 HeapListState 的困惑具体是什么呢？&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Jimmy Zhang &lt;13669299054@163.com&gt; 于2020年7月11日周六 下午4:50写道：&#010;&#010;嗯嗯，之前没有选择回复全部，不好意思。&#010;&#010;我看源码关于RocksDB这块确实是需要序列化的，所以肯定是多份保存，如果状态后端是heap呢，也是一样的吗？从我测试内存来看，感觉也是多份，只是heapliststate那个类给了我一些困惑😦&#010;&#010;&#010;在2020年07月11日 16:23，Congxian Qiu 写道：&#010;Hi&#010;&#010;&#010;每个窗口都是一个单独的 state，至于你认为的不同 state 仅保持引用是不对的。这个你可以使用&#010;RocksDBStateBackend&#010;来考虑，RocksDBStateBackend 中会把 state 序列化成 bytes，然后写到 RocksDB&#010;中，就是每个 State&#010;中都会有一份。&#010;&#010;&#010;PS：回复邮件的时候可以选择「全部回复」这样就能够加上 \"user-zh@flink.apache.org\"），这样我们的邮件所有人都能看到了&#010;&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;&#010;&#010;张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月7日周二 上午10:34写道：&#010;&#010;&#010;Hi,我通过看源码发现每条数据到达时，是分配给了所有的窗口，但是我理解这单条数据是不是只是传递给了每个窗口，其实在内存中只有一份，窗口状态保持对它的引用，触发一次窗口就删掉对这些数据的引用？&#010;很高兴与您探讨！&#010;&#010;&#010;&#010;&#010;| |&#010;张浩&#010;|&#010;|&#010;邮箱：zhanghao_waxm@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月06日 20:56，Congxian Qiu 写道：&#010;Hi&#010;&#010;&#010;我理解，如果只存取一份的话，state 的管理会变得麻烦一些（所有需要这份数据的窗口都需要去某个地方取，&#010;state&#010;什么时候清理逻辑也会变得麻烦一些）&#010;&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;&#010;&#010;张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月6日周一 下午1:57写道：&#010;&#010;你好，我的思考是便于在状态信息中清除或者提取每一个窗口的数据信息。&#010;不知道，我这样理解的对吗？&#010;另外，为什么我们不能只存储一份数据呢？&#010;非常感谢与您交流！&#010;&#010;&#010;&#010;&#010;| |&#010;张浩&#010;|&#010;|&#010;邮箱：zhanghao_waxm@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月06日 13:46，Congxian Qiu 写道：&#010;Hi&#010;现在的实现是这样的，每条数据会在每个窗口中存一份&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#010;&#010;Hi,all!&#010;由于第一次咨询，我不确定上一份邮件大家是否收到。&#010;想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#010;份？&#010;&#010;&#010;| |&#010;张浩&#010;|&#010;|&#010;13669299054@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;&#010;",
        "depth": "6",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<CAA8tFvst_jtxEweNixhgWB2OQ86YL6d-+SUYWpE6eEBcchOmOg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 11:58:00 GMT",
        "subject": "Re: 滑动窗口数据存储多份问题",
        "content": "Hi&#013;&#010;&#013;&#010;从 HeapListState#add 这里看是的，我跟了一个 WindowOperator 到最终 HeapListState&#013;&#010;的逻辑，这里确实是只有一份数据，没有拷贝。这个东西的实现可能是因为性能好，我尝试确认下这个原因，多谢你的提问。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Jimmy Zhang &lt;13669299054@163.com&gt; 于2020年7月12日周日 上午8:13写道：&#013;&#010;&#013;&#010;&gt; Hi,all！&#013;&#010;&gt;&#013;&#010;&gt; 从WindowOperator.java的processElement方法跟进去，使用windowState.add(element.getValue());添加数据，这里面找到add方法的HeapListState类的实现，&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; @Override&#013;&#010;&gt;  public void add(V value) {&#013;&#010;&gt;   Preconditions.checkNotNull(value, \"You cannot add null to a ListState.\");&#013;&#010;&gt;   final N namespace = currentNamespace;&#013;&#010;&gt;   final StateTable&lt;K, N, List&lt;V&gt;&gt; map = stateTable;&#013;&#010;&gt;   List&lt;V&gt; list = map.get(namespace);&#013;&#010;&gt;   if (list == null) {&#013;&#010;&gt;    list = new ArrayList&lt;&gt;();&#013;&#010;&gt;    map.put(namespace, list);&#013;&#010;&gt;   }&#013;&#010;&gt;   list.add(value);&#013;&#010;&gt;  }&#013;&#010;&gt; 就是这个方法，让我产生了 “此value只真实存在一份”的困惑！&#013;&#010;&gt; |&#013;&#010;&gt; Best,&#013;&#010;&gt; Jimmy&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月11日 21:02，Congxian Qiu&lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt; 你说的 HeapListState 的困惑具体是什么呢？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Jimmy Zhang &lt;13669299054@163.com&gt; 于2020年7月11日周六 下午4:50写道：&#013;&#010;&gt;&#013;&#010;&gt; 嗯嗯，之前没有选择回复全部，不好意思。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我看源码关于RocksDB这块确实是需要序列化的，所以肯定是多份保存，如果状态后端是heap呢，也是一样的吗？从我测试内存来看，感觉也是多份，只是heapliststate那个类给了我一些困惑😦&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月11日 16:23，Congxian Qiu 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 每个窗口都是一个单独的 state，至于你认为的不同 state 仅保持引用是不对的。这个你可以使用&#010;RocksDBStateBackend&#013;&#010;&gt; 来考虑，RocksDBStateBackend 中会把 state 序列化成 bytes，然后写到 RocksDB&#010;中，就是每个 State&#013;&#010;&gt; 中都会有一份。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; PS：回复邮件的时候可以选择「全部回复」这样就能够加上 \"user-zh@flink.apache.org\"），这样我们的邮件所有人都能看到了&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月7日周二 上午10:34写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi,我通过看源码发现每条数据到达时，是分配给了所有的窗口，但是我理解这单条数据是不是只是传递给了每个窗口，其实在内存中只有一份，窗口状态保持对它的引用，触发一次窗口就删掉对这些数据的引用？&#013;&#010;&gt; 很高兴与您探讨！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 张浩&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：zhanghao_waxm@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月06日 20:56，Congxian Qiu 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我理解，如果只存取一份的话，state 的管理会变得麻烦一些（所有需要这份数据的窗口都需要去某个地方取，&#010;state&#013;&#010;&gt; 什么时候清理逻辑也会变得麻烦一些）&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月6日周一 下午1:57写道：&#013;&#010;&gt;&#013;&#010;&gt; 你好，我的思考是便于在状态信息中清除或者提取每一个窗口的数据信息。&#013;&#010;&gt; 不知道，我这样理解的对吗？&#013;&#010;&gt; 另外，为什么我们不能只存储一份数据呢？&#013;&#010;&gt; 非常感谢与您交流！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 张浩&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：zhanghao_waxm@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月06日 13:46，Congxian Qiu 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt; 现在的实现是这样的，每条数据会在每个窗口中存一份&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#013;&#010;&gt;&#013;&#010;&gt; Hi,all!&#013;&#010;&gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#013;&#010;&gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#013;&#010;&gt; 份？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 张浩&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 13669299054@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<15ed1e7f.61a6.17348b48c38.Coremail.zhanghao_waxm@163.com>",
        "from": "&quot;Jimmy Zhang&quot; &lt;zhanghao_w...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 15:04:18 GMT",
        "subject": "回复：滑动窗口数据存储多份问题",
        "content": "Hi！&#010;好的，非常感谢！&#010;很期待你接下来的回复。&#010;&#010;&#010;|&#010;Best,&#010;Jimmy&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月13日 19:58，Congxian Qiu 写道：&#010;Hi&#010;&#010;从 HeapListState#add 这里看是的，我跟了一个 WindowOperator 到最终 HeapListState&#010;的逻辑，这里确实是只有一份数据，没有拷贝。这个东西的实现可能是因为性能好，我尝试确认下这个原因，多谢你的提问。&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Jimmy Zhang &lt;13669299054@163.com&gt; 于2020年7月12日周日 上午8:13写道：&#010;&#010;&gt; Hi,all！&#010;&gt;&#010;&gt; 从WindowOperator.java的processElement方法跟进去，使用windowState.add(element.getValue());添加数据，这里面找到add方法的HeapListState类的实现，&#010;&gt;&#010;&gt;&#010;&gt; @Override&#010;&gt;  public void add(V value) {&#010;&gt;   Preconditions.checkNotNull(value, \"You cannot add null to a ListState.\");&#010;&gt;   final N namespace = currentNamespace;&#010;&gt;   final StateTable&lt;K, N, List&lt;V&gt;&gt; map = stateTable;&#010;&gt;   List&lt;V&gt; list = map.get(namespace);&#010;&gt;   if (list == null) {&#010;&gt;    list = new ArrayList&lt;&gt;();&#010;&gt;    map.put(namespace, list);&#010;&gt;   }&#010;&gt;   list.add(value);&#010;&gt;  }&#010;&gt; 就是这个方法，让我产生了 “此value只真实存在一份”的困惑！&#010;&gt; |&#010;&gt; Best,&#010;&gt; Jimmy&#010;&gt; |&#010;&gt; 签名由网易邮箱大师定制&#010;&gt; 在2020年7月11日 21:02，Congxian Qiu&lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt; Hi&#010;&gt; 你说的 HeapListState 的困惑具体是什么呢？&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; Jimmy Zhang &lt;13669299054@163.com&gt; 于2020年7月11日周六 下午4:50写道：&#010;&gt;&#010;&gt; 嗯嗯，之前没有选择回复全部，不好意思。&#010;&gt;&#010;&gt;&#010;&gt; 我看源码关于RocksDB这块确实是需要序列化的，所以肯定是多份保存，如果状态后端是heap呢，也是一样的吗？从我测试内存来看，感觉也是多份，只是heapliststate那个类给了我一些困惑😦&#010;&gt;&#010;&gt;&#010;&gt; 在2020年07月11日 16:23，Congxian Qiu 写道：&#010;&gt; Hi&#010;&gt;&#010;&gt;&#010;&gt; 每个窗口都是一个单独的 state，至于你认为的不同 state 仅保持引用是不对的。这个你可以使用&#010;RocksDBStateBackend&#010;&gt; 来考虑，RocksDBStateBackend 中会把 state 序列化成 bytes，然后写到 RocksDB&#010;中，就是每个 State&#010;&gt; 中都会有一份。&#010;&gt;&#010;&gt;&#010;&gt; PS：回复邮件的时候可以选择「全部回复」这样就能够加上 \"user-zh@flink.apache.org\"），这样我们的邮件所有人都能看到了&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月7日周二 上午10:34写道：&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Hi,我通过看源码发现每条数据到达时，是分配给了所有的窗口，但是我理解这单条数据是不是只是传递给了每个窗口，其实在内存中只有一份，窗口状态保持对它的引用，触发一次窗口就删掉对这些数据的引用？&#010;&gt; 很高兴与您探讨！&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; 张浩&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：zhanghao_waxm@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; 在2020年07月06日 20:56，Congxian Qiu 写道：&#010;&gt; Hi&#010;&gt;&#010;&gt;&#010;&gt; 我理解，如果只存取一份的话，state 的管理会变得麻烦一些（所有需要这份数据的窗口都需要去某个地方取，&#010;state&#010;&gt; 什么时候清理逻辑也会变得麻烦一些）&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 张浩 &lt;zhanghao_waxm@163.com&gt; 于2020年7月6日周一 下午1:57写道：&#010;&gt;&#010;&gt; 你好，我的思考是便于在状态信息中清除或者提取每一个窗口的数据信息。&#010;&gt; 不知道，我这样理解的对吗？&#010;&gt; 另外，为什么我们不能只存储一份数据呢？&#010;&gt; 非常感谢与您交流！&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; 张浩&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：zhanghao_waxm@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; 在2020年07月06日 13:46，Congxian Qiu 写道：&#010;&gt; Hi&#010;&gt; 现在的实现是这样的，每条数据会在每个窗口中存一份&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; 张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#010;&gt;&#010;&gt; Hi,all!&#010;&gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#010;&gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#010;&gt; 份？&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; 张浩&#010;&gt; |&#010;&gt; |&#010;&gt; 13669299054@163.com&#010;&gt; |&#010;&gt; 签名由网易邮箱大师定制&#010;&gt;&#010;&gt;&#010;",
        "depth": "8",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<CAHtLNa-ws938nWUsmB=yrS3tU_sFukXyARcGJOMGnPrGib4XvQ@mail.gmail.com>",
        "from": "Tianwang Li &lt;litianw...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 01:53:56 GMT",
        "subject": "Re: 滑动窗口数据存储多份问题",
        "content": "| 为什么使用 datastream api 的话，滑动窗口对于每条数据都会在 state 中存&#010;size / slide&#013;&#010;可以使用blink 的SQL，是通过pane 实现的，输出的时候才合并每个pane。参考`PanedWindowAssigner`&#013;&#010;&#013;&#010;&#013;&#010;张浩 &lt;13669299054@163.com&gt; 于2020年7月6日周一 下午12:49写道：&#013;&#010;&#013;&#010;&gt; Hi,all!&#013;&#010;&gt; 由于第一次咨询，我不确定上一份邮件大家是否收到。&#013;&#010;&gt; 想咨询下大家，为什么使用 datastream api 的话，滑动窗口对于每条数据都会在&#010;state 中存 size / slide&#013;&#010;&gt; 份？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 张浩&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 13669299054@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;**************************************&#013;&#010; tivanli&#013;&#010;**************************************&#013;&#010;",
        "depth": "1",
        "reply": "<4db08dca.8edf.173227496d7.Coremail.13669299054@163.com>"
    },
    {
        "id": "<tencent_114A2EB197668B73835E1E611B5042E11708@qq.com>",
        "from": "&quot;Zhou Zach&quot; &lt;is...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 05:11:45 GMT",
        "subject": "flink cep result DataStream no data print",
        "content": "code:&#013;&#010;&#013;&#010;&#013;&#010;val inpurtDS = streamTableEnv.toAppendStream[BehaviorInfo](behaviorTable)inpurtDS.print()val&#010;pattern = Pattern.begin[BehaviorInfo](\"start\")&#013;&#010;  .where(_.clickCount &amp;gt; 7)val patternStream = CEP.pattern(inpurtDS, pattern)&#013;&#010;val result: DataStream[BehaviorInfo] = patternStream.process(&#013;&#010;  new PatternProcessFunction[BehaviorInfo, BehaviorInfo]() {&#013;&#010;    override def processMatch(&#013;&#010;                               matchPattern: util.Map[String, util.List[BehaviorInfo",
        "depth": "0",
        "reply": "<tencent_114A2EB197668B73835E1E611B5042E11708@qq.com>"
    },
    {
        "id": "<tencent_2E96BC0180B93ED14E88FE516BAC6A271107@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 08:59:35 GMT",
        "subject": "Flink状态调试",
        "content": "Hi， 各位大佬们，请教一下： Flink的checkpoint怎么调试啊，我想看程序目前的状态，拿到了checkpoint的文件，打开后有一些东西是乱码，没有结构性，有方法吗？",
        "depth": "0",
        "reply": "<tencent_2E96BC0180B93ED14E88FE516BAC6A271107@qq.com>"
    },
    {
        "id": "<CAA8tFvu0=LwgVqRLkxKtUKvp2FWVfeMopyYgoiHubCQjQbuCCg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 11:21:52 GMT",
        "subject": "Re: Flink状态调试",
        "content": "Hi&#013;&#010;想 debug checkpoint 文件的话，可以参考下这个 UT[1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://github.com/apache/flink/blob/master/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointMetadataLoadingTest.java&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Z-Z &lt;zz9876543210@qq.com&gt; 于2020年7月6日周一 下午4:59写道：&#013;&#010;&#013;&#010;&gt; Hi， 各位大佬们，请教一下：&#013;&#010;&gt; Flink的checkpoint怎么调试啊，我想看程序目前的状态，拿到了checkpoint的文件，打开后有一些东西是乱码，没有结构性，有方法吗？&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_2E96BC0180B93ED14E88FE516BAC6A271107@qq.com>"
    },
    {
        "id": "<CAOMLN=aMcg5EPDF7DH5gV_WiKNHgrbcvpwW6N6GVQKHarQdo8Q@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 11:33:49 GMT",
        "subject": "Re: Flink状态调试",
        "content": "Hi Z-Z,&#013;&#010;&#013;&#010;如果你想查看的是程序中的state内容，建议触发一次savepoint并搭配state&#010;processor api来查询。&#013;&#010;&#013;&#010;参考&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/libs/state_processor_api.html&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月6日周一 下午7:22写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt; 想 debug checkpoint 文件的话，可以参考下这个 UT[1]&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/master/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointMetadataLoadingTest.java&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Z-Z &lt;zz9876543210@qq.com&gt; 于2020年7月6日周一 下午4:59写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi， 各位大佬们，请教一下：&#013;&#010;&gt; &gt; Flink的checkpoint怎么调试啊，我想看程序目前的状态，拿到了checkpoint的文件，打开后有一些东西是乱码，没有结构性，有方法吗？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_2E96BC0180B93ED14E88FE516BAC6A271107@qq.com>"
    },
    {
        "id": "<CAOMLN=a8N85f1FbvhpGArFFj7ep8Zp1Qfv-3GZ4i47CRFY24ag@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 11:36:13 GMT",
        "subject": "Re: Flink状态调试",
        "content": "抱歉，我似乎回错邮件了，应该使用回复全部？&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月6日周一 下午7:22写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt; 想 debug checkpoint 文件的话，可以参考下这个 UT[1]&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/master/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointMetadataLoadingTest.java&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Z-Z &lt;zz9876543210@qq.com&gt; 于2020年7月6日周一 下午4:59写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi， 各位大佬们，请教一下：&#013;&#010;&gt; &gt; Flink的checkpoint怎么调试啊，我想看程序目前的状态，拿到了checkpoint的文件，打开后有一些东西是乱码，没有结构性，有方法吗？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_2E96BC0180B93ED14E88FE516BAC6A271107@qq.com>"
    },
    {
        "id": "<SG2PR02MB3226CB1A5F94EBD8149A4F5AF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>",
        "from": "王 outlook &lt;deadwi...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 09:06:47 GMT",
        "subject": "Flink SQL 关键字 user？",
        "content": "我发现我创建表 字段名为 user会报错。 user是关键字吗，还是其他原因&#013;&#010;&#013;&#010;&#013;&#010;\"CREATE TABLE ods_foo (\\n\" +&#013;&#010;\"    id INT,\\n\" +&#013;&#010;\"    user ARRAY&lt;ROW&lt;name STRING&gt;&gt;\\n\" +&#013;&#010;\") WITH (&#013;&#010;&#013;&#010;Exception in thread \"main\" org.apache.flink.table.api.SqlParserException: SQL parse failed.&#010;Encountered \"user\" at line 3, column 5.&#013;&#010;Was expecting one of:&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<SG2PR02MB3226CB1A5F94EBD8149A4F5AF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<CABKuJ_QcdhfZy7Tw0qcD18e-_Guf=rtbU3y8vo4uHufoG6ZEZQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 09:14:30 GMT",
        "subject": "Re: Flink SQL 关键字 user？",
        "content": "是的，user是关键字，关键字列表可以参考[1].&#013;&#010;&#013;&#010;如果遇到关键字，可以使用 ` 来处理，比如：&#013;&#010;CREATE TABLE `user` (...) WITH (...);&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/#reserved-keywords&#013;&#010;&#013;&#010;王 outlook &lt;deadwind4@outlook.com&gt; 于2020年7月6日周一 下午5:07写道：&#013;&#010;&#013;&#010;&gt; 我发现我创建表 字段名为 user会报错。 user是关键字吗，还是其他原因&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; \"CREATE TABLE ods_foo (\\n\" +&#013;&#010;&gt; \"    id INT,\\n\" +&#013;&#010;&gt; \"    user ARRAY&lt;ROW&lt;name STRING&gt;&gt;\\n\" +&#013;&#010;&gt; \") WITH (&#013;&#010;&gt;&#013;&#010;&gt; Exception in thread \"main\" org.apache.flink.table.api.SqlParserException:&#013;&#010;&gt; SQL parse failed. Encountered \"user\" at line 3, column 5.&#013;&#010;&gt; Was expecting one of:&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<SG2PR02MB3226CB1A5F94EBD8149A4F5AF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<9F3C61B9-2511-471C-BDE0-2E8DEAF28205@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 09:17:03 GMT",
        "subject": "Re: Flink SQL 关键字 user？",
        "content": "Hi，&#010;是的，用`user`转义处理下，&#010;完整的保留关键字参考[1]&#010;&#010;Best,&#010;Leonard&#010;[1] https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/ &lt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/&gt;&#010;&#010;&gt; 在 2020年7月6日，17:06，王 outlook &lt;deadwind4@outlook.com&gt; 写道：&#010;&gt; &#010;&gt; \"CREATE TABLE ods_foo (\\n\" +&#010;&gt; \"    id INT,\\n\" +&#010;&gt; \"    user ARRAY&lt;ROW&lt;name STRING&gt;&gt;\\n\" +&#010;&gt; \") WITH (&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<SG2PR02MB3226CB1A5F94EBD8149A4F5AF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<7bec36e8.a32f.173237c3931.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 09:36:50 GMT",
        "subject": "jemalloc dump 内存",
        "content": "hi，社区的各位，是否有配置过jemalloc？&#010;&#010;目前在docker容器中放入编译过后的jemalloc，在flink的配置文件加入如下配置&#010;&#010;containerized.taskmanager.env.LD_PRELOAD: /opt/jemalloc/lib/libjemalloc.so&#010;   containerized.taskmanager.env.MALLOC_CONF: prof:true,lg_prof_interval:25,lg_prof_sample:17,prof_prefix:/opt/state/jeprof.out&#010;&#010;结果生成不到对应的heap文件， /opt/state挂载在宿主机的磁盘上，权限给了。&#010;&#010;请问该如何操作才可以生产内存dump呢？&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "0",
        "reply": "<7bec36e8.a32f.173237c3931.Coremail.a511955993@163.com>"
    },
    {
        "id": "<65f2955d.2b0e.17323faea74.Coremail.milan183sansiro@126.com>",
        "from": "milan183sansiro &lt;milan183sans...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 11:55:13 GMT",
        "subject": "Flink从SavePoint启动任务，修改的代码不生效",
        "content": "各位好：&#010;        背景：Flink版本1.6.4，数据源为Kafka的主题A，B，消费者组相同&#010;        操作步骤：1.使用SavePoint取消任务。&#010;        2.修改代码将B去掉，只消费A主题。&#010;        3.从SavePoint启动任务，发现消费者组在B主题下的偏移量也回到了任务停止时的偏移量，之后偏移量马上变成了最新并继续消费。&#010;        想知道为什么修改代码不生效。&#010;&#010;",
        "depth": "0",
        "reply": "<65f2955d.2b0e.17323faea74.Coremail.milan183sansiro@126.com>"
    },
    {
        "id": "<tencent_377613F7DF05ED00751BA8BA71AF7E12A205@qq.com>",
        "from": "&quot;wujunxi&quot; &lt;462329...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 12:32:38 GMT",
        "subject": "回复：Flink从SavePoint启动任务，修改的代码不生效",
        "content": "你好，确认以下两个点&#013;&#010;1.是否给每个算子设置了id&#013;&#010;2.设置savepoint恢复的路径是否正确&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"milan183sansiro\"&lt;milan183sansiro@126.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 晚上7:55&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Flink从SavePoint启动任务，修改的代码不生效&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;各位好：&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 背景：Flink版本1.6.4，数据源为Kafka的主题A，B，消费者组相同&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 操作步骤：1.使用SavePoint取消任务。&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 2.修改代码将B去掉，只消费A主题。&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 3.从SavePoint启动任务，发现消费者组在B主题下的偏移量也回到了任务停止时的偏移量，之后偏移量马上变成了最新并继续消费。&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 想知道为什么修改代码不生效。",
        "depth": "1",
        "reply": "<65f2955d.2b0e.17323faea74.Coremail.milan183sansiro@126.com>"
    },
    {
        "id": "<4e9fd0da.2bb3.1732432613f.Coremail.milan183sansiro@126.com>",
        "from": "milan183sansiro &lt;milan183sans...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 12:55:48 GMT",
        "subject": "回复：Flink从SavePoint启动任务，修改的代码不生效",
        "content": "你好：&#010;    1.没有给算子手动设置id&#010;    2.设置savepoint恢复的路径是正确的&#010;&#010;&#010;在2020年7月6日 20:32，wujunxi&lt;462329521@qq.com&gt; 写道：&#010;你好，确认以下两个点&#010;1.是否给每个算子设置了id&#010;2.设置savepoint恢复的路径是否正确&#010;&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:&amp;nbsp;\"milan183sansiro\"&lt;milan183sansiro@126.com&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 晚上7:55&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Flink从SavePoint启动任务，修改的代码不生效&#010;&#010;&#010;&#010;各位好：&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 背景：Flink版本1.6.4，数据源为Kafka的主题A，B，消费者组相同&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 操作步骤：1.使用SavePoint取消任务。&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 2.修改代码将B去掉，只消费A主题。&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 3.从SavePoint启动任务，发现消费者组在B主题下的偏移量也回到了任务停止时的偏移量，之后偏移量马上变成了最新并继续消费。&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 想知道为什么修改代码不生效。",
        "depth": "2",
        "reply": "<65f2955d.2b0e.17323faea74.Coremail.milan183sansiro@126.com>"
    },
    {
        "id": "<625E5ECF-672C-4BA1-8A1B-68E563BAF516@gmail.com>",
        "from": "Paul Lam &lt;paullin3...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 02:28:49 GMT",
        "subject": "Re: Flink从SavePoint启动任务，修改的代码不生效",
        "content": "估计你是用同一个 Kafka Source 消费 A B 两个 Topic? 如果是，看起来像是&#010;Kafka Connector 早期的一个问题。&#013;&#010;&#013;&#010;作业停止的时候，Topic B 的 partition offset 被存储到 Savepoint 中，然后在恢复的时候，尽管代码中&#010;Topic B 已经被移除，但它的 partition offset 还是被恢复了。&#013;&#010;&#013;&#010;这个问题在后来的版本，估计是 1.8 或 1.9，被修复了。&#013;&#010;&#013;&#010;Best,&#013;&#010;Paul Lam&#013;&#010;&#013;&#010;&gt; 2020年7月6日 20:55，milan183sansiro &lt;milan183sansiro@126.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 你好：&#013;&#010;&gt;    1.没有给算子手动设置id&#013;&#010;&gt;    2.设置savepoint恢复的路径是正确的&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 在2020年7月6日 20:32，wujunxi&lt;462329521@qq.com&gt; 写道：&#013;&#010;&gt; 你好，确认以下两个点&#013;&#010;&gt; 1.是否给每个算子设置了id&#013;&#010;&gt; 2.设置savepoint恢复的路径是否正确&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"milan183sansiro\"&lt;milan183sansiro@126.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月6日(星期一) 晚上7:55&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt; &#013;&#010;&gt; 主题:&amp;nbsp;Flink从SavePoint启动任务，修改的代码不生效&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 各位好：&#013;&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 背景：Flink版本1.6.4，数据源为Kafka的主题A，B，消费者组相同&#013;&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 操作步骤：1.使用SavePoint取消任务。&#013;&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 2.修改代码将B去掉，只消费A主题。&#013;&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 3.从SavePoint启动任务，发现消费者组在B主题下的偏移量也回到了任务停止时的偏移量，之后偏移量马上变成了最新并继续消费。&#013;&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 想知道为什么修改代码不生效。&#013;&#010;&#013;&#010;",
        "depth": "3",
        "reply": "<65f2955d.2b0e.17323faea74.Coremail.milan183sansiro@126.com>"
    },
    {
        "id": "<773a7ef9.151a.17328d85652.Coremail.milan183sansiro@126.com>",
        "from": "milan183sansiro &lt;milan183sans...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:35:33 GMT",
        "subject": "回复： Flink从SavePoint启动任务，修改的代码不生效",
        "content": "好的，感谢。&#010;&#010;&#010;在2020年7月7日 10:28，Paul Lam&lt;paullin3280@gmail.com&gt; 写道：&#010;估计你是用同一个 Kafka Source 消费 A B 两个 Topic? 如果是，看起来像是&#010;Kafka Connector 早期的一个问题。&#010;&#010;作业停止的时候，Topic B 的 partition offset 被存储到 Savepoint 中，然后在恢复的时候，尽管代码中&#010;Topic B 已经被移除，但它的 partition offset 还是被恢复了。&#010;&#010;这个问题在后来的版本，估计是 1.8 或 1.9，被修复了。&#010;&#010;Best,&#010;Paul Lam&#010;&#010;2020年7月6日 20:55，milan183sansiro &lt;milan183sansiro@126.com&gt; 写道：&#010;&#010;你好：&#010;1.没有给算子手动设置id&#010;2.设置savepoint恢复的路径是正确的&#010;&#010;&#010;在2020年7月6日 20:32，wujunxi&lt;462329521@qq.com&gt; 写道：&#010;你好，确认以下两个点&#010;1.是否给每个算子设置了id&#010;2.设置savepoint恢复的路径是否正确&#010;&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:&amp;nbsp;\"milan183sansiro\"&lt;milan183sansiro@126.com&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 晚上7:55&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Flink从SavePoint启动任务，修改的代码不生效&#010;&#010;&#010;&#010;各位好：&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 背景：Flink版本1.6.4，数据源为Kafka的主题A，B，消费者组相同&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 操作步骤：1.使用SavePoint取消任务。&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 2.修改代码将B去掉，只消费A主题。&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 3.从SavePoint启动任务，发现消费者组在B主题下的偏移量也回到了任务停止时的偏移量，之后偏移量马上变成了最新并继续消费。&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 想知道为什么修改代码不生效。&#010;&#010;",
        "depth": "4",
        "reply": "<65f2955d.2b0e.17323faea74.Coremail.milan183sansiro@126.com>"
    },
    {
        "id": "<tencent_E6101F86411DD7EDAF2067AE75F79E73B408@qq.com>",
        "from": "&quot;claylin&quot; &lt;1012539...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 12:10:36 GMT",
        "subject": "关于jdbc connector扩展问题",
        "content": "hi all我这里有个需求需要从sql里面写数据到clickhouse里面，但是看源码，发现并不好扩展，https://github.com/apache/flink/blob/d04872d2c6b7570ea3ba02f8fc4fca02daa96118/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java#L30,&#010;这里我看直接写死仅支持DerbyDialect、MySQLDialect、PostgresDialect，而且这个类不支持注册jdbc新驱动，如果想在SQL里面支持其他类型的数据库的话，该怎么弄，求支招",
        "depth": "0",
        "reply": "<tencent_E6101F86411DD7EDAF2067AE75F79E73B408@qq.com>"
    },
    {
        "id": "<CAELO933wxUbnM+XuELA=FW8es-YnwrSE--fSW2thaa1gX5jyKg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 03:14:15 GMT",
        "subject": "Re: 关于jdbc connector扩展问题",
        "content": "Hi,&#013;&#010;&#013;&#010;目前 flink-connector-jdbc 还不支持注册&#013;&#010;dialect，社区有这方面的计划，但是目前还没有资源做这一块，这是个比较复杂的功能，需要对接口做细致的设计。&#013;&#010;目前你可以拿 flink-connector-jdbc 源码，加一个自己的 Dialect 类，在 JdbcDialects&#010;中注册进你的&#013;&#010;dialect，然后编译打包就可以了。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Mon, 6 Jul 2020 at 20:10, claylin &lt;1012539884@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi all我这里有个需求需要从sql里面写数据到clickhouse里面，但是看源码，发现并不好扩展，&#013;&#010;&gt; https://github.com/apache/flink/blob/d04872d2c6b7570ea3ba02f8fc4fca02daa96118/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java#L30,&#013;&#010;&gt; 这里我看直接写死仅支持DerbyDialect、MySQLDialect、PostgresDialect，而且这个类不支持注册jdbc新驱动，如果想在SQL里面支持其他类型的数据库的话，该怎么弄，求支招&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_E6101F86411DD7EDAF2067AE75F79E73B408@qq.com>"
    },
    {
        "id": "<SG2PR02MB32264C2A9DC0B95266FB628CF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>",
        "from": "王 outlook &lt;deadwi...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 13:28:55 GMT",
        "subject": "Flink SQL复杂JSON解析",
        "content": "像如下这种JSON输入，&#013;&#010;&#013;&#010;{&#013;&#010;  \"id\": 1,&#013;&#010;  \"many_names\": [&#013;&#010;    {\"name\": \"foo\"},&#013;&#010;    {\"name\": \"bar\"}&#013;&#010;  ]&#013;&#010;}&#013;&#010;&#013;&#010;输出表两行  id 1, name foo  |  id 1, name bar&#013;&#010;&#013;&#010;最佳实践是从Kafka读到后，调用TableFunction这个UDTF转出多行？&#013;&#010;还是Flink SQL有更方便的操作，从Source读出来就能把数组展开。&#013;&#010;&#013;&#010;&#013;&#010;来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;",
        "depth": "0",
        "reply": "<SG2PR02MB32264C2A9DC0B95266FB628CF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<CABKuJ_RNneCogfXVmhPbLUqr1nqLk3K_kx1kJZDhcZpM3YX8Mg@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 14:35:39 GMT",
        "subject": "Re: Flink SQL复杂JSON解析",
        "content": "我理解最佳实践是第一种，先读出来array，再用table function展开成多行。&#013;&#010;实际上把array转成多行是Flink 内置支持的，可以参考[1]的”Expanding arrays&#010;into a relation“部分&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#013;&#010;&#013;&#010;王 outlook &lt;deadwind4@outlook.com&gt; 于2020年7月6日周一 下午9:29写道：&#013;&#010;&#013;&#010;&gt; 像如下这种JSON输入，&#013;&#010;&gt;&#013;&#010;&gt; {&#013;&#010;&gt;   \"id\": 1,&#013;&#010;&gt;   \"many_names\": [&#013;&#010;&gt;     {\"name\": \"foo\"},&#013;&#010;&gt;     {\"name\": \"bar\"}&#013;&#010;&gt;   ]&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt; 输出表两行  id 1, name foo  |  id 1, name bar&#013;&#010;&gt;&#013;&#010;&gt; 最佳实践是从Kafka读到后，调用TableFunction这个UDTF转出多行？&#013;&#010;&gt; 还是Flink SQL有更方便的操作，从Source读出来就能把数组展开。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<SG2PR02MB32264C2A9DC0B95266FB628CF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<SG2PR02MB3226813EDB78AEAB1E1325E7F8670@SG2PR02MB3226.apcprd02.prod.outlook.com>",
        "from": "hua mulan &lt;deadwi...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 12:23:54 GMT",
        "subject": "回复: Flink SQL复杂JSON解析",
        "content": "我试了下 Array里是基本元素可以CROSS JOIN UNNEST直接解开。如果Array里是Row、POJO、Tuple这种复合类型我就只能UDTF了是吧。&#013;&#010;&#013;&#010;来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: Benchao Li &lt;libenchao@apache.org&gt;&#013;&#010;发送时间: 2020年7月6日 22:35&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: Flink SQL复杂JSON解析&#013;&#010;&#013;&#010;我理解最佳实践是第一种，先读出来array，再用table function展开成多行。&#013;&#010;实际上把array转成多行是Flink 内置支持的，可以参考[1]的”Expanding arrays&#010;into a relation“部分&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#013;&#010;&#013;&#010;王 outlook &lt;deadwind4@outlook.com&gt; 于2020年7月6日周一 下午9:29写道：&#013;&#010;&#013;&#010;&gt; 像如下这种JSON输入，&#013;&#010;&gt;&#013;&#010;&gt; {&#013;&#010;&gt;   \"id\": 1,&#013;&#010;&gt;   \"many_names\": [&#013;&#010;&gt;     {\"name\": \"foo\"},&#013;&#010;&gt;     {\"name\": \"bar\"}&#013;&#010;&gt;   ]&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt; 输出表两行  id 1, name foo  |  id 1, name bar&#013;&#010;&gt;&#013;&#010;&gt; 最佳实践是从Kafka读到后，调用TableFunction这个UDTF转出多行？&#013;&#010;&gt; 还是Flink SQL有更方便的操作，从Source读出来就能把数组展开。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;--&#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "2",
        "reply": "<SG2PR02MB32264C2A9DC0B95266FB628CF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<CABKuJ_SFqGVVS_3bAWgY3jTkXY348VXiFDe4DskXtxjwfeKcOw@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 12:46:44 GMT",
        "subject": "Re: Flink SQL复杂JSON解析",
        "content": "看代码应该是支持复合类型的，你可以试下。&#013;&#010;&#013;&#010;hua mulan &lt;deadwind4@outlook.com&gt; 于2020年7月8日周三 下午8:34写道：&#013;&#010;&#013;&#010;&gt; 我试了下 Array里是基本元素可以CROSS JOIN&#013;&#010;&gt; UNNEST直接解开。如果Array里是Row、POJO、Tuple这种复合类型我就只能UDTF了是吧。&#013;&#010;&gt;&#013;&#010;&gt; 来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Benchao Li &lt;libenchao@apache.org&gt;&#013;&#010;&gt; 发送时间: 2020年7月6日 22:35&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: Re: Flink SQL复杂JSON解析&#013;&#010;&gt;&#013;&#010;&gt; 我理解最佳实践是第一种，先读出来array，再用table function展开成多行。&#013;&#010;&gt; 实际上把array转成多行是Flink 内置支持的，可以参考[1]的”Expanding&#010;arrays into a relation“部分&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#013;&#010;&gt;&#013;&#010;&gt; 王 outlook &lt;deadwind4@outlook.com&gt; 于2020年7月6日周一 下午9:29写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 像如下这种JSON输入，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; {&#013;&#010;&gt; &gt;   \"id\": 1,&#013;&#010;&gt; &gt;   \"many_names\": [&#013;&#010;&gt; &gt;     {\"name\": \"foo\"},&#013;&#010;&gt; &gt;     {\"name\": \"bar\"}&#013;&#010;&gt; &gt;   ]&#013;&#010;&gt; &gt; }&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 输出表两行  id 1, name foo  |  id 1, name bar&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 最佳实践是从Kafka读到后，调用TableFunction这个UDTF转出多行？&#013;&#010;&gt; &gt; 还是Flink SQL有更方便的操作，从Source读出来就能把数组展开。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "3",
        "reply": "<SG2PR02MB32264C2A9DC0B95266FB628CF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<SG2PR02MB3226DDD29C7E7BDD071EA28AF8610@SG2PR02MB3226.apcprd02.prod.outlook.com>",
        "from": "hua mulan &lt;deadwi...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:21:28 GMT",
        "subject": "回复: Flink SQL复杂JSON解析",
        "content": "Hi&#013;&#010;&#013;&#010;那我觉得目前最佳实践就是，我用DataStream的API先把数据清洗成 json object&#010;in top level 在导入Kafka，之后再FlinkSQL 处理。&#013;&#010;&#013;&#010;可爱的木兰&#013;&#010;________________________________&#013;&#010;发件人: Benchao Li &lt;libenchao@apache.org&gt;&#013;&#010;发送时间: 2020年7月8日 20:46&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: Flink SQL复杂JSON解析&#013;&#010;&#013;&#010;看代码应该是支持复合类型的，你可以试下。&#013;&#010;&#013;&#010;hua mulan &lt;deadwind4@outlook.com&gt; 于2020年7月8日周三 下午8:34写道：&#013;&#010;&#013;&#010;&gt; 我试了下 Array里是基本元素可以CROSS JOIN&#013;&#010;&gt; UNNEST直接解开。如果Array里是Row、POJO、Tuple这种复合类型我就只能UDTF了是吧。&#013;&#010;&gt;&#013;&#010;&gt; 来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Benchao Li &lt;libenchao@apache.org&gt;&#013;&#010;&gt; 发送时间: 2020年7月6日 22:35&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: Re: Flink SQL复杂JSON解析&#013;&#010;&gt;&#013;&#010;&gt; 我理解最佳实践是第一种，先读出来array，再用table function展开成多行。&#013;&#010;&gt; 实际上把array转成多行是Flink 内置支持的，可以参考[1]的”Expanding&#010;arrays into a relation“部分&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#013;&#010;&gt;&#013;&#010;&gt; 王 outlook &lt;deadwind4@outlook.com&gt; 于2020年7月6日周一 下午9:29写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 像如下这种JSON输入，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; {&#013;&#010;&gt; &gt;   \"id\": 1,&#013;&#010;&gt; &gt;   \"many_names\": [&#013;&#010;&gt; &gt;     {\"name\": \"foo\"},&#013;&#010;&gt; &gt;     {\"name\": \"bar\"}&#013;&#010;&gt; &gt;   ]&#013;&#010;&gt; &gt; }&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 输出表两行  id 1, name foo  |  id 1, name bar&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 最佳实践是从Kafka读到后，调用TableFunction这个UDTF转出多行？&#013;&#010;&gt; &gt; 还是Flink SQL有更方便的操作，从Source读出来就能把数组展开。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 来自 Outlook&lt;http://aka.ms/weboutlook&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;--&#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "3",
        "reply": "<SG2PR02MB32264C2A9DC0B95266FB628CF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<076EC844-1305-4F42-95C2-5168210D2915@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 15:41:59 GMT",
        "subject": "Re: Flink SQL复杂JSON解析",
        "content": "Hi,&#010;&#010;Schema 里可以声明成array, 推荐有拆成多行数据的需求用UDTF处理，现在source里是没看到有拆分多行的实现。&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月6日，21:28，王 outlook &lt;deadwind4@outlook.com&gt; 写道：&#010;&gt; &#010;&gt; TableFunction这个UDTF&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<SG2PR02MB32264C2A9DC0B95266FB628CF8690@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<1e44462b.22f9.17327057eb1.Coremail.lydata_jia@163.com>",
        "from": "lydata &lt;lydata_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 02:05:38 GMT",
        "subject": "flink 1.10.1 入 hive 格式为parquet",
        "content": " Hi,&#010;&#010;可以提供一份flink1.10 入hive格式为parquet的例子吗？&#010;&#010;Best,&#010;lydata",
        "depth": "0",
        "reply": "<1e44462b.22f9.17327057eb1.Coremail.lydata_jia@163.com>"
    },
    {
        "id": "<CADH6UNTRBxoC8iUK1b-XoBdyPrawQ4kOeo-bXd-yD1shS2G4GA@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:35:47 GMT",
        "subject": "Re: flink 1.10.1 入 hive 格式为parquet",
        "content": "只要你的hive目标表创建为Parquet格式就行了哈，INSERT语句上跟其他类型的表没有区别的&#013;&#010;&#013;&#010;On Tue, Jul 7, 2020 at 10:05 AM lydata &lt;lydata_jia@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;  Hi,&#013;&#010;&gt;&#013;&#010;&gt; 可以提供一份flink1.10 入hive格式为parquet的例子吗？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; lydata&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "1",
        "reply": "<1e44462b.22f9.17327057eb1.Coremail.lydata_jia@163.com>"
    },
    {
        "id": "<tencent_DA2E9461096F13A23251B46B8A8926F0E606@qq.com>",
        "from": "&quot;吴磊&quot; &lt;wuleifl...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 02:25:25 GMT",
        "subject": "作业从flink1.9.0迁移到1.10.1，LogicalTypeRoot变更后无法从CP恢复：No enum constant org.apache.flink.table.types.logical.LogicalTypeRoot.ANY",
        "content": "各位好：&amp;nbsp; &amp;nbsp; 当我把作业从flink1.9.0迁移到1.10.1，且作业中使用了'group&#010;by'形式的语法时，会导致无法从cp/sp恢复，&#013;&#010;代码：&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;报错如下：&#013;&#010;switched from RUNNING to FAILED.switched from RUNNING to FAILED.java.lang.Exception: Exception&#010;while creating StreamOperatorStateContext. at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:989)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448) at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533) at java.lang.Thread.run(Thread.java:748)Caused&#010;by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedProcessOperator_d26553d858836b90d15e66f459fbcb50_(2/3)&#010;from any of the 1 provided restore options. at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;... 9 moreCaused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when&#010;trying to restore heap backend at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)&#010;at org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:529)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;... 11 moreCaused by: java.io.InvalidObjectException: enum constant ANY does not exist in&#010;class org.apache.flink.table.types.logical.LogicalTypeRoot at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2013)&#010;at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1569) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2286)&#010;at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2166) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2068)&#010;at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1572) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:430)&#010;at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) at&#010;org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:555) at org.apache.flink.table.runtime.typeutils.BaseRowSerializer$BaseRowSerializerSnapshot.readSnapshot(BaseRowSerializer.java:306)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.readVersionedSnapshot(TypeSerializerSnapshot.java:174)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.deserializeV2(TypeSerializerSnapshotSerializationUtil.java:179)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.read(TypeSerializerSnapshotSerializationUtil.java:150)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.readSerializerSnapshot(TypeSerializerSnapshotSerializationUtil.java:76)&#010;at org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotReadersWriters$CurrentReaderImpl.readStateMetaInfoSnapshot(StateMetaInfoSnapshotReadersWriters.java:219)&#010;at org.apache.flink.runtime.state.KeyedBackendSerializationProxy.read(KeyedBackendSerializationProxy.java:169)&#010;at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:133)&#010;at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)&#010;... 15 moreCaused by: java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.types.logical.LogicalTypeRoot.ANY&#010;at java.lang.Enum.valueOf(Enum.java:238) &#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; 烦请各位帮助，谢谢！",
        "depth": "0",
        "reply": "<tencent_DA2E9461096F13A23251B46B8A8926F0E606@qq.com>"
    },
    {
        "id": "<CAELO932T=vjpiprXT8ebmS2Uk2e+piTzNm1Y6FKmon1+E83WXg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 08:28:30 GMT",
        "subject": "Re: 作业从flink1.9.0迁移到1.10.1，LogicalTypeRoot变更后无法从CP恢复：No enum constant org.apache.flink.table.types.logical.LogicalTypeRoot.ANY",
        "content": "Hi,&#010;&#010;问一下，你是指用1.10去恢复 1.9 作业的 savepoint/checkpoint 吗？还是指迁移到&#010;1.10 后，无法从 failover&#010;中恢复？&#010;如果是前者的话，Flink SQL 目前没有保证跨大版本的 state 兼容性。所以当你从&#010;1.9 升级到 1.10 时，作业需要放弃状态重跑。&#010;&#010;Best,&#010;Jark&#010;&#010;On Tue, 7 Jul 2020 at 15:54, 吴磊 &lt;wuleiflink@foxmail.com&gt; wrote:&#010;&#010;&gt; 各位好：&#010;&gt;     当我把作业从flink1.9.0迁移到1.10.1，且作业中使用了'group by'形式的语法时，会导致无法从cp/sp恢复，&#010;&gt; 代码：&#010;&gt;&#010;&gt;&#010;&gt; 报错如下：&#010;&gt;&#010;&gt; switched from RUNNING to FAILED.switched from RUNNING to FAILED.java.lang.Exception:&#010;Exception while creating StreamOperatorStateContext. at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:989)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448) at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533) at java.lang.Thread.run(Thread.java:748)Caused&#010;by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for KeyedProcessOperator_d26553d858836b90d15e66f459fbcb50_(2/3)&#010;from any of the 1 provided restore options. at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;... 9 moreCaused by: org.apache.flink.runtime.state.BackendBuildingException: Failed when&#010;trying to restore heap backend at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)&#010;at org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:529)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;... 11 moreCaused by: java.io.InvalidObjectException: enum constant ANY does not exist in&#010;class org.apache.flink.table.types.logical.LogicalTypeRoot at java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2013)&#010;at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1569) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2286)&#010;at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2166) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2068)&#010;at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1572) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:430)&#010;at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576) at&#010;org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:555) at org.apache.flink.table.runtime.typeutils.BaseRowSerializer$BaseRowSerializerSnapshot.readSnapshot(BaseRowSerializer.java:306)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.readVersionedSnapshot(TypeSerializerSnapshot.java:174)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.deserializeV2(TypeSerializerSnapshotSerializationUtil.java:179)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.read(TypeSerializerSnapshotSerializationUtil.java:150)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.readSerializerSnapshot(TypeSerializerSnapshotSerializationUtil.java:76)&#010;at org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotReadersWriters$CurrentReaderImpl.readStateMetaInfoSnapshot(StateMetaInfoSnapshotReadersWriters.java:219)&#010;at org.apache.flink.runtime.state.KeyedBackendSerializationProxy.read(KeyedBackendSerializationProxy.java:169)&#010;at org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:133)&#010;at org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)&#010;... 15 moreCaused by: java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.types.logical.LogicalTypeRoot.ANY&#010;at java.lang.Enum.valueOf(Enum.java:238)&#010;&gt;&#010;&gt;&#010;&gt;   烦请各位帮助，谢谢！&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_DA2E9461096F13A23251B46B8A8926F0E606@qq.com>"
    },
    {
        "id": "<CAA8tFvsLyW2nFoRhGMn1jZo+5itNivKQVgGX1p+UJrZSMPR9NQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 08:09:12 GMT",
        "subject": "Re: 作业从flink1.9.0迁移到1.10.1，LogicalTypeRoot变更后无法从CP恢复：No enum constant org.apache.flink.table.types.logical.LogicalTypeRoot.ANY",
        "content": "Hi&#010;&#010;从错误栈来看，是因为 `No enum constant&#010;org.apache.flink.table.types.logical.LogicalTypeRoot.ANY ` 这个导致的无法正常&#010;restore。首先你需要看下是否是大版本的升级（像 Jark 说的那样），如果是小版本的升级，你需要看下为什么找不到这个&#010;LocigcalTypeRoot.ANY.&#010;&#010;PS: 贴代码/错误栈可以使用 gist[1] 或者 pastebin[2] 这样的服务，现在邮件里看到的栈信息没有很好的分行&#010;&#010;[1] https://gist.github.com/&#010;[2] https://pastebin.ubuntu.com/&#010;Best,&#010;Congxian&#010;&#010;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月7日周二 下午4:28写道：&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; 问一下，你是指用1.10去恢复 1.9 作业的 savepoint/checkpoint 吗？还是指迁移到&#010;1.10 后，无法从 failover&#010;&gt; 中恢复？&#010;&gt; 如果是前者的话，Flink SQL 目前没有保证跨大版本的 state 兼容性。所以当你从&#010;1.9 升级到 1.10 时，作业需要放弃状态重跑。&#010;&gt;&#010;&gt; Best,&#010;&gt; Jark&#010;&gt;&#010;&gt; On Tue, 7 Jul 2020 at 15:54, 吴磊 &lt;wuleiflink@foxmail.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; 各位好：&#010;&gt; &gt;     当我把作业从flink1.9.0迁移到1.10.1，且作业中使用了'group by'形式的语法时，会导致无法从cp/sp恢复，&#010;&gt; &gt; 代码：&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 报错如下：&#010;&gt; &gt;&#010;&gt; &gt; switched from RUNNING to FAILED.switched from RUNNING to&#010;&gt; FAILED.java.lang.Exception: Exception while creating&#010;&gt; StreamOperatorStateContext. at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:989)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)&#010;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708) at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533) at&#010;&gt; java.lang.Thread.run(Thread.java:748)Caused by:&#010;&gt; org.apache.flink.util.FlinkException: Could not restore keyed state backend&#010;&gt; for KeyedProcessOperator_d26553d858836b90d15e66f459fbcb50_(2/3) from any of&#010;&gt; the 1 provided restore options. at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; ... 9 moreCaused by:&#010;&gt; org.apache.flink.runtime.state.BackendBuildingException: Failed when trying&#010;&gt; to restore heap backend at&#010;&gt; org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:116)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.state.filesystem.FsStateBackend.createKeyedStateBackend(FsStateBackend.java:529)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; ... 11 moreCaused by: java.io.InvalidObjectException: enum constant ANY&#010;&gt; does not exist in class&#010;&gt; org.apache.flink.table.types.logical.LogicalTypeRoot at&#010;&gt; java.io.ObjectInputStream.readEnum(ObjectInputStream.java:2013) at&#010;&gt; java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1569) at&#010;&gt; java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2286) at&#010;&gt; java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2166) at&#010;&gt; java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2068)&#010;&gt; at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1572) at&#010;&gt; java.io.ObjectInputStream.readObject(ObjectInputStream.java:430) at&#010;&gt; org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:576)&#010;&gt; at&#010;&gt; org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:555)&#010;&gt; at&#010;&gt; org.apache.flink.table.runtime.typeutils.BaseRowSerializer$BaseRowSerializerSnapshot.readSnapshot(BaseRowSerializer.java:306)&#010;&gt; at&#010;&gt; org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.readVersionedSnapshot(TypeSerializerSnapshot.java:174)&#010;&gt; at&#010;&gt; org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.deserializeV2(TypeSerializerSnapshotSerializationUtil.java:179)&#010;&gt; at&#010;&gt; org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.read(TypeSerializerSnapshotSerializationUtil.java:150)&#010;&gt; at&#010;&gt; org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.readSerializerSnapshot(TypeSerializerSnapshotSerializationUtil.java:76)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.state.metainfo.StateMetaInfoSnapshotReadersWriters$CurrentReaderImpl.readStateMetaInfoSnapshot(StateMetaInfoSnapshotReadersWriters.java:219)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.state.KeyedBackendSerializationProxy.read(KeyedBackendSerializationProxy.java:169)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.state.heap.HeapRestoreOperation.restore(HeapRestoreOperation.java:133)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.state.heap.HeapKeyedStateBackendBuilder.build(HeapKeyedStateBackendBuilder.java:114)&#010;&gt; ... 15 moreCaused by: java.lang.IllegalArgumentException: No enum constant&#010;&gt; org.apache.flink.table.types.logical.LogicalTypeRoot.ANY at&#010;&gt; java.lang.Enum.valueOf(Enum.java:238)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;   烦请各位帮助，谢谢！&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_DA2E9461096F13A23251B46B8A8926F0E606@qq.com>"
    },
    {
        "id": "<tencent_8D53CEEA7B745C773A5EA0E4@qq.com>",
        "from": "&quot;seeksst&quot;&lt;seek...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 03:35:49 GMT",
        "subject": "回复： 求助：FLINKSQL1.10实时统计累计UV",
        "content": "我看你代码上是sqlUpdate，tableConfig是另外设置的，需要作为入参一同放入sqlUpdate中，&#010;使用方法sqlUpdate(str, config)&#010;另外如果你使用的是rocksdb，需要开启rocksdb的ttl&#010;state.backend.rocksdb.ttl.compaction.filter.enabled设置成true&#010;低版本这个参数默认是false&#010;&#010;&#010;&#010;&#010;原始邮件&#010;发件人:x35907418@qq.com&#010;收件人:user-zhuser-zh@flink.apache.org&#010;发送时间:2020年7月7日(周二) 10:46&#010;主题:回复： 求助：FLINKSQL1.10实时统计累计UV&#010;&#010;&#010;是blinkval setttings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;------------------nbsp;原始邮件nbsp;------------------ 发件人:nbsp;\"Benchao Li\"libenchao@apache.orggt;;&#010;发送时间:nbsp;2020年7月6日(星期一) 晚上11:11 收件人:nbsp;\"user-zh\"user-zh@flink.apache.orggt;;&#010;主题:nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV 感觉不太应该有这种情况，你用的是blink&#010;planner么？ x 35907418@qq.comgt; 于2020年7月6日周一 下午1:24写道： gt; sorry,我说错了，确实没有，都是group&#010;agg. gt; gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7))，但是状态还是越来越大，没有按既定配置自动清理.&#010;gt; gt; gt; ------------------amp;nbsp;原始邮件amp;nbsp;------------------ gt; 发件人:amp;nbsp;\"Benchao&#010;Li\"libenchao@apache.orgamp;gt;; gt; 发送时间:amp;nbsp;2020年7月6日(星期一) 中午12:52&#010;gt; 收件人:amp;nbsp;\"user-zh\"user-zh@flink.apache.orgamp;gt;; gt; gt; 主题:amp;nbsp;Re:&#010;求助：FLINKSQL1.10实时统计累计UV gt; gt; gt; gt; 我看你的SQL里面并没有用到窗口呀，只是一个普通的聚合。&#010;gt; 这种聚合需要设置合理的state retention[1]时间的，要不然状态默认是永远不清理的。&#010;gt; gt; [1] gt; gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html#idle-state-retention-time&#010;gt; gt; x 35907418@qq.comamp;gt; 于2020年7月6日周一 上午11:15写道： gt; gt; amp;gt;&#010;版本是1.10.1，最后sink的时候确实是一个window里面做count gt; amp;gt; distinct操作。请问是只要计算过程中含有一个window里面做count&#010;gt; amp;gt; gt; distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，groupamp;amp;nbsp;DATE_FORMAT(rowtm,&#010;gt; amp;gt; 'yyyy-MM-dd') 这个sql对应的状态很大。代码如下： gt; amp;gt; val&#010;rt_totaluv_view : Table = tabEnv.sqlQuery( gt; amp;gt;amp;nbsp;amp;nbsp; \"\"\" gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;SELECT MAX(DATE_FORMAT(rowtm, 'yyyy-MM-dd gt; HH:mm:00')) gt; amp;gt; time_str,COUNT(DISTINCT&#010;userkey) uv gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; FROM source gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;GROUP BY DATE_FORMAT(rowtm, 'yyyy-MM-dd') gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;\"\"\") gt; amp;gt; tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view) gt; amp;gt;&#010;gt; amp;gt; val totaluvTmp = gt; tabEnv.toRetractStream[(String,Long)](rt_totaluv_view) gt;&#010;amp;gt;amp;nbsp;amp;nbsp; .filter( line =amp;amp;gt; line._1 == true ).map( line gt; =amp;amp;gt;&#010;line._2 ) gt; amp;gt; gt; amp;gt; val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#010;gt; amp;gt; gt; amp;gt; tabEnv.sqlUpdate( gt; amp;gt;amp;nbsp;amp;nbsp; s\"\"\" gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;INSERT INTO mysql_totaluv gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; SELECT _1,MAX(_2)&#010;gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; FROM $totaluvTabTmp gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;GROUP BY _1 gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; \"\"\") gt; amp;gt; ------------------amp;amp;nbsp;原始邮件amp;amp;nbsp;------------------&#010;gt; amp;gt; 发件人:amp;amp;nbsp;\"Benchao Li\"libenchao@apache.orgamp;amp;gt;; gt; amp;gt;&#010;发送时间:amp;amp;nbsp;2020年7月3日(星期五) 晚上9:47 gt; amp;gt; 收件人:amp;amp;nbsp;\"user-zh\"user-zh@flink.apache.orgamp;amp;gt;;&#010;gt; amp;gt; gt; amp;gt; 主题:amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; gt; amp;gt; gt; amp;gt; gt; amp;gt; 你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题， gt; amp;gt; 这个已经在1.11中修复了。 gt; amp;gt; gt;&#010;amp;gt; [1] https://issues.apache.org/jira/browse/FLINK-17942 gt; amp;gt; gt; amp;gt; x 35907418@qq.comamp;amp;gt;&#010;于2020年7月3日周五 下午4:34写道： gt; amp;gt; gt; amp;gt; amp;amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; gt; ------------------amp;amp;amp;nbsp;原始邮件amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; 发件人:amp;amp;amp;nbsp;\"Jark Wu\"imjark@gmail.comamp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; 发送时间:amp;amp;amp;nbsp;2020年6月18日(星期四) 中午12:16&#010;gt; amp;gt; amp;amp;gt; 收件人:amp;amp;amp;nbsp;\"user-zh\"user-zh@flink.apache.org gt; amp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; 主题:amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;是的，我觉得这样子是能绕过的。 gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;On Thu, 18 Jun 2020 at 10:34, x 35907418@qq.comamp;amp;amp;gt; gt; wrote: gt; amp;gt; amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery( gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp; \"\"\" gt; amp;gt; amp;amp;gt;&#010;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; SELECT&#010;gt; amp;gt; MAX(DATE_FORMAT(ts, 'yyyy-MM-dd gt; amp;gt; amp;amp;gt; HH:mm:00')) gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv gt; amp;gt; amp;amp;gt; gt;&#010;amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; FROM gt;&#010;amp;gt; user_behavioramp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; GROUP BY gt; amp;gt;&#010;amp;amp;gt; DATE_FORMAT(ts, gt; 'yyyy-MM-dd')amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;\"\"\") gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val gt;&#010;amp;gt; resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab) gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; gt; .filter(line=amp;amp;amp;amp;gt;line._1==true).map(line=amp;amp;amp;amp;gt;line._2)&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; tabEnv.sqlUpdate( gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;s\"\"\" gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;INSERT gt; INTO gt; amp;gt; rt_totaluv gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;SELECT gt; _1,MAX(_2) gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;FROM gt; $res gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;GROUP gt; BY _1 gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;\"\"\") gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; gt; ------------------amp;amp;amp;amp;nbsp;原始邮件amp;amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 发件人:amp;amp;amp;amp;nbsp;\"Jark Wu\" gt; imjark@gmail.comamp;amp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 发送时间:amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午1:55 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 收件人:amp;amp;amp;amp;nbsp;\"user-zh\"&#010;gt; user-zh@flink.apache.org gt; amp;gt; amp;amp;amp;amp;gt;; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 主题:amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; CREATE TABLE&#010;mysql ( gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;time_str gt; STRING, gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;uv BIGINT, gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;PRIMARY gt; KEY (ts) NOT ENFORCED gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; ) WITH ( gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; gt; 'connector' = 'jdbc',&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; 'url' =&#010;gt; amp;gt; 'jdbc:mysql://localhost:3306/mydatabase', gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; gt; 'table-name' = 'myuv' gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; ); gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;INSERT INTO mysql gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#010;gt; HH:mm:00')), gt; amp;gt; amp;amp;gt; COUNT(DISTINCTamp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; user_id) gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; FROM user_behavior; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; On Wed, 17 Jun 2020 at&#010;13:49, x  gt; 35907418@qq.comamp;amp;amp;amp;gt; gt; amp;gt; wrote: gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; sink表这个样式 gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; tm uv gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 2020/06/17 13:46:00 10000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;2020/06/17 13:47:00 20000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; 2020/06/17&#010;13:48:00 30000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;group by 日期的话，分钟如何获取 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; ------------------amp;amp;amp;amp;amp;nbsp;原始邮件amp;amp;amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 发件人:amp;amp;amp;amp;amp;nbsp;\"Benchao&#010;Li\" gt; amp;gt; libenchao@apache.org gt; amp;gt; amp;amp;gt; amp;amp;amp;amp;amp;gt;; gt;&#010;amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 发送时间:amp;amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午11:46 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 收件人:amp;amp;amp;amp;amp;nbsp;\"user-zh\"&#010;gt; amp;gt; user-zh@flink.apache.org gt; amp;gt; amp;amp;gt; amp;amp;amp;amp;amp;gt;; gt;&#010;amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 主题:amp;amp;amp;amp;amp;nbsp;Re: gt; 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; Hi， gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 我感觉这种场景可以有两种方式， gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 1. 可以直接用group by + mini batch gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 2. window聚合 + fast emit gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; 对于#1，group&#010;gt; amp;gt; by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#010;gt; amp;gt; amp;amp;gt; 'yyyy-MM-dd')。 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;这种情况下的状态清理，需要配置state gt; retention时间，配置方法可以参考[1]&#010;gt; amp;gt; 。同时，mini gt; amp;gt; amp;amp;gt; batch的开启也需要 gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 用参数[2] 来打开。 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; fast gt; amp;gt; amp;amp;gt; gt;&#010;emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; table.exec.emit.early-fire.enabled&#010;= true gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; table.exec.emit.early-fire.delay&#010;= 60 s gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; [1] gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; [2] gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; x 35907418@qq.com gt; amp;amp;amp;amp;amp;gt; gt; amp;gt; 于2020年6月17日周三&#010;上午11:14写道： gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; CREATE&#010;gt; VIEW uv_per_10min AS gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;gt; SELECTamp;amp;amp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; gt; amp;amp;amp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; MAX(DATE_FORMAT(proctimeamp;amp;amp;amp;amp;amp;nbsp;,&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 'yyyy-MM-dd gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; HH:mm:00'))amp;amp;amp;amp;amp;amp;nbsp;OVER w gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; AS gt; amp;gt; time_str,amp;amp;amp;amp;amp;amp;nbsp;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; amp;amp;amp;amp;amp;amp;nbsp;&#010;gt; amp;gt; COUNT(DISTINCT user_id) OVER gt; amp;gt; amp;amp;gt; w AS uv gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; FROM gt; user_behavior gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; WINDOW w gt; AS (ORDER&#010;BY proctime gt; amp;gt; ROWS BETWEEN gt; amp;gt; amp;amp;gt; UNBOUNDED gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; PRECEDING AND gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; CURRENT gt; ROW); gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; 想请教一下，应该如何处理？&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; PARTITION&#010;gt; BY gt; amp;gt; DATE_FORMAT(rowtm, 'yyyy-MM-dd') gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;这样可以吗，另外状态应该如何清理？ gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#010;gt; amp;gt; VIEW吧 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;多谢 gt; amp;gt; gt; amp;gt; gt; amp;gt; gt; amp;gt; -- gt; amp;gt; gt; amp;gt; Best, gt;&#010;amp;gt; Benchao Li gt; gt; gt; gt; -- gt; gt; Best, gt; Benchao Li -- Best, Benchao Li",
        "depth": "1",
        "reply": "<tencent_8D53CEEA7B745C773A5EA0E4@qq.com>"
    },
    {
        "id": "<tencent_5D2D3BFD6E004826060E76E5CABAD795CE08@qq.com>",
        "from": "&quot;x&quot; &lt;35907...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:08:26 GMT",
        "subject": "回复： 求助：FLINKSQL1.10实时统计累计UV",
        "content": "您说的这种方式，V1.10.1 不支持吧，我看参数只有一个String类型的&#013;&#010;void sqlUpdate(String stmt);&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"seeksst\"&lt;seeksst@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月7日(星期二) 中午11:35&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复： 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我看你代码上是sqlUpdate，tableConfig是另外设置的，需要作为入参一同放入sqlUpdate中，&#013;&#010;使用方法sqlUpdate(str, config)&#013;&#010;另外如果你使用的是rocksdb，需要开启rocksdb的ttl&#013;&#010;state.backend.rocksdb.ttl.compaction.filter.enabled设置成true&#013;&#010;低版本这个参数默认是false&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;原始邮件&#013;&#010;发件人:x35907418@qq.com&#013;&#010;收件人:user-zhuser-zh@flink.apache.org&#013;&#010;发送时间:2020年7月7日(周二) 10:46&#013;&#010;主题:回复： 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&#013;&#010;&#013;&#010;是blinkval setttings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;------------------nbsp;原始邮件nbsp;------------------ 发件人:nbsp;\"Benchao Li\"libenchao@apache.orggt;;&#010;发送时间:nbsp;2020年7月6日(星期一) 晚上11:11 收件人:nbsp;\"user-zh\"user-zh@flink.apache.orggt;;&#010;主题:nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV 感觉不太应该有这种情况，你用的是blink&#010;planner么？ x 35907418@qq.comgt; 于2020年7月6日周一 下午1:24写道： gt; sorry,我说错了，确实没有，都是group&#010;agg. gt; gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7))，但是状态还是越来越大，没有按既定配置自动清理.&#010;gt; gt; gt; ------------------amp;nbsp;原始邮件amp;nbsp;------------------ gt; 发件人:amp;nbsp;\"Benchao&#010;Li\"libenchao@apache.orgamp;gt;; gt; 发送时间:amp;nbsp;2020年7月6日(星期一) 中午12:52&#010;gt; 收件人:amp;nbsp;\"user-zh\"user-zh@flink.apache.orgamp;gt;; gt; gt; 主题:amp;nbsp;Re:&#010;求助：FLINKSQL1.10实时统计累计UV gt; gt; gt; gt; 我看你的SQL里面并没有用到窗口呀，只是一个普通的聚合。&#010;gt; 这种聚合需要设置合理的state retention[1]时间的，要不然状态默认是永远不清理的。&#010;gt; gt; [1] gt; gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html#idle-state-retention-time&#010;gt; gt; x 35907418@qq.comamp;gt; 于2020年7月6日周一 上午11:15写道： gt; gt; amp;gt;&#010;版本是1.10.1，最后sink的时候确实是一个window里面做count gt; amp;gt; distinct操作。请问是只要计算过程中含有一个window里面做count&#010;gt; amp;gt; gt; distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，groupamp;amp;nbsp;DATE_FORMAT(rowtm,&#010;gt; amp;gt; 'yyyy-MM-dd') 这个sql对应的状态很大。代码如下： gt; amp;gt; val&#010;rt_totaluv_view : Table = tabEnv.sqlQuery( gt; amp;gt;amp;nbsp;amp;nbsp; \"\"\" gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;SELECT MAX(DATE_FORMAT(rowtm, 'yyyy-MM-dd gt; HH:mm:00')) gt; amp;gt; time_str,COUNT(DISTINCT&#010;userkey) uv gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; FROM source gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;GROUP BY DATE_FORMAT(rowtm, 'yyyy-MM-dd') gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;\"\"\") gt; amp;gt; tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view) gt; amp;gt;&#010;gt; amp;gt; val totaluvTmp = gt; tabEnv.toRetractStream[(String,Long)](rt_totaluv_view) gt;&#010;amp;gt;amp;nbsp;amp;nbsp; .filter( line =amp;amp;gt; line._1 == true ).map( line gt; =amp;amp;gt;&#010;line._2 ) gt; amp;gt; gt; amp;gt; val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#010;gt; amp;gt; gt; amp;gt; tabEnv.sqlUpdate( gt; amp;gt;amp;nbsp;amp;nbsp; s\"\"\" gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;INSERT INTO mysql_totaluv gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; SELECT _1,MAX(_2)&#010;gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; FROM $totaluvTabTmp gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;GROUP BY _1 gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; \"\"\") gt; amp;gt; ------------------amp;amp;nbsp;原始邮件amp;amp;nbsp;------------------&#010;gt; amp;gt; 发件人:amp;amp;nbsp;\"Benchao Li\"libenchao@apache.orgamp;amp;gt;; gt; amp;gt;&#010;发送时间:amp;amp;nbsp;2020年7月3日(星期五) 晚上9:47 gt; amp;gt; 收件人:amp;amp;nbsp;\"user-zh\"user-zh@flink.apache.orgamp;amp;gt;;&#010;gt; amp;gt; gt; amp;gt; 主题:amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; gt; amp;gt; gt; amp;gt; gt; amp;gt; 你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题， gt; amp;gt; 这个已经在1.11中修复了。 gt; amp;gt; gt;&#010;amp;gt; [1] https://issues.apache.org/jira/browse/FLINK-17942 gt; amp;gt; gt; amp;gt; x 35907418@qq.comamp;amp;gt;&#010;于2020年7月3日周五 下午4:34写道： gt; amp;gt; gt; amp;gt; amp;amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; gt; ------------------amp;amp;amp;nbsp;原始邮件amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; 发件人:amp;amp;amp;nbsp;\"Jark Wu\"imjark@gmail.comamp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; 发送时间:amp;amp;amp;nbsp;2020年6月18日(星期四) 中午12:16&#010;gt; amp;gt; amp;amp;gt; 收件人:amp;amp;amp;nbsp;\"user-zh\"user-zh@flink.apache.org gt; amp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; 主题:amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;是的，我觉得这样子是能绕过的。 gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;On Thu, 18 Jun 2020 at 10:34, x 35907418@qq.comamp;amp;amp;gt; gt; wrote: gt; amp;gt; amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery( gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp; \"\"\" gt; amp;gt; amp;amp;gt;&#010;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; SELECT&#010;gt; amp;gt; MAX(DATE_FORMAT(ts, 'yyyy-MM-dd gt; amp;gt; amp;amp;gt; HH:mm:00')) gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv gt; amp;gt; amp;amp;gt; gt;&#010;amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; FROM gt;&#010;amp;gt; user_behavioramp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; GROUP BY gt; amp;gt;&#010;amp;amp;gt; DATE_FORMAT(ts, gt; 'yyyy-MM-dd')amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;\"\"\") gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val gt;&#010;amp;gt; resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab) gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; gt; .filter(line=amp;amp;amp;amp;gt;line._1==true).map(line=amp;amp;amp;amp;gt;line._2)&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; tabEnv.sqlUpdate( gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;s\"\"\" gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;INSERT gt; INTO gt; amp;gt; rt_totaluv gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;SELECT gt; _1,MAX(_2) gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;FROM gt; $res gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;GROUP gt; BY _1 gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;\"\"\") gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; gt; ------------------amp;amp;amp;amp;nbsp;原始邮件amp;amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 发件人:amp;amp;amp;amp;nbsp;\"Jark Wu\" gt; imjark@gmail.comamp;amp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 发送时间:amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午1:55 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 收件人:amp;amp;amp;amp;nbsp;\"user-zh\"&#010;gt; user-zh@flink.apache.org gt; amp;gt; amp;amp;amp;amp;gt;; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 主题:amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; CREATE TABLE&#010;mysql ( gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;time_str gt; STRING, gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;uv BIGINT, gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;PRIMARY gt; KEY (ts) NOT ENFORCED gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; ) WITH ( gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; gt; 'connector' = 'jdbc',&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; 'url' =&#010;gt; amp;gt; 'jdbc:mysql://localhost:3306/mydatabase', gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; gt; 'table-name' = 'myuv' gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; ); gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;INSERT INTO mysql gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#010;gt; HH:mm:00')), gt; amp;gt; amp;amp;gt; COUNT(DISTINCTamp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; user_id) gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; FROM user_behavior; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; On Wed, 17 Jun 2020 at&#010;13:49, x&amp;nbsp; gt; 35907418@qq.comamp;amp;amp;amp;gt; gt; amp;gt; wrote: gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; sink表这个样式 gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; tm uv gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 2020/06/17 13:46:00 10000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;2020/06/17 13:47:00 20000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; 2020/06/17&#010;13:48:00 30000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;group by 日期的话，分钟如何获取 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; ------------------amp;amp;amp;amp;amp;nbsp;原始邮件amp;amp;amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 发件人:amp;amp;amp;amp;amp;nbsp;\"Benchao&#010;Li\" gt; amp;gt; libenchao@apache.org gt; amp;gt; amp;amp;gt; amp;amp;amp;amp;amp;gt;; gt;&#010;amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 发送时间:amp;amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午11:46 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 收件人:amp;amp;amp;amp;amp;nbsp;\"user-zh\"&#010;gt; amp;gt; user-zh@flink.apache.org gt; amp;gt; amp;amp;gt; amp;amp;amp;amp;amp;gt;; gt;&#010;amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 主题:amp;amp;amp;amp;amp;nbsp;Re: gt; 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; Hi， gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 我感觉这种场景可以有两种方式， gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 1. 可以直接用group by + mini batch gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 2. window聚合 + fast emit gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; 对于#1，group&#010;gt; amp;gt; by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#010;gt; amp;gt; amp;amp;gt; 'yyyy-MM-dd')。 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;这种情况下的状态清理，需要配置state gt; retention时间，配置方法可以参考[1]&#010;gt; amp;gt; 。同时，mini gt; amp;gt; amp;amp;gt; batch的开启也需要 gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 用参数[2] 来打开。 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; fast gt; amp;gt; amp;amp;gt; gt;&#010;emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; table.exec.emit.early-fire.enabled&#010;= true gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; table.exec.emit.early-fire.delay&#010;= 60 s gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; [1] gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; [2] gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; x 35907418@qq.com gt; amp;amp;amp;amp;amp;gt; gt; amp;gt; 于2020年6月17日周三&#010;上午11:14写道： gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; CREATE&#010;gt; VIEW uv_per_10min AS gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;gt; SELECTamp;amp;amp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; gt; amp;amp;amp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; MAX(DATE_FORMAT(proctimeamp;amp;amp;amp;amp;amp;nbsp;,&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 'yyyy-MM-dd gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; HH:mm:00'))amp;amp;amp;amp;amp;amp;nbsp;OVER w gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; AS gt; amp;gt; time_str,amp;amp;amp;amp;amp;amp;nbsp;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; amp;amp;amp;amp;amp;amp;nbsp;&#010;gt; amp;gt; COUNT(DISTINCT user_id) OVER gt; amp;gt; amp;amp;gt; w AS uv gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; FROM gt; user_behavior gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; WINDOW w gt; AS (ORDER&#010;BY proctime gt; amp;gt; ROWS BETWEEN gt; amp;gt; amp;amp;gt; UNBOUNDED gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; PRECEDING AND gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; CURRENT gt; ROW); gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; 想请教一下，应该如何处理？&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; PARTITION&#010;gt; BY gt; amp;gt; DATE_FORMAT(rowtm, 'yyyy-MM-dd') gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;这样可以吗，另外状态应该如何清理？ gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#010;gt; amp;gt; VIEW吧 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;多谢 gt; amp;gt; gt; amp;gt; gt; amp;gt; gt; amp;gt; -- gt; amp;gt; gt; amp;gt; Best, gt;&#010;amp;gt; Benchao Li gt; gt; gt; gt; -- gt; gt; Best, gt; Benchao Li -- Best, Benchao Li",
        "depth": "2",
        "reply": "<tencent_8D53CEEA7B745C773A5EA0E4@qq.com>"
    },
    {
        "id": "<tencent_650D8FFA87CC272EA648B4E3@qq.com>",
        "from": "&quot;seeksst&quot;&lt;seek...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 06:00:20 GMT",
        "subject": "回复： 求助：FLINKSQL1.10实时统计累计UV",
        "content": "你好，我刚刚切换到1.10.1版本，还是可以设置的，这个接口是在StreamTableEnvironment里面，使用的是flink-table-api-java-bridge，如果你使用的是scala版本，这个我不是很了解，理论应该差不多。&#010;在1.10版本中，我一般是这么写的：&#010;EnvironmentSettings settings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();&#010;StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);&#010;也不需要进行sqlUpdate传入，直接tableEnv.getConfig().setIdleStateRetentionTime()&#010;设置就可以了。&#010;另外，你指的状态越来越大需要描述的更详细一点，具体现象和状态后端的选择。之前也提到了，使用rocksdb做状态后端，需要开启ttl配置，&#010;否则现象就是checkpoint的文件大小会不断增大。&#010;&#010;&#010;原始邮件&#010;发件人:x35907418@qq.com&#010;收件人:user-zhuser-zh@flink.apache.org&#010;发送时间:2020年7月8日(周三) 11:08&#010;主题:回复： 求助：FLINKSQL1.10实时统计累计UV&#010;&#010;&#010;您说的这种方式，V1.10.1 不支持吧，我看参数只有一个String类型的 void&#010;sqlUpdate(String stmt); ------------------nbsp;原始邮件nbsp;------------------ 发件人:nbsp;\"seeksst\"seeksst@163.comgt;;&#010;发送时间:nbsp;2020年7月7日(星期二) 中午11:35 收件人:nbsp;\"user-zh\"user-zh@flink.apache.orggt;;&#010;主题:nbsp;回复： 求助：FLINKSQL1.10实时统计累计UV 我看你代码上是sqlUpdate，tableConfig是另外设置的，需要作为入参一同放入sqlUpdate中，&#010;使用方法sqlUpdate(str, config) 另外如果你使用的是rocksdb，需要开启rocksdb的ttl&#010;state.backend.rocksdb.ttl.compaction.filter.enabled设置成true 低版本这个参数默认是false&#010;原始邮件 发件人:x35907418@qq.com 收件人:user-zhuser-zh@flink.apache.org 发送时间:2020年7月7日(周二) 10:46&#010;主题:回复： 求助：FLINKSQL1.10实时统计累计UV 是blinkval setttings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;------------------nbsp;原始邮件nbsp;------------------ 发件人:nbsp;\"Benchao Li\"libenchao@apache.orggt;;&#010;发送时间:nbsp;2020年7月6日(星期一) 晚上11:11 收件人:nbsp;\"user-zh\"user-zh@flink.apache.orggt;;&#010;主题:nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV 感觉不太应该有这种情况，你用的是blink&#010;planner么？ x 35907418@qq.comgt; 于2020年7月6日周一 下午1:24写道： gt; sorry,我说错了，确实没有，都是group&#010;agg. gt; gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7))，但是状态还是越来越大，没有按既定配置自动清理.&#010;gt; gt; gt; ------------------amp;nbsp;原始邮件amp;nbsp;------------------ gt; 发件人:amp;nbsp;\"Benchao&#010;Li\"libenchao@apache.orgamp;gt;; gt; 发送时间:amp;nbsp;2020年7月6日(星期一) 中午12:52&#010;gt; 收件人:amp;nbsp;\"user-zh\"user-zh@flink.apache.orgamp;gt;; gt; gt; 主题:amp;nbsp;Re:&#010;求助：FLINKSQL1.10实时统计累计UV gt; gt; gt; gt; 我看你的SQL里面并没有用到窗口呀，只是一个普通的聚合。&#010;gt; 这种聚合需要设置合理的state retention[1]时间的，要不然状态默认是永远不清理的。&#010;gt; gt; [1] gt; gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html#idle-state-retention-time&#010;gt; gt; x 35907418@qq.comamp;gt; 于2020年7月6日周一 上午11:15写道： gt; gt; amp;gt;&#010;版本是1.10.1，最后sink的时候确实是一个window里面做count gt; amp;gt; distinct操作。请问是只要计算过程中含有一个window里面做count&#010;gt; amp;gt; gt; distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，groupamp;amp;nbsp;DATE_FORMAT(rowtm,&#010;gt; amp;gt; 'yyyy-MM-dd') 这个sql对应的状态很大。代码如下： gt; amp;gt; val&#010;rt_totaluv_view : Table = tabEnv.sqlQuery( gt; amp;gt;amp;nbsp;amp;nbsp; \"\"\" gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;SELECT MAX(DATE_FORMAT(rowtm, 'yyyy-MM-dd gt; HH:mm:00')) gt; amp;gt; time_str,COUNT(DISTINCT&#010;userkey) uv gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; FROM source gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;GROUP BY DATE_FORMAT(rowtm, 'yyyy-MM-dd') gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;\"\"\") gt; amp;gt; tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view) gt; amp;gt;&#010;gt; amp;gt; val totaluvTmp = gt; tabEnv.toRetractStream[(String,Long)](rt_totaluv_view) gt;&#010;amp;gt;amp;nbsp;amp;nbsp; .filter( line =amp;amp;gt; line._1 == true ).map( line gt; =amp;amp;gt;&#010;line._2 ) gt; amp;gt; gt; amp;gt; val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#010;gt; amp;gt; gt; amp;gt; tabEnv.sqlUpdate( gt; amp;gt;amp;nbsp;amp;nbsp; s\"\"\" gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;INSERT INTO mysql_totaluv gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; SELECT _1,MAX(_2)&#010;gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; FROM $totaluvTabTmp gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp;&#010;GROUP BY _1 gt; amp;gt;amp;nbsp;amp;nbsp;amp;nbsp;amp;nbsp; \"\"\") gt; amp;gt; ------------------amp;amp;nbsp;原始邮件amp;amp;nbsp;------------------&#010;gt; amp;gt; 发件人:amp;amp;nbsp;\"Benchao Li\"libenchao@apache.orgamp;amp;gt;; gt; amp;gt;&#010;发送时间:amp;amp;nbsp;2020年7月3日(星期五) 晚上9:47 gt; amp;gt; 收件人:amp;amp;nbsp;\"user-zh\"user-zh@flink.apache.orgamp;amp;gt;;&#010;gt; amp;gt; gt; amp;gt; 主题:amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; gt; amp;gt; gt; amp;gt; gt; amp;gt; 你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题， gt; amp;gt; 这个已经在1.11中修复了。 gt; amp;gt; gt;&#010;amp;gt; [1] https://issues.apache.org/jira/browse/FLINK-17942 gt; amp;gt; gt; amp;gt; x 35907418@qq.comamp;amp;gt;&#010;于2020年7月3日周五 下午4:34写道： gt; amp;gt; gt; amp;gt; amp;amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; gt; ------------------amp;amp;amp;nbsp;原始邮件amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; 发件人:amp;amp;amp;nbsp;\"Jark Wu\"imjark@gmail.comamp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; 发送时间:amp;amp;amp;nbsp;2020年6月18日(星期四) 中午12:16&#010;gt; amp;gt; amp;amp;gt; 收件人:amp;amp;amp;nbsp;\"user-zh\"user-zh@flink.apache.org gt; amp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; 主题:amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;是的，我觉得这样子是能绕过的。 gt; amp;gt; amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;On Thu, 18 Jun 2020 at 10:34, x 35907418@qq.comamp;amp;amp;gt; gt; wrote: gt; amp;gt; amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery( gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp; \"\"\" gt; amp;gt; amp;amp;gt;&#010;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; SELECT&#010;gt; amp;gt; MAX(DATE_FORMAT(ts, 'yyyy-MM-dd gt; amp;gt; amp;amp;gt; HH:mm:00')) gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv gt; amp;gt; amp;amp;gt; gt;&#010;amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; FROM gt;&#010;amp;gt; user_behavioramp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp; GROUP BY gt; amp;gt;&#010;amp;amp;gt; DATE_FORMAT(ts, gt; 'yyyy-MM-dd')amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;\"\"\") gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val gt;&#010;amp;gt; resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab) gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; gt; .filter(line=amp;amp;amp;amp;gt;line._1==true).map(line=amp;amp;amp;amp;gt;line._2)&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; tabEnv.sqlUpdate( gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;s\"\"\" gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;INSERT gt; INTO gt; amp;gt; rt_totaluv gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;SELECT gt; _1,MAX(_2) gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;FROM gt; $res gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;GROUP gt; BY _1 gt; amp;gt; amp;amp;gt; gt; amp;amp;amp;gt;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;amp;amp;amp;nbsp;&#010;\"\"\") gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; gt; ------------------amp;amp;amp;amp;nbsp;原始邮件amp;amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 发件人:amp;amp;amp;amp;nbsp;\"Jark Wu\" gt; imjark@gmail.comamp;amp;amp;amp;gt;;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 发送时间:amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午1:55 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 收件人:amp;amp;amp;amp;nbsp;\"user-zh\"&#010;gt; user-zh@flink.apache.org gt; amp;gt; amp;amp;amp;amp;gt;; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 主题:amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; CREATE TABLE&#010;mysql ( gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;time_str gt; STRING, gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;uv BIGINT, gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp;&#010;PRIMARY gt; KEY (ts) NOT ENFORCED gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; ) WITH ( gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; gt; 'connector' = 'jdbc',&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; 'url' =&#010;gt; amp;gt; 'jdbc:mysql://localhost:3306/mydatabase', gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;nbsp;amp;amp;amp;amp;nbsp; gt; 'table-name' = 'myuv' gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; ); gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;INSERT INTO mysql gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#010;gt; HH:mm:00')), gt; amp;gt; amp;amp;gt; COUNT(DISTINCTamp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; user_id) gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; FROM user_behavior; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; On Wed, 17 Jun 2020 at&#010;13:49, xnbsp; gt; 35907418@qq.comamp;amp;amp;amp;gt; gt; amp;gt; wrote: gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; sink表这个样式 gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; tm uv gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 2020/06/17 13:46:00 10000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;2020/06/17 13:47:00 20000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; 2020/06/17&#010;13:48:00 30000 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;group by 日期的话，分钟如何获取 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; ------------------amp;amp;amp;amp;amp;nbsp;原始邮件amp;amp;amp;amp;amp;nbsp;------------------&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 发件人:amp;amp;amp;amp;amp;nbsp;\"Benchao&#010;Li\" gt; amp;gt; libenchao@apache.org gt; amp;gt; amp;amp;gt; amp;amp;amp;amp;amp;gt;; gt;&#010;amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 发送时间:amp;amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午11:46 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 收件人:amp;amp;amp;amp;amp;nbsp;\"user-zh\"&#010;gt; amp;gt; user-zh@flink.apache.org gt; amp;gt; amp;amp;gt; amp;amp;amp;amp;amp;gt;; gt;&#010;amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 主题:amp;amp;amp;amp;amp;nbsp;Re: gt; 求助：FLINKSQL1.10实时统计累计UV&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; Hi， gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; 我感觉这种场景可以有两种方式， gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 1. 可以直接用group by + mini batch gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 2. window聚合 + fast emit gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; 对于#1，group&#010;gt; amp;gt; by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#010;gt; amp;gt; amp;amp;gt; 'yyyy-MM-dd')。 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;这种情况下的状态清理，需要配置state gt; retention时间，配置方法可以参考[1]&#010;gt; amp;gt; 。同时，mini gt; amp;gt; amp;amp;gt; batch的开启也需要 gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; 用参数[2] 来打开。 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; fast gt; amp;gt; amp;amp;gt; gt;&#010;emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; table.exec.emit.early-fire.enabled&#010;= true gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; table.exec.emit.early-fire.delay&#010;= 60 s gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; [1] gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; [2] gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; gt; amp;gt; gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; x 35907418@qq.com gt; amp;amp;amp;amp;amp;gt; gt; amp;gt; 于2020年6月17日周三&#010;上午11:14写道： gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt;&#010;需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; CREATE&#010;gt; VIEW uv_per_10min AS gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;gt; SELECTamp;amp;amp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; gt; amp;amp;amp;amp;amp;amp;nbsp; gt; amp;gt; amp;amp;gt; MAX(DATE_FORMAT(proctimeamp;amp;amp;amp;amp;amp;nbsp;,&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; 'yyyy-MM-dd gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; gt; HH:mm:00'))amp;amp;amp;amp;amp;amp;nbsp;OVER w gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; AS gt; amp;gt; time_str,amp;amp;amp;amp;amp;amp;nbsp;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; amp;amp;amp;amp;amp;amp;nbsp;&#010;gt; amp;gt; COUNT(DISTINCT user_id) OVER gt; amp;gt; amp;amp;gt; w AS uv gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; FROM gt; user_behavior gt; amp;gt;&#010;amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; WINDOW w gt; AS (ORDER&#010;BY proctime gt; amp;gt; ROWS BETWEEN gt; amp;gt; amp;amp;gt; UNBOUNDED gt; amp;gt; amp;amp;gt;&#010;amp;amp;amp;gt; PRECEDING AND gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; CURRENT gt; ROW); gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt;&#010;amp;amp;amp;amp;amp;gt; gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; 想请教一下，应该如何处理？&#010;gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; PARTITION&#010;gt; BY gt; amp;gt; DATE_FORMAT(rowtm, 'yyyy-MM-dd') gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;这样可以吗，另外状态应该如何清理？ gt; amp;gt; amp;amp;gt; amp;amp;amp;gt;&#010;amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt; gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#010;gt; amp;gt; VIEW吧 gt; amp;gt; amp;amp;gt; amp;amp;amp;gt; amp;amp;amp;amp;gt; amp;amp;amp;amp;amp;gt;&#010;多谢 gt; amp;gt; gt; amp;gt; gt; amp;gt; gt; amp;gt; -- gt; amp;gt; gt; amp;gt; Best, gt;&#010;amp;gt; Benchao Li gt; gt; gt; gt; -- gt; gt; Best, gt; Benchao Li -- Best, Benchao Li",
        "depth": "1",
        "reply": "<tencent_8D53CEEA7B745C773A5EA0E4@qq.com>"
    },
    {
        "id": "<202007071145178616921@163.com>",
        "from": "&quot;liuhy_email@163.com&quot; &lt;liuhy_em...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 03:45:18 GMT",
        "subject": "FlinkWebUI 参数疑问",
        "content": "Dear,&#013;&#010;    请问FlinkUI中TaskManager页面的参数含义是什么，有相关的文章描述吗？&#013;&#010;    &#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Thanks,&#013;&#010;Hongyang&#013;&#010;",
        "depth": "0",
        "reply": "<202007071145178616921@163.com>"
    },
    {
        "id": "<tencent_8E0E45008C7D3A05E365A7CEE65098762C09@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 04:14:57 GMT",
        "subject": "DataStream的state问题",
        "content": "Deal all:&#013;&#010;&#013;&#010;&#013;&#010;想问下,在给state设置ttl的时候，如下面的代码：&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; StateTtlConfig&#010;ttlConfig = StateTtlConfig&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .newBuilder(Time.days(1))&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .build();&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;设置了1天时间之后失效，例如2020-07-07 08:30:00点开始的job，那失效时间是这个时间段2020-07-07&#010;00:00:00~2020-07-07 23:59:59，还是job上线之后，2020-07-07 08:30:00~2020-07-08 08:30:00这个时间段?&#013;&#010;&#013;&#010;&#013;&#010;Thanks&#013;&#010;Jiazhi",
        "depth": "0",
        "reply": "<tencent_8E0E45008C7D3A05E365A7CEE65098762C09@qq.com>"
    },
    {
        "id": "<CAA8tFvspJMBj3_HW96k-82U9F7-VVPSXnwdujvAX-oc3oz_c6A@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 05:13:11 GMT",
        "subject": "Re: DataStream的state问题",
        "content": "Hi&#013;&#010;&#013;&#010;是最后一次 access 的时间到当前的时间超过了你设置的 ttl 间隔，比如你配置的是&#010;`OnCreateAndWrite`&#013;&#010;那么就是创建和写操作之后的 1 天，这个 state 会变成 expired，具体的可以参考文档[1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&gt; 于2020年7月7日周二 下午12:17写道：&#013;&#010;&#013;&#010;&gt; Deal all:&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 想问下,在给state设置ttl的时候，如下面的代码：&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; StateTtlConfig ttlConfig = StateTtlConfig&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .newBuilder(Time.days(1))&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&gt; .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; .build();&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 设置了1天时间之后失效，例如2020-07-07 08:30:00点开始的job，那失效时间是这个时间段2020-07-07&#013;&#010;&gt; 00:00:00~2020-07-07 23:59:59，还是job上线之后，2020-07-07 08:30:00~2020-07-08&#013;&#010;&gt; 08:30:00这个时间段?&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Thanks&#013;&#010;&gt; Jiazhi&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_8E0E45008C7D3A05E365A7CEE65098762C09@qq.com>"
    },
    {
        "id": "<tencent_7AE53F89EE301B12EA5929C42583365AC708@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:01:28 GMT",
        "subject": "flink sql 读写写kafka表的时候可以指定消息的key吗",
        "content": "hi：&#013;&#010;&amp;nbsp; flink sql 写kafka表的时候可以指定消息的key吗？&#013;&#010;看官网的kafka connector没有找到消息key相关的说明&#013;&#010;如果可以的话，如何指定？&#013;&#010;&amp;nbsp;谢谢",
        "depth": "0",
        "reply": "<tencent_7AE53F89EE301B12EA5929C42583365AC708@qq.com>"
    },
    {
        "id": "<37E141CE-99B1-4EED-B1A6-9752AF40F6D5@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:15:49 GMT",
        "subject": "Re: flink sql 读写写kafka表的时候可以指定消息的key吗",
        "content": "Hi,&#010;&#010;目前还不支持的，社区有一个 FLIP-107[1] 在计划做这个事情。&#010;&#010;祝好，&#010;Leonard Xu&#010;[1] https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records#FLIP107:Readingtablecolumnsfromdifferentpartsofsourcerecords-Kafka:ETL:read,transformandwritebackwithkey,value.Allfieldsofthekeyarepresentinthevalueaswell.&#010;&lt;https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records#FLIP107:Readingtablecolumnsfromdifferentpartsofsourcerecords-Kafka:ETL:read,transformandwritebackwithkey,value.Allfieldsofthekeyarepresentinthevalueaswell.&gt;&#010;&#010;&gt; 在 2020年7月7日，17:01，op &lt;520075694@qq.com&gt; 写道：&#010;&gt; &#010;&gt; hi：&#010;&gt; &amp;nbsp; flink sql 写kafka表的时候可以指定消息的key吗？&#010;&gt; 看官网的kafka connector没有找到消息key相关的说明&#010;&gt; 如果可以的话，如何指定？&#010;&gt; &amp;nbsp;谢谢&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_7AE53F89EE301B12EA5929C42583365AC708@qq.com>"
    },
    {
        "id": "<tencent_0AB53C39911AE640022E2B925874F0494B05@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:16:55 GMT",
        "subject": "回复： flink sql 读写写kafka表的时候可以指定消息的key吗",
        "content": "感谢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Leonard Xu\"&lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月7日(星期二) 下午5:15&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink sql 读写写kafka表的时候可以指定消息的key吗&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi,&#013;&#010;&#013;&#010;目前还不支持的，社区有一个 FLIP-107[1] 在计划做这个事情。&#013;&#010;&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;[1] https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records#FLIP107:Readingtablecolumnsfromdifferentpartsofsourcerecords-Kafka:ETL:read,transformandwritebackwithkey,value.Allfieldsofthekeyarepresentinthevalueaswell.&#010;&lt;https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records#FLIP107:Readingtablecolumnsfromdifferentpartsofsourcerecords-Kafka:ETL:read,transformandwritebackwithkey,value.Allfieldsofthekeyarepresentinthevalueaswell.&amp;gt;&#013;&#010;&#013;&#010;&amp;gt; 在 2020年7月7日，17:01，op &lt;520075694@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; hi：&#013;&#010;&amp;gt; &amp;amp;nbsp; flink sql 写kafka表的时候可以指定消息的key吗？&#013;&#010;&amp;gt; 看官网的kafka connector没有找到消息key相关的说明&#013;&#010;&amp;gt; 如果可以的话，如何指定？&#013;&#010;&amp;gt; &amp;amp;nbsp;谢谢",
        "depth": "2",
        "reply": "<tencent_7AE53F89EE301B12EA5929C42583365AC708@qq.com>"
    },
    {
        "id": "<CAELO9336=vRyo7F2_-h44yKJBZ+ko7WnGzo+seuRVBTz_kvz6w@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:01:26 GMT",
        "subject": "Re: flink sql 读写写kafka表的时候可以指定消息的key吗",
        "content": "Hi,&#013;&#010;&#013;&#010;可以描述下你的业务场景么？ 为什么一定要去获取 key 的信息呢，因为按照我的理解，一般来说&#010;key 的信息一般在 value 中也有。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Tue, 7 Jul 2020 at 17:17, op &lt;520075694@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 感谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Leonard Xu\"&lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月7日(星期二) 下午5:15&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: flink sql 读写写kafka表的时候可以指定消息的key吗&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 目前还不支持的，社区有一个 FLIP-107[1] 在计划做这个事情。&#013;&#010;&gt;&#013;&#010;&gt; 祝好，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; [1]&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records#FLIP107:Readingtablecolumnsfromdifferentpartsofsourcerecords-Kafka:ETL:read,transformandwritebackwithkey,value.Allfieldsofthekeyarepresentinthevalueaswell.&#013;&#010;&gt; &lt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records#FLIP107:Readingtablecolumnsfromdifferentpartsofsourcerecords-Kafka:ETL:read,transformandwritebackwithkey,value.Allfieldsofthekeyarepresentinthevalueaswell.&amp;gt&#013;&#010;&gt; ;&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 在 2020年7月7日，17:01，op &lt;520075694@qq.com&amp;gt; 写道：&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; hi：&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; flink sql 写kafka表的时候可以指定消息的key吗？&#013;&#010;&gt; &amp;gt; 看官网的kafka connector没有找到消息key相关的说明&#013;&#010;&gt; &amp;gt; 如果可以的话，如何指定？&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp;谢谢&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_7AE53F89EE301B12EA5929C42583365AC708@qq.com>"
    },
    {
        "id": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:12:41 GMT",
        "subject": "flink kafka connector中获取kafka元数据",
        "content": "hi、&#013;&#010;flink table/sql api中，有办法获取kafka元数据吗？&#013;&#010;&#013;&#010;tableEnvironment.sqlUpdate(CREATE TABLE MyUserTable (...) WITH&#013;&#010;('connector.type' = 'kafka','connector.version' = '0.11' ,...))&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>"
    },
    {
        "id": "<F153E364-0DA6-433B-85DD-F7FEA861FBD9@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:19:22 GMT",
        "subject": "Re: flink kafka connector中获取kafka元数据",
        "content": "Hi,&#010; kafka元数据 是指kafka记录里的 meta数据吗？ 比如kafka自带的timestamp，kafka的key信息。&#010;如果是这些信息的话， Table/SQL API 目前还没办法拿到， FLIP-107[1] 会支持这个事情。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;[1] https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records&#010;&lt;https://cwiki.apache.org/confluence/display/FLINK/FLIP-107:+Reading+table+columns+from+different+parts+of+source+records&gt;&#010;&#010;&gt; 在 2020年7月7日，17:12，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#010;&gt; &#010;&gt; kafka元数据&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>"
    },
    {
        "id": "<CAEZk042XNCU-ZvAqGm=SzWNFtORu+3GxHyB3nBUeVLC0JXTTPg@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:26:43 GMT",
        "subject": "Re: flink kafka connector中获取kafka元数据",
        "content": "hi&#013;&#010;是的，想以下面这种方式获取&#013;&#010;&#013;&#010;CREATE TABLE MyUserTable (key string,topic string,....,以及其他的数据字段) WITH&#013;&#010;('connector.type' = 'kafka','connector.version' = '0.11' ,...)&#013;&#010;&#013;&#010;&#013;&#010;On Tue, Jul 7, 2020 at 5:19 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;  kafka元数据 是指kafka记录里的 meta数据吗？ 比如kafka自带的timestamp，kafka的key信息。&#013;&#010;&gt; 如果是这些信息的话， Table/SQL API 目前还没办法拿到， FLIP-107[1]&#010;会支持这个事情。&#013;&#010;&gt;&#013;&#010;&gt; 祝好，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &lt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107:+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月7日，17:12，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; kafka元数据&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>"
    },
    {
        "id": "<C19B4DFC-BD1C-4955-BC77-7892E2E2A058@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:29:30 GMT",
        "subject": "Re: flink kafka connector中获取kafka元数据",
        "content": "嗯，这个在FLIP-107里会支持，目前没法拿到这些meta数据，可以关注下FLIP-107的进展。&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月7日，17:26，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#010;&gt; &#010;&gt; hi&#010;&gt; 是的，想以下面这种方式获取&#010;&gt; &#010;&gt; CREATE TABLE MyUserTable (key string,topic string,....,以及其他的数据字段) WITH&#010;&gt; ('connector.type' = 'kafka','connector.version' = '0.11' ,...)&#010;&gt; &#010;&gt; &#010;&gt; On Tue, Jul 7, 2020 at 5:19 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#010;&gt; &#010;&gt;&gt; Hi,&#010;&gt;&gt; kafka元数据 是指kafka记录里的 meta数据吗？ 比如kafka自带的timestamp，kafka的key信息。&#010;&gt;&gt; 如果是这些信息的话， Table/SQL API 目前还没办法拿到， FLIP-107[1]&#010;会支持这个事情。&#010;&gt;&gt; &#010;&gt;&gt; 祝好，&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt; &#010;&gt;&gt; [1]&#010;&gt;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records&#010;&gt;&gt; &lt;&#010;&gt;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107:+Reading+table+columns+from+different+parts+of+source+records&#010;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月7日，17:12，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; kafka元数据&#010;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "3",
        "reply": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>"
    },
    {
        "id": "<CAEZk042tqaAWt80RVh-wKKF3qqs6Qgk9k-1ckw-Ty5_FioiA4A@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:42:39 GMT",
        "subject": "Re: flink kafka connector中获取kafka元数据",
        "content": "好的&#013;&#010;&#013;&#010;On Tue, Jul 7, 2020 at 5:30 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 嗯，这个在FLIP-107里会支持，目前没法拿到这些meta数据，可以关注下FLIP-107的进展。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月7日，17:26，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hi&#013;&#010;&gt; &gt; 是的，想以下面这种方式获取&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; CREATE TABLE MyUserTable (key string,topic string,....,以及其他的数据字段)&#010;WITH&#013;&#010;&gt; &gt; ('connector.type' = 'kafka','connector.version' = '0.11' ,...)&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Tue, Jul 7, 2020 at 5:19 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi,&#013;&#010;&gt; &gt;&gt; kafka元数据 是指kafka记录里的 meta数据吗？ 比如kafka自带的timestamp，kafka的key信息。&#013;&#010;&gt; &gt;&gt; 如果是这些信息的话， Table/SQL API 目前还没办法拿到， FLIP-107[1]&#010;会支持这个事情。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 祝好，&#013;&#010;&gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &gt;&gt; &lt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107:+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 在 2020年7月7日，17:12，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; kafka元数据&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>"
    },
    {
        "id": "<CAELO933frvH5eUuztm-MnEqh7uiEChx4mnbsDEACWQgBERZ-qg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:14:35 GMT",
        "subject": "Re: flink kafka connector中获取kafka元数据",
        "content": "Hi,&#013;&#010;&#013;&#010;社区可能打算先支持 timestamp 和 key 的读写，其他 meta 信息（topic, partition,&#010;etc..）可能会以后再支持。&#013;&#010;你对其他 meta 信息的读取需求大吗？ 能具体说说你的业务场景不？感谢！&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;On Tue, 7 Jul 2020 at 17:42, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 好的&#013;&#010;&gt;&#013;&#010;&gt; On Tue, Jul 7, 2020 at 5:30 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; 嗯，这个在FLIP-107里会支持，目前没法拿到这些meta数据，可以关注下FLIP-107的进展。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Leonard Xu&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 在 2020年7月7日，17:26，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; hi&#013;&#010;&gt; &gt; &gt; 是的，想以下面这种方式获取&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; CREATE TABLE MyUserTable (key string,topic string,....,以及其他的数据字段)&#010;WITH&#013;&#010;&gt; &gt; &gt; ('connector.type' = 'kafka','connector.version' = '0.11' ,...)&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; On Tue, Jul 7, 2020 at 5:19 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; Hi,&#013;&#010;&gt; &gt; &gt;&gt; kafka元数据 是指kafka记录里的 meta数据吗？ 比如kafka自带的timestamp，kafka的key信息。&#013;&#010;&gt; &gt; &gt;&gt; 如果是这些信息的话， Table/SQL API 目前还没办法拿到，&#010;FLIP-107[1] 会支持这个事情。&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; 祝好，&#013;&#010;&gt; &gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; [1]&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &gt; &gt;&gt; &lt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107:+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; 在 2020年7月7日，17:12，Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&gt; kafka元数据&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>"
    },
    {
        "id": "<CAEZk042JS9RQuTjrECwL5Y39U3e=0mXPMy7Vx6OhLGCR1-1NVA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:31:47 GMT",
        "subject": "Re: flink kafka connector中获取kafka元数据",
        "content": "hi、&#013;&#010;其实我这面对key和timestamp需求不大，而对topic、partition、offset需求大一点，主要有两种场景：&#013;&#010;1、根据offset筛选出一组数据的最新值，这个之所以不用时间戳来筛选是因为同一个时间戳可能对应多条数据&#013;&#010;2、这个场景有点像kafka数据的备份，在存储端要存储topic、partition、offset元数据信息&#013;&#010;&#013;&#010;On Tue, Jul 7, 2020 at 6:14 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 社区可能打算先支持 timestamp 和 key 的读写，其他 meta 信息（topic,&#010;partition, etc..）可能会以后再支持。&#013;&#010;&gt; 你对其他 meta 信息的读取需求大吗？ 能具体说说你的业务场景不？感谢！&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; On Tue, 7 Jul 2020 at 17:42, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; 好的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Tue, Jul 7, 2020 at 5:30 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 嗯，这个在FLIP-107里会支持，目前没法拿到这些meta数据，可以关注下FLIP-107的进展。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Leonard Xu&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在 2020年7月7日，17:26，Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; hi&#013;&#010;&gt; &gt; &gt; &gt; 是的，想以下面这种方式获取&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; CREATE TABLE MyUserTable (key string,topic string,....,以及其他的数据字段)&#013;&#010;&gt; WITH&#013;&#010;&gt; &gt; &gt; &gt; ('connector.type' = 'kafka','connector.version' = '0.11' ,...)&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; On Tue, Jul 7, 2020 at 5:19 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; Hi,&#013;&#010;&gt; &gt; &gt; &gt;&gt; kafka元数据 是指kafka记录里的 meta数据吗？ 比如kafka自带的timestamp，kafka的key信息。&#013;&#010;&gt; &gt; &gt; &gt;&gt; 如果是这些信息的话， Table/SQL API 目前还没办法拿到，&#010;FLIP-107[1] 会支持这个事情。&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; 祝好，&#013;&#010;&gt; &gt; &gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt; [1]&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &gt; &gt; &gt;&gt; &lt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-107:+Reading+table+columns+from+different+parts+of+source+records&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; 在 2020年7月7日，17:12，Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&gt; kafka元数据&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<CAEZk042PA6UGmd-gt9Cj2NH04h=xZPboj=+x3e5P8tu0M7WG=A@mail.gmail.com>"
    },
    {
        "id": "<1594113600748-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:20:00 GMT",
        "subject": "一个source多个sink的同步问题",
        "content": "source是kafka，有一个rowtime定义：&#010;&#010;            .field(\"rowtime\", DataTypes.TIMESTAMP(0))&#010;            .rowtime(Rowtime()&#010;                .timestamps_from_field(\"actionTime\")&#010;                .watermarks_periodic_bounded(60000)&#010;            )&#010;&#010;有两个sink，第一个sink是直接把kafa的数据保存到postgres。&#010;第二个sink是定义一个1小时的tumble window，然后定义了一个udf，udf里面去查询第一个sink保存的数据。&#010;    st_env.scan(\"source\") \\&#010;         .window(Tumble.over(\"1.hour\").on(\"rowtime\").alias(\"hourlywindow\"))&#010;\\&#010;         .group_by(\"hourlywindow\") \\&#010;         .select(\"udf(...)\")&#010;         ...&#010;&#010;&#010;现在的问题是：第二个sink的tumble window触发的时候，数据库里面的数据已经保存了下一个小时的数据了。&#010;&#010;有什么办法让tumble window在一个小时结束后马上触发？现在观察的是需要下一个小时的数据来了，才能触发上一个小时的窗口。&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1594113600748-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO932RoJgak0m6Tcgifq8guE6RBaAr8eXA787CpSxYfZ-q=A@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:08:13 GMT",
        "subject": "Re: 一个source多个sink的同步问题",
        "content": "watermark 的计算是跟数据上的 event-time 相关的。你的数据是不是间隔一小时来一波的呢？&#013;&#010;比如 10:00 的数据之后，就是 11:00 的数据，但是要1小时后才到来？&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Tue, 7 Jul 2020 at 17:20, lgs &lt;9925174@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; source是kafka，有一个rowtime定义：&#013;&#010;&gt;&#013;&#010;&gt;             .field(\"rowtime\", DataTypes.TIMESTAMP(0))&#013;&#010;&gt;             .rowtime(Rowtime()&#013;&#010;&gt;                 .timestamps_from_field(\"actionTime\")&#013;&#010;&gt;                 .watermarks_periodic_bounded(60000)&#013;&#010;&gt;             )&#013;&#010;&gt;&#013;&#010;&gt; 有两个sink，第一个sink是直接把kafa的数据保存到postgres。&#013;&#010;&gt; 第二个sink是定义一个1小时的tumble window，然后定义了一个udf，udf里面去查询第一个sink保存的数据。&#013;&#010;&gt;     st_env.scan(\"source\") \\&#013;&#010;&gt;          .window(Tumble.over(\"1.hour\").on(\"rowtime\").alias(\"hourlywindow\"))&#013;&#010;&gt; \\&#013;&#010;&gt;          .group_by(\"hourlywindow\") \\&#013;&#010;&gt;          .select(\"udf(...)\")&#013;&#010;&gt;          ...&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 现在的问题是：第二个sink的tumble window触发的时候，数据库里面的数据已经保存了下一个小时的数据了。&#013;&#010;&gt;&#013;&#010;&gt; 有什么办法让tumble window在一个小时结束后马上触发？现在观察的是需要下一个小时的数据来了，才能触发上一个小时的窗口。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<1594113600748-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594118663805-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:44:23 GMT",
        "subject": "Re: 一个source多个sink的同步问题",
        "content": "是1个小时才到来。10：00- 11：00的数据，11：01分到来。&#013;&#010;&#013;&#010;但是现在的问题是这个数据来了，我的第一个sink马上就保存到数据库了，&#010;11：02进数据库。但是第二个sink，因为有tumble&#010;window，所以10：00- 11：00的数据，需要到12：01，才会触发这个窗口。&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "2",
        "reply": "<1594113600748-0.post@n8.nabble.com>"
    },
    {
        "id": "<20bb3b.ab1e.1732f516ef6.Coremail.17626017841@163.com>",
        "from": "&quot;Sun.Zhu&quot; &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 16:45:32 GMT",
        "subject": "回复： 一个source多个sink的同步问题",
        "content": "&#010;&#010;窗口的触发逻辑就是这样的，必须watermark达到了窗口结束时间才会触发，可能10-11点的窗口中的数据最大只有10：59呢&#010;| |&#010;Sun.Zhu&#010;|&#010;|&#010;17626017841@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;&#010;&#010;在2020年07月7日 18:44，lgs&lt;9925174@qq.com&gt; 写道：&#010;是1个小时才到来。10：00- 11：00的数据，11：01分到来。&#010;&#010;但是现在的问题是这个数据来了，我的第一个sink马上就保存到数据库了，&#010;11：02进数据库。但是第二个sink，因为有tumble&#010;window，所以10：00- 11：00的数据，需要到12：01，才会触发这个窗口。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "3",
        "reply": "<1594113600748-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>",
        "from": "&quot;noake&quot;&lt;no...@sina.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:25:45 GMT",
        "subject": "如何在Flink SQL中使用周期性水印?",
        "content": "Dear All：&#010;&#010;&#010;大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#010;我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。",
        "depth": "0",
        "reply": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>"
    },
    {
        "id": "<CAELO9322_vWqyq5nwSN5evuZ+qTXuU-EHtseio1NpBnBJVR80A@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:09:38 GMT",
        "subject": "Re: 如何在Flink SQL中使用周期性水印?",
        "content": "Hi,&#013;&#010;&#013;&#010;这个问题我理解其实和周期性水印没有关系，是属于 idle source&#013;&#010;的问题，你可以尝试下加上配置 table.exec.source.idle-timeout = 10s 能不能解决你的问题。[1]&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;[1]:&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&#013;&#010;&#013;&#010;On Tue, 7 Jul 2020 at 17:35, noake &lt;noake@sina.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; Dear All：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#013;&#010;&gt; 我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>"
    },
    {
        "id": "<tencent_26DEC0719116734BB6076DCABE875C693907@qq.com>",
        "from": "&quot;1193216154&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:08:08 GMT",
        "subject": "回复： 如何在Flink SQL中使用周期性水印?",
        "content": "hi Jark Wu.&#013;&#010;我的理解是table.exec.source.idle-timeout只能解决watermark对齐的时候去忽略某个没有watermark的并行度。但是在每个并行度都没有watermark的时候，还是无法更新watermark。&#013;&#010;我觉得题主的意思应该是，在kafka的所有分区都没有数据的时候，最后一个窗口无法触发（因为没有watermark大于最后那个窗口结束时间了）。&#013;&#010;有没有可以设置在eventTime情况下，周期性生成当前时间的一个waterMark(和数据无关),因为可能没有新数据到来了。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月7日(星期二) 晚上6:09&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi,&#013;&#010;&#013;&#010;这个问题我理解其实和周期性水印没有关系，是属于 idle source&#013;&#010;的问题，你可以尝试下加上配置 table.exec.source.idle-timeout = 10s 能不能解决你的问题。[1]&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;[1]:&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&#013;&#010;&#013;&#010;On Tue, 7 Jul 2020 at 17:35, noake &lt;noake@sina.cn&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; Dear All：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#013;&#010;&amp;gt; 我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。",
        "depth": "2",
        "reply": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>"
    },
    {
        "id": "<CAELO932h_uK4R+NiFOmFc3ucSxzfp_K5SyDWvui1dUc=2VXcSA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:48:35 GMT",
        "subject": "Re: 如何在Flink SQL中使用周期性水印?",
        "content": "如果所有 partition 都没有数据，还希望 watermark 往前走，那 idle source 确实解决不了这个问题。&#013;&#010;目前确实没有太好的解决办法。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Wed, 8 Jul 2020 at 11:08, 1193216154 &lt;1193216154@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi Jark Wu.&#013;&#010;&gt;&#013;&#010;&gt; 我的理解是table.exec.source.idle-timeout只能解决watermark对齐的时候去忽略某个没有watermark的并行度。但是在每个并行度都没有watermark的时候，还是无法更新watermark。&#013;&#010;&gt; 我觉得题主的意思应该是，在kafka的所有分区都没有数据的时候，最后一个窗口无法触发（因为没有watermark大于最后那个窗口结束时间了）。&#013;&#010;&gt; 有没有可以设置在eventTime情况下，周期性生成当前时间的一个waterMark(和数据无关),因为可能没有新数据到来了。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月7日(星期二) 晚上6:09&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 这个问题我理解其实和周期性水印没有关系，是属于 idle source&#013;&#010;&gt; 的问题，你可以尝试下加上配置 table.exec.source.idle-timeout = 10s 能不能解决你的问题。[1]&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; [1]:&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&#013;&#010;&gt;&#013;&#010;&gt; On Tue, 7 Jul 2020 at 17:35, noake &lt;noake@sina.cn&amp;gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; Dear All：&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#013;&#010;&gt; &amp;gt; 我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>"
    },
    {
        "id": "<tencent_2B7C8D8D503BB5EF2657564E2D5052A4C905@qq.com>",
        "from": "&quot;1193216154&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 04:01:00 GMT",
        "subject": "回复： 如何在Flink SQL中使用周期性水印?",
        "content": "&amp;nbsp; &amp;nbsp;Jark，flink有没有必要去支持这个特性？我感觉还是有一些应用场景&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月8日(星期三) 中午11:48&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;如果所有 partition 都没有数据，还希望 watermark 往前走，那 idle source 确实解决不了这个问题。&#013;&#010;目前确实没有太好的解决办法。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Wed, 8 Jul 2020 at 11:08, 1193216154 &lt;1193216154@qq.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; hi Jark Wu.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我的理解是table.exec.source.idle-timeout只能解决watermark对齐的时候去忽略某个没有watermark的并行度。但是在每个并行度都没有watermark的时候，还是无法更新watermark。&#013;&#010;&amp;gt; 我觉得题主的意思应该是，在kafka的所有分区都没有数据的时候，最后一个窗口无法触发（因为没有watermark大于最后那个窗口结束时间了）。&#013;&#010;&amp;gt; 有没有可以设置在eventTime情况下，周期性生成当前时间的一个waterMark(和数据无关),因为可能没有新数据到来了。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月7日(星期二) 晚上6:09&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Hi,&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 这个问题我理解其实和周期性水印没有关系，是属于 idle source&#013;&#010;&amp;gt; 的问题，你可以尝试下加上配置 table.exec.source.idle-timeout = 10s 能不能解决你的问题。[1]&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Jark&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; [1]:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; On Tue, 7 Jul 2020 at 17:35, noake &lt;noake@sina.cn&amp;amp;gt; wrote:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Dear All：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#013;&#010;&amp;gt; &amp;amp;gt; 我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。",
        "depth": "4",
        "reply": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>"
    },
    {
        "id": "<CAELO932qAKjudOHQFxzxW26DO5KqTneMfzvP-GvapCxJKGZoXQ@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 05:26:53 GMT",
        "subject": "Re: 如何在Flink SQL中使用周期性水印?",
        "content": "嗯， 可以在 JIRA 中开个 issue 描述下你的需求~&#013;&#010;&#013;&#010;On Wed, 8 Jul 2020 at 12:01, 1193216154 &lt;1193216154@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;Jark，flink有没有必要去支持这个特性？我感觉还是有一些应用场景&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月8日(星期三) 中午11:48&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 如果所有 partition 都没有数据，还希望 watermark 往前走，那 idle source&#010;确实解决不了这个问题。&#013;&#010;&gt; 目前确实没有太好的解决办法。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Wed, 8 Jul 2020 at 11:08, 1193216154 &lt;1193216154@qq.com&amp;gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; hi Jark Wu.&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; 我的理解是table.exec.source.idle-timeout只能解决watermark对齐的时候去忽略某个没有watermark的并行度。但是在每个并行度都没有watermark的时候，还是无法更新watermark。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; 我觉得题主的意思应该是，在kafka的所有分区都没有数据的时候，最后一个窗口无法触发（因为没有watermark大于最后那个窗口结束时间了）。&#013;&#010;&gt; &amp;gt; 有没有可以设置在eventTime情况下，周期性生成当前时间的一个waterMark(和数据无关),因为可能没有新数据到来了。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; 发件人:&amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;gt;;&#013;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月7日(星期二) 晚上6:09&#013;&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; Hi,&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 这个问题我理解其实和周期性水印没有关系，是属于 idle source&#013;&#010;&gt; &amp;gt; 的问题，你可以尝试下加上配置 table.exec.source.idle-timeout =&#010;10s 能不能解决你的问题。[1]&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; Best,&#013;&#010;&gt; &amp;gt; Jark&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; [1]:&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&#013;&#010;&gt; &amp;gt&#013;&#010;&gt; &lt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&amp;gt&gt;&#013;&#010;&gt; ;&#013;&#010;&gt; &amp;gt; On Tue, 7 Jul 2020 at 17:35, noake &lt;noake@sina.cn&amp;amp;gt; wrote:&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; Dear All：&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。&#013;&#010;",
        "depth": "5",
        "reply": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>"
    },
    {
        "id": "<tencent_884C04C55C8A6D473BA90F2D174C2E259005@qq.com>",
        "from": "&quot;1193216154&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:08:18 GMT",
        "subject": "回复： 如何在Flink SQL中使用周期性水印?",
        "content": "欢迎加入讨论&amp;nbsp; https://issues.apache.org/jira/browse/FLINK-18523&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月8日(星期三) 中午1:26&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;嗯， 可以在 JIRA 中开个 issue 描述下你的需求~&#013;&#010;&#013;&#010;On Wed, 8 Jul 2020 at 12:01, 1193216154 &lt;1193216154@qq.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;Jark，flink有没有必要去支持这个特性？我感觉还是有一些应用场景&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月8日(星期三) 中午11:48&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 如果所有 partition 都没有数据，还希望 watermark 往前走，那 idle&#010;source 确实解决不了这个问题。&#013;&#010;&amp;gt; 目前确实没有太好的解决办法。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Jark&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; On Wed, 8 Jul 2020 at 11:08, 1193216154 &lt;1193216154@qq.com&amp;amp;gt; wrote:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; hi Jark Wu.&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 我的理解是table.exec.source.idle-timeout只能解决watermark对齐的时候去忽略某个没有watermark的并行度。但是在每个并行度都没有watermark的时候，还是无法更新watermark。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 我觉得题主的意思应该是，在kafka的所有分区都没有数据的时候，最后一个窗口无法触发（因为没有watermark大于最后那个窗口结束时间了）。&#013;&#010;&amp;gt; &amp;amp;gt; 有没有可以设置在eventTime情况下，周期性生成当前时间的一个waterMark(和数据无关),因为可能没有新数据到来了。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月7日(星期二) 晚上6:09&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 如何在Flink SQL中使用周期性水印?&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Hi,&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 这个问题我理解其实和周期性水印没有关系，是属于&#010;idle source&#013;&#010;&amp;gt; &amp;amp;gt; 的问题，你可以尝试下加上配置 table.exec.source.idle-timeout&#010;= 10s 能不能解决你的问题。[1]&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; Jark&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; [1]:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&#013;&#010;&amp;gt; &amp;amp;gt&#013;&#010;&amp;gt; &lt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&amp;amp;gt&amp;gt;&#013;&#010;&amp;gt; ;&#013;&#010;&amp;gt; &amp;amp;gt; On Tue, 7 Jul 2020 at 17:35, noake &lt;noake@sina.cn&amp;amp;amp;gt;&#010;wrote:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Dear All：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。",
        "depth": "6",
        "reply": "<tencent_4DBA987E9CE70279AFA7976F@qq.com>"
    },
    {
        "id": "<CANYrj=LNkKyv96BKXK4rd9eym2yjjtUGZcmb+jsTvX4XRe3BDQ@mail.gmail.com>",
        "from": "Jun Zou &lt;nianjun...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:37:59 GMT",
        "subject": "嵌套 json 中string 数组的解析异常",
        "content": "Hi all：&#013;&#010;我使用 flink 1.9 处理嵌套 json， 它嵌套了一个string数组，构造出的 table&#010;schema结构为：&#013;&#010;Row(parsedResponse: BasicArrayTypeInfo&lt;String&gt;, timestamp: Long)&#013;&#010;执行作业后会发生报错如下，出现 object 类型和string 类型的转换错误&#013;&#010;Caused by: java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast&#013;&#010;to [Ljava.lang.String;&#013;&#010;at&#013;&#010;org.apache.flink.api.common.typeutils.base.array.StringArraySerializer.copy(StringArraySerializer.java:35)&#013;&#010;at&#013;&#010;org.apache.flink.api.java.typeutils.runtime.RowSerializer.copy(RowSerializer.java:93)&#013;&#010;at&#013;&#010;org.apache.flink.api.java.typeutils.runtime.RowSerializer.copy(RowSerializer.java:44)&#013;&#010;&#013;&#010;大佬们知道该怎么修改么？&#013;&#010;&#013;&#010;我的json 的结构如下：&#013;&#010;{\"parsedResponse\":[\"apple\", \"banana\", \"orange\"], \"timestamp\": \"1522253345\"}&#013;&#010; P.S:&#013;&#010;如果把 string 数组改为 long 数组或者 double 数组执行对应的操作可以正确运行，目前来看只有&#010;string 数组出现问题。&#013;&#010;",
        "depth": "0",
        "reply": "<CANYrj=LNkKyv96BKXK4rd9eym2yjjtUGZcmb+jsTvX4XRe3BDQ@mail.gmail.com>"
    },
    {
        "id": "<111AECEC-239B-43D6-AE3D-113F4CFB8769@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:48:16 GMT",
        "subject": "Re: 嵌套 json 中string 数组的解析异常",
        "content": "Hi,&#013;&#010;&#013;&#010;方便把 SQL 也贴下吗？看起来像个bug。&#013;&#010;&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<CANYrj=LNkKyv96BKXK4rd9eym2yjjtUGZcmb+jsTvX4XRe3BDQ@mail.gmail.com>"
    },
    {
        "id": "<CANYrj=+0bvFwFGsmaHgeQ3+qdFw6pJrMWn95UfzaTA1=FhwMcQ@mail.gmail.com>",
        "from": "Jun Zou &lt;nianjun...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 11:30:11 GMT",
        "subject": "Re: 嵌套 json 中string 数组的解析异常",
        "content": "Hi, Leonard Xu:&#010;&#010;我使用的 sql 如下，&#010;&#010;&gt; SELECT TUMBLE_START(rowtime, INTERVAL '30' SECOND) AS ts, fruit,&#010;&gt; COUNT(`fruit`) AS `cnt`&#010;&gt; FROM mysource, UNNEST(mysource.parsedResponse) AS A(fruit)&#010;&gt; GROUP BY TUMBLE(rowtime, INTERVAL '30' SECOND), fruit&#010;&#010;&#010;从调试日志来看，应该是一开始就挂掉了，我贴一下相关的日志&#010;&#010;INFO - Initializing heap keyed state backend with stream factory.&#010;&#010;INFO - Source: Custom Source -&gt; Timestamps/Watermarks -&gt; from:&#010;&gt; (parsedResponse, rowtime) -&gt; correlate:&#010;&gt; table(explode($cor0.parsedResponse)), select: parsedResponse, rowtime, f0&#010;&gt; -&gt; select: (rowtime, fruit) -&gt; time attribute: (rowtime) (1/1)&#010;&gt; (d8c5f92b850811595dbdc130c04f9e58) switched from RUNNING to FAILED.&#010;&gt; java.lang.Exception:&#010;&gt; org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException:&#010;&gt; Could not forward element to next operator&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.checkThrowSourceExecutionException(SourceStreamTask.java:212)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.SourceStreamTask.performDefaultAction(SourceStreamTask.java:132)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:298)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:403)&#010;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)&#010;&gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)&#010;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by:&#010;&gt; org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException:&#010;&gt; Could not forward element to next operator&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:651)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:612)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:592)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:727)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:705)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollectWithTimestamp(StreamSourceContexts.java:310)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collectWithTimestamp(StreamSourceContexts.java:409)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.connectors.CommonConsumer.run(CommonConsumer.java:49)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:202)&#010;&gt; Caused by: java.lang.ClassCastException: [Ljava.lang.Object; cannot be&#010;&gt; cast to [Ljava.lang.String;&#010;&gt; at&#010;&gt; org.apache.flink.api.common.typeutils.base.array.StringArraySerializer.copy(StringArraySerializer.java:35)&#010;&gt; at&#010;&gt; org.apache.flink.api.java.typeutils.runtime.RowSerializer.copy(RowSerializer.java:93)&#010;&gt; at&#010;&gt; org.apache.flink.api.java.typeutils.runtime.RowSerializer.copy(RowSerializer.java:44)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:635)&#010;&gt; ... 10 more&#010;&gt;&#010;&#010;另外，如果我把string 数组的类型从  BasicArrayTypeInfo.STRING_ARRAY_TYPE_INFO&#010;改为 ObjectArrayTypeInfo.getInfoFor(Types.STRING)， 即schema 从&#010;&#010;&gt; root&#010;&gt;  |-- parsedResponse: LEGACY(BasicArrayTypeInfo&lt;String&gt;)&#010;&gt;  |-- rowtime: TIMESTAMP(3) *ROWTIME*&#010;&gt;&#010;变为&#010;&#010;&gt; root&#010;&gt;  |-- parsedResponse: ARRAY&lt;STRING&gt;&#010;&gt;  |-- rowtime: TIMESTAMP(3) *ROWTIME*&#010;&gt;&#010;&#010;也仍然会发生相同的错误，但日志执行有些不同&#010;&#010;&gt; INFO - Source: Custom Source -&gt; Timestamps/Watermarks -&gt; from:&#010;&gt; (parsedResponse, rowtime) -&gt; correlate:&#010;&gt; table(explode($cor0.parsedResponse)), select: parsedResponse, rowtime, f0&#010;&gt; -&gt; select: (rowtime, fruit) -&gt; time attribute: (rowtime) (1/1)&#010;&gt; (36b79032354b9e9ab70a30d98b1de903) switched from RUNNING to FAILED.&#010;&gt; java.lang.Exception:&#010;&gt; org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException:&#010;&gt; Could not forward element to next operator&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.checkThrowSourceExecutionException(SourceStreamTask.java:212)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.SourceStreamTask.performDefaultAction(SourceStreamTask.java:132)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.run(StreamTask.java:298)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:403)&#010;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:705)&#010;&gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:530)&#010;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by:&#010;&gt; org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException:&#010;&gt; Could not forward element to next operator&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:654)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:612)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:592)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:727)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:705)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollectWithTimestamp(StreamSourceContexts.java:310)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collectWithTimestamp(StreamSourceContexts.java:409)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.connectors.CommonConsumer.run(CommonConsumer.java:49)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:202)&#010;&gt; Caused by:&#010;&gt; org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException:&#010;&gt; Could not forward element to next operator&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:651)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:612)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:592)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:727)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:705)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.operators.TimestampsAndPeriodicWatermarksOperator.processElement(TimestampsAndPeriodicWatermarksOperator.java:67)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:637)&#010;&gt; ... 10 more&#010;&gt; Caused by: java.lang.ClassCastException: [Ljava.lang.Object; cannot be&#010;&gt; cast to [Ljava.lang.String;&#010;&gt; at DataStreamSourceConversion$5.processElement(Unknown Source)&#010;&gt; at&#010;&gt; org.apache.flink.table.runtime.CRowOutputProcessRunner.processElement(CRowOutputProcessRunner.scala:70)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.api.operators.ProcessOperator.processElement(ProcessOperator.java:66)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:637)&#010;&gt; ... 16 more&#010;&gt;&#010;&#010;我尝试使用 JsonRowSchemaConverter对 schema 进行转换，得到的schema和上一封邮件里面是一致的，即：&#010;&#010;&gt; Row(parsedResponse: BasicArrayTypeInfo&lt;String&gt;, timestamp: Timestamp)&#010;&gt;&#010;所以是我的操作在哪里出现了问题呢？&#010;&#010;感谢您的回复！&#010;&#010;祝好！&#010;&#010;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月7日周二 下午5:48写道：&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; 方便把 SQL 也贴下吗？看起来像个bug。&#010;&gt;&#010;&gt; 祝好，&#010;&gt; Leonard Xu&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CANYrj=LNkKyv96BKXK4rd9eym2yjjtUGZcmb+jsTvX4XRe3BDQ@mail.gmail.com>"
    },
    {
        "id": "<51BC620D-ACFE-4FC6-9AE5-541B815DD6B5@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 13:48:57 GMT",
        "subject": "Re: 嵌套 json 中string 数组的解析异常",
        "content": "Hi, &#010;&#010;看了下代码，这确实是Flink 1.9里面的一个bug[1], 原因没有 source 没有正确处理legacy&#010;type 和新的 type，这个issue没有在1.9的分支上修复，可以升级到1.10.1试下。&#010;&#010;祝好，&#010;Leonard Xu&#010;[1]https://issues.apache.org/jira/browse/FLINK-16622 &lt;https://issues.apache.org/jira/browse/FLINK-16622?focusedCommentId=17061790&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17061790&gt;&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<CANYrj=LNkKyv96BKXK4rd9eym2yjjtUGZcmb+jsTvX4XRe3BDQ@mail.gmail.com>"
    },
    {
        "id": "<CANYrj=JpEJaaSgdkp42tMtEAUVSdwCD9sdopK5960ekfagZpnw@mail.gmail.com>",
        "from": "Jun Zou &lt;nianjun...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 02:31:41 GMT",
        "subject": "Re: 嵌套 json 中string 数组的解析异常",
        "content": "Hi，&#013;&#010;感谢您的指导！&#013;&#010;&#013;&#010;祝好！&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月7日周二 下午9:49写道：&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 看了下代码，这确实是Flink 1.9里面的一个bug[1], 原因没有 source 没有正确处理legacy&#010;type 和新的&#013;&#010;&gt; type，这个issue没有在1.9的分支上修复，可以升级到1.10.1试下。&#013;&#010;&gt;&#013;&#010;&gt; 祝好，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; [1]https://issues.apache.org/jira/browse/FLINK-16622 &lt;&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-16622?focusedCommentId=17061790&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17061790&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CANYrj=LNkKyv96BKXK4rd9eym2yjjtUGZcmb+jsTvX4XRe3BDQ@mail.gmail.com>"
    },
    {
        "id": "<5037bd09.29c3.17329847305.Coremail.kevinyunhe@163.com>",
        "from": "邹云鹤 &lt;kevinyu...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 13:43:32 GMT",
        "subject": "Flink 1.11 SQL作业中调用UDTF 出现“No match found for function signature ”异常",
        "content": "&#010;&#010;hi all&#010;本人基于Flink 1.11 SNAPSHOT 在 Flink sql 作业中使用 UDTF, UDTF 的定义如下：&#010;&#010;&#010;@FunctionHint(&#010;        input = {@DataTypeHint(\"STRING\"), @DataTypeHint(\"STRING\")},&#010;        output = @DataTypeHint(\"STRING\")&#010;)&#010;public class Split extends TableFunction&lt;String&gt; {&#010;  public Split(){}&#010;  public void eval(String str, String ch) {&#010;    if (str == null || str.isEmpty()) {&#010;      return;&#010;    } else {&#010;      String[] ss = str.split(ch);&#010;      for (String s : ss) {&#010;        collect(s);&#010;      }&#010;    }&#010;  }&#010;} &#010;&#010;&#010;在flink sql中通过 create function splitByChar as '**.**.Split' 来创建这个function，在tableEnv&#010;中调用executeSql(....) 来完成对这个 function的注册，在sql 后面的计算逻辑中&#010;通过以下方式来调用这个UDTF &#010;create view view_source_1 as select `dateTime，`itime`,  lng,lat,net,event_info, cast(split_index(T.s,&#010;'_', 0) as int) as time_page from view_source as a left join LATERAL TABLE (splitByChar('a,b,c',','))&#010;as T(s) on true;&#010;&#010;&#010;结果一直出现以下错误信息：&#010;org.apache.flink.table.api.ValidationException: SQL validation failed. From line 3, column&#010;25 to line 3, column 47: No match found for function signature splitByChar(&lt;CHARACTER&gt;,&#010;&lt;CHARACTER&gt;)&#010;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)&#010;at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:629)&#010;....................&#010;Caused by: org.apache.calcite.runtime.CalciteContextException: From line 3, column 25 to line&#010;3, column 47: r(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;... 8 more&#010;Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No match found for function&#010;signature splitByChar(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&#010;&#010;之前在flink 1.10 里面使用是正常的， 问下各位大佬有没有在flink 1.11 遇到过这个错误，&#010;麻烦提供一下帮助。&#010;| |&#010;邹云鹤&#010;|&#010;|&#010;kevinyunhe@163.com&#010;|&#010;签名由网易邮箱大师定制",
        "depth": "0",
        "reply": "<5037bd09.29c3.17329847305.Coremail.kevinyunhe@163.com>"
    },
    {
        "id": "<CABKuJ_SmTKfpmYrwL91Mdzi_f2+KWJQ2RXr_G6xgq5bA0FwLpA@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 15:27:51 GMT",
        "subject": "Re: Flink 1.11 SQL作业中调用UDTF 出现“No match found for function signature ”异常",
        "content": "我感觉这应该是新版本的udf的bug，我在本地也可以复现。&#010;已经建了一个issue[1] 来跟进。&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18520&#010;&#010;邹云鹤 &lt;kevinyunhe@163.com&gt; 于2020年7月7日周二 下午9:43写道：&#010;&#010;&gt;&#010;&gt;&#010;&gt; hi all&#010;&gt; 本人基于Flink 1.11 SNAPSHOT 在 Flink sql 作业中使用 UDTF, UDTF 的定义如下：&#010;&gt;&#010;&gt;&#010;&gt; @FunctionHint(&#010;&gt;         input = {@DataTypeHint(\"STRING\"), @DataTypeHint(\"STRING\")},&#010;&gt;         output = @DataTypeHint(\"STRING\")&#010;&gt; )&#010;&gt; public class Split extends TableFunction&lt;String&gt; {&#010;&gt;   public Split(){}&#010;&gt;   public void eval(String str, String ch) {&#010;&gt;     if (str == null || str.isEmpty()) {&#010;&gt;       return;&#010;&gt;     } else {&#010;&gt;       String[] ss = str.split(ch);&#010;&gt;       for (String s : ss) {&#010;&gt;         collect(s);&#010;&gt;       }&#010;&gt;     }&#010;&gt;   }&#010;&gt; }&#010;&gt;&#010;&gt;&#010;&gt; 在flink sql中通过 create function splitByChar as '**.**.Split'&#010;&gt; 来创建这个function，在tableEnv 中调用executeSql(....) 来完成对这个 function的注册，在sql&#010;&gt; 后面的计算逻辑中 通过以下方式来调用这个UDTF&#010;&gt; create view view_source_1 as select `dateTime，`itime`,&#010;&gt; lng,lat,net,event_info, cast(split_index(T.s, '_', 0) as int) as time_page&#010;&gt; from view_source as a left join LATERAL TABLE (splitByChar('a,b,c',',')) as&#010;&gt; T(s) on true;&#010;&gt;&#010;&gt;&#010;&gt; 结果一直出现以下错误信息：&#010;&gt; org.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt; From line 3, column 25 to line 3, column 47: No match found for function&#010;&gt; signature splitByChar(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)&#010;&gt; at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:629)&#010;&gt; ....................&#010;&gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From line&#010;&gt; 3, column 25 to line 3, column 47: r(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;&gt; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&gt; at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt; at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt; at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt; at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt; at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; ... 8 more&#010;&gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No match&#010;&gt; found for function signature splitByChar(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;&gt; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&gt; at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt;&#010;&gt;&#010;&gt; 之前在flink 1.10 里面使用是正常的， 问下各位大佬有没有在flink 1.11&#010;遇到过这个错误， 麻烦提供一下帮助。&#010;&gt; | |&#010;&gt; 邹云鹤&#010;&gt; |&#010;&gt; |&#010;&gt; kevinyunhe@163.com&#010;&gt; |&#010;&gt; 签名由网易邮箱大师定制&#010;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "1",
        "reply": "<5037bd09.29c3.17329847305.Coremail.kevinyunhe@163.com>"
    },
    {
        "id": "<65f1fde1.2c05.17329f2c8db.Coremail.kevinyunhe@163.com>",
        "from": "邹云鹤 &lt;kevinyu...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 15:44:03 GMT",
        "subject": "回复：Flink 1.11 SQL作业中调用UDTF 出现“No match found for function signature ”异常",
        "content": "好的&#010;&#010;&#010;&#010;&#010;| |&#010;邹云鹤&#010;|&#010;|&#010;邮箱：kevinyunhe@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月07日 23:27，Benchao Li 写道：&#010;我感觉这应该是新版本的udf的bug，我在本地也可以复现。&#010;已经建了一个issue[1] 来跟进。&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18520&#010;&#010;邹云鹤 &lt;kevinyunhe@163.com&gt; 于2020年7月7日周二 下午9:43写道：&#010;&#010;&gt;&#010;&gt;&#010;&gt; hi all&#010;&gt; 本人基于Flink 1.11 SNAPSHOT 在 Flink sql 作业中使用 UDTF, UDTF 的定义如下：&#010;&gt;&#010;&gt;&#010;&gt; @FunctionHint(&#010;&gt;         input = {@DataTypeHint(\"STRING\"), @DataTypeHint(\"STRING\")},&#010;&gt;         output = @DataTypeHint(\"STRING\")&#010;&gt; )&#010;&gt; public class Split extends TableFunction&lt;String&gt; {&#010;&gt;   public Split(){}&#010;&gt;   public void eval(String str, String ch) {&#010;&gt;     if (str == null || str.isEmpty()) {&#010;&gt;       return;&#010;&gt;     } else {&#010;&gt;       String[] ss = str.split(ch);&#010;&gt;       for (String s : ss) {&#010;&gt;         collect(s);&#010;&gt;       }&#010;&gt;     }&#010;&gt;   }&#010;&gt; }&#010;&gt;&#010;&gt;&#010;&gt; 在flink sql中通过 create function splitByChar as '**.**.Split'&#010;&gt; 来创建这个function，在tableEnv 中调用executeSql(....) 来完成对这个 function的注册，在sql&#010;&gt; 后面的计算逻辑中 通过以下方式来调用这个UDTF&#010;&gt; create view view_source_1 as select `dateTime，`itime`,&#010;&gt; lng,lat,net,event_info, cast(split_index(T.s, '_', 0) as int) as time_page&#010;&gt; from view_source as a left join LATERAL TABLE (splitByChar('a,b,c',',')) as&#010;&gt; T(s) on true;&#010;&gt;&#010;&gt;&#010;&gt; 结果一直出现以下错误信息：&#010;&gt; org.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt; From line 3, column 25 to line 3, column 47: No match found for function&#010;&gt; signature splitByChar(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:66)&#010;&gt; at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:629)&#010;&gt; ....................&#010;&gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From line&#010;&gt; 3, column 25 to line 3, column 47: r(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;&gt; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&gt; at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt; at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt; at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt; at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt; at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; ... 8 more&#010;&gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No match&#010;&gt; found for function signature splitByChar(&lt;CHARACTER&gt;, &lt;CHARACTER&gt;)&#010;&gt; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&gt; at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt;&#010;&gt;&#010;&gt; 之前在flink 1.10 里面使用是正常的， 问下各位大佬有没有在flink 1.11&#010;遇到过这个错误， 麻烦提供一下帮助。&#010;&gt; | |&#010;&gt; 邹云鹤&#010;&gt; |&#010;&gt; |&#010;&gt; kevinyunhe@163.com&#010;&gt; |&#010;&gt; 签名由网易邮箱大师定制&#010;&#010;&#010;&#010;--&#010;&#010;Best,&#010;Benchao Li&#010;",
        "depth": "2",
        "reply": "<5037bd09.29c3.17329847305.Coremail.kevinyunhe@163.com>"
    },
    {
        "id": "<tencent_68BDB727BB29BA3D474A361C@qq.com>",
        "from": "&quot;noake&quot;&lt;no...@sina.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 14:15:05 GMT",
        "subject": "Re: Re: Re: Re: flink 1.11 作业执行异常",
        "content": "我在1.11.0中遇到了同样的问题， pom中加了下面的依赖就没解决了&#010;dependency&#010; groupIdorg.apache.flink/groupId&#010; artifactIdflink-clients_${scala.binary.version}/artifactId&#010; version${flink.version}/version&#010;/dependency&#010;&#010;&#010;原始邮件&#010;发件人:Congxian Qiuqcx978132955@gmail.com&#010;收件人:user-zhuser-zh@flink.apache.org&#010;抄送:Jark Wuimjark@gmail.com; Jun Zhangzhangjunemail100@gmail.com&#010;发送时间:2020年7月7日(周二) 19:35&#010;主题:Re: Re: Re: Re: flink 1.11 作业执行异常&#010;&#010;&#010;Hi 从这个报错看上去是尝试通过 serviceLoader 加载一些 factory 的时候出错了（找不到），可以看看对应的&#010;module 的 resources 文件下是否有对应的 resource 文件 Best, Congxian sunfulin sunfulin0321@163.com&#010;于2020年7月7日周二 下午6:29写道：     hi,  我的pom文件本地执行时，scope的provided都是去掉的。&#010; dependency  groupIdorg.apache.flink/groupId   artifactIdflink-table-planner-blink_${scala.binary.version}/artifactId&#010; version${flink.version}/version  /dependency    确实比较诡异。org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010; 这个异常在啥情况下会触发到。                在 2020-07-07 18:10:58，\"Jark&#010;Wu\" imjark@gmail.com 写道：  如果是在 IDEA 中运行的话，你看看 blink planner&#010;这个依赖的 scope 是不是被 provided 掉了？ 去掉  provided  再试试看？  &#010; Best,  Jark    On Tue, 7 Jul 2020 at 18:01, sunfulin sunfulin0321@163.com wrote:     hi,&#010;  @Jun Zhang 我一直使用的就是blink planner，这个jar包一直都有的。     @Jark&#010;Wu 我是在本地idea中直接运行的，还没有打包到集群跑。跟这个有关系么？&#010;                                      在 2020-07-07 15:40:17，\"Jark Wu\" imjark@gmail.com&#010;写道：   Hi,      你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？&#010;     Best,   Jark      On Tue, 7 Jul 2020 at 15:31, Jun Zhang zhangjunemail100@gmail.com &#010; wrote:       hi.sunfulin    你有没有导入blink的planner呢，加入这个试试   &#010;   dependency    groupIdorg.apache.flink/groupId       artifactIdflink-table-planner-blink_${scala.binary.version}/artifactId&#010;   version${flink.version}/version    /dependency          sunfulin sunfulin0321@163.com 于2020年7月7日周二&#010;下午3:21写道：                hi, jark    我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#010;   configuration里的DeployOptions.TARGET    （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#010;         //构建StreamExecutionEnvironment    public static final StreamExecutionEnvironment&#010;env =    StreamExecutionEnvironment.getExecutionEnvironment();       //构建EnvironmentSettings&#010;并指定Blink Planner    private static final EnvironmentSettings bsSettings =       EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;      //构建StreamTableEnvironment    public static final StreamTableEnvironment tEnv =&#010;   StreamTableEnvironment.create(env, bsSettings);                   tEnv.executeSql(“ddl&#010;sql”);                //source注册成表       tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#010;   $(\"f1\").as(\"first\"), $(\"p\").proctime());                //join语句       Table table&#010;= tEnv.sqlQuery(\"select b.* from test a left  join    my_dim FOR SYSTEM_TIME AS OF a.p AS&#010;b on a.first = b.userId\");                //输出       tEnv.toAppendStream(table,  Row.class).print(\"LookUpJoinJob\");&#010;               env.execute(\"LookUpJoinJob\");                            在 2020-07-06 14:59:17，\"Jark&#010;Wu\" imjark@gmail.com 写道：    能分享下复现的作业代码不？        Best,   &#010;Jark        On Mon, 6 Jul 2020 at 11:00, sunfulin sunfulin0321@163.com  wrote:         Hi，&#010;    我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：  &#010;  org.apache.flink.table.api.TableExecution: Failed to execute sql             caused by :&#010;java.lang.IlleagalStateException: No ExecutorFactory   found    to     execute the application.&#010;    at           org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;            想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。",
        "depth": "0",
        "reply": "<tencent_68BDB727BB29BA3D474A361C@qq.com>"
    },
    {
        "id": "<2b4deb27.1765.1732c02ffb8.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 01:21:00 GMT",
        "subject": "Re:Re: Re: Re: Re: flink 1.11 作业执行异常",
        "content": "hi, noake&#010;感谢分享。我加了这个依赖后也OK了。周知下大家。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-07 22:15:05，\"noake\" &lt;noake@sina.cn&gt; 写道：&#010;&gt;我在1.11.0中遇到了同样的问题， pom中加了下面的依赖就没解决了&#010;&gt;dependency&#010;&gt; groupIdorg.apache.flink/groupId&#010;&gt; artifactIdflink-clients_${scala.binary.version}/artifactId&#010;&gt; version${flink.version}/version&#010;&gt;/dependency&#010;&gt;&#010;&gt;&#010;&gt;原始邮件&#010;&gt;发件人:Congxian Qiuqcx978132955@gmail.com&#010;&gt;收件人:user-zhuser-zh@flink.apache.org&#010;&gt;抄送:Jark Wuimjark@gmail.com; Jun Zhangzhangjunemail100@gmail.com&#010;&gt;发送时间:2020年7月7日(周二) 19:35&#010;&gt;主题:Re: Re: Re: Re: flink 1.11 作业执行异常&#010;&gt;&#010;&gt;&#010;&gt;Hi 从这个报错看上去是尝试通过 serviceLoader 加载一些 factory 的时候出错了（找不到），可以看看对应的&#010;module 的 resources 文件下是否有对应的 resource 文件 Best, Congxian sunfulin sunfulin0321@163.com&#010;于2020年7月7日周二 下午6:29写道：     hi,  我的pom文件本地执行时，scope的provided都是去掉的。&#010; dependency  groupIdorg.apache.flink/groupId   artifactIdflink-table-planner-blink_${scala.binary.version}/artifactId&#010; version${flink.version}/version  /dependency    确实比较诡异。org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010; 这个异常在啥情况下会触发到。                在 2020-07-07 18:10:58，\"Jark&#010;Wu\" imjark@gmail.com 写道：  如果是在 IDEA 中运行的话，你看看 blink planner&#010;这个依赖的 scope 是不是被 provided 掉了？ 去掉  provided  再试试看？  &#010; Best,  Jark    On Tue, 7 Jul 2020 at 18:01, sunfulin sunfulin0321@163.com wrote:     hi,&#010;  @Jun Zhang 我一直使用的就是blink planner，这个jar包一直都有的。     @Jark&#010;Wu 我是在本地idea中直接运行的，还没有打包到集群跑。跟这个有关系么？&#010;                                      在 2020-07-07 15:40:17，\"Jark Wu\" imjark@gmail.com&#010;写道：   Hi,      你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？&#010;     Best,   Jark      On Tue, 7 Jul 2020 at 15:31, Jun Zhang zhangjunemail100@gmail.com &#010; wrote:       hi.sunfulin    你有没有导入blink的planner呢，加入这个试试   &#010;   dependency    groupIdorg.apache.flink/groupId       artifactIdflink-table-planner-blink_${scala.binary.version}/artifactId&#010;   version${flink.version}/version    /dependency          sunfulin sunfulin0321@163.com 于2020年7月7日周二&#010;下午3:21写道：                hi, jark    我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#010;   configuration里的DeployOptions.TARGET    （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#010;         //构建StreamExecutionEnvironment    public static final StreamExecutionEnvironment&#010;env =    StreamExecutionEnvironment.getExecutionEnvironment();       //构建EnvironmentSettings&#010;并指定Blink Planner    private static final EnvironmentSettings bsSettings =       EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;      //构建StreamTableEnvironment    public static final StreamTableEnvironment tEnv =&#010;   StreamTableEnvironment.create(env, bsSettings);                   tEnv.executeSql(“ddl&#010;sql”);                //source注册成表       tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#010;   $(\"f1\").as(\"first\"), $(\"p\").proctime());                //join语句       Table table&#010;= tEnv.sqlQuery(\"select b.* from test a left  join    my_dim FOR SYSTEM_TIME AS OF a.p AS&#010;b on a.first = b.userId\");                //输出       tEnv.toAppendStream(table,  Row.class).print(\"LookUpJoinJob\");&#010;               env.execute(\"LookUpJoinJob\");                            在 2020-07-06 14:59:17，\"Jark&#010;Wu\" imjark@gmail.com 写道：    能分享下复现的作业代码不？        Best,   &#010;Jark        On Mon, 6 Jul 2020 at 11:00, sunfulin sunfulin0321@163.com  wrote:         Hi，&#010;    我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：  &#010;  org.apache.flink.table.api.TableExecution: Failed to execute sql             caused by :&#010;java.lang.IlleagalStateException: No ExecutorFactory   found    to     execute the application.&#010;    at           org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;            想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#010;",
        "depth": "1",
        "reply": "<tencent_68BDB727BB29BA3D474A361C@qq.com>"
    },
    {
        "id": "<CAELO930S1rXyfKZyHxUDHLSX0iu4xn_KgkYkQuZYh=ksNnJScA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 04:04:09 GMT",
        "subject": "Re: Re: Re: Re: Re: flink 1.11 作业执行异常",
        "content": "估计是这个导致的：&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/release-notes/flink-1.11.html#reversed-dependency-from-flink-streaming-java-to-flink-client-flink-15090&#013;&#010;&#013;&#010;On Wed, 8 Jul 2020 at 09:21, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi, noake&#013;&#010;&gt; 感谢分享。我加了这个依赖后也OK了。周知下大家。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-07 22:15:05，\"noake\" &lt;noake@sina.cn&gt; 写道：&#013;&#010;&gt; &gt;我在1.11.0中遇到了同样的问题， pom中加了下面的依赖就没解决了&#013;&#010;&gt; &gt;dependency&#013;&#010;&gt; &gt; groupIdorg.apache.flink/groupId&#013;&#010;&gt; &gt; artifactIdflink-clients_${scala.binary.version}/artifactId&#013;&#010;&gt; &gt; version${flink.version}/version&#013;&#010;&gt; &gt;/dependency&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;原始邮件&#013;&#010;&gt; &gt;发件人:Congxian Qiuqcx978132955@gmail.com&#013;&#010;&gt; &gt;收件人:user-zhuser-zh@flink.apache.org&#013;&#010;&gt; &gt;抄送:Jark Wuimjark@gmail.com; Jun Zhangzhangjunemail100@gmail.com&#013;&#010;&gt; &gt;发送时间:2020年7月7日(周二) 19:35&#013;&#010;&gt; &gt;主题:Re: Re: Re: Re: flink 1.11 作业执行异常&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Hi 从这个报错看上去是尝试通过 serviceLoader 加载一些 factory 的时候出错了（找不到），可以看看对应的&#010;module 的&#013;&#010;&gt; resources 文件下是否有对应的 resource 文件 Best, Congxian sunfulin&#013;&#010;&gt; sunfulin0321@163.com 于2020年7月7日周二 下午6:29写道：     hi,&#013;&#010;&gt; 我的pom文件本地执行时，scope的provided都是去掉的。  dependency&#013;&#010;&gt; groupIdorg.apache.flink/groupId&#013;&#010;&gt;  artifactIdflink-table-planner-blink_${scala.binary.version}/artifactId&#013;&#010;&gt; version${flink.version}/version  /dependency&#013;&#010;&gt; 确实比较诡异。org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt; 这个异常在啥情况下会触发到。                在 2020-07-07 18:10:58，\"Jark&#010;Wu\"&#013;&#010;&gt; imjark@gmail.com 写道：  如果是在 IDEA 中运行的话，你看看 blink planner&#010;这个依赖的 scope 是不是被&#013;&#010;&gt; provided 掉了？ 去掉  provided  再试试看？    Best,  Jark    On Tue, 7 Jul&#010;2020 at&#013;&#010;&gt; 18:01, sunfulin sunfulin0321@163.com wrote:     hi,   @Jun Zhang&#013;&#010;&gt; 我一直使用的就是blink planner，这个jar包一直都有的。     @Jark Wu&#013;&#010;&gt; 我是在本地idea中直接运行的，还没有打包到集群跑。跟这个有关系么？&#010;                                      在&#013;&#010;&gt; 2020-07-07 15:40:17，\"Jark Wu\" imjark@gmail.com 写道：   Hi,&#013;&#010;&gt; 你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？      Best,&#010;  Jark      On Tue, 7 Jul 2020 at&#013;&#010;&gt; 15:31, Jun Zhang zhangjunemail100@gmail.com   wrote:       hi.sunfulin&#013;&#010;&gt; 你有没有导入blink的planner呢，加入这个试试       dependency&#013;&#010;&gt; groupIdorg.apache.flink/groupId&#013;&#010;&gt;  artifactIdflink-table-planner-blink_${scala.binary.version}/artifactId&#013;&#010;&gt; version${flink.version}/version    /dependency          sunfulin&#013;&#010;&gt; sunfulin0321@163.com 于2020年7月7日周二 下午3:21写道：                hi,&#010;jark&#013;&#010;&gt; 我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#013;&#010;&gt; configuration里的DeployOptions.TARGET&#013;&#010;&gt; （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#013;&#010;&gt; //构建StreamExecutionEnvironment    public static final&#013;&#010;&gt; StreamExecutionEnvironment env =&#013;&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt;  //构建EnvironmentSettings 并指定Blink Planner    private static final&#013;&#010;&gt; EnvironmentSettings bsSettings =&#013;&#010;&gt;  EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt;      //构建StreamTableEnvironment    public static final&#013;&#010;&gt; StreamTableEnvironment tEnv =    StreamTableEnvironment.create(env,&#013;&#010;&gt; bsSettings);                   tEnv.executeSql(“ddl sql”);&#013;&#010;&gt; //source注册成表       tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#013;&#010;&gt;   $(\"f1\").as(\"first\"), $(\"p\").proctime());                //join语句&#013;&#010;&gt;  Table table = tEnv.sqlQuery(\"select b.* from test a left  join    my_dim&#013;&#010;&gt; FOR SYSTEM_TIME AS OF a.p AS b on a.first = b.userId\");&#013;&#010;&gt; //输出       tEnv.toAppendStream(table,  Row.class).print(\"LookUpJoinJob\");&#013;&#010;&gt;               env.execute(\"LookUpJoinJob\");                            在&#013;&#010;&gt; 2020-07-06 14:59:17，\"Jark Wu\" imjark@gmail.com 写道：    能分享下复现的作业代码不？&#013;&#010;&gt;   Best,    Jark        On Mon, 6 Jul 2020 at 11:00, sunfulin&#013;&#010;&gt; sunfulin0321@163.com  wrote:         Hi，     我使用目前最新的Flink 1.11&#013;&#010;&gt; rc4来测试我的作业。报了如下异常：     org.apache.flink.table.api.TableExecution:&#010;Failed to&#013;&#010;&gt; execute sql             caused by : java.lang.IlleagalStateException: No&#013;&#010;&gt; ExecutorFactory   found    to     execute the application.     at&#013;&#010;&gt;  org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt;            想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_68BDB727BB29BA3D474A361C@qq.com>"
    },
    {
        "id": "<tencent_5654DD5C3ED3B2458E8899B17E67443CD206@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 14:27:35 GMT",
        "subject": "Flink DataStream 统计UV问题",
        "content": "大家好！&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;谢谢！&#013;&#010;Jiazhi",
        "depth": "0",
        "reply": "<tencent_5654DD5C3ED3B2458E8899B17E67443CD206@qq.com>"
    },
    {
        "id": "<CAOMLN=Zv-JKKw_jADNmR3QHbN7Cfi2AW05AdFY9DSayq=Dj0HA@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:22:59 GMT",
        "subject": "Re: Flink DataStream 统计UV问题",
        "content": "Hi Jiazhi,&#013;&#010;&#013;&#010;1.如果数据流量不是很大的话，按每条数据触发也没问题。另外，基于事件时间的情况，提前触发可以选择ContinuousEventTimeTrigger，可以查看Trigger接口的实现找到你想要的trigger。&#013;&#010;2.窗口结束后会自动释放。一般对于Global窗口需要手动设置TTL&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&gt; 于2020年7月7日周二 下午10:27写道：&#013;&#010;&#013;&#010;&gt; 大家好！&#013;&#010;&gt;&#013;&#010;&gt;      想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;&gt; 1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;&gt; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;&gt; 2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 谢谢！&#013;&#010;&gt; Jiazhi&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_5654DD5C3ED3B2458E8899B17E67443CD206@qq.com>"
    },
    {
        "id": "<CALL9TYKHPcNREvK+WmP-Y9Qprqi55k8EMEvD4Pm8_BdzZjTWgw@mail.gmail.com>",
        "from": "tison &lt;wander4...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 04:30:32 GMT",
        "subject": "Re: Flink DataStream 统计UV问题",
        "content": "你这个需求貌似是要看一天的 UV 的实时更新量，可以看一下 sliding window。如果是每天&#010;0 点清零，实时看今天的&#013;&#010;UV，那就是另一个问题了，应该需要自己定义 trigger &amp; evictor&#013;&#010;&#013;&#010;每条触发一次 window...看你数据量吧&#013;&#010;&#013;&#010;Best,&#013;&#010;tison.&#013;&#010;&#013;&#010;&#013;&#010;shizk233 &lt;wangwangdaxian233@gmail.com&gt; 于2020年7月10日周五 上午10:23写道：&#013;&#010;&#013;&#010;&gt; Hi Jiazhi,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 1.如果数据流量不是很大的话，按每条数据触发也没问题。另外，基于事件时间的情况，提前触发可以选择ContinuousEventTimeTrigger，可以查看Trigger接口的实现找到你想要的trigger。&#013;&#010;&gt; 2.窗口结束后会自动释放。一般对于Global窗口需要手动设置TTL&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&gt; 于2020年7月7日周二 下午10:27写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 大家好！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;      想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;&gt; &gt; 1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;&gt; &gt; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;&gt; &gt; 2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 谢谢！&#013;&#010;&gt; &gt; Jiazhi&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_5654DD5C3ED3B2458E8899B17E67443CD206@qq.com>"
    },
    {
        "id": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 14:32:51 GMT",
        "subject": "DataStream统计uv问题",
        "content": "大家好！&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&#013;&#010;&#013;&#010;DataStream&lt;UvPer10Min&amp;gt; uvPer10MinDataStream = userBehaviorSource&#013;&#010;.windowAll(TumblingProcessingTimeWindows.of(Time.days(1L)))&#013;&#010;.trigger(CountTrigger.of(1L))&#013;&#010;.evictor(CountEvictor.of(0L, true))&#013;&#010;.process(new ProcessAllWindowFunction&lt;UserBehavior, UvPer10Min, TimeWindow&amp;gt;() {&#013;&#010;private transient MapState&lt;String, String&amp;gt; userIdState;&#013;&#010;private transient ValueState&lt;Long&amp;gt; uvCountState;&#013;&#010;&amp;nbsp; &amp;nbsp;&#013;&#010;谢谢！&#013;&#010;Jiazhi",
        "depth": "0",
        "reply": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>"
    },
    {
        "id": "<63f455cf-6012-48ff-9011-6db556ea66e5.yungao.gy@aliyun.com>",
        "from": "&quot;Yun Gao&quot; &lt;yungao...@aliyun.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 02:07:55 GMT",
        "subject": "Re: DataStream统计uv问题",
        "content": "&#010;您好：&#010;   1. 这个应该是预期内的用法，如果执行起来没有遇到问题应该是Ok的。&#010;   2. 窗口的状态会在超过最大时间+最大允许的延迟时间之后被清理。------------------------------------------------------------------&#010;Sender:ゞ野蠻遊戲χ&lt;zhoujiazhi1985@vip.qq.com&gt;&#010;Date:2020/07/07 22:32:51&#010;Recipient:user-zh&lt;user-zh@flink.apache.org&gt;&#010;Theme:DataStream统计uv问题&#010;&#010;大家好！&#010;&#010;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#010;&#010;&#010;DataStream&lt;UvPer10Min&amp;gt; uvPer10MinDataStream = userBehaviorSource&#010;.windowAll(TumblingProcessingTimeWindows.of(Time.days(1L)))&#010;.trigger(CountTrigger.of(1L))&#010;.evictor(CountEvictor.of(0L, true))&#010;.process(new ProcessAllWindowFunction&lt;UserBehavior, UvPer10Min, TimeWindow&amp;gt;() {&#010;private transient MapState&lt;String, String&amp;gt; userIdState;&#010;private transient ValueState&lt;Long&amp;gt; uvCountState;&#010;&amp;nbsp; &amp;nbsp;&#010;谢谢！&#010;Jiazhi&#010;",
        "depth": "1",
        "reply": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>"
    },
    {
        "id": "<tencent_ECC0024DC4C1F8B4B32F9E688A7BCE876905@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 02:15:48 GMT",
        "subject": "回复： DataStream统计uv问题",
        "content": "意思是当我使用滚动窗口之后，在第一个滚动窗口中的state自动会被清除，第二个滚动窗口进来之后，获取相同的Descriptor，里面的值是null？&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Yun Gao\"&lt;yungao.gy@aliyun.com.INVALID&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 上午10:07&#013;&#010;收件人:&amp;nbsp;\"ゞ野蠻遊戲χ\"&lt;zhoujiazhi1985@vip.qq.com&amp;gt;;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: DataStream统计uv问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;您好：&#013;&#010;&amp;nbsp;&amp;nbsp; 1. 这个应该是预期内的用法，如果执行起来没有遇到问题应该是Ok的。&#013;&#010;&amp;nbsp;&amp;nbsp; 2. 窗口的状态会在超过最大时间+最大允许的延迟时间之后被清理。------------------------------------------------------------------&#013;&#010;Sender:ゞ野蠻遊戲χ&lt;zhoujiazhi1985@vip.qq.com&amp;gt;&#013;&#010;Date:2020/07/07 22:32:51&#013;&#010;Recipient:user-zh&lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;Theme:DataStream统计uv问题&#013;&#010;&#013;&#010;大家好！&#013;&#010;&#013;&#010;&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&#013;&#010;&#013;&#010;DataStream&lt;UvPer10Min&amp;amp;gt; uvPer10MinDataStream = userBehaviorSource&#013;&#010;.windowAll(TumblingProcessingTimeWindows.of(Time.days(1L)))&#013;&#010;.trigger(CountTrigger.of(1L))&#013;&#010;.evictor(CountEvictor.of(0L, true))&#013;&#010;.process(new ProcessAllWindowFunction&lt;UserBehavior, UvPer10Min, TimeWindow&amp;amp;gt;()&#010;{&#013;&#010;private transient MapState&lt;String, String&amp;amp;gt; userIdState;&#013;&#010;private transient ValueState&lt;Long&amp;amp;gt; uvCountState;&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;谢谢！&#013;&#010;Jiazhi",
        "depth": "2",
        "reply": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>"
    },
    {
        "id": "<77dae4ed-fe00-4080-9713-0883e9a293aa.yungao.gy@aliyun.com>",
        "from": "&quot;Yun Gao&quot; &lt;yungao...@aliyun.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 02:18:21 GMT",
        "subject": "Re: 回复： DataStream统计uv问题",
        "content": "逻辑上这么说也没有问题；可以认为它是先把收到的记录（根据key和time）分配到特定的窗口中，然后不同的窗口的状态和计算都是独立的&#010;&#010;&#010;&#010; ------------------Original Mail ------------------&#010;Sender:ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&gt;&#010;Send Date:Thu Jul 9 10:16:06 2020&#010;Recipients:user-zh &lt;user-zh@flink.apache.org&gt;&#010;Subject:回复： DataStream统计uv问题&#010;意思是当我使用滚动窗口之后，在第一个滚动窗口中的state自动会被清除，第二个滚动窗口进来之后，获取相同的Descriptor，里面的值是null？&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:&amp;nbsp;\"Yun Gao\"&lt;yungao.gy@aliyun.com.INVALID&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 上午10:07&#010;收件人:&amp;nbsp;\"ゞ野蠻遊戲χ\"&lt;zhoujiazhi1985@vip.qq.com&amp;gt;;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Re: DataStream统计uv问题&#010;&#010;&#010;&#010;&#010;您好：&#010;&amp;nbsp;&amp;nbsp; 1. 这个应该是预期内的用法，如果执行起来没有遇到问题应该是Ok的。&#010;&amp;nbsp;&amp;nbsp; 2. 窗口的状态会在超过最大时间+最大允许的延迟时间之后被清理。------------------------------------------------------------------&#010;Sender:ゞ野蠻遊戲χ&lt;zhoujiazhi1985@vip.qq.com&amp;gt;&#010;Date:2020/07/07 22:32:51&#010;Recipient:user-zh&lt;user-zh@flink.apache.org&amp;gt;&#010;Theme:DataStream统计uv问题&#010;&#010;大家好！&#010;&#010;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#010;&#010;&#010;DataStream&lt;UvPer10Min&amp;amp;gt; uvPer10MinDataStream = userBehaviorSource&#010;.windowAll(TumblingProcessingTimeWindows.of(Time.days(1L)))&#010;.trigger(CountTrigger.of(1L))&#010;.evictor(CountEvictor.of(0L, true))&#010;.process(new ProcessAllWindowFunction&lt;UserBehavior, UvPer10Min, TimeWindow&amp;amp;gt;()&#010;{&#010;private transient MapState&lt;String, String&amp;amp;gt; userIdState;&#010;private transient ValueState&lt;Long&amp;amp;gt; uvCountState;&#010;&amp;amp;nbsp; &amp;amp;nbsp;&#010;谢谢！&#010;Jiazhi",
        "depth": "2",
        "reply": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>"
    },
    {
        "id": "<tencent_A7608F489AB90A5B80B64C9F46EEA99C4E06@qq.com>",
        "from": "&quot;1193216154&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 02:25:29 GMT",
        "subject": "回复： 回复： DataStream统计uv问题",
        "content": "我建议你用ContinuousEventTimeTrigger，可以在窗口范围内，连续触发。&#013;&#010;你这个countTrigger，促发次数太多了，而且你后面是processWindowFunction，导致计算压力比较大。&#013;&#010;建议你用aggregateWindowFuntion&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Yun Gao\"&lt;yungao.gy@aliyun.com.INVALID&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 上午10:18&#013;&#010;收件人:&amp;nbsp;\"ゞ野蠻遊戲χ\"&lt;zhoujiazhi1985@vip.qq.com&amp;gt;;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 回复： DataStream统计uv问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;逻辑上这么说也没有问题；可以认为它是先把收到的记录（根据key和time）分配到特定的窗口中，然后不同的窗口的状态和计算都是独立的&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp;------------------Original Mail ------------------&#013;&#010;Sender:ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&amp;gt;&#013;&#010;Send Date:Thu Jul 9 10:16:06 2020&#013;&#010;Recipients:user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;Subject:回复： DataStream统计uv问题&#013;&#010;意思是当我使用滚动窗口之后，在第一个滚动窗口中的state自动会被清除，第二个滚动窗口进来之后，获取相同的Descriptor，里面的值是null？&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;发件人:&amp;amp;nbsp;\"Yun Gao\"&lt;yungao.gy@aliyun.com.INVALID&amp;amp;gt;;&#013;&#010;发送时间:&amp;amp;nbsp;2020年7月9日(星期四) 上午10:07&#013;&#010;收件人:&amp;amp;nbsp;\"ゞ野蠻遊戲χ\"&lt;zhoujiazhi1985@vip.qq.com&amp;amp;gt;;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;amp;nbsp;Re: DataStream统计uv问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;您好：&#013;&#010;&amp;amp;nbsp;&amp;amp;nbsp; 1. 这个应该是预期内的用法，如果执行起来没有遇到问题应该是Ok的。&#013;&#010;&amp;amp;nbsp;&amp;amp;nbsp; 2. 窗口的状态会在超过最大时间+最大允许的延迟时间之后被清理。------------------------------------------------------------------&#013;&#010;Sender:ゞ野蠻遊戲χ&lt;zhoujiazhi1985@vip.qq.com&amp;amp;gt;&#013;&#010;Date:2020/07/07 22:32:51&#013;&#010;Recipient:user-zh&lt;user-zh@flink.apache.org&amp;amp;gt;&#013;&#010;Theme:DataStream统计uv问题&#013;&#010;&#013;&#010;大家好！&#013;&#010;&#013;&#010;&#013;&#010;&amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;&amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;&amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;&amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&#013;&#010;&#013;&#010;DataStream&lt;UvPer10Min&amp;amp;amp;gt; uvPer10MinDataStream = userBehaviorSource&#013;&#010;.windowAll(TumblingProcessingTimeWindows.of(Time.days(1L)))&#013;&#010;.trigger(CountTrigger.of(1L))&#013;&#010;.evictor(CountEvictor.of(0L, true))&#013;&#010;.process(new ProcessAllWindowFunction&lt;UserBehavior, UvPer10Min, TimeWindow&amp;amp;amp;gt;()&#010;{&#013;&#010;private transient MapState&lt;String, String&amp;amp;amp;gt; userIdState;&#013;&#010;private transient ValueState&lt;Long&amp;amp;amp;gt; uvCountState;&#013;&#010;&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;谢谢！&#013;&#010;Jiazhi",
        "depth": "3",
        "reply": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>"
    },
    {
        "id": "<tencent_624376D6C6074D19CC6932B95A2C88081A0A@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 05:03:16 GMT",
        "subject": "回复： 回复： DataStream统计uv问题",
        "content": "不错的想法&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"1193216154\"&lt;1193216154@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 上午10:25&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复： 回复： DataStream统计uv问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我建议你用ContinuousEventTimeTrigger，可以在窗口范围内，连续触发。&#013;&#010;你这个countTrigger，促发次数太多了，而且你后面是processWindowFunction，导致计算压力比较大。&#013;&#010;建议你用aggregateWindowFuntion&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;发件人:&amp;amp;nbsp;\"Yun Gao\"&lt;yungao.gy@aliyun.com.INVALID&amp;amp;gt;;&#013;&#010;发送时间:&amp;amp;nbsp;2020年7月9日(星期四) 上午10:18&#013;&#010;收件人:&amp;amp;nbsp;\"ゞ野蠻遊戲χ\"&lt;zhoujiazhi1985@vip.qq.com&amp;amp;gt;;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;amp;nbsp;Re: 回复： DataStream统计uv问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;逻辑上这么说也没有问题；可以认为它是先把收到的记录（根据key和time）分配到特定的窗口中，然后不同的窗口的状态和计算都是独立的&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&amp;amp;nbsp;------------------Original Mail ------------------&#013;&#010;Sender:ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&amp;amp;gt;&#013;&#010;Send Date:Thu Jul 9 10:16:06 2020&#013;&#010;Recipients:user-zh &lt;user-zh@flink.apache.org&amp;amp;gt;&#013;&#010;Subject:回复： DataStream统计uv问题&#013;&#010;意思是当我使用滚动窗口之后，在第一个滚动窗口中的state自动会被清除，第二个滚动窗口进来之后，获取相同的Descriptor，里面的值是null？&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;发件人:&amp;amp;amp;nbsp;\"Yun Gao\"&lt;yungao.gy@aliyun.com.INVALID&amp;amp;amp;gt;;&#013;&#010;发送时间:&amp;amp;amp;nbsp;2020年7月9日(星期四) 上午10:07&#013;&#010;收件人:&amp;amp;amp;nbsp;\"ゞ野蠻遊戲χ\"&lt;zhoujiazhi1985@vip.qq.com&amp;amp;amp;gt;;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;amp;amp;nbsp;Re: DataStream统计uv问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;您好：&#013;&#010;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 1. 这个应该是预期内的用法，如果执行起来没有遇到问题应该是Ok的。&#013;&#010;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 2. 窗口的状态会在超过最大时间+最大允许的延迟时间之后被清理。------------------------------------------------------------------&#013;&#010;Sender:ゞ野蠻遊戲χ&lt;zhoujiazhi1985@vip.qq.com&amp;amp;amp;gt;&#013;&#010;Date:2020/07/07 22:32:51&#013;&#010;Recipient:user-zh&lt;user-zh@flink.apache.org&amp;amp;amp;gt;&#013;&#010;Theme:DataStream统计uv问题&#013;&#010;&#013;&#010;大家好！&#013;&#010;&#013;&#010;&#013;&#010;&amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;&amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;&amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&amp;amp;amp;amp;nbsp; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;&amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&#013;&#010;&#013;&#010;DataStream&lt;UvPer10Min&amp;amp;amp;amp;gt; uvPer10MinDataStream = userBehaviorSource&#013;&#010;.windowAll(TumblingProcessingTimeWindows.of(Time.days(1L)))&#013;&#010;.trigger(CountTrigger.of(1L))&#013;&#010;.evictor(CountEvictor.of(0L, true))&#013;&#010;.process(new ProcessAllWindowFunction&lt;UserBehavior, UvPer10Min, TimeWindow&amp;amp;amp;amp;gt;()&#010;{&#013;&#010;private transient MapState&lt;String, String&amp;amp;amp;amp;gt; userIdState;&#013;&#010;private transient ValueState&lt;Long&amp;amp;amp;amp;gt; uvCountState;&#013;&#010;&amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;谢谢！&#013;&#010;Jiazhi",
        "depth": "4",
        "reply": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>"
    },
    {
        "id": "<tencent_F6702D5933E1A78AEDBB2BCDA7E658B70E07@qq.com>",
        "from": "&quot;Yichao Yang&quot; &lt;1048262...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 06:18:19 GMT",
        "subject": "回复：DataStream统计uv问题",
        "content": "Hi,&#013;&#010;&#013;&#010;&#013;&#010;我看了下你统计uv的方法，是不是可以思考转换一下计算uv的方式，开一天的窗口可能不是一个合适的方法，可以参考[1]中的方案。&#013;&#010;&#013;&#010;&#013;&#010;[1]&amp;nbsp;https://lists.apache.org/thread.html/rbe00ee38e2d07310d4e3c796de86c65205d1f5deecfc1678d9ebbdea%40%3Cuser-zh.flink.apache.org%3E&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"ゞ野蠻遊戲χ\"&lt;zhoujiazhi1985@vip.qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月7日(星期二) 晚上10:32&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;DataStream统计uv问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;大家好！&#013;&#010;&#013;&#010;&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;想问下，现在在用DataStream的api来统计每天的UV，代码如下，有2个使用问题：&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;1、在使用Tumbling窗口的时候，由于使用窗口跨度是1天（Time.days(1)），只有以一天结束的时候，才能输出一个uv值，&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; 这样时间等待太长了，所以加了一个trigger，每来一条都触发一次窗口，不知道这样的用法没有问题。&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;2、还有想问下在窗口结束后，里面的state状态会自动释放吗？还是要自己手动设置TTL的。&#013;&#010;&#013;&#010;&#013;&#010;DataStream&lt;UvPer10Min&amp;amp;gt; uvPer10MinDataStream = userBehaviorSource&#013;&#010;.windowAll(TumblingProcessingTimeWindows.of(Time.days(1L)))&#013;&#010;.trigger(CountTrigger.of(1L))&#013;&#010;.evictor(CountEvictor.of(0L, true))&#013;&#010;.process(new ProcessAllWindowFunction&lt;UserBehavior, UvPer10Min, TimeWindow&amp;amp;gt;()&#010;{&#013;&#010;private transient MapState&lt;String, String&amp;amp;gt; userIdState;&#013;&#010;private transient ValueState&lt;Long&amp;amp;gt; uvCountState;&#013;&#010;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;谢谢！&#013;&#010;Jiazhi",
        "depth": "1",
        "reply": "<tencent_A118AE33345E756A7A0E8749D52249E4C005@qq.com>"
    },
    {
        "id": "<5576e514.55b1.1732bc73d81.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 00:15:44 GMT",
        "subject": "flink 1.11 connector jdbc 依赖解析失败",
        "content": "hi all，&#010;flink升级到1.11，flink-connector-jdbc idea解析失败，去maven仓库查也没查到，请问是不是要手动编译1.11的源码的方式安装依赖的&#010;&#010;",
        "depth": "0",
        "reply": "<5576e514.55b1.1732bc73d81.Coremail.wander669@163.com>"
    },
    {
        "id": "<917D4DEB-E37E-447A-A2CC-B7BEF97617BD@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 01:35:10 GMT",
        "subject": "Re: flink 1.11 connector jdbc 依赖解析失败",
        "content": "Hello,&#010;&#010;我看下了maven仓库里有的[1], 官网文档里也有下载链接[2]，是不是pom里的依赖没有写对？1.11&#010;jdbc connector 的module名从 flink-jdbc 规范到了 flink-connector-jdbc。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;[1] https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-jdbc_2.11/1.11.0/&#010;&lt;https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-jdbc_2.11/1.11.0/&gt;&#010;[2] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html&gt;&#010;&#010;&#010;&gt; 在 2020年7月8日，08:15，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; hi all，&#010;&gt; flink升级到1.11，flink-connector-jdbc idea解析失败，去maven仓库查也没查到，请问是不是要手动编译1.11的源码的方式安装依赖的&#010;&gt; &#010;&#010;&#010;",
        "depth": "1",
        "reply": "<5576e514.55b1.1732bc73d81.Coremail.wander669@163.com>"
    },
    {
        "id": "<3b017fb2.6a5f.1732d4860f3.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:16:24 GMT",
        "subject": "Re:Re: flink 1.11 connector jdbc 依赖解析失败",
        "content": "感谢提醒，&#010;&#010;&#010;&#010;&#010;我是在https://mvnrepository.com/这个上面搜没搜到对应的包的，不过，module名改成flink-connector-jdbc，可以了，感谢提醒&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-08 09:35:10，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hello,&#010;&gt;&#010;&gt;我看下了maven仓库里有的[1], 官网文档里也有下载链接[2]，是不是pom里的依赖没有写对？1.11&#010;jdbc connector 的module名从 flink-jdbc 规范到了 flink-connector-jdbc。&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;&gt;&#010;&gt;[1] https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-jdbc_2.11/1.11.0/&#010;&lt;https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-jdbc_2.11/1.11.0/&gt;&#010;&gt;[2] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html&gt;&#010;&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月8日，08:15，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; hi all，&#010;&gt;&gt; flink升级到1.11，flink-connector-jdbc idea解析失败，去maven仓库查也没查到，请问是不是要手动编译1.11的源码的方式安装依赖的&#010;&gt;&gt; &#010;&gt;&#010;",
        "depth": "2",
        "reply": "<5576e514.55b1.1732bc73d81.Coremail.wander669@163.com>"
    },
    {
        "id": "<CAFTKPZpeVB8NGE0sw_r4UeqrTrHG36rueTcaspOV=kULZ4wemQ@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 02:18:36 GMT",
        "subject": "Re: [Third-party Tool] Flink memory calculator",
        "content": "Hi, there,&#010;&#010;As Flink 1.11.0 released, we provide a new calculator[1] for this&#010;version. Feel free to try it and any feedback or suggestion is&#010;welcomed!&#010;&#010;[1] https://github.com/KarmaGYZ/flink-memory-calculator/blob/master/calculator-1.11.sh&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Wed, Apr 1, 2020 at 9:45 PM Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; @Marta&#010;&gt; Thanks for the tip! I'll do that.&#010;&gt;&#010;&gt; Best,&#010;&gt; Yangze Guo&#010;&gt;&#010;&gt; On Wed, Apr 1, 2020 at 8:05 PM Marta Paes Moreira &lt;marta@ververica.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; Hey, Yangze.&#010;&gt; &gt;&#010;&gt; &gt; I'd like to suggest that you submit this tool to Flink Community Pages [1]. That&#010;way it can get more exposure and it'll be easier for users to find it.&#010;&gt; &gt;&#010;&gt; &gt; Thanks for your contribution!&#010;&gt; &gt;&#010;&gt; &gt; [1] https://flink-packages.org/&#010;&gt; &gt;&#010;&gt; &gt; On Tue, Mar 31, 2020 at 9:09 AM Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Hi, there.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; In the latest version, the calculator supports dynamic options. You&#010;&gt; &gt;&gt; could append all your dynamic options to the end of \"bin/calculator.sh&#010;&gt; &gt;&gt; [-h]\".&#010;&gt; &gt;&gt; Since \"-tm\" will be deprecated eventually, please replace it with&#010;&gt; &gt;&gt; \"-Dtaskmanager.memory.process.size=\".&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Best,&#010;&gt; &gt;&gt; Yangze Guo&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; On Mon, Mar 30, 2020 at 12:57 PM Xintong Song &lt;tonysong820@gmail.com&gt;&#010;wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; Hi Jeff,&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; I think the purpose of this tool it to allow users play with the memory&#010;configurations without needing to actually deploy the Flink cluster or even have a job. For&#010;sanity checks, we currently have them in the start-up scripts (for standalone clusters) and&#010;resource managers (on K8s/Yarn/Mesos).&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; I think it makes sense do the checks earlier, i.e. on the client side.&#010;But I'm not sure if JobListener is the right place. IIUC, JobListener is invoked before submitting&#010;a specific job, while the mentioned checks validate Flink's cluster level configurations.&#010;It might be okay for a job cluster, but does not cover the scenarios of session clusters.&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; Thank you~&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; Xintong Song&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; On Mon, Mar 30, 2020 at 12:03 PM Yangze Guo &lt;karmagyz@gmail.com&gt;&#010;wrote:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Thanks for your feedbacks, @Xintong and @Jeff.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; @Jeff&#010;&gt; &gt;&gt; &gt;&gt; I think it would always be good to leverage exist logic in Flink, such&#010;&gt; &gt;&gt; &gt;&gt; as JobListener. However, this calculator does not only target to check&#010;&gt; &gt;&gt; &gt;&gt; the conflict, it also targets to provide the calculating result to&#010;&gt; &gt;&gt; &gt;&gt; user before the job is actually deployed in case there is any&#010;&gt; &gt;&gt; &gt;&gt; unexpected configuration. It's a good point that we need to parse the&#010;&gt; &gt;&gt; &gt;&gt; dynamic configs. I prefer to parse the dynamic configs and cli&#010;&gt; &gt;&gt; &gt;&gt; commands in bash instead of adding hook in JobListener.&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Best,&#010;&gt; &gt;&gt; &gt;&gt; Yangze Guo&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; On Mon, Mar 30, 2020 at 10:32 AM Jeff Zhang &lt;zjffdu@gmail.com&gt;&#010;wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; Hi Yangze,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; Does this tool just parse the configuration in flink-conf.yaml&#010;?  Maybe it could be done in JobListener [1] (we should enhance it via adding hook before&#010;job submission), so that it could all the cases (e.g. parameters coming from command line)&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; [1] https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/core/execution/JobListener.java#L35&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; Yangze Guo &lt;karmagyz@gmail.com&gt; 于2020年3月30日周一&#010;上午9:40写道：&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Hi, Yun,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; I'm sorry that it currently could not handle it. But I think&#010;it is a&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; really good idea and that feature would be added to the next&#010;version.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; Yangze Guo&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; On Mon, Mar 30, 2020 at 12:21 AM Yun Tang &lt;myasuka@live.com&gt;&#010;wrote:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Very interesting and convenient tool, just a quick question:&#010;could this tool also handle deployment cluster commands like \"-tm\" mixed with configuration&#010;in `flink-conf.yaml` ?&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Best&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Yun Tang&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; ________________________________&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; From: Yangze Guo &lt;karmagyz@gmail.com&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Sent: Friday, March 27, 2020 18:00&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; To: user &lt;user@flink.apache.org&gt;; user-zh@flink.apache.org&#010;&lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Subject: [Third-party Tool] Flink memory calculator&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Hi, there.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; In release-1.10, the memory setup of task managers has&#010;changed a lot.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; I would like to provide here a third-party tool to simulate&#010;and get&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; the calculation result of Flink's memory configuration.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;  Although there is already a detailed setup guide[1]&#010;and migration&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; guide[2] officially, the calculator could further allow&#010;users to:&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; - Verify if there is any conflict in their configuration.&#010;The&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; calculator is more lightweight than starting a Flink&#010;cluster,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; especially when running Flink on Yarn/Kubernetes. User&#010;could make sure&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; their configuration is correct locally before deploying&#010;it to external&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; resource managers.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; - Get all of the memory configurations before deploying.&#010;User may set&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; taskmanager.memory.task.heap.size and taskmanager.memory.managed.size.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; But they also want to know the total memory consumption&#010;of Flink. With&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; this tool, users could get all of the memory configurations&#010;they are&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; interested in. If anything is unexpected, they would&#010;not need to&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; re-deploy a Flink cluster.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; The repo link of this tool is&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; https://github.com/KarmaGYZ/flink-memory-calculator.&#010;It reuses the&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; BashJavaUtils.jar of Flink and ensures the calculation&#010;result is&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; exactly the same as your Flink dist. For more details,&#010;please take a&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; look at the README.&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Any feedback or suggestion is welcomed!&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; [1] https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_setup.html&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; [2] https://ci.apache.org/projects/flink/flink-docs-master/ops/memory/mem_migration.html&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Best,&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; &gt; Yangze Guo&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; --&#010;&gt; &gt;&gt; &gt;&gt; &gt; Best Regards&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; Jeff Zhang&#010;&#010;",
        "depth": "1",
        "reply": "<CAFTKPZpeVB8NGE0sw_r4UeqrTrHG36rueTcaspOV=kULZ4wemQ@mail.gmail.com>"
    },
    {
        "id": "<1594175446740-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 02:30:46 GMT",
        "subject": "flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "场景：    canal解析binlog后，将db1实例内的多张表（表数据有关联）的变化发送到kafka的单topic，单分区中，从而保证有序；&#010;  &#010;若我想做数据同步至另一个mysql实例db2中，怎么用flink sql操作多张表，同时保证表与表之间有序呢？&#010; &#010;例如mysql实例db1中有表test, statusCREATE TABLE `test` (  `id` int(11) NOT NULL, &#010;`name` varchar(255) NOT NULL,  `time` datetime NOT NULL,  `status` int(11)&#010;NOT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE&#010;TABLE `status` (  `status` int(11) NOT NULL,  `name` varchar(255) NOT NULL, &#010;PRIMARY KEY (`status`)) ENGINE=InnoDB DEFAULT CHARSET=utf8比如，我用flink&#010;sql，可以设置对应的一张test表，然后sink到mysql镜像实例db2的镜像表test，和表status做同步，但status表要怎么操作呢？如何保证有序？我目前能实现单表，确实方便，求助，多表的怎么做有序同步？CREATE&#010;TABLE test (`id` INT,`name` VARCHAR(255),`time` TIMESTAMP(3),`status`&#010;INT,PRIMARY KEY(id) NOT ENFORCED ) WITH ( 'connector'='kafka',&#010;'topic'='test', 'properties.group.id'='c_mysql_binlog_postgres',&#010;'properties.bootstrap.servers'='localhost:9092',&#010;'scan.startup.mode'='earliest-offset', 'format'='canal-json',&#010;'canal-json.ignore-parse-errors'='true');CREATE TABLE status (`status`&#010;INT,`name` VARCHAR(255),PRIMARY KEY(name) NOT ENFORCED ) WITH (&#010;'connector'='kafka', 'topic'='test',&#010;'properties.group.id'='c_mysql_binlog_postgres',&#010;'properties.bootstrap.servers'='localhost:9092',&#010;'scan.startup.mode'='earliest-offset', 'format'='canal-json',&#010;'canal-json.ignore-parse-errors'='true');&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;",
        "depth": "0",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO933pJ8kUD4aYAcapqhfJHzwikoq8NSHvJeQ5_mAO8Sd_yA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:59:06 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "Hi,&#013;&#010;&#013;&#010;我想先问一下你使用的是刚发布的 1.11.0 版本吗？ 还是自己 build 的 release-1.11&#010;分支呢？&#013;&#010;&#013;&#010;另外，我理解下你的需求是  db1.test 同步到 db2.test,  db1.status 同步到 db2.status？&#013;&#010;多表的*有序*同步是指？&#013;&#010;我理解你只需要像定义 db1.test -&gt; db2.test 一样，定义好 db1.status binlog&#010;table 然后 insert&#013;&#010;into 到 db2.status mysql table就行了。&#013;&#010;&#013;&#010;感谢反馈使用体验。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;&#013;&#010;On Wed, 8 Jul 2020 at 10:30, jindy_liu &lt;286729788@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 场景：    canal解析binlog后，将db1实例内的多张表（表数据有关联）的变化发送到kafka的单topic，单分区中，从而保证有序；&#013;&#010;&gt; 若我想做数据同步至另一个mysql实例db2中，怎么用flink sql操作多张表，同时保证表与表之间有序呢？&#013;&#010;&gt; 例如mysql实例db1中有表test, statusCREATE TABLE `test` (  `id` int(11) NOT NULL,&#013;&#010;&gt; `name` varchar(255) NOT NULL,  `time` datetime NOT NULL,  `status` int(11)&#013;&#010;&gt; NOT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE&#013;&#010;&gt; TABLE `status` (  `status` int(11) NOT NULL,  `name` varchar(255) NOT&#013;&#010;&gt; NULL,&#013;&#010;&gt; PRIMARY KEY (`status`)) ENGINE=InnoDB DEFAULT CHARSET=utf8比如，我用flink&#013;&#010;&gt;&#013;&#010;&gt; sql，可以设置对应的一张test表，然后sink到mysql镜像实例db2的镜像表test，和表status做同步，但status表要怎么操作呢？如何保证有序？我目前能实现单表，确实方便，求助，多表的怎么做有序同步？CREATE&#013;&#010;&gt; TABLE test (`id` INT,`name` VARCHAR(255),`time` TIMESTAMP(3),`status`&#013;&#010;&gt; INT,PRIMARY KEY(id) NOT ENFORCED ) WITH ( 'connector'='kafka',&#013;&#010;&gt; 'topic'='test', 'properties.group.id'='c_mysql_binlog_postgres',&#013;&#010;&gt; 'properties.bootstrap.servers'='localhost:9092',&#013;&#010;&gt; 'scan.startup.mode'='earliest-offset', 'format'='canal-json',&#013;&#010;&gt; 'canal-json.ignore-parse-errors'='true');CREATE TABLE status (`status`&#013;&#010;&gt; INT,`name` VARCHAR(255),PRIMARY KEY(name) NOT ENFORCED ) WITH (&#013;&#010;&gt; 'connector'='kafka', 'topic'='test',&#013;&#010;&gt; 'properties.group.id'='c_mysql_binlog_postgres',&#013;&#010;&gt; 'properties.bootstrap.servers'='localhost:9092',&#013;&#010;&gt; 'scan.startup.mode'='earliest-offset', 'format'='canal-json',&#013;&#010;&gt; 'canal-json.ignore-parse-errors'='true');&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;",
        "depth": "1",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<BAA21305-DBB4-42E0-82F7-5B9DE883BA21@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 04:20:37 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "Hello,&#010;我理解下你场景：d1的 test 表 和 status 表两者之间有关联，比如外键，比如&#010;test 更新一条数据后 status也需要级联地更新一条数据。&#010;希望通过 Flink 的CDC功能同步这两张表到db2后，任意时刻，这两张表的状态是原子的（两张表对应&#010;d1中两张表的一个快照版本）， 是这种场景吗？&#010;&#010;如果是这种场景，现在是还没有支持的。&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月8日，11:59，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,&#010;&gt; &#010;&gt; 我想先问一下你使用的是刚发布的 1.11.0 版本吗？ 还是自己 build&#010;的 release-1.11 分支呢？&#010;&gt; &#010;&gt; 另外，我理解下你的需求是  db1.test 同步到 db2.test,  db1.status 同步到&#010;db2.status？&#010;&gt; 多表的*有序*同步是指？&#010;&gt; 我理解你只需要像定义 db1.test -&gt; db2.test 一样，定义好 db1.status&#010;binlog table 然后 insert&#010;&gt; into 到 db2.status mysql table就行了。&#010;&gt; &#010;&gt; 感谢反馈使用体验。&#010;&gt; &#010;&gt; Best,&#010;&gt; Jark&#010;&gt; &#010;&gt; &#010;&gt; On Wed, 8 Jul 2020 at 10:30, jindy_liu &lt;286729788@qq.com&gt; wrote:&#010;&gt; &#010;&gt;&gt; 场景：    canal解析binlog后，将db1实例内的多张表（表数据有关联）的变化发送到kafka的单topic，单分区中，从而保证有序；&#010;&gt;&gt; 若我想做数据同步至另一个mysql实例db2中，怎么用flink sql操作多张表，同时保证表与表之间有序呢？&#010;&gt;&gt; 例如mysql实例db1中有表test, statusCREATE TABLE `test` (  `id` int(11) NOT&#010;NULL,&#010;&gt;&gt; `name` varchar(255) NOT NULL,  `time` datetime NOT NULL,  `status` int(11)&#010;&gt;&gt; NOT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE&#010;&gt;&gt; TABLE `status` (  `status` int(11) NOT NULL,  `name` varchar(255) NOT&#010;&gt;&gt; NULL,&#010;&gt;&gt; PRIMARY KEY (`status`)) ENGINE=InnoDB DEFAULT CHARSET=utf8比如，我用flink&#010;&gt;&gt; &#010;&gt;&gt; sql，可以设置对应的一张test表，然后sink到mysql镜像实例db2的镜像表test，和表status做同步，但status表要怎么操作呢？如何保证有序？我目前能实现单表，确实方便，求助，多表的怎么做有序同步？CREATE&#010;&gt;&gt; TABLE test (`id` INT,`name` VARCHAR(255),`time` TIMESTAMP(3),`status`&#010;&gt;&gt; INT,PRIMARY KEY(id) NOT ENFORCED ) WITH ( 'connector'='kafka',&#010;&gt;&gt; 'topic'='test', 'properties.group.id'='c_mysql_binlog_postgres',&#010;&gt;&gt; 'properties.bootstrap.servers'='localhost:9092',&#010;&gt;&gt; 'scan.startup.mode'='earliest-offset', 'format'='canal-json',&#010;&gt;&gt; 'canal-json.ignore-parse-errors'='true');CREATE TABLE status (`status`&#010;&gt;&gt; INT,`name` VARCHAR(255),PRIMARY KEY(name) NOT ENFORCED ) WITH (&#010;&gt;&gt; 'connector'='kafka', 'topic'='test',&#010;&gt;&gt; 'properties.group.id'='c_mysql_binlog_postgres',&#010;&gt;&gt; 'properties.bootstrap.servers'='localhost:9092',&#010;&gt;&gt; 'scan.startup.mode'='earliest-offset', 'format'='canal-json',&#010;&gt;&gt; 'canal-json.ignore-parse-errors'='true');&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; --&#010;&gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594185497781-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 05:18:17 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "对的，只不过生产中，有些设计的时候外键没有显示声明，都是用流程保证更新表的顺序。&#013;&#010;所以消费数据变化的时候，也是要按顺序消费。不然使用镜像数据的人，可能会出问题。&#013;&#010;&#013;&#010;求教：除flink sql 的cdc功能外，flink的其它特性能否较好的支持这种场景呢？&#010; 需要写再底层点的api吗？&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "3",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<7B6ABA1E-508C-452B-807A-0DC7F101913F@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 05:35:00 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "Hello,&#013;&#010; &#013;&#010;很有意思的话题，我理解这需要保证多个CDC数据源 的全局一致性， 多个业务表的&#010;bin-log 通过 cdc接入flink后，得保证 每个数据源的写入目标库的时候有一个全局一致性的保证，这个底层的APi应该也支持不了的。&#013;&#010;一种可能的思路是 抽取cdc 记录 的metadata里的 committed ts (原始数据库中每次变更的时间，&#010;debezuim 的source.ts_ms字段， canal的es 字段)，通过这个时间来协调 多个&#010;CDC 数据源的处理速度，这只是我的一个想法。&#013;&#010;&#013;&#010;不过可以确定的是，目前的API应该拿不到这个信息，现在的 Flink 框架没法处理这个数据,&#010;可以看下 一些CDC框架是否能做这个事情。&#013;&#010;&#013;&#010;Best,&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;&gt; 在 2020年7月8日，13:18，jindy_liu &lt;286729788@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 对的，只不过生产中，有些设计的时候外键没有显示声明，都是用流程保证更新表的顺序。&#013;&#010;&gt; 所以消费数据变化的时候，也是要按顺序消费。不然使用镜像数据的人，可能会出问题。&#013;&#010;&gt; &#013;&#010;&gt; 求教：除flink sql 的cdc功能外，flink的其它特性能否较好的支持这种场景呢？&#010; 需要写再底层点的api吗？&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&#013;&#010;",
        "depth": "4",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594192233206-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:10:33 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "恩，主要是看flink 的发布里说flink&#010;支持cdc了，感觉这个功能好像是我要的，感觉好像我要做的事情能用flink都搞定。就不用多个开源平台切换与维护多个组件了。&#013;&#010;&#013;&#010;我原本还想先基于flink sql 将数据存量数据先全量导一次异构存储（如hbase,&#010;pgsql等）（批量），然后再flink cdc&#010;把mysql的bin-log变化数据搬运到异构存储(如hbase,&#010;pgsql等)后（增量），同时再镜像一份cdc后的kafka里的json数据到下游（变化通知）。&#013;&#010;&#013;&#010;那么下游再基于镜像的kafka里的数据（变化）+异构的镜像数据，再基于flink去做一些实时计算的场景需求（比如最近一个月内的前多少名的数据等），不用都挤在mysql的从库在做一些分析了，并且有些分析也不适合在mysql上搞，一些olap类的。&#013;&#010;&#013;&#010;但实际demo了吧，光一个数据的实时搬运里，要解决的问题还挺多的，光flink好像不太行（可能是我不太熟悉，我接触flink时间较短）&#013;&#010;问题：&#013;&#010;1、存量+实时数据怎么结合起来，目前语义上只能做到“至少一次”，先存量搬运，再binlog实时迁移，但难以定位存量搬运完后对应的kafka的起始消费位置。（但业务场景如果只需要“至少一次”，还是可以用的，业务大部分是只需“至少一次”）&#013;&#010;&#013;&#010;2、db里多表有序：这里有kafka性能问题和有序保证问题；目前业务场景db表变化不太快，一天1百w行数据的变更，可以搞定，同时也可以按需的N张表有序，不用整个db实例里的全部表。但这个有序感觉用flink&#010;sql cdc还不太好搞多表。如果直接写程序去消费&#013;&#010;&#013;&#010;3、多sink怎么保证数据一致性：具体来说，在增量同步的时候，flink需要先sink&#010;异构存储（先），后要sink&#010;kafka（后），怎么保证两个sink的先后次序与原子性？&#013;&#010;&#013;&#010;现请问下，flink 的sink能定义先后吗?&#010;如上面的，将kafka里的canal-json数据取出后，能先写pgsql成功，再把json数据原封不动写kafka吗？如果目前不支持，可否自己改造下支持？&#013;&#010;&#013;&#010;&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "5",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594193562718-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:32:42 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "如果用变更时间来有序，可能会有一个问题，是如果数据变更太快了，两条先后数据的时间可能是一样的？&#013;&#010;&#013;&#010;这个问题不知道是不是可以用bin-log的gtid来搞？至少是递增的?&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "5",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<C3BF7C0B-40EE-4747-82FC-3228ACCF7681@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:43:12 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "&#013;&#010;如果是同一个数据库（集群）的表，gtid应该是全局唯一且递增的，用gtid是更好的，异构的数据源就没有一个全局的id了，你可以试下.&#010;^_^&#013;&#010;&#013;&#010;祝好&#013;&#010;&#013;&#010;&gt; 在 2020年7月8日，15:32，jindy_liu &lt;286729788@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 如果用变更时间来有序，可能会有一个问题，是如果数据变更太快了，两条先后数据的时间可能是一样的？&#013;&#010;&gt; &#013;&#010;&gt; 这个问题不知道是不是可以用bin-log的gtid来搞？至少是递增的?&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&#013;&#010;",
        "depth": "6",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594185129706-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 05:12:09 GMT",
        "subject": "Re: flink 1.11 中cdc功能中，使用flink sql来操作一个kafka topic中的多张表，并保证有序？",
        "content": "https://github.com/apache/flink/tree/release-1.11.0我看github上tag下已经发布了release-1.11.0，我就编了下tag下的release-1.11.0。最近在做实时计算的一些调研，我们第一步就是要做数据的实时搬运（异构存储），看flink&#010;1.11有cdc功能，我关注了下。看发布了就立即试用了下，看看能不能用你们这个做变化数据的实时同步。1、体验了下，若mysql的binlog按单表有序到kafka，单topic,单分区，flink&#010;cdc的同步确实很方便，几条sql语句就搞定了。2、若mysql binlog按db实例，多表有序到kafka&#010;单topic,单分区，感觉不知道要怎么样定义这个ddl,&#010;同时怎么保证按序同步。（比如表与表之前的数据存在逻辑上的外键约束等等，具体来说test表的status字端就是个外键，如果关联记录都有更新，那更新顺序就比较重要了，要严格按binlog顺序来）。今天看了下，源码里canal-json的解析，好像只解析到了json里的feild&#010;和 operate 类型。感觉这个多表有序的场景应该也是比较多的需求的。&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "2",
        "reply": "<1594175446740-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 02:30:52 GMT",
        "subject": "State里面用guava Cache",
        "content": "大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&#013;&#010;&#013;&#010;比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1",
        "depth": "0",
        "reply": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>"
    },
    {
        "id": "<CAA8tFvs2U3PduojeV=F=xsDk=HACfPVx=4qhhXbeJ54SxXyg2w@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:50:49 GMT",
        "subject": "Re: State里面用guava Cache",
        "content": "你好，为什么需要在 State 里面再用 cache 呢？单纯的 State 不能满足需求吗？需求是什么呢？&#013;&#010;另外，除了 ValueState，其他的 ListState/MapState 能否满足你的需求呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月8日周三 上午10:31写道：&#013;&#010;&#013;&#010;&gt; 大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>"
    },
    {
        "id": "<tencent_2C79132B61562496B24489F30661B43ACA09@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:52:45 GMT",
        "subject": "回复： State里面用guava Cache",
        "content": "您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月8日(星期三) 下午3:50&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;你好，为什么需要在 State 里面再用 cache 呢？单纯的 State 不能满足需求吗？需求是什么呢？&#013;&#010;另外，除了 ValueState，其他的 ListState/MapState 能否满足你的需求呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&amp;gt; 于2020年7月8日周三 上午10:31写道：&#013;&#010;&#013;&#010;&amp;gt; 大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1",
        "depth": "2",
        "reply": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>"
    },
    {
        "id": "<CAA8tFvuFJ6W3QXb-mjcmGp=r-nZBvm-KZdKMT+LJrL6_CQ+BVg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:56:00 GMT",
        "subject": "Re: State里面用guava Cache",
        "content": "TTL state[1] 满足你的需求吗? 如果不满足的话，能否描述下你的需求呢？&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月8日周三 下午3:53写道：&#013;&#010;&#013;&#010;&gt; 您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月8日(星期三) 下午3:50&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: State里面用guava Cache&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你好，为什么需要在 State 里面再用 cache 呢？单纯的 State 不能满足需求吗？需求是什么呢？&#013;&#010;&gt; 另外，除了 ValueState，其他的 ListState/MapState 能否满足你的需求呢？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; op &lt;520075694@qq.com&amp;gt; 于2020年7月8日周三 上午10:31写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>"
    },
    {
        "id": "<tencent_D8689197E724B6C1344970676910FB0CA20A@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:07:17 GMT",
        "subject": "回复： State里面用guava Cache",
        "content": "您好，是这样的，我想再程序里面关联一些用户id，使用cache缓存一些热数据，设置每个id写入多久后自动清理掉，关联的时候首先访问缓存，访问不到再去访问外部存储；&#013;&#010;业务中的key会一直出现，也就是说ttl可能不会生效，这样没办法使用state&#010;ttl对吧？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月8日(星期三) 下午3:56&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;TTL state[1] 满足你的需求吗? 如果不满足的话，能否描述下你的需求呢？&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&amp;gt; 于2020年7月8日周三 下午3:53写道：&#013;&#010;&#013;&#010;&amp;gt; 您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月8日(星期三) 下午3:50&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 你好，为什么需要在 State 里面再用 cache 呢？单纯的 State 不能满足需求吗？需求是什么呢？&#013;&#010;&amp;gt; 另外，除了 ValueState，其他的 ListState/MapState 能否满足你的需求呢？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Congxian&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; op &lt;520075694@qq.com&amp;amp;gt; 于2020年7月8日周三 上午10:31写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1",
        "depth": "4",
        "reply": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>"
    },
    {
        "id": "<CAA8tFvsYR7Xr63qmkC_BdSgfa1KofnBAV5pDxCevx0nKd67goQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 09:52:13 GMT",
        "subject": "Re: State里面用guava Cache",
        "content": "我尝试理解一下你的需求：&#013;&#010;你希望从外部存储同步一些信息，由于访问外部存储效率不高，所以希望加一个&#010;cache，然后 cache&#013;&#010;中的数据希望在一定时间后过期，过期后重新去外部存储同步一次信息。&#013;&#010;&#013;&#010;但是还有一些信息不太明白，那这里你打算在什么地方使用 state 呢？state&#010;存放什么数据呢？或者说，你自己维护这个状态之后，为什么还有使用&#013;&#010;state 呢？&#013;&#010;&#013;&#010;不管怎么说使用 Flink 之后，还是建议尽量使用 state，而不是使用外存，flink&#010;提供的 state 方便做一些容错处理。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月8日周三 下午4:07写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 您好，是这样的，我想再程序里面关联一些用户id，使用cache缓存一些热数据，设置每个id写入多久后自动清理掉，关联的时候首先访问缓存，访问不到再去访问外部存储；&#013;&#010;&gt; 业务中的key会一直出现，也就是说ttl可能不会生效，这样没办法使用state&#010;ttl对吧？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月8日(星期三) 下午3:56&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: State里面用guava Cache&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; TTL state[1] 满足你的需求吗? 如果不满足的话，能否描述下你的需求呢？&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; op &lt;520075694@qq.com&amp;gt; 于2020年7月8日周三 下午3:53写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; 发件人:&amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月8日(星期三) 下午3:50&#013;&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 你好，为什么需要在 State 里面再用 cache 呢？单纯的 State 不能满足需求吗？需求是什么呢？&#013;&#010;&gt; &amp;gt; 另外，除了 ValueState，其他的 ListState/MapState 能否满足你的需求呢？&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; Best,&#013;&#010;&gt; &amp;gt; Congxian&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; op &lt;520075694@qq.com&amp;amp;gt; 于2020年7月8日周三 上午10:31写道：&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1&#013;&#010;",
        "depth": "5",
        "reply": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>"
    },
    {
        "id": "<tencent_DC62154C83845C160C71C820A79664394B06@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 09:59:28 GMT",
        "subject": "回复： State里面用guava Cache",
        "content": "比如数据里来了一个id我需要去判断这个id是新的还是已经存在的，由于历史数据量比较大，所以放全部state里面不太好。&#013;&#010;把最近活跃的id放到ValueState[Cache]里面，可以在内存里关联到绝大部分的id，避免频繁访问外部存储。&#013;&#010;如果不使用state保存的的话，重启作业后cache会重置，这段时间通过外部存储去关联id会很慢&#013;&#010;&amp;nbsp;谢谢&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月8日(星期三) 下午5:52&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我尝试理解一下你的需求：&#013;&#010;你希望从外部存储同步一些信息，由于访问外部存储效率不高，所以希望加一个&#010;cache，然后 cache&#013;&#010;中的数据希望在一定时间后过期，过期后重新去外部存储同步一次信息。&#013;&#010;&#013;&#010;但是还有一些信息不太明白，那这里你打算在什么地方使用 state 呢？state&#010;存放什么数据呢？或者说，你自己维护这个状态之后，为什么还有使用&#013;&#010;state 呢？&#013;&#010;&#013;&#010;不管怎么说使用 Flink 之后，还是建议尽量使用 state，而不是使用外存，flink&#010;提供的 state 方便做一些容错处理。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&amp;gt; 于2020年7月8日周三 下午4:07写道：&#013;&#010;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 您好，是这样的，我想再程序里面关联一些用户id，使用cache缓存一些热数据，设置每个id写入多久后自动清理掉，关联的时候首先访问缓存，访问不到再去访问外部存储；&#013;&#010;&amp;gt; 业务中的key会一直出现，也就是说ttl可能不会生效，这样没办法使用state&#010;ttl对吧？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月8日(星期三) 下午3:56&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; TTL state[1] 满足你的需求吗? 如果不满足的话，能否描述下你的需求呢？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; [1]&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Congxian&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; op &lt;520075694@qq.com&amp;amp;gt; 于2020年7月8日周三 下午3:53写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月8日(星期三) 下午3:50&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 你好，为什么需要在 State 里面再用 cache 呢？单纯的&#010;State 不能满足需求吗？需求是什么呢？&#013;&#010;&amp;gt; &amp;amp;gt; 另外，除了 ValueState，其他的 ListState/MapState 能否满足你的需求呢？&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; Congxian&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; op &lt;520075694@qq.com&amp;amp;amp;gt; 于2020年7月8日周三 上午10:31写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1",
        "depth": "6",
        "reply": "<tencent_2A9172A5C442986C89B32C41E29A4128AD07@qq.com>"
    },
    {
        "id": "<202007081059058293894@163.com>",
        "from": "&quot;18579099920@163.com&quot; &lt;18579099...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 02:59:06 GMT",
        "subject": "FlinkKafkaProducer没有写入多个topic的功能",
        "content": "我有一个需求是通过读取一个kafka的主题的数据经过flink处理再写入到多个kafka的主题中（写入的主题是动态的，数据中能解析到需要写入到的目的地主题），&#013;&#010;但是FlinkKafkaProducer好像只能写入一个主题里面？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;18579099920@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007081059058293894@163.com>"
    },
    {
        "id": "<95958487-4196-4b6b-be1f-fdc887dc3f75.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:04:40 GMT",
        "subject": "回复：FlinkKafkaProducer没有写入多个topic的功能",
        "content": "你好,可以尝试自定义KafkaSerializationSchema来实现你的业务场景&#010;class DemoSerializationSchema extends KafkaSerializationSchema[DemoBean] {&#010;  override def serialize(element: DemoBean, timestamp: lang.Long): ProducerRecord[Array[Byte],&#010;Array[Byte",
        "depth": "1",
        "reply": "<202007081059058293894@163.com>"
    },
    {
        "id": "<63de103c.3069.1732c657a44.Coremail.18579099920@163.com>",
        "from": "flink小猪 &lt;18579099...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:08:34 GMT",
        "subject": "Re:回复：FlinkKafkaProducer没有写入多个topic的功能",
        "content": "兄弟，感谢&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-08 11:04:40，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&#010;你好,可以尝试自定义KafkaSerializationSchema来实现你的业务场景&#010;class DemoSerializationSchema extends KafkaSerializationSchema[DemoBean] {&#010;override def serialize(element: DemoBean, timestamp: lang.Long): ProducerRecord[Array[Byte],&#010;Array[Byte",
        "depth": "2",
        "reply": "<202007081059058293894@163.com>"
    },
    {
        "id": "<78E20179-D2BF-4A70-9251-294AF977651F@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:38:42 GMT",
        "subject": "Re: FlinkKafkaProducer没有写入多个topic的功能",
        "content": "Hi,&#010;&#010;夏帅的方案是ok的，因为Kafka 默认支持写入topic不存在时自动创建[1],&#010;这个配置是默认开启的，所以只用实现下自定义KafkaSerializationSchema就可以满足你的需求。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;[1] https://docs.confluent.io/current/installation/configuration/broker-configs.html#auto.create.topics.enable&#010;&lt;https://docs.confluent.io/current/installation/configuration/broker-configs.html#auto.create.topics.enable&gt;&#010;&#010;&gt; 在 2020年7月8日，11:08，flink小猪 &lt;18579099920@163.com&gt; 写道：&#010;&gt; &#010;&gt; 兄弟，感谢&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在 2020-07-08 11:04:40，\"夏帅\" &lt;jkillers@dingtalk.com&gt; 写道：&#010;&gt; &#010;&gt; 你好,可以尝试自定义KafkaSerializationSchema来实现你的业务场景&#010;&gt; class DemoSerializationSchema extends KafkaSerializationSchema[DemoBean] {&#010;&gt; override def serialize(element: DemoBean, timestamp: lang.Long): ProducerRecord[Array[Byte],&#010;Array[Byte",
        "depth": "3",
        "reply": "<202007081059058293894@163.com>"
    },
    {
        "id": "<tencent_41001E7D789D74FD27D690D534222179E508@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 02:59:20 GMT",
        "subject": "Flink Hadoop依赖",
        "content": "Hi， 各位大佬们，有个问题，Flink 1.10.0版本中，已经在jobmanager的lib文件夹添加了flink-shaded-hadoop-2-uber-2.7.5-10.0.jar文件，通过webui上传可以正常运行任务，但通过cli命令，提交任务后报Could&#010;not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported&#010;by Flink and no Hadoop file system to support this scheme could be loaded.有谁知道是怎么回事吗？",
        "depth": "0",
        "reply": "<tencent_41001E7D789D74FD27D690D534222179E508@qq.com>"
    },
    {
        "id": "<CAHsnkPuuEVO9xTSFh+eMr_TB+g+3=bZj-9gT9wG8q6i83Zcsew@mail.gmail.com>",
        "from": "Xintong Song &lt;tonysong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:09:21 GMT",
        "subject": "Re: Flink Hadoop依赖",
        "content": "你说的 “jobmanager的lib文件夹” 是指哪里？Flink 的部署方式是怎样的？CLI&#010;运行在哪里？&#013;&#010;&#013;&#010;Thank you~&#013;&#010;&#013;&#010;Xintong Song&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;On Wed, Jul 8, 2020 at 10:59 AM Z-Z &lt;zz9876543210@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi， 各位大佬们，有个问题，Flink&#013;&#010;&gt; 1.10.0版本中，已经在jobmanager的lib文件夹添加了flink-shaded-hadoop-2-uber-2.7.5-10.0.jar文件，通过webui上传可以正常运行任务，但通过cli命令，提交任务后报Could&#013;&#010;&gt; not find a file system implementation for scheme 'hdfs'. The scheme is not&#013;&#010;&gt; directly supported by Flink and no Hadoop file system to support this&#013;&#010;&gt; scheme could be loaded.有谁知道是怎么回事吗？&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_41001E7D789D74FD27D690D534222179E508@qq.com>"
    },
    {
        "id": "<64a1ede.78a7.1732c766320.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:27:02 GMT",
        "subject": "作业升级到flink1.11，idea运行失败",
        "content": "&#010;hi&#010;&#010;作业的依赖从1.10.1升级到1.11.0，在idea运行的时候报错&#010;&#010;Exception in thread \"main\" java.lang.IllegalStateException: No ExecutorFactory found to execute&#010;the application.&#010;     at org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;     at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1803)&#010;     at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1713)&#010;     at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment（）&#010;&#010;是哪里出问题了呢&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "0",
        "reply": "<64a1ede.78a7.1732c766320.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAFTKPZp2xUPqkEcPSgYh7mT8yEZs2QCQBq-cL0t=hGJ=TVqm5Q@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 03:30:49 GMT",
        "subject": "Re: 作业升级到flink1.11，idea运行失败",
        "content": "尝试加一下这个依赖&#010;groupId: org.apache.flink&#010;artifactId: flink-clients_${scala.binary.version}&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Wed, Jul 8, 2020 at 11:27 AM SmileSmile &lt;a511955993@163.com&gt; wrote:&#010;&gt;&#010;&gt;&#010;&gt; hi&#010;&gt;&#010;&gt; 作业的依赖从1.10.1升级到1.11.0，在idea运行的时候报错&#010;&gt;&#010;&gt; Exception in thread \"main\" java.lang.IllegalStateException: No ExecutorFactory found&#010;to execute the application.&#010;&gt;      at org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;&gt;      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1803)&#010;&gt;      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1713)&#010;&gt;      at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment（）&#010;&gt;&#010;&gt; 是哪里出问题了呢&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&#010;",
        "depth": "1",
        "reply": "<64a1ede.78a7.1732c766320.Coremail.a511955993@163.com>"
    },
    {
        "id": "<6a4b0e5f.7f6f.1732cb5944e.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 04:36:04 GMT",
        "subject": "回复：作业升级到flink1.11，idea运行失败",
        "content": "添加依赖后正常了。应该是这个导致的&#010;&#010;https://ci.apache.org/projects/flink/flink-docs-master/release-notes/flink-1.11.html#reversed-dependency-from-flink-streaming-java-to-flink-client-flink-15090&#010;&#010;thanks&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月08日 11:30，Yangze Guo 写道：&#010;尝试加一下这个依赖&#010;groupId: org.apache.flink&#010;artifactId: flink-clients_${scala.binary.version}&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Wed, Jul 8, 2020 at 11:27 AM SmileSmile &lt;a511955993@163.com&gt; wrote:&#010;&gt;&#010;&gt;&#010;&gt; hi&#010;&gt;&#010;&gt; 作业的依赖从1.10.1升级到1.11.0，在idea运行的时候报错&#010;&gt;&#010;&gt; Exception in thread \"main\" java.lang.IllegalStateException: No ExecutorFactory found&#010;to execute the application.&#010;&gt;      at org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;&gt;      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1803)&#010;&gt;      at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1713)&#010;&gt;      at org.apache.flink.streaming.api.environment.LocalStreamEnvironment.execute(LocalStreamEnvironment（）&#010;&gt;&#010;&gt; 是哪里出问题了呢&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;",
        "depth": "2",
        "reply": "<64a1ede.78a7.1732c766320.Coremail.a511955993@163.com>"
    },
    {
        "id": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 05:03:16 GMT",
        "subject": "flink 1.11  on kubernetes 构建失败 ",
        "content": "hi&#010;&#010;按照文档[1]的方法部署session cluster on kubernetes，集群构建的时候出现了如下报错&#010;&#010;&#010;Starting Task Manager&#010;sed: couldn't open temporary file /opt/flink/conf/sedVdyy6Q: Read-only file system&#010;sed: couldn't open temporary file /opt/flink/conf/sedcj5VKQ: Read-only file system&#010;/docker-entrypoint.sh: 72: /docker-entrypoint.sh: cannot create /opt/flink/conf/flink-conf.yaml:&#010;Permission denied&#010;sed: couldn't open temporary file /opt/flink/conf/sedB5eynR: Read-only file system&#010;/docker-entrypoint.sh: 120: /docker-entrypoint.sh: cannot create /opt/flink/conf/flink-conf.yaml.tmp:&#010;Read-only file system&#010;[ERROR] The execution result is empty.&#010;[ERROR] Could not get JVM parameters and dynamic configurations properly.&#010;&#010;&#010;是否有遇到同样的问题，支个招&#010;&#010;&#010;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html#session-cluster-resource-definitions&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "0",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CY4PR20MB122109ACE0384FD22E13D376DA670@CY4PR20MB1221.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:29:57 GMT",
        "subject": "Re: flink 1.11  on kubernetes 构建失败 ",
        "content": "Hi&#013;&#010;&#013;&#010;你是不是对 /opt/flink/conf 目录下的文件进行了sed相关写操作？社区文档中使用的方法是将configmap挂载成本地的flink-conf.yaml&#010;等文件，而这个挂载的目录其实是不可写的。&#013;&#010;直接修改configmap里面的内容，这样挂载时候就会自动更新了。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Wednesday, July 8, 2020 13:03&#013;&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: flink 1.11 on kubernetes 构建失败&#013;&#010;&#013;&#010;hi&#013;&#010;&#013;&#010;按照文档[1]的方法部署session cluster on kubernetes，集群构建的时候出现了如下报错&#013;&#010;&#013;&#010;&#013;&#010;Starting Task Manager&#013;&#010;sed: couldn't open temporary file /opt/flink/conf/sedVdyy6Q: Read-only file system&#013;&#010;sed: couldn't open temporary file /opt/flink/conf/sedcj5VKQ: Read-only file system&#013;&#010;/docker-entrypoint.sh: 72: /docker-entrypoint.sh: cannot create /opt/flink/conf/flink-conf.yaml:&#010;Permission denied&#013;&#010;sed: couldn't open temporary file /opt/flink/conf/sedB5eynR: Read-only file system&#013;&#010;/docker-entrypoint.sh: 120: /docker-entrypoint.sh: cannot create /opt/flink/conf/flink-conf.yaml.tmp:&#010;Read-only file system&#013;&#010;[ERROR] The execution result is empty.&#013;&#010;[ERROR] Could not get JVM parameters and dynamic configurations properly.&#013;&#010;&#013;&#010;&#013;&#010;是否有遇到同样的问题，支个招&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html#session-cluster-resource-definitions&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "1",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<534765f0.9a25.1732d937575.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:38:25 GMT",
        "subject": "回复：flink 1.11  on kubernetes 构建失败",
        "content": "hi yun tang！&#010;&#010;没有对 /opt/flink/config 目录下的文件做写操作。 只是按照官网上的配置文件进行部署，镜像用的也是社区的镜像。&#010;best！&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月08日 16:29，Yun Tang 写道：&#010;Hi&#010;&#010;你是不是对 /opt/flink/conf 目录下的文件进行了sed相关写操作？社区文档中使用的方法是将configmap挂载成本地的flink-conf.yaml&#010;等文件，而这个挂载的目录其实是不可写的。&#010;直接修改configmap里面的内容，这样挂载时候就会自动更新了。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Wednesday, July 8, 2020 13:03&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: flink 1.11 on kubernetes 构建失败&#010;&#010;hi&#010;&#010;按照文档[1]的方法部署session cluster on kubernetes，集群构建的时候出现了如下报错&#010;&#010;&#010;Starting Task Manager&#010;sed: couldn't open temporary file /opt/flink/conf/sedVdyy6Q: Read-only file system&#010;sed: couldn't open temporary file /opt/flink/conf/sedcj5VKQ: Read-only file system&#010;/docker-entrypoint.sh: 72: /docker-entrypoint.sh: cannot create /opt/flink/conf/flink-conf.yaml:&#010;Permission denied&#010;sed: couldn't open temporary file /opt/flink/conf/sedB5eynR: Read-only file system&#010;/docker-entrypoint.sh: 120: /docker-entrypoint.sh: cannot create /opt/flink/conf/flink-conf.yaml.tmp:&#010;Read-only file system&#010;[ERROR] The execution result is empty.&#010;[ERROR] Could not get JVM parameters and dynamic configurations properly.&#010;&#010;&#010;是否有遇到同样的问题，支个招&#010;&#010;&#010;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html#session-cluster-resource-definitions&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "2",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<67dc0480.d9e5.1733216b21e.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 05:40:15 GMT",
        "subject": "回复：flink 1.11  on kubernetes 构建失败",
        "content": "hi&#010;&#010;按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#010;&#010;&#010;目前看差别在于1.11启动jm和tm是通过args: [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#010;本地挂载的flink-configuration-configmap.yaml导致失败。&#010;&#010;&#010;1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#010;&#010;command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#010;         while :;&#010;         do&#010;           if [[ -f $(find log -name '*jobmanager*.log' -print -quit) ",
        "depth": "3",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf36Po1gDLCdRcqqagrwQ5sSiz0hjweG+1H4TOt0cuvz_Dw@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 12:02:03 GMT",
        "subject": "Re: flink 1.11 on kubernetes 构建失败",
        "content": "sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#013;&#010;才会这样[1]&#013;&#010;&#013;&#010;你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#013;&#010;如果不是，需要你把你的yaml发出来&#013;&#010;&#013;&#010;&#013;&#010;[1].&#013;&#010;https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#013;&#010;[2].&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt;&#013;&#010;&gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 目前看差别在于1.11启动jm和tm是通过args:&#013;&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#013;&#010;&gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#013;&#010;&gt;&#013;&#010;&gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#013;&#010;&gt;          while :;&#013;&#010;&gt;          do&#013;&#010;&gt;            if [[ -f $(find log -name '*jobmanager*.log' -print -quit) ",
        "depth": "4",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<674a83da.13d6.173369a38fa.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 02:42:23 GMT",
        "subject": "回复：flink 1.11 on kubernetes 构建失败",
        "content": "hi yang wang&#010;&#010;1.11版本的on kubernetes在hostname上有做什么变化吗？&#010;&#010;作业运行的时候 flink ui上 tm变成ip：端口&#010;，在1.10版本，ui上是 podname：端口。&#010;&#010;作业启动的时候，jm日志一直在刷&#010;&#010;No hostname could be resolved for the IP address 10.35.160.5, using IP address as host name.&#010;Local input split assignment (such as for HDFS files) may be impacted&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月09日 20:02，Yang Wang 写道：&#010;sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#010;才会这样[1]&#010;&#010;你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#010;如果不是，需要你把你的yaml发出来&#010;&#010;&#010;[1].&#010;https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#010;[2].&#010;https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#010;&#010;&gt; hi&#010;&gt;&#010;&gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#010;&gt;&#010;&gt;&#010;&gt; 目前看差别在于1.11启动jm和tm是通过args:&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#010;&gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#010;&gt;&#010;&gt;&#010;&gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#010;&gt;&#010;&gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#010;&gt;          while :;&#010;&gt;          do&#010;&gt;            if [[ -f $(find log -name '*jobmanager*.log' -print -quit) ",
        "depth": "5",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf36_5-VQwgBdaDcUjziJGZyJfvELkdyed_h82aetTWKz2w@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 04:13:08 GMT",
        "subject": "Re: flink 1.11 on kubernetes 构建失败",
        "content": "我记得1.11里面对host这个地方应该是没有改动，taskmanager.network.bind-policy的&#013;&#010;默认值一会都是ip。所以你说的UI上是podname，这个是哪里的？正常TM列表akka地址&#013;&#010;都是ip地址的&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 上午10:42写道：&#013;&#010;&#013;&#010;&gt; hi yang wang&#013;&#010;&gt;&#013;&#010;&gt; 1.11版本的on kubernetes在hostname上有做什么变化吗？&#013;&#010;&gt;&#013;&#010;&gt; 作业运行的时候 flink ui上 tm变成ip：端口&#013;&#010;&gt; ，在1.10版本，ui上是 podname：端口。&#013;&#010;&gt;&#013;&#010;&gt; 作业启动的时候，jm日志一直在刷&#013;&#010;&gt;&#013;&#010;&gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#013;&#010;&gt; address as host name. Local input split assignment (such as for HDFS files)&#013;&#010;&gt; may be impacted&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月09日 20:02，Yang Wang 写道：&#013;&#010;&gt; sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#013;&#010;&gt; 才会这样[1]&#013;&#010;&gt;&#013;&#010;&gt; 你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#013;&#010;&gt; 如果不是，需要你把你的yaml发出来&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1].&#013;&#010;&gt; https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#013;&#010;&gt; [2].&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 目前看差别在于1.11启动jm和tm是通过args:&#013;&#010;&gt; &gt;&#013;&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#013;&#010;&gt; &gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#013;&#010;&gt; &gt;          while :;&#013;&#010;&gt; &gt;          do&#013;&#010;&gt; &gt;            if [[ -f $(find log -name '*jobmanager*.log' -print -quit) ",
        "depth": "6",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<4e18b58.2141.1733721493c.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 05:09:55 GMT",
        "subject": "回复：flink 1.11 on kubernetes 构建失败",
        "content": "hi Yang&#010;&#010;在1.10版本，running的作业点击拓普图中随便一个operation，有detail subtasks&#010;taskmanagers xxx x 这行，taskmanagers这栏里的host，显示的是 podname：端口&#010;&#010;在1.11变成ip：端口&#010;&#010;目前我这边遇到的情况是，构建了一个有120slot的集群，作业并行度是120。&#010;提交到jm后jm就失联了，jm timeout。观察jm日志，疯狂在刷&#010;&#010;&#010;No hostname could be resolved for the IP address 10.35.160.5, using IP address as host name.&#010;Local input split assignment (such as for HDFS files) may be impacted&#010;&#010;&#010;目前观察到的改变主要是这块podname和ip的区别，其他不确定&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月10日 12:13，Yang Wang 写道：&#010;我记得1.11里面对host这个地方应该是没有改动，taskmanager.network.bind-policy的&#010;默认值一会都是ip。所以你说的UI上是podname，这个是哪里的？正常TM列表akka地址&#010;都是ip地址的&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 上午10:42写道：&#010;&#010;&gt; hi yang wang&#010;&gt;&#010;&gt; 1.11版本的on kubernetes在hostname上有做什么变化吗？&#010;&gt;&#010;&gt; 作业运行的时候 flink ui上 tm变成ip：端口&#010;&gt; ，在1.10版本，ui上是 podname：端口。&#010;&gt;&#010;&gt; 作业启动的时候，jm日志一直在刷&#010;&gt;&#010;&gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#010;&gt; address as host name. Local input split assignment (such as for HDFS files)&#010;&gt; may be impacted&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; 在2020年07月09日 20:02，Yang Wang 写道：&#010;&gt; sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#010;&gt; 才会这样[1]&#010;&gt;&#010;&gt; 你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#010;&gt; 如果不是，需要你把你的yaml发出来&#010;&gt;&#010;&gt;&#010;&gt; [1].&#010;&gt; https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#010;&gt; [2].&#010;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#010;&gt;&#010;&gt; &gt; hi&#010;&gt; &gt;&#010;&gt; &gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 目前看差别在于1.11启动jm和tm是通过args:&#010;&gt; &gt;&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#010;&gt; &gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#010;&gt; &gt;&#010;&gt; &gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#010;&gt; &gt;          while :;&#010;&gt; &gt;          do&#010;&gt; &gt;            if [[ -f $(find log -name '*jobmanager*.log' -print -quit) ",
        "depth": "7",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf34Zo28SectRuBqnJ7OCekovWCgzKr0cjH1A=h6OH+VWvQ@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 10:18:04 GMT",
        "subject": "Re: flink 1.11 on kubernetes 构建失败",
        "content": "抱歉回复晚了&#013;&#010;&#013;&#010;我这边也验证了一下，在你所说的地方确实是ip:port，但是提交任务都是正常的&#013;&#010;&#013;&#010;如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#013;&#010;有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#013;&#010;可能是coredns有问题&#013;&#010;&#013;&#010;&#013;&#010;Flink里面用的是InetAddress#getHostFromNameService来跟进IP地址获取FQDN的&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 下午1:10写道：&#013;&#010;&#013;&#010;&gt; hi Yang&#013;&#010;&gt;&#013;&#010;&gt; 在1.10版本，running的作业点击拓普图中随便一个operation，有detail subtasks&#010;taskmanagers xxx x&#013;&#010;&gt; 这行，taskmanagers这栏里的host，显示的是 podname：端口&#013;&#010;&gt;&#013;&#010;&gt; 在1.11变成ip：端口&#013;&#010;&gt;&#013;&#010;&gt; 目前我这边遇到的情况是，构建了一个有120slot的集群，作业并行度是120。&#010;提交到jm后jm就失联了，jm timeout。观察jm日志，疯狂在刷&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#013;&#010;&gt; address as host name. Local input split assignment (such as for HDFS files)&#013;&#010;&gt; may be impacted&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 目前观察到的改变主要是这块podname和ip的区别，其他不确定&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月10日 12:13，Yang Wang 写道：&#013;&#010;&gt; 我记得1.11里面对host这个地方应该是没有改动，taskmanager.network.bind-policy的&#013;&#010;&gt; 默认值一会都是ip。所以你说的UI上是podname，这个是哪里的？正常TM列表akka地址&#013;&#010;&gt; 都是ip地址的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 上午10:42写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi yang wang&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 1.11版本的on kubernetes在hostname上有做什么变化吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 作业运行的时候 flink ui上 tm变成ip：端口&#013;&#010;&gt; &gt; ，在1.10版本，ui上是 podname：端口。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 作业启动的时候，jm日志一直在刷&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#013;&#010;&gt; &gt; address as host name. Local input split assignment (such as for HDFS&#013;&#010;&gt; files)&#013;&#010;&gt; &gt; may be impacted&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在2020年07月09日 20:02，Yang Wang 写道：&#013;&#010;&gt; &gt; sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#013;&#010;&gt; &gt; 才会这样[1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#013;&#010;&gt; &gt; 如果不是，需要你把你的yaml发出来&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1].&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#013;&#010;&gt; &gt; [2].&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Yang&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; hi&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 目前看差别在于1.11启动jm和tm是通过args:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#013;&#010;&gt; &gt; &gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#013;&#010;&gt; &gt; &gt;          while :;&#013;&#010;&gt; &gt; &gt;          do&#013;&#010;&gt; &gt; &gt;            if [[ -f $(find log -name '*jobmanager*.log' -print -quit)&#013;&#010;&gt; ",
        "depth": "8",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<1e1a61b6.442e.173765efa2a.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 11:53:22 GMT",
        "subject": "回复：flink 1.11 on kubernetes 构建失败",
        "content": "&#010;Hi，Yang Wang！&#010;&#010;很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&#010;在JM报错的地方，No hostname could be resolved for ip address xxxxx ，报出来的ip是k8s分配给flink&#010;pod的内网ip，不是宿主机的ip。请问这个问题是出在哪里呢&#010;&#010;Best！&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月22日 18:18，Yang Wang 写道：&#010;抱歉回复晚了&#010;&#010;我这边也验证了一下，在你所说的地方确实是ip:port，但是提交任务都是正常的&#010;&#010;如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#010;有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;可能是coredns有问题&#010;&#010;&#010;Flink里面用的是InetAddress#getHostFromNameService来跟进IP地址获取FQDN的&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 下午1:10写道：&#010;&#010;&gt; hi Yang&#010;&gt;&#010;&gt; 在1.10版本，running的作业点击拓普图中随便一个operation，有detail subtasks&#010;taskmanagers xxx x&#010;&gt; 这行，taskmanagers这栏里的host，显示的是 podname：端口&#010;&gt;&#010;&gt; 在1.11变成ip：端口&#010;&gt;&#010;&gt; 目前我这边遇到的情况是，构建了一个有120slot的集群，作业并行度是120。&#010;提交到jm后jm就失联了，jm timeout。观察jm日志，疯狂在刷&#010;&gt;&#010;&gt;&#010;&gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#010;&gt; address as host name. Local input split assignment (such as for HDFS files)&#010;&gt; may be impacted&#010;&gt;&#010;&gt;&#010;&gt; 目前观察到的改变主要是这块podname和ip的区别，其他不确定&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; 在2020年07月10日 12:13，Yang Wang 写道：&#010;&gt; 我记得1.11里面对host这个地方应该是没有改动，taskmanager.network.bind-policy的&#010;&gt; 默认值一会都是ip。所以你说的UI上是podname，这个是哪里的？正常TM列表akka地址&#010;&gt; 都是ip地址的&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 上午10:42写道：&#010;&gt;&#010;&gt; &gt; hi yang wang&#010;&gt; &gt;&#010;&gt; &gt; 1.11版本的on kubernetes在hostname上有做什么变化吗？&#010;&gt; &gt;&#010;&gt; &gt; 作业运行的时候 flink ui上 tm变成ip：端口&#010;&gt; &gt; ，在1.10版本，ui上是 podname：端口。&#010;&gt; &gt;&#010;&gt; &gt; 作业启动的时候，jm日志一直在刷&#010;&gt; &gt;&#010;&gt; &gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#010;&gt; &gt; address as host name. Local input split assignment (such as for HDFS&#010;&gt; files)&#010;&gt; &gt; may be impacted&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; | |&#010;&gt; &gt; a511955993&#010;&gt; &gt; |&#010;&gt; &gt; |&#010;&gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; |&#010;&gt; &gt;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt;&#010;&gt; &gt; 在2020年07月09日 20:02，Yang Wang 写道：&#010;&gt; &gt; sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#010;&gt; &gt; 才会这样[1]&#010;&gt; &gt;&#010;&gt; &gt; 你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#010;&gt; &gt; 如果不是，需要你把你的yaml发出来&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; [1].&#010;&gt; &gt;&#010;&gt; https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#010;&gt; &gt; [2].&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Yang&#010;&gt; &gt;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 目前看差别在于1.11启动jm和tm是通过args:&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#010;&gt; &gt; &gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#010;&gt; &gt; &gt;          while :;&#010;&gt; &gt; &gt;          do&#010;&gt; &gt; &gt;            if [[ -f $(find log -name '*jobmanager*.log' -print -quit)&#010;&gt; ",
        "depth": "9",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<2fde7b1d.446d.17376644e0a.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 11:59:11 GMT",
        "subject": "回复：flink 1.11 on kubernetes 构建失败",
        "content": "Hi Yang Wang！&#010;&#010;你提到了Flink里面用的是InetAddress#getHostFromNameService来跟进IP地址获取FQDN的。&#010;&#010;这个在1.10和1.11版本是否有发生变化？这段报错只在1.11才出现，1.10不存在。如果core&#010;dns有问题，应该两个版本都有有异常&#010;&#010;Best！&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月22日 18:18，Yang Wang 写道：&#010;抱歉回复晚了&#010;&#010;我这边也验证了一下，在你所说的地方确实是ip:port，但是提交任务都是正常的&#010;&#010;如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#010;有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;可能是coredns有问题&#010;&#010;&#010;Flink里面用的是InetAddress#getHostFromNameService来跟进IP地址获取FQDN的&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 下午1:10写道：&#010;&#010;&gt; hi Yang&#010;&gt;&#010;&gt; 在1.10版本，running的作业点击拓普图中随便一个operation，有detail subtasks&#010;taskmanagers xxx x&#010;&gt; 这行，taskmanagers这栏里的host，显示的是 podname：端口&#010;&gt;&#010;&gt; 在1.11变成ip：端口&#010;&gt;&#010;&gt; 目前我这边遇到的情况是，构建了一个有120slot的集群，作业并行度是120。&#010;提交到jm后jm就失联了，jm timeout。观察jm日志，疯狂在刷&#010;&gt;&#010;&gt;&#010;&gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#010;&gt; address as host name. Local input split assignment (such as for HDFS files)&#010;&gt; may be impacted&#010;&gt;&#010;&gt;&#010;&gt; 目前观察到的改变主要是这块podname和ip的区别，其他不确定&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; 在2020年07月10日 12:13，Yang Wang 写道：&#010;&gt; 我记得1.11里面对host这个地方应该是没有改动，taskmanager.network.bind-policy的&#010;&gt; 默认值一会都是ip。所以你说的UI上是podname，这个是哪里的？正常TM列表akka地址&#010;&gt; 都是ip地址的&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 上午10:42写道：&#010;&gt;&#010;&gt; &gt; hi yang wang&#010;&gt; &gt;&#010;&gt; &gt; 1.11版本的on kubernetes在hostname上有做什么变化吗？&#010;&gt; &gt;&#010;&gt; &gt; 作业运行的时候 flink ui上 tm变成ip：端口&#010;&gt; &gt; ，在1.10版本，ui上是 podname：端口。&#010;&gt; &gt;&#010;&gt; &gt; 作业启动的时候，jm日志一直在刷&#010;&gt; &gt;&#010;&gt; &gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#010;&gt; &gt; address as host name. Local input split assignment (such as for HDFS&#010;&gt; files)&#010;&gt; &gt; may be impacted&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; | |&#010;&gt; &gt; a511955993&#010;&gt; &gt; |&#010;&gt; &gt; |&#010;&gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; |&#010;&gt; &gt;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt;&#010;&gt; &gt; 在2020年07月09日 20:02，Yang Wang 写道：&#010;&gt; &gt; sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#010;&gt; &gt; 才会这样[1]&#010;&gt; &gt;&#010;&gt; &gt; 你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#010;&gt; &gt; 如果不是，需要你把你的yaml发出来&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; [1].&#010;&gt; &gt;&#010;&gt; https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#010;&gt; &gt; [2].&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Yang&#010;&gt; &gt;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 目前看差别在于1.11启动jm和tm是通过args:&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#010;&gt; &gt; &gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#010;&gt; &gt; &gt;          while :;&#010;&gt; &gt; &gt;          do&#010;&gt; &gt; &gt;            if [[ -f $(find log -name '*jobmanager*.log' -print -quit)&#010;&gt; ",
        "depth": "9",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf35Y8_JapXqdGPA1pK8qA7ZB0aNBbvukq6B5FDiTsHZjSA@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 02:44:43 GMT",
        "subject": "Re: flink 1.11 on kubernetes 构建失败",
        "content": "这个地方是没有变化的，你可以看TaskManagerRunner的代码，一直是使用ip地址来向JM注册的&#013;&#010;&#013;&#010;你需要确认coredns解析这个IP到底是否可以成功，另外我验证了一下，你说的detail&#010;subtasks taskmanagers xxx x 这行&#013;&#010;显示的其实目前也是hostname，是解析ip之后得到的，例如我这边看到的是172-20-0-50，是因为我执行nslookup查询的结果是&#013;&#010;kubectl run -i -t busybox --image=busybox --restart=Never&#013;&#010;/ # nslookup 172.20.0.50&#013;&#010;Server: 172.21.0.10&#013;&#010;Address: 172.21.0.10:53&#013;&#010;&#013;&#010;50.0.20.172.in-addr.arpa name =&#013;&#010;172-20-0-50.flink-jobmanager.default.svc.cluster.local&#013;&#010;&#013;&#010;&#013;&#010;你最好先确认下你这边K8s集群的变更以及coredns的问题吧&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:59写道：&#013;&#010;&#013;&#010;&gt; Hi Yang Wang！&#013;&#010;&gt;&#013;&#010;&gt; 你提到了Flink里面用的是InetAddress#getHostFromNameService来跟进IP地址获取FQDN的。&#013;&#010;&gt;&#013;&#010;&gt; 这个在1.10和1.11版本是否有发生变化？这段报错只在1.11才出现，1.10不存在。如果core&#010;dns有问题，应该两个版本都有有异常&#013;&#010;&gt;&#013;&#010;&gt; Best！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; a511955993&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt;&#013;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&gt;&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt;&#010;定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月22日 18:18，Yang Wang &lt;danrtsey.wy@gmail.com&gt; 写道：&#013;&#010;&gt; 抱歉回复晚了&#013;&#010;&gt;&#013;&#010;&gt; 我这边也验证了一下，在你所说的地方确实是ip:port，但是提交任务都是正常的&#013;&#010;&gt;&#013;&#010;&gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#013;&#010;&gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#013;&#010;&gt; 可能是coredns有问题&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Flink里面用的是InetAddress#getHostFromNameService来跟进IP地址获取FQDN的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 下午1:10写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi Yang&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在1.10版本，running的作业点击拓普图中随便一个operation，有detail&#010;subtasks taskmanagers xxx&#013;&#010;&gt; x&#013;&#010;&gt; &gt; 这行，taskmanagers这栏里的host，显示的是 podname：端口&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在1.11变成ip：端口&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 目前我这边遇到的情况是，构建了一个有120slot的集群，作业并行度是120。&#010;提交到jm后jm就失联了，jm&#013;&#010;&gt; timeout。观察jm日志，疯狂在刷&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#013;&#010;&gt; &gt; address as host name. Local input split assignment (such as for HDFS&#013;&#010;&gt; files)&#013;&#010;&gt; &gt; may be impacted&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 目前观察到的改变主要是这块podname和ip的区别，其他不确定&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在2020年07月10日 12:13，Yang Wang 写道：&#013;&#010;&gt; &gt; 我记得1.11里面对host这个地方应该是没有改动，taskmanager.network.bind-policy的&#013;&#010;&gt; &gt; 默认值一会都是ip。所以你说的UI上是podname，这个是哪里的？正常TM列表akka地址&#013;&#010;&gt; &gt; 都是ip地址的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Yang&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月10日周五 上午10:42写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; hi yang wang&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 1.11版本的on kubernetes在hostname上有做什么变化吗？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 作业运行的时候 flink ui上 tm变成ip：端口&#013;&#010;&gt; &gt; &gt; ，在1.10版本，ui上是 podname：端口。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 作业启动的时候，jm日志一直在刷&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; No hostname could be resolved for the IP address 10.35.160.5, using IP&#013;&#010;&gt; &gt; &gt; address as host name. Local input split assignment (such as for HDFS&#013;&#010;&gt; &gt; files)&#013;&#010;&gt; &gt; &gt; may be impacted&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; | |&#013;&#010;&gt; &gt; &gt; a511955993&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 在2020年07月09日 20:02，Yang Wang 写道：&#013;&#010;&gt; &gt; &gt; sed替换报错应该不是Pod启动失败的根本原因，因为目前的docker-entrypoint.sh做了修改&#013;&#010;&gt; &gt; &gt; 才会这样[1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 你这个报错看着是执行bash-java-utils.jar报的错，确认你用的是社区的yaml文件[2]，我运行是没有问题的。&#013;&#010;&gt; &gt; &gt; 如果不是，需要你把你的yaml发出来&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; [1].&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://github.com/apache/flink-docker/blob/dev-master/docker-entrypoint.sh&#013;&#010;&gt; &gt; &gt; [2].&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/kubernetes.html&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Yang&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月9日周四 下午1:40写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; hi&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 按照新版本的部署文件[1]，会部署失败.如果将部署文件改用1.10版本，只是修改镜像文件和log4j文件，可以成功构建[2]。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 目前看差别在于1.11启动jm和tm是通过args:&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; [\"jobmanager\"]的方法，通过docker-entrypoint.sh[3]看到调用set_common_options方法的时候会sed&#013;&#010;&gt; &gt; &gt; &gt; 本地挂载的flink-configuration-configmap.yaml导致失败。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 1.10 版本是通过$FLINK_HOME/bin/jobmanager.sh启动。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; command: [\"/bin/bash\", \"-c\", \"$FLINK_HOME/bin/jobmanager.sh start;\\&#013;&#010;&gt; &gt; &gt; &gt;          while :;&#013;&#010;&gt; &gt; &gt; &gt;          do&#013;&#010;&gt; &gt; &gt; &gt;            if [[ -f $(find log -name '*jobmanager*.log' -print&#013;&#010;&gt; -quit)&#013;&#010;&gt; &gt; ",
        "depth": "10",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<202007101433430229400@teld.cn>",
        "from": "&quot;liugh@teld.cn&quot; &lt;li...@teld.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 06:33:43 GMT",
        "subject": "退订",
        "content": "退订&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<27555EC6-4F2D-4424-9ABB-1C12D7A2B23E@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 06:39:14 GMT",
        "subject": "Re: 退订",
        "content": "Hello, &#010;&#010;退订邮件组的邮件，可以发送任意内容的邮件到  user-zh-unsubscribe@flink.apache.org&#010; 取消订阅来自 user-zh@flink.apache.org 邮件组的邮件&#010;&#010;邮件组的订阅管理，可以参考[1]&#010;&#010;祝好，&#010;Leonard Xu&#010;[1] https://flink.apache.org/community.html#how-to-subscribe-to-a-mailing-list &lt;https://flink.apache.org/community.html#how-to-subscribe-to-a-mailing-list&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<317d4d4b.80f3.1732cce7caf.Coremail.a511955993@163.com>"
    },
    {
        "id": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:29:50 GMT",
        "subject": "flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "代码在flink 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;Exception in thread \"main\" java.lang.IllegalStateException: No operators defined in streaming&#010;topology. Cannot generate StreamGraph.&#010;at org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;at org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&#010;&#010;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&#010;&#010;&#010;&#010;query：&#010;streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE `user` (&#010;        |    uid BIGINT,&#010;        |    sex VARCHAR,&#010;        |    age INT,&#010;        |    created_time TIMESTAMP(3),&#010;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;        |) WITH (&#010;        |    'connector.type' = 'kafka',&#010;        |    'connector.version' = 'universal',&#010;        |    -- 'connector.topic' = 'user',&#010;        |    'connector.topic' = 'user_long',&#010;        |    'connector.startup-mode' = 'latest-offset',&#010;        |    'connector.properties.group.id' = 'user_flink',&#010;        |    'format.type' = 'json',&#010;        |    'format.derive-schema' = 'true'&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;&#010;&#010;&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE user_hbase3(&#010;        |    rowkey BIGINT,&#010;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;        |) WITH (&#010;        |    'connector.type' = 'hbase',&#010;        |    'connector.version' = '2.1.0',&#010;        |    'connector.table-name' = 'user_hbase2',&#010;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;        |    'connector.write.buffer-flush.interval' = '2s'&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |insert into user_hbase3&#010;        |SELECT uid,&#010;        |&#010;        |  ROW(sex, age, created_time ) as cf&#010;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as created_time from `user`)&#010;        |&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;",
        "depth": "0",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<a8ed3538-bc3b-4099-9629-7e8312d87a93.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:40:41 GMT",
        "subject": "回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "你好,&#010;可以看看你的代码结构是不是以下这种&#010;    val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;    val bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build&#010;    val tableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)&#010;  ......&#010;    tableEnv.execute(\"\")&#010;如果是的话,可以尝试使用bsEnv.execute(\"\")&#010;1.11对于两者的execute代码实现有改动&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;发送时间：2020年7月8日(星期三) 15:30&#010;收件人：Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;主　题：flink Sql 1.11 executeSql报No operators defined in streaming topology&#010;&#010;代码在flink 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;Exception in thread \"main\" java.lang.IllegalStateException: No operators defined in streaming&#010;topology. Cannot generate StreamGraph.&#010;at org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;at org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&#010;&#010;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&#010;&#010;&#010;&#010;query：&#010;streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE `user` (&#010;        |    uid BIGINT,&#010;        |    sex VARCHAR,&#010;        |    age INT,&#010;        |    created_time TIMESTAMP(3),&#010;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;        |) WITH (&#010;        |    'connector.type' = 'kafka',&#010;        |    'connector.version' = 'universal',&#010;        |    -- 'connector.topic' = 'user',&#010;        |    'connector.topic' = 'user_long',&#010;        |    'connector.startup-mode' = 'latest-offset',&#010;        |    'connector.properties.group.id' = 'user_flink',&#010;        |    'format.type' = 'json',&#010;        |    'format.derive-schema' = 'true'&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;&#010;&#010;&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE user_hbase3(&#010;        |    rowkey BIGINT,&#010;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;        |) WITH (&#010;        |    'connector.type' = 'hbase',&#010;        |    'connector.version' = '2.1.0',&#010;        |    'connector.table-name' = 'user_hbase2',&#010;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;        |    'connector.write.buffer-flush.interval' = '2s'&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |insert into user_hbase3&#010;        |SELECT uid,&#010;        |&#010;        |  ROW(sex, age, created_time ) as cf&#010;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as created_time from `user`)&#010;        |&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<6642c41d.7628.1732d6c8fc1.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:55:56 GMT",
        "subject": "Re:回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "&#010;&#010;&#010;代码结构改成这样的了：&#010;&#010;&#010;&#010;&#010;val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&#010;val blinkEnvSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&#010;val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv, blinkEnvSettings)&#010;&#010;&#010;&#010;&#010;&#010;streamExecutionEnv.execute(\"from kafka sink hbase\")&#010;&#010;&#010;&#010;&#010;还是报一样的错&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-08 15:40:41，\"夏帅\" &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt;你好,&#010;&gt;可以看看你的代码结构是不是以下这种&#010;&gt;    val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;    val bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build&#010;&gt;    val tableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)&#010;&gt;  ......&#010;&gt;    tableEnv.execute(\"\")&#010;&gt;如果是的话,可以尝试使用bsEnv.execute(\"\")&#010;&gt;1.11对于两者的execute代码实现有改动&#010;&gt;&#010;&gt;&#010;&gt;------------------------------------------------------------------&#010;&gt;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;发送时间：2020年7月8日(星期三) 15:30&#010;&gt;收件人：Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;&gt;主　题：flink Sql 1.11 executeSql报No operators defined in streaming topology&#010;&gt;&#010;&gt;代码在flink 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;&gt;Exception in thread \"main\" java.lang.IllegalStateException: No operators defined in streaming&#010;topology. Cannot generate StreamGraph.&#010;&gt;at org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt;at org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt;at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;&gt;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&gt;&#010;&gt;&#010;&gt;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;query：&#010;&gt;streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE `user` (&#010;&gt;        |    uid BIGINT,&#010;&gt;        |    sex VARCHAR,&#010;&gt;        |    age INT,&#010;&gt;        |    created_time TIMESTAMP(3),&#010;&gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;        |) WITH (&#010;&gt;        |    'connector.type' = 'kafka',&#010;&gt;        |    'connector.version' = 'universal',&#010;&gt;        |    -- 'connector.topic' = 'user',&#010;&gt;        |    'connector.topic' = 'user_long',&#010;&gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt;        |    'format.type' = 'json',&#010;&gt;        |    'format.derive-schema' = 'true'&#010;&gt;        |)&#010;&gt;        |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE user_hbase3(&#010;&gt;        |    rowkey BIGINT,&#010;&gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt;        |) WITH (&#010;&gt;        |    'connector.type' = 'hbase',&#010;&gt;        |    'connector.version' = '2.1.0',&#010;&gt;        |    'connector.table-name' = 'user_hbase2',&#010;&gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;&gt;        |    'connector.write.buffer-flush.interval' = '2s'&#010;&gt;        |)&#010;&gt;        |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |insert into user_hbase3&#010;&gt;        |SELECT uid,&#010;&gt;        |&#010;&gt;        |  ROW(sex, age, created_time ) as cf&#010;&gt;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as created_time from&#010;`user`)&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;",
        "depth": "2",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<CABi+2jRA3cEmAcLMhUacVJBg=_qE9Z28LpX3Ew=Pv-WzCTS=tw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:07:17 GMT",
        "subject": "Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "Hi,&#010;&#010;你的代码里：streamTableEnv.executeSql，它的意思就是已经提交到集群异步的去执行了。&#010;&#010;所以你后面 \"streamExecutionEnv.execute(\"from kafka sink hbase\")\"&#010;并没有真正的物理节点。你不用再调用了。&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Wed, Jul 8, 2020 at 3:56 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 代码结构改成这样的了：&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&#010;&gt; val blinkEnvSettings =&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;&#010;&gt; val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt; blinkEnvSettings)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; streamExecutionEnv.execute(\"from kafka sink hbase\")&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 还是报一样的错&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-08 15:40:41，\"夏帅\" &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt; &gt;你好,&#010;&gt; &gt;可以看看你的代码结构是不是以下这种&#010;&gt; &gt;    val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;    val bsSettings =&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build&#010;&gt; &gt;    val tableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)&#010;&gt; &gt;  ......&#010;&gt; &gt;    tableEnv.execute(\"\")&#010;&gt; &gt;如果是的话,可以尝试使用bsEnv.execute(\"\")&#010;&gt; &gt;1.11对于两者的execute代码实现有改动&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;------------------------------------------------------------------&#010;&gt; &gt;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;发送时间：2020年7月8日(星期三) 15:30&#010;&gt; &gt;收件人：Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;主 题：flink Sql 1.11 executeSql报No operators defined in streaming topology&#010;&gt; &gt;&#010;&gt; &gt;代码在flink&#010;&gt; 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;&gt; &gt;Exception in thread \"main\" java.lang.IllegalStateException: No operators&#010;&gt; defined in streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt;at&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt;at&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt;at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;query：&#010;&gt; &gt;streamTableEnv.executeSql(&#010;&gt; &gt;      \"\"\"&#010;&gt; &gt;        |&#010;&gt; &gt;        |CREATE TABLE `user` (&#010;&gt; &gt;        |    uid BIGINT,&#010;&gt; &gt;        |    sex VARCHAR,&#010;&gt; &gt;        |    age INT,&#010;&gt; &gt;        |    created_time TIMESTAMP(3),&#010;&gt; &gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; SECOND&#010;&gt; &gt;        |) WITH (&#010;&gt; &gt;        |    'connector.type' = 'kafka',&#010;&gt; &gt;        |    'connector.version' = 'universal',&#010;&gt; &gt;        |    -- 'connector.topic' = 'user',&#010;&gt; &gt;        |    'connector.topic' = 'user_long',&#010;&gt; &gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;        |    'format.type' = 'json',&#010;&gt; &gt;        |    'format.derive-schema' = 'true'&#010;&gt; &gt;        |)&#010;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt; &gt;      \"\"\"&#010;&gt; &gt;        |&#010;&gt; &gt;        |CREATE TABLE user_hbase3(&#010;&gt; &gt;        |    rowkey BIGINT,&#010;&gt; &gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt; &gt;        |) WITH (&#010;&gt; &gt;        |    'connector.type' = 'hbase',&#010;&gt; &gt;        |    'connector.version' = '2.1.0',&#010;&gt; &gt;        |    'connector.table-name' = 'user_hbase2',&#010;&gt; &gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt; &gt;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt; &gt;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;&gt; &gt;        |    'connector.write.buffer-flush.interval' = '2s'&#010;&gt; &gt;        |)&#010;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt; &gt;      \"\"\"&#010;&gt; &gt;        |&#010;&gt; &gt;        |insert into user_hbase3&#010;&gt; &gt;        |SELECT uid,&#010;&gt; &gt;        |&#010;&gt; &gt;        |  ROW(sex, age, created_time ) as cf&#010;&gt; &gt;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as&#010;&gt; created_time from `user`)&#010;&gt; &gt;        |&#010;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "3",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<435b4f52.7d2a.1732d81a52f.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:18:57 GMT",
        "subject": "Re:Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "去掉就好了，感谢解答&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-08 16:07:17，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;你的代码里：streamTableEnv.executeSql，它的意思就是已经提交到集群异步的去执行了。&#010;&gt;&#010;&gt;所以你后面 \"streamExecutionEnv.execute(\"from kafka sink hbase\")\"&#010;&gt;并没有真正的物理节点。你不用再调用了。&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Wed, Jul 8, 2020 at 3:56 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 代码结构改成这样的了：&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt;&#010;&gt;&gt; val blinkEnvSettings =&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;&gt;&#010;&gt;&gt; val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt;&gt; blinkEnvSettings)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; streamExecutionEnv.execute(\"from kafka sink hbase\")&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 还是报一样的错&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-08 15:40:41，\"夏帅\" &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt;&gt; &gt;你好,&#010;&gt;&gt; &gt;可以看看你的代码结构是不是以下这种&#010;&gt;&gt; &gt;    val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt; &gt;    val bsSettings =&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build&#010;&gt;&gt; &gt;    val tableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)&#010;&gt;&gt; &gt;  ......&#010;&gt;&gt; &gt;    tableEnv.execute(\"\")&#010;&gt;&gt; &gt;如果是的话,可以尝试使用bsEnv.execute(\"\")&#010;&gt;&gt; &gt;1.11对于两者的execute代码实现有改动&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;------------------------------------------------------------------&#010;&gt;&gt; &gt;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; &gt;发送时间：2020年7月8日(星期三) 15:30&#010;&gt;&gt; &gt;收件人：Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; &gt;主 题：flink Sql 1.11 executeSql报No operators defined in streaming topology&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;代码在flink&#010;&gt;&gt; 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;&gt;&gt; &gt;Exception in thread \"main\" java.lang.IllegalStateException: No operators&#010;&gt;&gt; defined in streaming topology. Cannot generate StreamGraph.&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;&gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;query：&#010;&gt;&gt; &gt;streamTableEnv.executeSql(&#010;&gt;&gt; &gt;      \"\"\"&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |CREATE TABLE `user` (&#010;&gt;&gt; &gt;        |    uid BIGINT,&#010;&gt;&gt; &gt;        |    sex VARCHAR,&#010;&gt;&gt; &gt;        |    age INT,&#010;&gt;&gt; &gt;        |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt;&gt; SECOND&#010;&gt;&gt; &gt;        |) WITH (&#010;&gt;&gt; &gt;        |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;        |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;        |    -- 'connector.topic' = 'user',&#010;&gt;&gt; &gt;        |    'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;        |    'format.type' = 'json',&#010;&gt;&gt; &gt;        |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;        |)&#010;&gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt;&gt; &gt;      \"\"\"&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |CREATE TABLE user_hbase3(&#010;&gt;&gt; &gt;        |    rowkey BIGINT,&#010;&gt;&gt; &gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt;&gt; &gt;        |) WITH (&#010;&gt;&gt; &gt;        |    'connector.type' = 'hbase',&#010;&gt;&gt; &gt;        |    'connector.version' = '2.1.0',&#010;&gt;&gt; &gt;        |    'connector.table-name' = 'user_hbase2',&#010;&gt;&gt; &gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt;&gt; &gt;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt;&gt; &gt;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;&gt;&gt; &gt;        |    'connector.write.buffer-flush.interval' = '2s'&#010;&gt;&gt; &gt;        |)&#010;&gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt;&gt; &gt;      \"\"\"&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |insert into user_hbase3&#010;&gt;&gt; &gt;        |SELECT uid,&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |  ROW(sex, age, created_time ) as cf&#010;&gt;&gt; &gt;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as&#010;&gt;&gt; created_time from `user`)&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "4",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<CADQYLGvo8+tc+FVhvyagLOMja85uOG2C0Eniv0HrkJpvOvbg7Q@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 09:19:44 GMT",
        "subject": "Re: Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "1.11 对 StreamTableEnvironment.execute()&#010;和 StreamExecutionEnvironment.execute() 的执行方式有所调整，&#010;简单概述为：&#010;1. StreamTableEnvironment.execute() 只能执行 sqlUpdate 和 insertInto 方法执行作业；&#010;2. Table 转化为 DataStream 后只能通过 StreamExecutionEnvironment.execute() 来执行作业；&#010;3. 新引入的 TableEnvironment.executeSql() 方法是直接执行sql作业&#010;(异步提交作业)，不需要再调用 StreamTableEnvironment.execute()&#010;或 StreamExecutionEnvironment.execute()&#010;&#010;详细可以参考 [1] [2]&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%89%A7%E8%A1%8C%E6%9F%A5%E8%AF%A2&#010;[2]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/common.html#%E5%B0%86%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90-datastream-%E6%88%96-dataset&#010;&#010;Best,&#010;Godfrey&#010;&#010;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月8日周三 下午4:19写道：&#010;&#010;&gt; 去掉就好了，感谢解答&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-08 16:07:17，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;Hi,&#010;&gt; &gt;&#010;&gt; &gt;你的代码里：streamTableEnv.executeSql，它的意思就是已经提交到集群异步的去执行了。&#010;&gt; &gt;&#010;&gt; &gt;所以你后面 \"streamExecutionEnv.execute(\"from kafka sink hbase\")\"&#010;&gt; &gt;并没有真正的物理节点。你不用再调用了。&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Wed, Jul 8, 2020 at 3:56 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 代码结构改成这样的了：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; val blinkEnvSettings =&#010;&gt; &gt;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt; &gt;&gt; blinkEnvSettings)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; streamExecutionEnv.execute(\"from kafka sink hbase\")&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 还是报一样的错&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-08 15:40:41，\"夏帅\" &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt; &gt;&gt; &gt;你好,&#010;&gt; &gt;&gt; &gt;可以看看你的代码结构是不是以下这种&#010;&gt; &gt;&gt; &gt;    val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt; &gt;    val bsSettings =&#010;&gt; &gt;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build&#010;&gt; &gt;&gt; &gt;    val tableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)&#010;&gt; &gt;&gt; &gt;  ......&#010;&gt; &gt;&gt; &gt;    tableEnv.execute(\"\")&#010;&gt; &gt;&gt; &gt;如果是的话,可以尝试使用bsEnv.execute(\"\")&#010;&gt; &gt;&gt; &gt;1.11对于两者的execute代码实现有改动&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;------------------------------------------------------------------&#010;&gt; &gt;&gt; &gt;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; &gt;发送时间：2020年7月8日(星期三) 15:30&#010;&gt; &gt;&gt; &gt;收件人：Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; &gt;主 题：flink Sql 1.11 executeSql报No operators defined in streaming&#010;&gt; topology&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;代码在flink&#010;&gt; &gt;&gt;&#010;&gt; 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;&gt; &gt;&gt; &gt;Exception in thread \"main\" java.lang.IllegalStateException: No&#010;&gt; operators&#010;&gt; &gt;&gt; defined in streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt;&gt; &gt;at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt;&gt; &gt;at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt;&gt; &gt;at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;&gt; &gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;query：&#010;&gt; &gt;&gt; &gt;streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;      \"\"\"&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |CREATE TABLE `user` (&#010;&gt; &gt;&gt; &gt;        |    uid BIGINT,&#010;&gt; &gt;&gt; &gt;        |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;        |    age INT,&#010;&gt; &gt;&gt; &gt;        |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;        |) WITH (&#010;&gt; &gt;&gt; &gt;        |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;        |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;        |    -- 'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;        |    'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;        |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;        |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;        |)&#010;&gt; &gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;      \"\"\"&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |CREATE TABLE user_hbase3(&#010;&gt; &gt;&gt; &gt;        |    rowkey BIGINT,&#010;&gt; &gt;&gt; &gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt; &gt;&gt; &gt;        |) WITH (&#010;&gt; &gt;&gt; &gt;        |    'connector.type' = 'hbase',&#010;&gt; &gt;&gt; &gt;        |    'connector.version' = '2.1.0',&#010;&gt; &gt;&gt; &gt;        |    'connector.table-name' = 'user_hbase2',&#010;&gt; &gt;&gt; &gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt; &gt;&gt; &gt;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt; &gt;&gt; &gt;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;&gt; &gt;&gt; &gt;        |    'connector.write.buffer-flush.interval' = '2s'&#010;&gt; &gt;&gt; &gt;        |)&#010;&gt; &gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;      \"\"\"&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |insert into user_hbase3&#010;&gt; &gt;&gt; &gt;        |SELECT uid,&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |  ROW(sex, age, created_time ) as cf&#010;&gt; &gt;&gt; &gt;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as&#010;&gt; &gt;&gt; created_time from `user`)&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<b0e63902-1fac-49e4-bfe1-48857f48ae65.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 09:23:18 GMT",
        "subject": "回复：Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "感谢&#010;",
        "depth": "5",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<1595498443533-0.post@n8.nabble.com>",
        "from": "WeiXubin &lt;18925434...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 10:00:43 GMT",
        "subject": "Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "Hi，&#010;我想请问下使用 streamExecutionEnv.execute(\"from kafka sink&#010;hbase\")，通过这种方式可以给Job指定名称。&#010;而当使用streamTableEnv.executeSql(sql)之后似乎无法给Job定义名称。&#010;请问有什么解决方案吗？谢谢&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "4",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<CADQYLGt64W1AzUe5tr=YJVMY7b8uPVG+0rO9jtKPwtxrMbHZ8w@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 10:05:08 GMT",
        "subject": "Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "这个问题的已经有一个issue：https://issues.apache.org/jira/browse/FLINK-18545，请关注&#013;&#010;&#013;&#010;WeiXubin &lt;18925434719@163.com&gt; 于2020年7月23日周四 下午6:00写道：&#013;&#010;&#013;&#010;&gt; Hi，&#013;&#010;&gt; 我想请问下使用 streamExecutionEnv.execute(\"from kafka sink&#013;&#010;&gt; hbase\")，通过这种方式可以给Job指定名称。&#013;&#010;&gt; 而当使用streamTableEnv.executeSql(sql)之后似乎无法给Job定义名称。&#013;&#010;&gt; 请问有什么解决方案吗？谢谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<65663a2d.7021.1737b2368dc.Coremail.18925434719@163.com>",
        "from": "Weixubin  &lt;18925434...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 10:06:24 GMT",
        "subject": "Re:Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "Hi，&#010;我想请教下，使用streamExecutionEnv.execute(\"from kafka sink hbase\") 是可以指定Job的名称。&#010;而当改用streamTableEnv.executeSql(sql)的方式时，似乎无法定义Job的名称。&#010;请问有什么解决的方法吗？&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-08 16:07:17，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;你的代码里：streamTableEnv.executeSql，它的意思就是已经提交到集群异步的去执行了。&#010;&gt;&#010;&gt;所以你后面 \"streamExecutionEnv.execute(\"from kafka sink hbase\")\"&#010;&gt;并没有真正的物理节点。你不用再调用了。&#010;&gt;&#010;&gt;Best,&#010;&gt;Jingsong&#010;&gt;&#010;&gt;On Wed, Jul 8, 2020 at 3:56 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 代码结构改成这样的了：&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt;&#010;&gt;&gt; val blinkEnvSettings =&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;&gt;&#010;&gt;&gt; val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt;&gt; blinkEnvSettings)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; streamExecutionEnv.execute(\"from kafka sink hbase\")&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 还是报一样的错&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-08 15:40:41，\"夏帅\" &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt;&gt; &gt;你好,&#010;&gt;&gt; &gt;可以看看你的代码结构是不是以下这种&#010;&gt;&gt; &gt;    val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;&gt; &gt;    val bsSettings =&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build&#010;&gt;&gt; &gt;    val tableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)&#010;&gt;&gt; &gt;  ......&#010;&gt;&gt; &gt;    tableEnv.execute(\"\")&#010;&gt;&gt; &gt;如果是的话,可以尝试使用bsEnv.execute(\"\")&#010;&gt;&gt; &gt;1.11对于两者的execute代码实现有改动&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;------------------------------------------------------------------&#010;&gt;&gt; &gt;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt;&gt; &gt;发送时间：2020年7月8日(星期三) 15:30&#010;&gt;&gt; &gt;收件人：Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; &gt;主 题：flink Sql 1.11 executeSql报No operators defined in streaming topology&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;代码在flink&#010;&gt;&gt; 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;&gt;&gt; &gt;Exception in thread \"main\" java.lang.IllegalStateException: No operators&#010;&gt;&gt; defined in streaming topology. Cannot generate StreamGraph.&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;&gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;query：&#010;&gt;&gt; &gt;streamTableEnv.executeSql(&#010;&gt;&gt; &gt;      \"\"\"&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |CREATE TABLE `user` (&#010;&gt;&gt; &gt;        |    uid BIGINT,&#010;&gt;&gt; &gt;        |    sex VARCHAR,&#010;&gt;&gt; &gt;        |    age INT,&#010;&gt;&gt; &gt;        |    created_time TIMESTAMP(3),&#010;&gt;&gt; &gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt;&gt; SECOND&#010;&gt;&gt; &gt;        |) WITH (&#010;&gt;&gt; &gt;        |    'connector.type' = 'kafka',&#010;&gt;&gt; &gt;        |    'connector.version' = 'universal',&#010;&gt;&gt; &gt;        |    -- 'connector.topic' = 'user',&#010;&gt;&gt; &gt;        |    'connector.topic' = 'user_long',&#010;&gt;&gt; &gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt; &gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt; &gt;        |    'format.type' = 'json',&#010;&gt;&gt; &gt;        |    'format.derive-schema' = 'true'&#010;&gt;&gt; &gt;        |)&#010;&gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt;&gt; &gt;      \"\"\"&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |CREATE TABLE user_hbase3(&#010;&gt;&gt; &gt;        |    rowkey BIGINT,&#010;&gt;&gt; &gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt;&gt; &gt;        |) WITH (&#010;&gt;&gt; &gt;        |    'connector.type' = 'hbase',&#010;&gt;&gt; &gt;        |    'connector.version' = '2.1.0',&#010;&gt;&gt; &gt;        |    'connector.table-name' = 'user_hbase2',&#010;&gt;&gt; &gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt;&gt; &gt;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt;&gt; &gt;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;&gt;&gt; &gt;        |    'connector.write.buffer-flush.interval' = '2s'&#010;&gt;&gt; &gt;        |)&#010;&gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt;&gt; &gt;      \"\"\"&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |insert into user_hbase3&#010;&gt;&gt; &gt;        |SELECT uid,&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |  ROW(sex, age, created_time ) as cf&#010;&gt;&gt; &gt;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as&#010;&gt;&gt; created_time from `user`)&#010;&gt;&gt; &gt;        |&#010;&gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;Best, Jingsong Lee&#010;",
        "depth": "4",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<CADQYLGty0t33t+9mrvy4NYsYsiRL5s-cEqQMfqJu2aq+m5mpCg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 12:17:10 GMT",
        "subject": "Re: Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "hi,&#010;目前没有解决办法，insert job根据sink表名自动生成job name。&#010;后续解法关注 https://issues.apache.org/jira/browse/FLINK-18545&#010;&#010;Weixubin &lt;18925434719@163.com&gt; 于2020年7月23日周四 下午6:07写道：&#010;&#010;&gt; Hi，&#010;&gt; 我想请教下，使用streamExecutionEnv.execute(\"from kafka sink hbase\") 是可以指定Job的名称。&#010;&gt; 而当改用streamTableEnv.executeSql(sql)的方式时，似乎无法定义Job的名称。&#010;&gt; 请问有什么解决的方法吗？&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-08 16:07:17，\"Jingsong Li\" &lt;jingsonglee0@gmail.com&gt; 写道：&#010;&gt; &gt;Hi,&#010;&gt; &gt;&#010;&gt; &gt;你的代码里：streamTableEnv.executeSql，它的意思就是已经提交到集群异步的去执行了。&#010;&gt; &gt;&#010;&gt; &gt;所以你后面 \"streamExecutionEnv.execute(\"from kafka sink hbase\")\"&#010;&gt; &gt;并没有真正的物理节点。你不用再调用了。&#010;&gt; &gt;&#010;&gt; &gt;Best,&#010;&gt; &gt;Jingsong&#010;&gt; &gt;&#010;&gt; &gt;On Wed, Jul 8, 2020 at 3:56 PM Zhou Zach &lt;wander669@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 代码结构改成这样的了：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; val streamExecutionEnv =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; val blinkEnvSettings =&#010;&gt; &gt;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv,&#010;&gt; &gt;&gt; blinkEnvSettings)&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; streamExecutionEnv.execute(\"from kafka sink hbase\")&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 还是报一样的错&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-08 15:40:41，\"夏帅\" &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt; &gt;&gt; &gt;你好,&#010;&gt; &gt;&gt; &gt;可以看看你的代码结构是不是以下这种&#010;&gt; &gt;&gt; &gt;    val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt; &gt;&gt; &gt;    val bsSettings =&#010;&gt; &gt;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build&#010;&gt; &gt;&gt; &gt;    val tableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)&#010;&gt; &gt;&gt; &gt;  ......&#010;&gt; &gt;&gt; &gt;    tableEnv.execute(\"\")&#010;&gt; &gt;&gt; &gt;如果是的话,可以尝试使用bsEnv.execute(\"\")&#010;&gt; &gt;&gt; &gt;1.11对于两者的execute代码实现有改动&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;------------------------------------------------------------------&#010;&gt; &gt;&gt; &gt;发件人：Zhou Zach &lt;wander669@163.com&gt;&#010;&gt; &gt;&gt; &gt;发送时间：2020年7月8日(星期三) 15:30&#010;&gt; &gt;&gt; &gt;收件人：Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; &gt;主 题：flink Sql 1.11 executeSql报No operators defined in streaming&#010;&gt; topology&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;代码在flink&#010;&gt; &gt;&gt;&#010;&gt; 1.10.1是可以正常运行的，升级到1.11.0时，提示streamTableEnv.sqlUpdate弃用，改成executeSql了，程序启动2秒后，报异常：&#010;&gt; &gt;&gt; &gt;Exception in thread \"main\" java.lang.IllegalStateException: No&#010;&gt; operators&#010;&gt; &gt;&gt; defined in streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt;&gt; &gt;at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt;&gt; &gt;at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt;&gt; &gt;at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase$.main(FromKafkaSinkHbase.scala:79)&#010;&gt; &gt;&gt; &gt;at org.rabbit.sql.FromKafkaSinkHbase.main(FromKafkaSinkHbase.scala)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;但是，数据是正常sink到了hbase，是不是executeSql误报了。。。&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;query：&#010;&gt; &gt;&gt; &gt;streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;      \"\"\"&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |CREATE TABLE `user` (&#010;&gt; &gt;&gt; &gt;        |    uid BIGINT,&#010;&gt; &gt;&gt; &gt;        |    sex VARCHAR,&#010;&gt; &gt;&gt; &gt;        |    age INT,&#010;&gt; &gt;&gt; &gt;        |    created_time TIMESTAMP(3),&#010;&gt; &gt;&gt; &gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3'&#010;&gt; &gt;&gt; SECOND&#010;&gt; &gt;&gt; &gt;        |) WITH (&#010;&gt; &gt;&gt; &gt;        |    'connector.type' = 'kafka',&#010;&gt; &gt;&gt; &gt;        |    'connector.version' = 'universal',&#010;&gt; &gt;&gt; &gt;        |    -- 'connector.topic' = 'user',&#010;&gt; &gt;&gt; &gt;        |    'connector.topic' = 'user_long',&#010;&gt; &gt;&gt; &gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt; &gt;&gt; &gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt; &gt;&gt; &gt;        |    'format.type' = 'json',&#010;&gt; &gt;&gt; &gt;        |    'format.derive-schema' = 'true'&#010;&gt; &gt;&gt; &gt;        |)&#010;&gt; &gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;      \"\"\"&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |CREATE TABLE user_hbase3(&#010;&gt; &gt;&gt; &gt;        |    rowkey BIGINT,&#010;&gt; &gt;&gt; &gt;        |    cf ROW(sex VARCHAR, age INT, created_time VARCHAR)&#010;&gt; &gt;&gt; &gt;        |) WITH (&#010;&gt; &gt;&gt; &gt;        |    'connector.type' = 'hbase',&#010;&gt; &gt;&gt; &gt;        |    'connector.version' = '2.1.0',&#010;&gt; &gt;&gt; &gt;        |    'connector.table-name' = 'user_hbase2',&#010;&gt; &gt;&gt; &gt;        |    'connector.zookeeper.znode.parent' = '/hbase',&#010;&gt; &gt;&gt; &gt;        |    'connector.write.buffer-flush.max-size' = '10mb',&#010;&gt; &gt;&gt; &gt;        |    'connector.write.buffer-flush.max-rows' = '1000',&#010;&gt; &gt;&gt; &gt;        |    'connector.write.buffer-flush.interval' = '2s'&#010;&gt; &gt;&gt; &gt;        |)&#010;&gt; &gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;    streamTableEnv.executeSql(&#010;&gt; &gt;&gt; &gt;      \"\"\"&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |insert into user_hbase3&#010;&gt; &gt;&gt; &gt;        |SELECT uid,&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |  ROW(sex, age, created_time ) as cf&#010;&gt; &gt;&gt; &gt;        |  FROM  (select uid,sex,age, cast(created_time as VARCHAR) as&#010;&gt; &gt;&gt; created_time from `user`)&#010;&gt; &gt;&gt; &gt;        |&#010;&gt; &gt;&gt; &gt;        |\"\"\".stripMargin)&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;--&#010;&gt; &gt;Best, Jingsong Lee&#010;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<1595554723644-0.post@n8.nabble.com>",
        "from": "WeiXubin &lt;18925434...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 01:38:43 GMT",
        "subject": "Re: Re: 回复：flink Sql 1.11 executeSql报No operators defined in streaming topology",
        "content": "感谢&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "6",
        "reply": "<1b9e4bb7.6e4c.1732d54a9fe.Coremail.wander669@163.com>"
    },
    {
        "id": "<tencent_B191D404E1202D68415F1693C1C5EAAA0507@qq.com>",
        "from": "&quot;Robert.Zhang&quot; &lt;173603...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:41:01 GMT",
        "subject": "flink state使用",
        "content": "Hello,all&#013;&#010;目前遇到一个关于state的使用问题&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; DataStream&lt;Tuple2&lt;String, String&amp;gt;&amp;gt; result&#010;= iteration.closeWith(&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; iteration.keyBy(0).process(new&#010;RichMapFunc());&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我在richmapfunc里使用了state，那么每一次迭代的时候，这个state还是同一个吗？&#013;&#010;如果我对result这个结果stream继续使用了keyby(0)的话，state也是同一个么？&#013;&#010;此外，如果我对一个stream进行keyby之后，key一直是不变的，&#013;&#010;但是map等operator之后返回的并非是keyedstream（operator不对key进行操作），是否只能继续keyby，还是有更好的方式",
        "depth": "0",
        "reply": "<tencent_B191D404E1202D68415F1693C1C5EAAA0507@qq.com>"
    },
    {
        "id": "<CAA8tFvvR=eC80EkBB-TrJ9GkG7Nwq9K+qrqiPbyrUbZ8Loz6xA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:48:34 GMT",
        "subject": "Re: flink state使用",
        "content": "Hi&#013;&#010;&#013;&#010;KeyedState 的操作实际会针对当前的 key，也就是 keyBy 之后得到的 key，但是这个&#010;key 用户看不到。在一个 operator&#013;&#010;中 state 的数目是你创建的数目，但是每个 state 可以有多个 KV 对，其中&#010;K 是 keyby 的 key，V 可以是&#013;&#010;value，list，map 等。同一个 operator 上不同的 key 的 state 保证能够正确读写的。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Robert.Zhang &lt;173603082@qq.com&gt; 于2020年7月8日周三 下午3:41写道：&#013;&#010;&#013;&#010;&gt; Hello,all&#013;&#010;&gt; 目前遇到一个关于state的使用问题&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; DataStream&lt;Tuple2&lt;String, String&amp;gt;&amp;gt;&#010;result =&#013;&#010;&gt; iteration.closeWith(&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&gt; iteration.keyBy(0).process(new RichMapFunc());&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我在richmapfunc里使用了state，那么每一次迭代的时候，这个state还是同一个吗？&#013;&#010;&gt; 如果我对result这个结果stream继续使用了keyby(0)的话，state也是同一个么？&#013;&#010;&gt; 此外，如果我对一个stream进行keyby之后，key一直是不变的，&#013;&#010;&gt; 但是map等operator之后返回的并非是keyedstream（operator不对key进行操作），是否只能继续keyby，还是有更好的方式&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_B191D404E1202D68415F1693C1C5EAAA0507@qq.com>"
    },
    {
        "id": "<tencent_802C80A6BE08D769BD0922B0740F1A558F0A@qq.com>",
        "from": "&quot;Robert.Zhang&quot; &lt;173603...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 07:58:40 GMT",
        "subject": "回复： flink state使用",
        "content": "那就是在说，在iterativestream中，这个state可以正确的在每一次的迭代operator中传递并读取，那么结束迭代之后，假设得到的结果stream再进行keyby操作，因为operator不是同一个，此时就无法读取到前述迭代过程中的state，即便是同一个key&#013;&#010;这样的理解对吗？&#013;&#010;&#013;&#010;&#013;&#010;Best Regards&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人: \"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;; &#013;&#010;发送时间: 2020年7月8日(星期三) 下午3:48&#013;&#010;收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;; &#013;&#010;主题: Re: flink state使用&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;KeyedState 的操作实际会针对当前的 key，也就是 keyBy 之后得到的 key，但是这个&#010;key 用户看不到。在一个 operator&#013;&#010;中 state 的数目是你创建的数目，但是每个 state 可以有多个 KV 对，其中&#010;K 是 keyby 的 key，V 可以是&#013;&#010;value，list，map 等。同一个 operator 上不同的 key 的 state 保证能够正确读写的。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Robert.Zhang &lt;173603082@qq.com&amp;gt; 于2020年7月8日周三 下午3:41写道：&#013;&#010;&#013;&#010;&amp;gt; Hello,all&#013;&#010;&amp;gt; 目前遇到一个关于state的使用问题&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; DataStream&lt;Tuple2&lt;String, String&amp;amp;gt;&amp;amp;gt;&#010;result =&#013;&#010;&amp;gt; iteration.closeWith(&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; iteration.keyBy(0).process(new RichMapFunc());&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我在richmapfunc里使用了state，那么每一次迭代的时候，这个state还是同一个吗？&#013;&#010;&amp;gt; 如果我对result这个结果stream继续使用了keyby(0)的话，state也是同一个么？&#013;&#010;&amp;gt; 此外，如果我对一个stream进行keyby之后，key一直是不变的，&#013;&#010;&amp;gt; 但是map等operator之后返回的并非是keyedstream（operator不对key进行操作），是否只能继续keyby，还是有更好的方式",
        "depth": "2",
        "reply": "<tencent_B191D404E1202D68415F1693C1C5EAAA0507@qq.com>"
    },
    {
        "id": "<CAA8tFvvqFRYv0dZKUewG=+1pCF58aQrAr4jm+bYUpYgKccYrQw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 11:27:17 GMT",
        "subject": "Re: flink state使用",
        "content": "State 可以简单理解为一个 HashMap，Key 是 curretnly key（也就是 keyby 的 key)），value&#010;是 state&#013;&#010;存的值（可以是 value，list，map 等）&#013;&#010;&#013;&#010;所有 state 的读写都有一个 currently，只能读到 currently key 对应的值。&#013;&#010;&#013;&#010;在同一个 operator 中，同一个 key 能访问到之前存储过的 state 值，但是不能读取到其他&#010;key 对应的值&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Robert.Zhang &lt;173603082@qq.com&gt; 于2020年7月8日周三 下午3:58写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 那就是在说，在iterativestream中，这个state可以正确的在每一次的迭代operator中传递并读取，那么结束迭代之后，假设得到的结果stream再进行keyby操作，因为operator不是同一个，此时就无法读取到前述迭代过程中的state，即便是同一个key&#013;&#010;&gt; 这样的理解对吗？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best Regards&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人: \"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间: 2020年7月8日(星期三) 下午3:48&#013;&#010;&gt; 收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt; 主题: Re: flink state使用&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; KeyedState 的操作实际会针对当前的 key，也就是 keyBy 之后得到的 key，但是这个&#010;key 用户看不到。在一个 operator&#013;&#010;&gt; 中 state 的数目是你创建的数目，但是每个 state 可以有多个 KV 对，其中&#010;K 是 keyby 的 key，V 可以是&#013;&#010;&gt; value，list，map 等。同一个 operator 上不同的 key 的 state 保证能够正确读写的。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Robert.Zhang &lt;173603082@qq.com&amp;gt; 于2020年7月8日周三 下午3:41写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; Hello,all&#013;&#010;&gt; &amp;gt; 目前遇到一个关于state的使用问题&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; DataStream&lt;Tuple2&lt;String,&#013;&#010;&gt; String&amp;amp;gt;&amp;amp;gt; result =&#013;&#010;&gt; &amp;gt; iteration.closeWith(&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; iteration.keyBy(0).process(new RichMapFunc());&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 我在richmapfunc里使用了state，那么每一次迭代的时候，这个state还是同一个吗？&#013;&#010;&gt; &amp;gt; 如果我对result这个结果stream继续使用了keyby(0)的话，state也是同一个么？&#013;&#010;&gt; &amp;gt; 此外，如果我对一个stream进行keyby之后，key一直是不变的，&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; 但是map等operator之后返回的并非是keyedstream（operator不对key进行操作），是否只能继续keyby，还是有更好的方式&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_B191D404E1202D68415F1693C1C5EAAA0507@qq.com>"
    },
    {
        "id": "<tencent_A73A93C68904B5BB2DD40F62@qq.com>",
        "from": "&quot;noake&quot;&lt;no...@sina.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:37:39 GMT",
        "subject": "Re: 如何在Flink SQL中使用周期性水印?",
        "content": "非常感谢大家的回复。&#010;&#010;&#010;我们的业务场景已经解决了。&#010;目前的做法是在TableSource中配置水印策略， 在WatermarkGenerator中判断是否需要发射新的水印&#010;&#010;&#010;&#010;&#010;原始邮件&#010;发件人:Jark Wuimjark@gmail.com&#010;收件人:user-zhuser-zh@flink.apache.org&#010;发送时间:2020年7月8日(周三) 13:26&#010;主题:Re: 如何在Flink SQL中使用周期性水印?&#010;&#010;&#010;嗯， 可以在 JIRA 中开个 issue 描述下你的需求~ On Wed, 8 Jul 2020 at 12:01,&#010;1193216154 1193216154@qq.com wrote:  nbsp; nbsp;Jark，flink有没有必要去支持这个特性？我感觉还是有一些应用场景&#010;     ------------------nbsp;原始邮件nbsp;------------------  发件人:nbsp;\"Jark Wu\"imjark@gmail.comgt;;&#010; 发送时间:nbsp;2020年7月8日(星期三) 中午11:48  收件人:nbsp;\"user-zh\"user-zh@flink.apache.orggt;;&#010;  主题:nbsp;Re: 如何在Flink SQL中使用周期性水印?     如果所有 partition 都没有数据，还希望&#010;watermark 往前走，那 idle source 确实解决不了这个问题。  目前确实没有太好的解决办法。&#010;  Best,  Jark   On Wed, 8 Jul 2020 at 11:08, 1193216154 1193216154@qq.comgt; wrote:   gt;&#010;hi Jark Wu.  gt;  gt;  我的理解是table.exec.source.idle-timeout只能解决watermark对齐的时候去忽略某个没有watermark的并行度。但是在每个并行度都没有watermark的时候，还是无法更新watermark。&#010; gt;  我觉得题主的意思应该是，在kafka的所有分区都没有数据的时候，最后一个窗口无法触发（因为没有watermark大于最后那个窗口结束时间了）。&#010; gt; 有没有可以设置在eventTime情况下，周期性生成当前时间的一个waterMark(和数据无关),因为可能没有新数据到来了。&#010; gt;  gt;  gt;  gt;  gt; ------------------amp;nbsp;原始邮件amp;nbsp;------------------&#010; gt; 发件人:amp;nbsp;\"Jark Wu\"imjark@gmail.comamp;gt;;  gt; 发送时间:amp;nbsp;2020年7月7日(星期二)&#010;晚上6:09  gt; 收件人:amp;nbsp;\"user-zh\"user-zh@flink.apache.orgamp;gt;;  gt;  gt; 主题:amp;nbsp;Re:&#010;如何在Flink SQL中使用周期性水印?  gt;  gt;  gt;  gt; Hi,  gt;  gt; 这个问题我理解其实和周期性水印没有关系，是属于&#010;idle source  gt; 的问题，你可以尝试下加上配置 table.exec.source.idle-timeout&#010;= 10s 能不能解决你的问题。[1]  gt;  gt; Best,  gt; Jark  gt;  gt; [1]:  gt;  gt;&#010; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeout&#010; gt  \"&gt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/config.html#table-exec-source-idle-timeoutgt&#010; ;  gt; On Tue, 7 Jul 2020 at 17:35, noake noake@sina.cnamp;gt; wrote:  gt;  gt; amp;gt; Dear&#010;All：  gt; amp;gt;  gt; amp;gt;  gt; amp;gt; 大佬们， 请教下如何在Flink SQL中使用周期性的水印。&#010; gt; amp;gt; 我们在消费kafka时， 想设置在没有数据时水印时间也能继续向前走，&#010;用的是Flink SQL。",
        "depth": "0",
        "reply": "<tencent_A73A93C68904B5BB2DD40F62@qq.com>"
    },
    {
        "id": "<202007081832175225754@163.com>",
        "from": "&quot;hdxg1101300123@163.com&quot; &lt;hdxg1101300...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 10:32:18 GMT",
        "subject": "关于 richfunction中初始化数据库连接的问题",
        "content": "您好：&#013;&#010;        我使用flink1.10.1版本streamapi编写程序时，在不同的richfunction中 分别使用Class.forName(\"*****\");&#010;来加载数据库驱动。是不同的两个数据库驱动；这样会导致程序卡住不往下执行；有人遇到吗？&#013;&#010;&#013;&#010;&#013;&#010;hdxg1101300123@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007081832175225754@163.com>"
    },
    {
        "id": "<tencent_B98A7045C5FA5DB8A67E73895B06B897EC06@qq.com>",
        "from": "&quot;Yichao Yang&quot; &lt;1048262...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 10:36:56 GMT",
        "subject": "回复：关于 richfunction中初始化数据库连接的问题",
        "content": "Hi,&#013;&#010;&#013;&#010;&#013;&#010;是执行到哪步出现了问题？可以提供下面一些内容来帮忙定位问题吗？&#013;&#010;1.截图或者日志&#013;&#010;2.不同的数据库都是哪些数据库，以及版本是哪些&#013;&#010;3.单写一个测试用例加载两个数据库是否能够加载成功&#013;&#010;4.代码伪编码&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yichao Yang&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: hdxg1101300123@163.com &lt;hdxg1101300123@163.com&amp;gt;&#013;&#010;发送时间: 2020年7月8日 18:32&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：关于 richfunction中初始化数据库连接的问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;您好：&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 我使用flink1.10.1版本streamapi编写程序时，在不同的richfunction中&#010;分别使用Class.forName(\"*****\"); 来加载数据库驱动。是不同的两个数据库驱动；这样会导致程序卡住不往下执行；有人遇到吗？&#013;&#010;&#013;&#010;&#013;&#010;hdxg1101300123@163.com",
        "depth": "1",
        "reply": "<202007081832175225754@163.com>"
    },
    {
        "id": "<36e8bbbf.9f56.1732e09237f.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 10:46:57 GMT",
        "subject": "回复：关于 richfunction中初始化数据库连接的问题",
        "content": "hi&#010;具体是卡在什么地方了呢？可以打印日志定位一下 理论上是不会有这样的问题&#010;还有单个执行的话可以吗？&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月08日 18:32，hdxg1101300123@163.com 写道：&#010;您好：&#010;       我使用flink1.10.1版本streamapi编写程序时，在不同的richfunction中 分别使用Class.forName(\"*****\");&#010;来加载数据库驱动。是不同的两个数据库驱动；这样会导致程序卡住不往下执行；有人遇到吗？&#010;&#010;&#010;hdxg1101300123@163.com&#010;",
        "depth": "1",
        "reply": "<202007081832175225754@163.com>"
    },
    {
        "id": "<202007091131444852309@163.com>",
        "from": "&quot;hdxg1101300123@163.com&quot; &lt;hdxg1101300...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 03:31:54 GMT",
        "subject": "回复: 回复：关于 richfunction中初始化数据库连接的问题",
        "content": "&#013;&#010;FlinkKafkaConsumer&lt;Bill&gt; kafkaConsumer = new FlinkKafkaConsumer&lt;&gt;(TrafficConstants.BILLTOPIC,new&#010;SchargeConsumerSchema(), props);&#013;&#010;kafkaConsumer.setStartFromLatest();&#013;&#010;SingleOutputStreamOperator&lt;Bill&gt; process = env.addSource(kafkaConsumer).setParallelism(4)&#013;&#010;.filter(new HiveFilterFunction(TrafficConstants.HIVEURL, TrafficConstants.HIVEUSERNAME, TrafficConstants.HIVEPASSWORD)).name(\"流量费过滤\")&#013;&#010;.keyBy((KeySelector&lt;Bill, String&gt;) value -&gt; value.getUser_id() + value.getSerial_number()&#010;+ value.getProvince_code())&#013;&#010;.process(***);&#013;&#010;SingleOutputStreamOperator&lt;BillInfo&gt; map = process.map();&#013;&#010;map.addSink(new RdsFlowSink(TrafficConstants.URL, TrafficConstants.USERNAME, TrafficConstants.PASSWORD))&#013;&#010;.setParallelism(1).name(\"sinkRds\");&#013;&#010;&#013;&#010;这是主要逻辑：Kafka取数--&gt;自定义richfilter函数加载hive维表数据来过滤数据--&gt;keyby--&gt;process--&gt;自定义sink函数&#013;&#010;&#013;&#010;public class HiveFilterFunction extends RichFilterFunction&lt;Bill&gt; {&#013;&#010;    Logger LOG = LoggerFactory.getLogger(HiveFilterFunction.class);&#013;&#010;    private final String jdbcUrl;&#013;&#010;    private final String username;&#013;&#010;    private final String password;&#013;&#010;    private transient volatile Statement sts;&#013;&#010;    private transient volatile Connection connection;&#013;&#010;    Map&lt;String, String&gt; map = new ConcurrentHashMap();&#013;&#010;&#013;&#010;    public HiveFilterFunction(String jdbcUrl, String username, String password) {&#013;&#010;        this.jdbcUrl = jdbcUrl;&#013;&#010;        this.username = username;&#013;&#010;        this.password = password;&#013;&#010;    }&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void open(Configuration parameters) throws Exception {&#013;&#010;        super.open(parameters);&#013;&#010;        Class.forName(\"org.apache.hive.jdbc.HiveDriver\");&#013;&#010;        connection = DriverManager.getConnection(jdbcUrl, username, password);&#013;&#010;        LOG.info(\"hive connection --- \" + connection);&#013;&#010;        sts = connection.createStatement();&#013;&#010;        query();&#013;&#010;    }&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public boolean filter(Bill value) {&#013;&#010;        return map.containsKey(value.getIntegrate_item_code())&#013;&#010;                &amp;&amp; TrafficConstants.getProCode().contains(value.getProvince_code());&#013;&#010;    }&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void close() throws Exception {&#013;&#010;        super.close();&#013;&#010;        assert null != sts ;&#013;&#010;        assert null != connection ;&#013;&#010;        sts.close();&#013;&#010;        connection.close();&#013;&#010;    }&#013;&#010;&#013;&#010;    private void query() throws Exception {&#013;&#010;        ResultSet resultSet = null;&#013;&#010;        try {&#013;&#010;            sts.execute(TrafficConstants.SETSQL);&#013;&#010;            resultSet = sts.executeQuery(TrafficConstants.CODESQL);&#013;&#010;            while (resultSet.next()) {&#013;&#010;                map.put(resultSet.getString(\"charge_code_cbss\"), \"\");&#013;&#010;            }&#013;&#010;        } catch (Exception e) {&#013;&#010;            LOG.error(\"hive error\", e);&#013;&#010;            throw new Exception(e);&#013;&#010;        } finally {&#013;&#010;            assert resultSet != null;&#013;&#010;            resultSet.close();&#013;&#010;        }&#013;&#010;        LOG.info(\"hive 维表数据加载完成\");&#013;&#010;    }&#013;&#010;}&#013;&#010;&#013;&#010;public class RdsFlowSink extends RichSinkFunction&lt;BillInfo&gt;{&#013;&#010;    Logger LOG = LoggerFactory.getLogger(RdsFlowSink.class);&#013;&#010;    private final String url;&#013;&#010;    private final String name;&#013;&#010;    private final String password;&#013;&#010;&#013;&#010;    private transient volatile PreparedStatement insertStatement;&#013;&#010;    private transient volatile Connection connection;&#013;&#010;    private transient volatile Counter counter = null;&#013;&#010; &#013;&#010;    public RdsFlowSink(String url, String name, String password) {&#013;&#010;        this.url = url;&#013;&#010;        this.name = name;&#013;&#010;        this.password = password;&#013;&#010;    }&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void open(Configuration parameters) throws Exception {&#013;&#010;        Class.forName(\"com.mysql.jdbc.Driver\");&#013;&#010;        connection = DriverManager.getConnection(url,name,password);&#013;&#010;        LOG.info(\"connection --- \" + connection);&#013;&#010;        counter = getRuntimeContext().getMetricGroup().counter(\"counter\");&#013;&#010;        insertStatement = connection.prepareStatement(TrafficConstants.FLOWSQL);&#013;&#010;     &#013;&#010;    }&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void invoke(BillInfo value, Context context) throws Exception {&#013;&#010;        try {&#013;&#010;            insertStatement.setString(1,value.getSerial_number());&#013;&#010;            insertStatement.setString(2,value.getUser_id());&#013;&#010;            insertStatement.setString(3,value.getIntegrate_item_code());&#013;&#010;            insertStatement.setString(4,value.getFee());&#013;&#010;            insertStatement.setString(5,value.getCity_code());&#013;&#010;            counter.inc(1);&#013;&#010;            insertStatement.execute();&#013;&#010;          &#013;&#010;        }catch (Exception e){      &#013;&#010;            LOG.info(\"invoke  --- \" + connection);&#013;&#010;            LOG.error(e.getMessage());&#013;&#010;            throw new Exception(e);&#013;&#010;        }&#013;&#010;    }&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void close() throws Exception {&#013;&#010;        super.close();&#013;&#010;        assert insertStatement != null;&#013;&#010;        assert connection != null;&#013;&#010;        insertStatement.close();&#013;&#010;        connection.close();&#013;&#010;    }&#013;&#010;}&#013;&#010;&#013;&#010;执行的时候程序会卡在 Class.forName(\"org.apache.hive.jdbc.HiveDriver\"); 或者 Class.forName(\"com.mysql.jdbc.Driver\");&#010;这里&#013;&#010;&#013;&#010;&#013;&#010;hdxg1101300123@163.com&#013;&#010; &#013;&#010;发件人： JasonLee&#013;&#010;发送时间： 2020-07-08 18:46&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复：关于 richfunction中初始化数据库连接的问题&#013;&#010;hi&#013;&#010;具体是卡在什么地方了呢？可以打印日志定位一下 理论上是不会有这样的问题&#010;还有单个执行的话可以吗？&#013;&#010; &#013;&#010; &#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010; &#013;&#010;Signature is customized by Netease Mail Master&#013;&#010; &#013;&#010;在2020年07月08日 18:32，hdxg1101300123@163.com 写道：&#013;&#010;您好：&#013;&#010;       我使用flink1.10.1版本streamapi编写程序时，在不同的richfunction中 分别使用Class.forName(\"*****\");&#010;来加载数据库驱动。是不同的两个数据库驱动；这样会导致程序卡住不往下执行；有人遇到吗？&#013;&#010; &#013;&#010; &#013;&#010;hdxg1101300123@163.com&#013;&#010;",
        "depth": "1",
        "reply": "<202007081832175225754@163.com>"
    },
    {
        "id": "<tencent_002640B3939B23B8812CB8F05485E55D8106@qq.com>",
        "from": "&quot;Yichao Yang&quot; &lt;1048262...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 05:11:36 GMT",
        "subject": "回复： 回复：关于 richfunction中初始化数据库连接的问题",
        "content": "Hi,&#013;&#010;&#013;&#010;&#013;&#010;可以先写单元测试看下是否能同时加载两个数据库，先排除数据库连接本身的问题。&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yichao Yang&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"hdxg1101300123@163.com\"&lt;hdxg1101300123@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 中午11:31&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复: 回复：关于 richfunction中初始化数据库连接的问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;FlinkKafkaConsumer&lt;Bill&amp;gt; kafkaConsumer = new FlinkKafkaConsumer&lt;&amp;gt;(TrafficConstants.BILLTOPIC,new&#010;SchargeConsumerSchema(), props);&#013;&#010;kafkaConsumer.setStartFromLatest();&#013;&#010;SingleOutputStreamOperator&lt;Bill&amp;gt; process = env.addSource(kafkaConsumer).setParallelism(4)&#013;&#010;.filter(new HiveFilterFunction(TrafficConstants.HIVEURL, TrafficConstants.HIVEUSERNAME, TrafficConstants.HIVEPASSWORD)).name(\"流量费过滤\")&#013;&#010;.keyBy((KeySelector&lt;Bill, String&amp;gt;) value -&amp;gt; value.getUser_id() + value.getSerial_number()&#010;+ value.getProvince_code())&#013;&#010;.process(***);&#013;&#010;SingleOutputStreamOperator&lt;BillInfo&amp;gt; map = process.map();&#013;&#010;map.addSink(new RdsFlowSink(TrafficConstants.URL, TrafficConstants.USERNAME, TrafficConstants.PASSWORD))&#013;&#010;.setParallelism(1).name(\"sinkRds\");&#013;&#010;&#013;&#010;这是主要逻辑：Kafka取数--&amp;gt;自定义richfilter函数加载hive维表数据来过滤数据--&amp;gt;keyby--&amp;gt;process--&amp;gt;自定义sink函数&#013;&#010;&#013;&#010;public class HiveFilterFunction extends RichFilterFunction&lt;Bill&amp;gt; {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; Logger LOG = LoggerFactory.getLogger(HiveFilterFunction.class);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String jdbcUrl;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String username;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String password;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Statement sts;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Connection connection;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; Map&lt;String, String&amp;gt; map = new ConcurrentHashMap();&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public HiveFilterFunction(String jdbcUrl, String username,&#010;String password) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.jdbcUrl = jdbcUrl;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.username = username;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.password = password;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void open(Configuration parameters) throws Exception&#010;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; super.open(parameters);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Class.forName(\"org.apache.hive.jdbc.HiveDriver\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection = DriverManager.getConnection(jdbcUrl,&#010;username, password);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; LOG.info(\"hive connection&#010;--- \" + connection);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; sts = connection.createStatement();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; query();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public boolean filter(Bill value) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; return map.containsKey(value.getIntegrate_item_code())&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&amp;amp;&amp;amp; TrafficConstants.getProCode().contains(value.getProvince_code());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void close() throws Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; super.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert null != sts&#010;;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert null != connection&#010;;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; sts.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private void query() throws Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ResultSet resultSet&#010;= null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; try {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;sts.execute(TrafficConstants.SETSQL);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;resultSet = sts.executeQuery(TrafficConstants.CODESQL);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;while (resultSet.next()) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;map.put(resultSet.getString(\"charge_code_cbss\"), \"\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;}&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; } catch (Exception&#010;e) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;LOG.error(\"hive error\", e);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;throw new Exception(e);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; } finally {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;assert resultSet != null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;resultSet.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; LOG.info(\"hive 维表数据加载完成\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;}&#013;&#010;&#013;&#010;public class RdsFlowSink extends RichSinkFunction&lt;BillInfo&amp;gt;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; Logger LOG = LoggerFactory.getLogger(RdsFlowSink.class);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String url;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String name;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String password;&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile PreparedStatement insertStatement;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Connection connection;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Counter counter = null;&#013;&#010;&amp;nbsp;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public RdsFlowSink(String url, String name, String password)&#010;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.url = url;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.name = name;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.password = password;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void open(Configuration parameters) throws Exception&#010;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Class.forName(\"com.mysql.jdbc.Driver\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection = DriverManager.getConnection(url,name,password);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; LOG.info(\"connection&#010;--- \" + connection);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; counter = getRuntimeContext().getMetricGroup().counter(\"counter\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; insertStatement = connection.prepareStatement(TrafficConstants.FLOWSQL);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void invoke(BillInfo value, Context context) throws&#010;Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; try {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(1,value.getSerial_number());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(2,value.getUser_id());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(3,value.getIntegrate_item_code());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(4,value.getFee());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(5,value.getCity_code());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;counter.inc(1);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.execute();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }catch (Exception e){&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;LOG.info(\"invoke&amp;nbsp; --- \" + connection);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;LOG.error(e.getMessage());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;throw new Exception(e);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void close() throws Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; super.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert insertStatement&#010;!= null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert connection !=&#010;null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; insertStatement.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;}&#013;&#010;&#013;&#010;执行的时候程序会卡在 Class.forName(\"org.apache.hive.jdbc.HiveDriver\"); 或者 Class.forName(\"com.mysql.jdbc.Driver\");&#010;这里&#013;&#010;&#013;&#010;&#013;&#010;hdxg1101300123@163.com&#013;&#010;&amp;nbsp;&#013;&#010;发件人： JasonLee&#013;&#010;发送时间： 2020-07-08 18:46&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复：关于 richfunction中初始化数据库连接的问题&#013;&#010;hi&#013;&#010;具体是卡在什么地方了呢？可以打印日志定位一下 理论上是不会有这样的问题&#010;还有单个执行的话可以吗？&#013;&#010;&amp;nbsp;&#013;&#010;&amp;nbsp;&#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010;&amp;nbsp;&#013;&#010;Signature is customized by Netease Mail Master&#013;&#010;&amp;nbsp;&#013;&#010;在2020年07月08日 18:32，hdxg1101300123@163.com 写道：&#013;&#010;您好：&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 我使用flink1.10.1版本streamapi编写程序时，在不同的richfunction中&#010;分别使用Class.forName(\"*****\"); 来加载数据库驱动。是不同的两个数据库驱动；这样会导致程序卡住不往下执行；有人遇到吗？&#013;&#010;&amp;nbsp;&#013;&#010;&amp;nbsp;&#013;&#010;hdxg1101300123@163.com",
        "depth": "2",
        "reply": "<202007081832175225754@163.com>"
    },
    {
        "id": "<202007121303062384625@163.com>",
        "from": "&quot;hdxg1101300123@163.com&quot; &lt;hdxg1101300...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 12 Jul 2020 05:03:25 GMT",
        "subject": "回复: 回复：关于 richfunction中初始化数据库连接的问题，导致程序卡主不执行",
        "content": "我測試發現，但flink的算子发送并行度变化时 会出现问题，如果我的并行度一致则没有问题！测试代码如下；（最近比较慢，没回复）&#013;&#010;整体的并行度为1，其中map后改变并行度为2， 程序就会卡主不执行；&#013;&#010;（我的实际项目过滤后再计算出的数据很少所以sink的并行度是1）&#013;&#010;&#013;&#010;&#013;&#010;import org.apache.flink.api.common.functions.RichFilterFunction;&#013;&#010;import org.apache.flink.api.java.tuple.Tuple2;&#013;&#010;import org.apache.flink.configuration.Configuration;&#013;&#010;import org.apache.flink.streaming.api.TimeCharacteristic;&#013;&#010;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#013;&#010;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;&#013;&#010;import org.apache.flink.streaming.api.functions.source.SourceFunction;&#013;&#010;import org.slf4j.Logger;&#013;&#010;import org.slf4j.LoggerFactory;&#013;&#010;&#013;&#010;import java.sql.Connection;&#013;&#010;import java.sql.DriverManager;&#013;&#010;import java.sql.PreparedStatement;&#013;&#010;import java.sql.Statement;&#013;&#010;import java.util.ArrayList;&#013;&#010;import java.util.List;&#013;&#010;import java.util.Random;&#013;&#010;&#013;&#010;public class TimerMain4 {&#013;&#010;    public static void main(String[] args) throws Exception {&#013;&#010;        Logger LOG = LoggerFactory.getLogger(TimerMain4.class);&#013;&#010;        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;        env.setParallelism(1);&#013;&#010;        env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);&#013;&#010;        env.addSource(new MySourceTuple())&#013;&#010;                .filter(new RichFilterFunction&lt;Tuple2&lt;String, Long&gt;&gt;() {&#013;&#010;                    private transient volatile Statement sts1;&#013;&#010;                    private transient volatile Connection conn1;&#013;&#010;&#013;&#010;                    @Override&#013;&#010;                    public void open(Configuration parameters) throws Exception {&#013;&#010;                        super.open(parameters);&#013;&#010;                        Class.forName(\"org.apache.hive.jdbc.HiveDriver\");&#013;&#010;                        conn1 = DriverManager.getConnection(\"\", \"\", \"\");&#013;&#010;                        LOG.info(\"connection --- \" + conn1);&#013;&#010;                        sts1 = conn1.createStatement();&#013;&#010;                    }&#013;&#010;&#013;&#010;                    @Override&#013;&#010;                    public boolean filter(Tuple2&lt;String, Long&gt; value) {&#013;&#010;                        return true;&#013;&#010;                    }&#013;&#010;&#013;&#010;                    @Override&#013;&#010;                    public void close() throws Exception {&#013;&#010;                        super.close();&#013;&#010;                        sts1.close();&#013;&#010;                        conn1.close();&#013;&#010;                    }&#013;&#010;                })&#013;&#010;                .map(Tuple2::toString)&#013;&#010;                .setParallelism(1)&#013;&#010;                .addSink(new RichSinkFunction&lt;String&gt;() {&#013;&#010;                    private transient volatile PreparedStatement sts2;&#013;&#010;                    private transient volatile Connection conn2;&#013;&#010;&#013;&#010;                    @Override&#013;&#010;                    public void open(Configuration parameters) throws Exception {&#013;&#010;                        super.open(parameters);&#013;&#010;                        Class.forName(\"com.mysql.jdbc.Driver\");&#013;&#010;                        conn2 = DriverManager.getConnection(\"\", \"\", \"\");&#013;&#010;                        LOG.info(\"connection --- \" + conn2);&#013;&#010;                        sts2 = conn2.prepareStatement(\"\");&#013;&#010;                    }&#013;&#010;&#013;&#010;                    @Override&#013;&#010;                    public void close() throws Exception {&#013;&#010;                        super.close();&#013;&#010;                        sts2.close();&#013;&#010;                        conn2.close();&#013;&#010;                    }&#013;&#010;&#013;&#010;                    @Override&#013;&#010;                    public void invoke(String value, Context context) {&#013;&#010;                        LOG.info(value);&#013;&#010;                    }&#013;&#010;                }).setParallelism(1);&#013;&#010;        env.execute();&#013;&#010;    }&#013;&#010;&#013;&#010;}&#013;&#010;&#013;&#010;class MySourceTuple implements SourceFunction&lt;Tuple2&lt;String, Long&gt;&gt; {&#013;&#010;&#013;&#010;    private Boolean isRunning = true;&#013;&#010;    List&lt;String&gt; names = new ArrayList();&#013;&#010;&#013;&#010;    private final Random random = new Random();&#013;&#010;    Long number = 1L;&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void run(SourceContext&lt;Tuple2&lt;String, Long&gt;&gt; ctx) throws Exception&#010;{&#013;&#010;        names.add(\"张\");&#013;&#010;        names.add(\"王\");&#013;&#010;        names.add(\"李\");&#013;&#010;        names.add(\"赵\");&#013;&#010;        while (isRunning) {&#013;&#010;            int index = random.nextInt(4);&#013;&#010;            ctx.collect(new Tuple2&lt;&gt;(names.get(index), number));&#013;&#010;            number += 1;&#013;&#010;            Thread.sleep(1000);&#013;&#010;        }&#013;&#010;    }&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void cancel() {&#013;&#010;        isRunning = false;&#013;&#010;    }&#013;&#010;}&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hdxg1101300123@163.com&#013;&#010; &#013;&#010;发件人： Yichao Yang&#013;&#010;发送时间： 2020-07-09 13:11&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复： 回复：关于 richfunction中初始化数据库连接的问题&#013;&#010;Hi,&#013;&#010; &#013;&#010; &#013;&#010;可以先写单元测试看下是否能同时加载两个数据库，先排除数据库连接本身的问题。&#013;&#010; &#013;&#010; &#013;&#010;Best,&#013;&#010;Yichao Yang&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"hdxg1101300123@163.com\"&lt;hdxg1101300123@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月9日(星期四) 中午11:31&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010; &#013;&#010;主题:&amp;nbsp;回复: 回复：关于 richfunction中初始化数据库连接的问题&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;FlinkKafkaConsumer&lt;Bill&amp;gt; kafkaConsumer = new FlinkKafkaConsumer&lt;&amp;gt;(TrafficConstants.BILLTOPIC,new&#010;SchargeConsumerSchema(), props);&#013;&#010;kafkaConsumer.setStartFromLatest();&#013;&#010;SingleOutputStreamOperator&lt;Bill&amp;gt; process = env.addSource(kafkaConsumer).setParallelism(4)&#013;&#010;.filter(new HiveFilterFunction(TrafficConstants.HIVEURL, TrafficConstants.HIVEUSERNAME, TrafficConstants.HIVEPASSWORD)).name(\"流量费过滤\")&#013;&#010;.keyBy((KeySelector&lt;Bill, String&amp;gt;) value -&amp;gt; value.getUser_id() + value.getSerial_number()&#010;+ value.getProvince_code())&#013;&#010;.process(***);&#013;&#010;SingleOutputStreamOperator&lt;BillInfo&amp;gt; map = process.map();&#013;&#010;map.addSink(new RdsFlowSink(TrafficConstants.URL, TrafficConstants.USERNAME, TrafficConstants.PASSWORD))&#013;&#010;.setParallelism(1).name(\"sinkRds\");&#013;&#010; &#013;&#010;这是主要逻辑：Kafka取数--&amp;gt;自定义richfilter函数加载hive维表数据来过滤数据--&amp;gt;keyby--&amp;gt;process--&amp;gt;自定义sink函数&#013;&#010; &#013;&#010;public class HiveFilterFunction extends RichFilterFunction&lt;Bill&amp;gt; {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; Logger LOG = LoggerFactory.getLogger(HiveFilterFunction.class);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String jdbcUrl;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String username;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String password;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Statement sts;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Connection connection;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; Map&lt;String, String&amp;gt; map = new ConcurrentHashMap();&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public HiveFilterFunction(String jdbcUrl, String username,&#010;String password) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.jdbcUrl = jdbcUrl;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.username = username;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.password = password;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void open(Configuration parameters) throws Exception&#010;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; super.open(parameters);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Class.forName(\"org.apache.hive.jdbc.HiveDriver\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection = DriverManager.getConnection(jdbcUrl,&#010;username, password);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; LOG.info(\"hive connection&#010;--- \" + connection);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; sts = connection.createStatement();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; query();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public boolean filter(Bill value) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; return map.containsKey(value.getIntegrate_item_code())&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&amp;amp;&amp;amp; TrafficConstants.getProCode().contains(value.getProvince_code());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void close() throws Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; super.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert null != sts&#010;;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert null != connection&#010;;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; sts.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private void query() throws Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ResultSet resultSet&#010;= null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; try {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;sts.execute(TrafficConstants.SETSQL);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;resultSet = sts.executeQuery(TrafficConstants.CODESQL);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;while (resultSet.next()) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;map.put(resultSet.getString(\"charge_code_cbss\"), \"\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;}&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; } catch (Exception&#010;e) {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;LOG.error(\"hive error\", e);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;throw new Exception(e);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; } finally {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;assert resultSet != null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;resultSet.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; LOG.info(\"hive 维表数据加载完成\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;}&#013;&#010; &#013;&#010;public class RdsFlowSink extends RichSinkFunction&lt;BillInfo&amp;gt;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; Logger LOG = LoggerFactory.getLogger(RdsFlowSink.class);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String url;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String name;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private final String password;&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile PreparedStatement insertStatement;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Connection connection;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient volatile Counter counter = null;&#013;&#010;&amp;nbsp;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public RdsFlowSink(String url, String name, String password)&#010;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.url = url;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.name = name;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.password = password;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void open(Configuration parameters) throws Exception&#010;{&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Class.forName(\"com.mysql.jdbc.Driver\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection = DriverManager.getConnection(url,name,password);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; LOG.info(\"connection&#010;--- \" + connection);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; counter = getRuntimeContext().getMetricGroup().counter(\"counter\");&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; insertStatement = connection.prepareStatement(TrafficConstants.FLOWSQL);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void invoke(BillInfo value, Context context) throws&#010;Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; try {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(1,value.getSerial_number());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(2,value.getUser_id());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(3,value.getIntegrate_item_code());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(4,value.getFee());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.setString(5,value.getCity_code());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;counter.inc(1);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;insertStatement.execute();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }catch (Exception e){&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;LOG.info(\"invoke&amp;nbsp; --- \" + connection);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;LOG.error(e.getMessage());&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;throw new Exception(e);&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010; &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void close() throws Exception {&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; super.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert insertStatement&#010;!= null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; assert connection !=&#010;null;&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; insertStatement.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; connection.close();&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;}&#013;&#010; &#013;&#010;执行的时候程序会卡在 Class.forName(\"org.apache.hive.jdbc.HiveDriver\"); 或者 Class.forName(\"com.mysql.jdbc.Driver\");&#010;这里&#013;&#010; &#013;&#010; &#013;&#010;hdxg1101300123@163.com&#013;&#010;&amp;nbsp;&#013;&#010;发件人： JasonLee&#013;&#010;发送时间： 2020-07-08 18:46&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复：关于 richfunction中初始化数据库连接的问题&#013;&#010;hi&#013;&#010;具体是卡在什么地方了呢？可以打印日志定位一下 理论上是不会有这样的问题&#010;还有单个执行的话可以吗？&#013;&#010;&amp;nbsp;&#013;&#010;&amp;nbsp;&#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010;&amp;nbsp;&#013;&#010;Signature is customized by Netease Mail Master&#013;&#010;&amp;nbsp;&#013;&#010;在2020年07月08日 18:32，hdxg1101300123@163.com 写道：&#013;&#010;您好：&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 我使用flink1.10.1版本streamapi编写程序时，在不同的richfunction中&#010;分别使用Class.forName(\"*****\"); 来加载数据库驱动。是不同的两个数据库驱动；这样会导致程序卡住不往下执行；有人遇到吗？&#013;&#010;&amp;nbsp;&#013;&#010;&amp;nbsp;&#013;&#010;hdxg1101300123@163.com&#013;&#010;",
        "depth": "1",
        "reply": "<202007081832175225754@163.com>"
    },
    {
        "id": "<tencent_3F5C73B959CB6D02BC487642B8A2E2E92E05@qq.com>",
        "from": "&quot;Yichao Yang&quot; &lt;1048262...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 10:45:06 GMT",
        "subject": "回复： State里面用guava Cache",
        "content": "Hi,&#013;&#010;每次cache的长度都是一有没有可能并发比较大，每一个1都是不同的算子输出的。&#013;&#010;&#013;&#010;&#013;&#010;你的场景我们实践中的方法是按照用户id keyby之后再做localcache，并且如果用户id是long类型的话，localcache可以使用roaringbitmap，效率会比单纯的cache效率更好，占用内存更小。&#013;&#010;&#013;&#010;&#013;&#010;并且频繁update state在资源有限的情况下是会有性能瓶颈的，这种场景下建议开窗口，窗口结束时update一次state即可。&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yichao Yang&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: user-zh-return-5056-1048262223=qq.com &lt;520075694@qq.com&amp;gt;&#013;&#010;发送时间: 2020年7月8日 18:09&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复： State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;比如数据里来了一个id我需要去判断这个id是新的还是已经存在的，由于历史数据量比较大，所以放全部state里面不太好。&#013;&#010;把最近活跃的id放到ValueState[Cache]里面，可以在内存里关联到绝大部分的id，避免频繁访问外部存储。&#013;&#010;如果不使用state保存的的话，重启作业后cache会重置，这段时间通过外部存储去关联id会很慢&#013;&#010;&amp;amp;nbsp;谢谢&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;发件人:&amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;发送时间:&amp;amp;nbsp;2020年7月8日(星期三) 下午5:52&#013;&#010;收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我尝试理解一下你的需求：&#013;&#010;你希望从外部存储同步一些信息，由于访问外部存储效率不高，所以希望加一个&#010;cache，然后 cache&#013;&#010;中的数据希望在一定时间后过期，过期后重新去外部存储同步一次信息。&#013;&#010;&#013;&#010;但是还有一些信息不太明白，那这里你打算在什么地方使用 state 呢？state&#010;存放什么数据呢？或者说，你自己维护这个状态之后，为什么还有使用&#013;&#010;state 呢？&#013;&#010;&#013;&#010;不管怎么说使用 Flink 之后，还是建议尽量使用 state，而不是使用外存，flink&#010;提供的 state 方便做一些容错处理。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&amp;amp;gt; 于2020年7月8日周三 下午4:07写道：&#013;&#010;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; 您好，是这样的，我想再程序里面关联一些用户id，使用cache缓存一些热数据，设置每个id写入多久后自动清理掉，关联的时候首先访问缓存，访问不到再去访问外部存储；&#013;&#010;&amp;amp;gt; 业务中的key会一直出现，也就是说ttl可能不会生效，这样没办法使用state&#010;ttl对吧？&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月8日(星期三) 下午3:56&#013;&#010;&amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; TTL state[1] 满足你的需求吗? 如果不满足的话，能否描述下你的需求呢？&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; [1]&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;&amp;amp;gt; Best,&#013;&#010;&amp;amp;gt; Congxian&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; op &lt;520075694@qq.com&amp;amp;amp;gt; 于2020年7月8日周三 下午3:53写道：&#013;&#010;&amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; 您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月8日(星期三)&#010;下午3:50&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;amp;gt;;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; 你好，为什么需要在 State 里面再用 cache 呢？单纯的&#010;State 不能满足需求吗？需求是什么呢？&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; 另外，除了 ValueState，其他的 ListState/MapState 能否满足你的需求呢？&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; Best,&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; Congxian&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; op &lt;520075694@qq.com&amp;amp;amp;amp;gt; 于2020年7月8日周三&#010;上午10:31写道：&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 大家好，我想使用一个 ValueState[Cache]的状态，但是发现这个状态的value&#010;没办法更新，&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1",
        "depth": "0",
        "reply": "<tencent_3F5C73B959CB6D02BC487642B8A2E2E92E05@qq.com>"
    },
    {
        "id": "<tencent_EB948F26E2C0384BA0661513AB0DEE6AC50A@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 10:49:54 GMT",
        "subject": "回复： State里面用guava Cache",
        "content": "谢谢，&#013;&#010;&amp;nbsp; 这是我在本地把并行度都设置成1测试的，先不管了。。。&#013;&#010;&amp;nbsp; 这个roaringbitmap也是保存到state中吗？我后面试试&#013;&#010;&amp;nbsp; 开窗口这个可以，十分感谢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Yichao Yang\"&lt;1048262223@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月8日(星期三) 晚上6:45&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复： State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi,&#013;&#010;每次cache的长度都是一有没有可能并发比较大，每一个1都是不同的算子输出的。&#013;&#010;&#013;&#010;&#013;&#010;你的场景我们实践中的方法是按照用户id keyby之后再做localcache，并且如果用户id是long类型的话，localcache可以使用roaringbitmap，效率会比单纯的cache效率更好，占用内存更小。&#013;&#010;&#013;&#010;&#013;&#010;并且频繁update state在资源有限的情况下是会有性能瓶颈的，这种场景下建议开窗口，窗口结束时update一次state即可。&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yichao Yang&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: user-zh-return-5056-1048262223=qq.com &lt;520075694@qq.com&amp;amp;gt;&#013;&#010;发送时间: 2020年7月8日 18:09&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;amp;gt;&#013;&#010;主题: 回复： State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;比如数据里来了一个id我需要去判断这个id是新的还是已经存在的，由于历史数据量比较大，所以放全部state里面不太好。&#013;&#010;把最近活跃的id放到ValueState[Cache]里面，可以在内存里关联到绝大部分的id，避免频繁访问外部存储。&#013;&#010;如果不使用state保存的的话，重启作业后cache会重置，这段时间通过外部存储去关联id会很慢&#013;&#010;&amp;amp;amp;nbsp;谢谢&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;发件人:&amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;amp;gt;;&#013;&#010;发送时间:&amp;amp;amp;nbsp;2020年7月8日(星期三) 下午5:52&#013;&#010;收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我尝试理解一下你的需求：&#013;&#010;你希望从外部存储同步一些信息，由于访问外部存储效率不高，所以希望加一个&#010;cache，然后 cache&#013;&#010;中的数据希望在一定时间后过期，过期后重新去外部存储同步一次信息。&#013;&#010;&#013;&#010;但是还有一些信息不太明白，那这里你打算在什么地方使用 state 呢？state&#010;存放什么数据呢？或者说，你自己维护这个状态之后，为什么还有使用&#013;&#010;state 呢？&#013;&#010;&#013;&#010;不管怎么说使用 Flink 之后，还是建议尽量使用 state，而不是使用外存，flink&#010;提供的 state 方便做一些容错处理。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&amp;amp;amp;gt; 于2020年7月8日周三 下午4:07写道：&#013;&#010;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; 您好，是这样的，我想再程序里面关联一些用户id，使用cache缓存一些热数据，设置每个id写入多久后自动清理掉，关联的时候首先访问缓存，访问不到再去访问外部存储；&#013;&#010;&amp;amp;amp;gt; 业务中的key会一直出现，也就是说ttl可能不会生效，这样没办法使用state&#010;ttl对吧？&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月8日(星期三) 下午3:56&#013;&#010;&amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;amp;gt;;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; TTL state[1] 满足你的需求吗? 如果不满足的话，能否描述下你的需求呢？&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; [1]&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;&amp;amp;amp;gt; Best,&#013;&#010;&amp;amp;amp;gt; Congxian&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; op &lt;520075694@qq.com&amp;amp;amp;amp;gt; 于2020年7月8日周三 下午3:53写道：&#013;&#010;&amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; 您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月8日(星期三)&#010;下午3:50&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;Re: State里面用guava&#010;Cache&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好，为什么需要在 State 里面再用 cache&#010;呢？单纯的 State 不能满足需求吗？需求是什么呢？&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; 另外，除了 ValueState，其他的 ListState/MapState&#010;能否满足你的需求呢？&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; Best,&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; Congxian&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; op &lt;520075694@qq.com&amp;amp;amp;amp;amp;gt; 于2020年7月8日周三&#010;上午10:31写道：&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 大家好，我想使用一个&#010;ValueState[Cache]的状态，但是发现这个状态的value 没办法更新，&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;amp;amp;gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1",
        "depth": "1",
        "reply": "<tencent_3F5C73B959CB6D02BC487642B8A2E2E92E05@qq.com>"
    },
    {
        "id": "<CAA8tFvv3-YbRtYtFDovqmncNz3R2rwPjvbpk6tbczzfj9Fff8g@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 11:23:21 GMT",
        "subject": "Re: State里面用guava Cache",
        "content": "State 也可以理解为一个 cache，如果打算自己维护 cache（比如使用 guava&#010;cache）的话，那么就可以不使用&#013;&#010;State，当然如果你还希望做状态容错的话，也可以使用类似 YiChao 的方法，积攒一批数据然后一起更新&#010;State&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月8日周三 下午6:50写道：&#013;&#010;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; &amp;nbsp; 这是我在本地把并行度都设置成1测试的，先不管了。。。&#013;&#010;&gt; &amp;nbsp; 这个roaringbitmap也是保存到state中吗？我后面试试&#013;&#010;&gt; &amp;nbsp; 开窗口这个可以，十分感谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Yichao Yang\"&lt;1048262223@qq.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月8日(星期三) 晚上6:45&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;回复： State里面用guava Cache&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi,&#013;&#010;&gt; 每次cache的长度都是一有没有可能并发比较大，每一个1都是不同的算子输出的。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你的场景我们实践中的方法是按照用户id&#013;&#010;&gt; keyby之后再做localcache，并且如果用户id是long类型的话，localcache可以使用roaringbitmap，效率会比单纯的cache效率更好，占用内存更小。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 并且频繁update state在资源有限的情况下是会有性能瓶颈的，这种场景下建议开窗口，窗口结束时update一次state即可。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yichao Yang&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------ 原始邮件 ------------------&#013;&#010;&gt; 发件人: user-zh-return-5056-1048262223=qq.com &lt;520075694@qq.com&amp;amp;gt;&#013;&#010;&gt; 发送时间: 2020年7月8日 18:09&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;amp;gt;&#013;&#010;&gt; 主题: 回复： State里面用guava Cache&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 比如数据里来了一个id我需要去判断这个id是新的还是已经存在的，由于历史数据量比较大，所以放全部state里面不太好。&#013;&#010;&gt; 把最近活跃的id放到ValueState[Cache]里面，可以在内存里关联到绝大部分的id，避免频繁访问外部存储。&#013;&#010;&gt; 如果不使用state保存的的话，重启作业后cache会重置，这段时间通过外部存储去关联id会很慢&#013;&#010;&gt; &amp;amp;amp;nbsp;谢谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;amp;amp;gt;;&#013;&#010;&gt; 发送时间:&amp;amp;amp;nbsp;2020年7月8日(星期三) 下午5:52&#013;&#010;&gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我尝试理解一下你的需求：&#013;&#010;&gt; 你希望从外部存储同步一些信息，由于访问外部存储效率不高，所以希望加一个&#010;cache，然后 cache&#013;&#010;&gt; 中的数据希望在一定时间后过期，过期后重新去外部存储同步一次信息。&#013;&#010;&gt;&#013;&#010;&gt; 但是还有一些信息不太明白，那这里你打算在什么地方使用 state&#010;呢？state 存放什么数据呢？或者说，你自己维护这个状态之后，为什么还有使用&#013;&#010;&gt; state 呢？&#013;&#010;&gt;&#013;&#010;&gt; 不管怎么说使用 Flink 之后，还是建议尽量使用 state，而不是使用外存，flink&#010;提供的 state 方便做一些容错处理。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; op &lt;520075694@qq.com&amp;amp;amp;gt; 于2020年7月8日周三 下午4:07写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; 您好，是这样的，我想再程序里面关联一些用户id，使用cache缓存一些热数据，设置每个id写入多久后自动清理掉，关联的时候首先访问缓存，访问不到再去访问外部存储；&#013;&#010;&gt; &amp;amp;amp;gt; 业务中的key会一直出现，也就是说ttl可能不会生效，这样没办法使用state&#010;ttl对吧？&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&#013;&#010;&gt; &amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年7月8日(星期三) 下午3:56&#013;&#010;&gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&gt; &amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: State里面用guava Cache&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; TTL state[1] 满足你的需求吗? 如果不满足的话，能否描述下你的需求呢？&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; [1]&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;&gt; &amp;amp;amp;gt&#013;&#010;&gt; &lt;https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&amp;amp;amp;gt&gt;;&#013;&#010;&gt; Best,&#013;&#010;&gt; &amp;amp;amp;gt; Congxian&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; op &lt;520075694@qq.com&amp;amp;amp;amp;gt; 于2020年7月8日周三&#010;下午3:53写道：&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 您好，我主要是觉得Cache的自动过期比较好用&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;amp;nbsp;\"Congxian Qiu\"&lt;&#013;&#010;&gt; qcx978132955@gmail.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年7月8日(星期三)&#013;&#010;&gt; 下午3:50&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&gt; user-zh@flink.apache.org&amp;amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;Re: State里面用guava&#013;&#010;&gt; Cache&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 你好，为什么需要在 State 里面再用&#010;cache 呢？单纯的 State&#013;&#010;&gt; 不能满足需求吗？需求是什么呢？&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 另外，除了 ValueState，其他的 ListState/MapState&#013;&#010;&gt; 能否满足你的需求呢？&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; Best,&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; Congxian&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; op &lt;520075694@qq.com&amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; 于2020年7月8日周三 上午10:31写道：&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 大家好，我想使用一个&#013;&#010;&gt; ValueState[Cache]的状态，但是发现这个状态的value 没办法更新，&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&#013;&#010;&gt; 比如我在map里面每次往cache里面put一个字符串，然后update这个state，输出cache的长度，为什么每次输出长度都是1&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_3F5C73B959CB6D02BC487642B8A2E2E92E05@qq.com>"
    },
    {
        "id": "<CAJkeMpigE4eFQ8WBCVWQTCLOxhzD+5p5v3ZYaWejf1A-MonKmw@mail.gmail.com>",
        "from": "zilong xiao &lt;acidzz...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 12:54:58 GMT",
        "subject": "Flink SQL如何将多个表的查询结果(列不同)聚合成一张表",
        "content": "列如下面这样，需要查询table1 &amp; table2，分别查询不同的字段&#013;&#010;在最外层做比值，flink貌似语法检查不通过，应该怎么写这样的SQL呢，有前辈可以指导下不~&#013;&#010;select a.table_tmp1.r1 / a.table_tmp2.r2 as value0 from&#013;&#010;(&#013;&#010;(SELECT r1 FROM table1) AS table_tmp1, (SELECT r2 FROM table2) AS&#013;&#010;table_tmp2,&#013;&#010;)as a&#013;&#010;",
        "depth": "0",
        "reply": "<CAJkeMpigE4eFQ8WBCVWQTCLOxhzD+5p5v3ZYaWejf1A-MonKmw@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGsYdL0apquv=3j7s3=LTf03+eXrn5B=xj6rRRUOw7iz3A@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 13:24:46 GMT",
        "subject": "Re: Flink SQL如何将多个表的查询结果(列不同)聚合成一张表",
        "content": "select a.table_tmp1.r1 / a.table_tmp2.r2&#013;&#010;这个是对同一行的数据进行操作，所以你需要先对table_tmp1和table_tmp2做一个join，将两个表的数据根据条件合并成一张表。&#013;&#010;&#013;&#010;&#013;&#010;zilong xiao &lt;acidzz163@gmail.com&gt; 于2020年7月8日周三 下午8:55写道：&#013;&#010;&#013;&#010;&gt; 列如下面这样，需要查询table1 &amp; table2，分别查询不同的字段&#013;&#010;&gt; 在最外层做比值，flink貌似语法检查不通过，应该怎么写这样的SQL呢，有前辈可以指导下不~&#013;&#010;&gt; select a.table_tmp1.r1 / a.table_tmp2.r2 as value0 from&#013;&#010;&gt; (&#013;&#010;&gt; (SELECT r1 FROM table1) AS table_tmp1, (SELECT r2 FROM table2) AS&#013;&#010;&gt; table_tmp2,&#013;&#010;&gt; )as a&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAJkeMpigE4eFQ8WBCVWQTCLOxhzD+5p5v3ZYaWejf1A-MonKmw@mail.gmail.com>"
    },
    {
        "id": "<tencent_C1FAE5B4E824C24E533239F3F442388B0E09@qq.com>",
        "from": "&quot;cs&quot; &lt;58683...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 01:48:15 GMT",
        "subject": "回复： Flink SQL如何将多个表的查询结果(列不同)聚合成一张表",
        "content": "你得有个join条件连接两张表的&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"godfrey he\"&lt;godfreyhe@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月8日(星期三) 晚上9:24&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: Flink SQL如何将多个表的查询结果(列不同)聚合成一张表&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;select a.table_tmp1.r1 / a.table_tmp2.r2&#013;&#010;这个是对同一行的数据进行操作，所以你需要先对table_tmp1和table_tmp2做一个join，将两个表的数据根据条件合并成一张表。&#013;&#010;&#013;&#010;&#013;&#010;zilong xiao &lt;acidzz163@gmail.com&amp;gt; 于2020年7月8日周三 下午8:55写道：&#013;&#010;&#013;&#010;&amp;gt; 列如下面这样，需要查询table1 &amp;amp; table2，分别查询不同的字段&#013;&#010;&amp;gt; 在最外层做比值，flink貌似语法检查不通过，应该怎么写这样的SQL呢，有前辈可以指导下不~&#013;&#010;&amp;gt; select a.table_tmp1.r1 / a.table_tmp2.r2 as value0 from&#013;&#010;&amp;gt; (&#013;&#010;&amp;gt; (SELECT r1 FROM table1) AS table_tmp1, (SELECT r2 FROM table2) AS&#013;&#010;&amp;gt; table_tmp2,&#013;&#010;&amp;gt; )as a&#013;&#010;&amp;gt;",
        "depth": "2",
        "reply": "<CAJkeMpigE4eFQ8WBCVWQTCLOxhzD+5p5v3ZYaWejf1A-MonKmw@mail.gmail.com>"
    },
    {
        "id": "<tencent_C754DD5C1213930593C8A33143A751FCCD06@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 15:29:25 GMT",
        "subject": "flink-state问题",
        "content": "Deal all&#013;&#010;&#013;&#010;&#013;&#010;官网上讲这些 keyed state（ValueState&lt;T&amp;gt;，ReducingState&lt;T&amp;gt; ，ListState&lt;T&amp;gt;&#010;，AggregatingState&lt;IN, OUT&amp;gt; ，MapState&lt;UK, UV&amp;gt; ）&#013;&#010;支持在keyed stream中使用，言下之意就是只能在KeyedProcessFunction中使用？但是实际使用中，我在ProcessAllWindowFunction和ProcessWindowFunction也能使用上述这些State，这是什么原因？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;谢谢&#013;&#010;jiazhi",
        "depth": "0",
        "reply": "<tencent_C754DD5C1213930593C8A33143A751FCCD06@qq.com>"
    },
    {
        "id": "<CAA8tFvt2SCxFYEYbyf_wdBxBC1HJa5SQ2CtGu5-ercThQNA4bQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 02:08:26 GMT",
        "subject": "Re: flink-state问题",
        "content": "Hi&#013;&#010;&#013;&#010;keyed state 只能在 keyeed stream 中使用。ProcessAllWindowFunction 和&#013;&#010;ProcessWindowFunction 这两个都是 Window 上的 function，window 已经是 keyed stream&#010;了&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&gt; 于2020年7月8日周三 下午11:29写道：&#013;&#010;&#013;&#010;&gt; Deal all&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 官网上讲这些 keyed state（ValueState&lt;T&amp;gt;，ReducingState&lt;T&amp;gt;&#010;，ListState&lt;T&amp;gt;&#013;&#010;&gt; ，AggregatingState&lt;IN, OUT&amp;gt; ，MapState&lt;UK, UV&amp;gt; ）&#013;&#010;&gt; 支持在keyed&#013;&#010;&gt; stream中使用，言下之意就是只能在KeyedProcessFunction中使用？但是实际使用中，我在ProcessAllWindowFunction和ProcessWindowFunction也能使用上述这些State，这是什么原因？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 谢谢&#013;&#010;&gt; jiazhi&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_C754DD5C1213930593C8A33143A751FCCD06@qq.com>"
    },
    {
        "id": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>",
        "from": "&quot;Evan&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 01:38:01 GMT",
        "subject": "代码中如何取消正在运行的Flink Streaming作业",
        "content": "这个问题之前看到过有人在问，但是没有看到答案，我想问一下，Flink&#010;Streaming API有没有提供类似的接口，调用后就能停止这个Stream作业呢？",
        "depth": "0",
        "reply": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>"
    },
    {
        "id": "<CADQYLGuGUKoTcfBXf_kgpu_HjPVoYPy9+eEimjT6vrQ_hvsk_g@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 02:07:33 GMT",
        "subject": "Re: 代码中如何取消正在运行的Flink Streaming作业",
        "content": "可以通过 StreamExecutionEnvironment#executeAsync 提交作业，返回 JobClient [1], 通过&#013;&#010;JobClient 可以 cancel 作业，获取 job status。&#013;&#010;&#013;&#010;[1]&#013;&#010;https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月9日周四 上午9:40写道：&#013;&#010;&#013;&#010;&gt; 这个问题之前看到过有人在问，但是没有看到答案，我想问一下，Flink&#010;Streaming&#013;&#010;&gt; API有没有提供类似的接口，调用后就能停止这个Stream作业呢？&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>"
    },
    {
        "id": "<CAA8tFvsHS1=inU27OXQ0PYeFc-zWWf8iZvCAOC9pdAnaUO2QWw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 08:19:33 GMT",
        "subject": "Re: 代码中如何取消正在运行的Flink Streaming作业",
        "content": "Hi&#013;&#010;&#013;&#010;如果你是想做一个作业管理的平台，可以尝试看一下 CliFrontend[1] 中相关的逻辑，对于&#010;On Yarn&#013;&#010;的作业，简单地说你需要能够正确的初始化一个 client 和 Yarn RM 交互，然后你需要知道&#010;applicationId，另外你还需要知道&#013;&#010;flink 的 JobId，接下来就是调用 Flink 的接口了&#013;&#010;&#013;&#010;如果像更多的了解参数如从和命令行传到 java 代码的，你可以自己写一个单元测试，单步调试一下整个流程。&#013;&#010;&#013;&#010;[1]&#013;&#010;https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月9日周四 上午10:08写道：&#013;&#010;&#013;&#010;&gt; 可以通过 StreamExecutionEnvironment#executeAsync 提交作业，返回 JobClient&#010;[1], 通过&#013;&#010;&gt; JobClient 可以 cancel 作业，获取 job status。&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Godfrey&#013;&#010;&gt;&#013;&#010;&gt; Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月9日周四 上午9:40写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 这个问题之前看到过有人在问，但是没有看到答案，我想问一下，Flink&#010;Streaming&#013;&#010;&gt; &gt; API有没有提供类似的接口，调用后就能停止这个Stream作业呢？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>"
    },
    {
        "id": "<768ab633.34de.1733d1354f9.Coremail.rjianxu@163.com>",
        "from": "jianxu &lt;rjia...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 08:52:23 GMT",
        "subject": "回复： 代码中如何取消正在运行的Flink Streaming作业",
        "content": "Hi:&#010;    我想，你可能打算通过API的方式来取消正在运行的流任务。Flink任务提交时需要构建ClusterClient，提交成功后会返回任务对应的JobId。任务取消时，通过调用ClusterClient的cancel(JobID&#010;jobId)取消流任务。&#010;    Flink源码可以看看 CliFrontend[1]中的逻辑，如果觉得比较麻烦可以参考https://github.com/todd5167/flink-spark-submiter项目的任务提交部分，取消任务时构建ClusterClient即可。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;| |&#010;jianxu&#010;|&#010;|&#010;rjianxu@163.com&#010;|&#010;&#010;&#010;&#010;&#010;在2020年07月11日 16:19，Congxian Qiu&lt;qcx978132955@gmail.com&gt; 写道：&#010;Hi&#010;&#010;如果你是想做一个作业管理的平台，可以尝试看一下 CliFrontend[1] 中相关的逻辑，对于&#010;On Yarn&#010;的作业，简单地说你需要能够正确的初始化一个 client 和 Yarn RM 交互，然后你需要知道&#010;applicationId，另外你还需要知道&#010;flink 的 JobId，接下来就是调用 Flink 的接口了&#010;&#010;如果像更多的了解参数如从和命令行传到 java 代码的，你可以自己写一个单元测试，单步调试一下整个流程。&#010;&#010;[1]&#010;https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月9日周四 上午10:08写道：&#010;&#010;可以通过 StreamExecutionEnvironment#executeAsync 提交作业，返回 JobClient [1],&#010;通过&#010;JobClient 可以 cancel 作业，获取 job status。&#010;&#010;[1]&#010;&#010;https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API&#010;&#010;Best,&#010;Godfrey&#010;&#010;Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月9日周四 上午9:40写道：&#010;&#010;这个问题之前看到过有人在问，但是没有看到答案，我想问一下，Flink&#010;Streaming&#010;API有没有提供类似的接口，调用后就能停止这个Stream作业呢？&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>"
    },
    {
        "id": "<CAADy7x5aRSComw0ZZ7MThZsF6e-xBKVm_L+q=RzROjFkpPziuw@mail.gmail.com>",
        "from": "Jeff Zhang &lt;zjf...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 11 Jul 2020 15:23:18 GMT",
        "subject": "Re: 代码中如何取消正在运行的Flink Streaming作业",
        "content": "Zeppelin 能够帮你提交和cancel job，就是通过上面jianxu说的ClusterClient&#013;&#010;api来做到的，对zeppelin感兴趣的话，可以参考这个视频&#013;&#010;&#013;&#010;https://www.bilibili.com/video/BV1Te411W73b?p=21&#013;&#010;&#013;&#010;&#013;&#010;jianxu &lt;rjianxu@163.com&gt; 于2020年7月11日周六 下午4:52写道：&#013;&#010;&#013;&#010;&gt; Hi:&#013;&#010;&gt;&#013;&#010;&gt; 我想，你可能打算通过API的方式来取消正在运行的流任务。Flink任务提交时需要构建ClusterClient，提交成功后会返回任务对应的JobId。任务取消时，通过调用ClusterClient的cancel(JobID&#013;&#010;&gt; jobId)取消流任务。&#013;&#010;&gt;     Flink源码可以看看 CliFrontend[1]中的逻辑，如果觉得比较麻烦可以参考&#013;&#010;&gt; https://github.com/todd5167/flink-spark-submiter&#013;&#010;&gt; 项目的任务提交部分，取消任务时构建ClusterClient即可。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; jianxu&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; rjianxu@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月11日 16:19，Congxian Qiu&lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 如果你是想做一个作业管理的平台，可以尝试看一下 CliFrontend[1]&#010;中相关的逻辑，对于 On Yarn&#013;&#010;&gt; 的作业，简单地说你需要能够正确的初始化一个 client 和 Yarn RM 交互，然后你需要知道&#010;applicationId，另外你还需要知道&#013;&#010;&gt; flink 的 JobId，接下来就是调用 Flink 的接口了&#013;&#010;&gt;&#013;&#010;&gt; 如果像更多的了解参数如从和命令行传到 java 代码的，你可以自己写一个单元测试，单步调试一下整个流程。&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月9日周四 上午10:08写道：&#013;&#010;&gt;&#013;&#010;&gt; 可以通过 StreamExecutionEnvironment#executeAsync 提交作业，返回 JobClient&#010;[1], 通过&#013;&#010;&gt; JobClient 可以 cancel 作业，获取 job status。&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Godfrey&#013;&#010;&gt;&#013;&#010;&gt; Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月9日周四 上午9:40写道：&#013;&#010;&gt;&#013;&#010;&gt; 这个问题之前看到过有人在问，但是没有看到答案，我想问一下，Flink&#010;Streaming&#013;&#010;&gt; API有没有提供类似的接口，调用后就能停止这个Stream作业呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;Best Regards&#013;&#010;&#013;&#010;Jeff Zhang&#013;&#010;",
        "depth": "4",
        "reply": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>"
    },
    {
        "id": "<CAMhjQvjW1KWD-BP6tk7yvTfNvTW8TMCG9tsAKpwhOaBg8npG6Q@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:53:04 GMT",
        "subject": "Re: 代码中如何取消正在运行的Flink Streaming作业",
        "content": "如果是 on yarn 的话，也可以直接调用 yarn 的 kill 命令停止作业&#013;&#010;&#013;&#010;Jeff Zhang &lt;zjffdu@gmail.com&gt; 于2020年7月11日周六 下午11:23写道：&#013;&#010;&#013;&#010;&gt; Zeppelin 能够帮你提交和cancel job，就是通过上面jianxu说的ClusterClient&#013;&#010;&gt; api来做到的，对zeppelin感兴趣的话，可以参考这个视频&#013;&#010;&gt;&#013;&#010;&gt; https://www.bilibili.com/video/BV1Te411W73b?p=21&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; jianxu &lt;rjianxu@163.com&gt; 于2020年7月11日周六 下午4:52写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我想，你可能打算通过API的方式来取消正在运行的流任务。Flink任务提交时需要构建ClusterClient，提交成功后会返回任务对应的JobId。任务取消时，通过调用ClusterClient的cancel(JobID&#013;&#010;&gt; &gt; jobId)取消流任务。&#013;&#010;&gt; &gt;     Flink源码可以看看 CliFrontend[1]中的逻辑，如果觉得比较麻烦可以参考&#013;&#010;&gt; &gt; https://github.com/todd5167/flink-spark-submiter&#013;&#010;&gt; &gt; 项目的任务提交部分，取消任务时构建ClusterClient即可。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; jianxu&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; rjianxu@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在2020年07月11日 16:19，Congxian Qiu&lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如果你是想做一个作业管理的平台，可以尝试看一下 CliFrontend[1]&#010;中相关的逻辑，对于 On Yarn&#013;&#010;&gt; &gt; 的作业，简单地说你需要能够正确的初始化一个 client 和 Yarn&#010;RM 交互，然后你需要知道 applicationId，另外你还需要知道&#013;&#010;&gt; &gt; flink 的 JobId，接下来就是调用 Flink 的接口了&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如果像更多的了解参数如从和命令行传到 java 代码的，你可以自己写一个单元测试，单步调试一下整个流程。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月9日周四 上午10:08写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 可以通过 StreamExecutionEnvironment#executeAsync 提交作业，返回 JobClient&#010;[1], 通过&#013;&#010;&gt; &gt; JobClient 可以 cancel 作业，获取 job status。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月9日周四 上午9:40写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 这个问题之前看到过有人在问，但是没有看到答案，我想问一下，Flink&#010;Streaming&#013;&#010;&gt; &gt; API有没有提供类似的接口，调用后就能停止这个Stream作业呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best Regards&#013;&#010;&gt;&#013;&#010;&gt; Jeff Zhang&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>"
    },
    {
        "id": "<CAA8tFvt8fziHjPT-s_R+3W5OBNSAE+kG_ok=hwbDVZtDvrtz5A@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 05:48:34 GMT",
        "subject": "Re: 代码中如何取消正在运行的Flink Streaming作业",
        "content": "Hi&#013;&#010;&#013;&#010;如果可以的话，建议先调用 RestClient 的 stop 等命令（这样可以在最后做一次&#010;savepoint，或者 checkpoint -- 这个&#013;&#010;FLINK-12619 想做），然后失败再使用 yarn 的 kill 命令，这样能够减少后续启动时的回放数据量&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月14日周二 下午12:53写道：&#013;&#010;&#013;&#010;&gt; 如果是 on yarn 的话，也可以直接调用 yarn 的 kill 命令停止作业&#013;&#010;&gt;&#013;&#010;&gt; Jeff Zhang &lt;zjffdu@gmail.com&gt; 于2020年7月11日周六 下午11:23写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Zeppelin 能够帮你提交和cancel job，就是通过上面jianxu说的ClusterClient&#013;&#010;&gt; &gt; api来做到的，对zeppelin感兴趣的话，可以参考这个视频&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; https://www.bilibili.com/video/BV1Te411W73b?p=21&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; jianxu &lt;rjianxu@163.com&gt; 于2020年7月11日周六 下午4:52写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我想，你可能打算通过API的方式来取消正在运行的流任务。Flink任务提交时需要构建ClusterClient，提交成功后会返回任务对应的JobId。任务取消时，通过调用ClusterClient的cancel(JobID&#013;&#010;&gt; &gt; &gt; jobId)取消流任务。&#013;&#010;&gt; &gt; &gt;     Flink源码可以看看 CliFrontend[1]中的逻辑，如果觉得比较麻烦可以参考&#013;&#010;&gt; &gt; &gt; https://github.com/todd5167/flink-spark-submiter&#013;&#010;&gt; &gt; &gt; 项目的任务提交部分，取消任务时构建ClusterClient即可。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; | |&#013;&#010;&gt; &gt; &gt; jianxu&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; rjianxu@163.com&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 在2020年07月11日 16:19，Congxian Qiu&lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 如果你是想做一个作业管理的平台，可以尝试看一下 CliFrontend[1]&#010;中相关的逻辑，对于 On Yarn&#013;&#010;&gt; &gt; &gt; 的作业，简单地说你需要能够正确的初始化一个 client 和&#010;Yarn RM 交互，然后你需要知道&#013;&#010;&gt; applicationId，另外你还需要知道&#013;&#010;&gt; &gt; &gt; flink 的 JobId，接下来就是调用 Flink 的接口了&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 如果像更多的了解参数如从和命令行传到 java 代码的，你可以自己写一个单元测试，单步调试一下整个流程。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; [1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/master/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontend.java&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月9日周四 上午10:08写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 可以通过 StreamExecutionEnvironment#executeAsync 提交作业，返回&#010;JobClient [1], 通过&#013;&#010;&gt; &gt; &gt; JobClient 可以 cancel 作业，获取 job status。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; [1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://cwiki.apache.org/confluence/display/FLINK/FLIP-74%3A+Flink+JobClient+API&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Godfrey&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月9日周四 上午9:40写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 这个问题之前看到过有人在问，但是没有看到答案，我想问一下，Flink&#010;Streaming&#013;&#010;&gt; &gt; &gt; API有没有提供类似的接口，调用后就能停止这个Stream作业呢？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best Regards&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Jeff Zhang&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<tencent_E464A70C95F4D39BF5B3D62F96BE995A2908@qq.com>"
    },
    {
        "id": "<1594263335204-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 02:55:35 GMT",
        "subject": "Re: Re: Flink 多Sink 数据一致性保证",
        "content": "请问下，你这个最后是怎么做到的，能share下源码吗？&#013;&#010;是需要将两个sink合并到一个sink里，然后再实现下二阶段提交吗？&#013;&#010;我也遇到个多sink的原子性场景。&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "1",
        "reply": "<1594263335204-0.post@n8.nabble.com>"
    },
    {
        "id": "<fb39b4b9-3470-4932-a5b4-952e88c2dd4f.yungao.gy@aliyun.com>",
        "from": "&quot;Yun Gao&quot; &lt;yungao...@aliyun.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 10 Jul 2020 12:56:31 GMT",
        "subject": "Re: Re: Re: Flink 多Sink 数据一致性保证",
        "content": "多个sink如果都按TwoPhaseCommitSinkFunction来做的话，是可以实现多sink一致性的。大体上可以认为只要有一个sink出错，整个作业都会failover，其它sink当前的事务也会跟着abort掉，然后整个作业回退到上一次checkpoint开始执行。&#010;&#010;&#010;------------------------------------------------------------------&#010;Sender:jindy_liu&lt;286729788@qq.com&gt;&#010;Date:2020/07/09 10:55:35&#010;Recipient:&lt;user-zh@flink.apache.org&gt;&#010;Theme:Re: Re: Flink 多Sink 数据一致性保证&#010;&#010;请问下，你这个最后是怎么做到的，能share下源码吗？&#010;是需要将两个sink合并到一个sink里，然后再实现下二阶段提交吗？&#010;我也遇到个多sink的原子性场景。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;",
        "depth": "1",
        "reply": "<1594263335204-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594608567829-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 02:49:27 GMT",
        "subject": "Re: Re: Re: Flink 多Sink 数据一致性保证",
        "content": "原理大概理解了，想自己实现一个。比如kafka与mysql的实现，并想最大程度的复用些代码。&#013;&#010;看了下源码，感觉要把现在的connector（kafka，&#010;jdbc）中的代码都看一下，然后扣出来，再去按twophasecommitsinkfunction的实现，重组一些代码，一个个方法实现。&#013;&#010;另外问一下，好像现在源码里的jdbc只是at-least-once实现?&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "2",
        "reply": "<1594263335204-0.post@n8.nabble.com>"
    },
    {
        "id": "<4d605a46-9366-4a57-a9fd-b62a93d49e1b.yungao.gy@aliyun.com>",
        "from": "&quot;Yun Gao&quot; &lt;yungao...@aliyun.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 05:24:47 GMT",
        "subject": "Re: Re: Re: Re: Flink 多Sink 数据一致性保证",
        "content": "是的，社区现在正在加exactly-oncer jdbc sink实现[1]。&#010;&#010;另外，如果要实现两阶段提交的sink的话，总是需要有能跨session的transaction机制，就是在作业挂了之后，下次起来的时候这个事务还可以abort掉或者继续提交（取决于是否已经snapshot过了）。像jdbc必须要用xa事务，用单纯的jdbc事务应该就是有问题的，因为即使在snapshot的时候precommit过了，如果作业挂掉连接中断这个事务仍然会被abort掉。&#010;&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-15578&#010;&#010;------------------------------------------------------------------&#010;Sender:jindy_liu&lt;286729788@qq.com&gt;&#010;Date:2020/07/13 10:49:27&#010;Recipient:&lt;user-zh@flink.apache.org&gt;&#010;Theme:Re: Re: Re: Flink 多Sink 数据一致性保证&#010;&#010;原理大概理解了，想自己实现一个。比如kafka与mysql的实现，并想最大程度的复用些代码。&#010;看了下源码，感觉要把现在的connector（kafka，&#010;jdbc）中的代码都看一下，然后扣出来，再去按twophasecommitsinkfunction的实现，重组一些代码，一个个方法实现。&#010;另外问一下，好像现在源码里的jdbc只是at-least-once实现?&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;",
        "depth": "2",
        "reply": "<1594263335204-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_E43662AE412AADB9F2D64422180789DBAC07@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 03:25:27 GMT",
        "subject": "kafka connector问题",
        "content": "官网给的kafka table配置里的scan.startup.mode&amp;nbsp;CREATE TABLE kafkaTable ( user_id&#010;BIGINT,  item_id BIGINT,  category_id BIGINT,  behavior STRING,  ts TIMESTAMP(3) ) WITH (&#010; 'connector' = 'kafka',  'topic' = 'user_behavior',  'properties.bootstrap.servers' = 'localhost:9092',&#010; 'properties.group.id' = 'testGroup',  'format' = 'csv',  'scan.startup.mode' = 'earliest-offset'&#010;)看了总共有以下几总'earliest-offset',&amp;nbsp;'latest-offset',&amp;nbsp;'group-offsets',&amp;nbsp;'timestamp'&amp;nbsp;and&amp;nbsp;'specific-offsets'如果我作业重启的话选择group-offsets能否从上次消费到的位置开始？这种情况下需要配置提交offset到kafka&#010;broker相关的东西吗？有没有从savepoint保存的offset继续消费的配置？",
        "depth": "0",
        "reply": "<tencent_E43662AE412AADB9F2D64422180789DBAC07@qq.com>"
    },
    {
        "id": "<CABKuJ_RKyu2wQLPiUQvKKJ6x3ry0dkf2S_r8SF1ssBweNXccZQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 09 Jul 2020 06:23:08 GMT",
        "subject": "Re: kafka connector问题",
        "content": "首先，从checkpoint/savepoint&#013;&#010;恢复的话，一定会以checkpoint/savepoint中的offset为准，所以它的优先级是最高的，&#013;&#010;不管你配置哪种startup mode。&#013;&#010;如果你没有开启checkpoint，那么如果你用了group-offsets，那它就会从保存在kafka中的offset进行启动。&#013;&#010;提交offset到kafka这个应该是默认就开了的。&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月9日周四 上午11:25写道：&#013;&#010;&#013;&#010;&gt; 官网给的kafka table配置里的scan.startup.mode&amp;nbsp;CREATE TABLE kafkaTable&#010;(&#013;&#010;&gt; user_id BIGINT,  item_id BIGINT,  category_id BIGINT,  behavior STRING,  ts&#013;&#010;&gt; TIMESTAMP(3) ) WITH (  'connector' = 'kafka',  'topic' = 'user_behavior',&#013;&#010;&gt; 'properties.bootstrap.servers' = 'localhost:9092',  'properties.group.id'&#013;&#010;&gt; = 'testGroup',  'format' = 'csv',  'scan.startup.mode' = 'earliest-offset'&#013;&#010;&gt; )看了总共有以下几总'earliest-offset',&amp;nbsp;'latest-offset',&amp;nbsp;'group-offsets',&amp;nbsp;'timestamp'&amp;nbsp;and&amp;nbsp;'specific-offsets'如果我作业重启的话选择group-offsets能否从上次消费到的位置开始？这种情况下需要配置提交offset到kafka&#013;&#010;&gt; broker相关的东西吗？有没有从savepoint保存的offset继续消费的配置？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_E43662AE412AADB9F2D64422180789DBAC07@qq.com>"
    }
]