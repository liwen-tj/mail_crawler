[
    {
        "id": "<CANWTSLAcGZAQYO5NXOo2cnDiGBjm5NAFkEz7WSzKFyf1p6OohA@mail.gmail.com>",
        "from": "Eleanore Jin &lt;eleanore....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 00:25:39 GMT",
        "subject": "Flink Training - why cannot keyBy hour?",
        "content": "Hi experts,&#010;&#010;I am going through Ververica flink training, and when doing the lab with&#010;window (https://training.ververica.com/exercises/windows), basically it&#010;requires to compute within an hour which driver earns the most tip.&#010;&#010;The logic is to&#010;0. keyBy driverId&#010;1. create 1 hour window based on eventTime&#010;2. sum up all the tips for this driver within this 1 hour window&#010;3. create an 1 hour globalWindow for all drivers&#010;4. find the max tips&#010;&#010;sample code shown as below.&#010;&#010;SingleOutputStreamOperator&lt;Tuple3&lt;Long, Long, Float&gt;&gt;&#010;aggregatedTipsPerDriver = fares.keyBy(rides -&gt; rides.driverId)&#010; .window(TumblingEventTimeWindows.of(Time.hours(1)))&#010; .process(new SumTipsFunction());&#010;&#010;// Tuple3: reporting the timestamp for the end of the hour, the&#010;driverId, and the total of that driver's tips for that hour&#010;SingleOutputStreamOperator&lt;Tuple3&lt;Long, Long, Float&gt;&gt; hourlyMax =&#010; aggregatedTipsPerDriver.windowAll(TumblingEventTimeWindows.of(Time.hours(1)))&#010; .maxBy(2);&#010;&#010;&#010;The question is shown as 4th slide: why we cannot keyed by the hour?&#010;&#010;If I change the implementation to keyBy hour and run the HourlyTipsTest,&#010;&#010;the test of testMaxAcrossDrivers will fail:&#010;&#010;// (946688400000,1,6.0) -&gt; for timestamp window: 946688400000,&#010;driverId: 1, earns most tip: 6.0&#010;&#010;Expected :[(946688400000,1,6.0), (946692000000,2,20.0)]&#010;Actual   :[(946688400000,1,6.0), (946692000000,2,20.0), (946692000000,2,20.0)]&#010;&#010;&#010;[image: image.png]&#010;&#010;Thanks a lot!&#010;Eleanore&#010;&#010;",
        "depth": "0",
        "reply": "<CANWTSLAcGZAQYO5NXOo2cnDiGBjm5NAFkEz7WSzKFyf1p6OohA@mail.gmail.com>"
    },
    {
        "id": "<CANWTSLA0jA8rnxv5gFUHe_AgUVnEzHXspp=dCrw-fCdF_=GH7Q@mail.gmail.com>",
        "from": "Eleanore Jin &lt;eleanore....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 17:11:14 GMT",
        "subject": "Re: Flink Training - why cannot keyBy hour?",
        "content": "Hi David,&#010;&#010;Thanks a lot for the explanation!&#010;&#010;Eleanore&#010;&#010;On Thu, Jul 2, 2020 at 6:30 AM David Anderson &lt;david@alpinegizmo.com&gt; wrote:&#010;&#010;&gt; Eleanore,&#010;&gt;&#010;&gt; Yes, if you change the implementation in the way that is suggested by the&#010;&gt; slide, the tests will fail. But it's more interesting to observe the&#010;&gt; behavior in the console.&#010;&gt;&#010;&gt; The notes that go with that slide explain the situation in more detail.&#010;&gt; (Use alt-p or option-p to see the notes). But to recap here, there are two&#010;&gt; related effects:&#010;&gt;&#010;&gt; (1) Instead of producing a single result at the end of the window, this&#010;&gt; alternative implementation produces a result for every event. In other&#010;&gt; words, it produces a stream that eventually arrives at the same maximum&#010;&gt; value produced by the timeWindowAll.&#010;&gt;&#010;&gt; (2) With timeWindowAll, once the results for a given hour have been&#010;&gt; produced, Flink frees the state associated with the window for that hour.&#010;&gt; It knows, based on the watermarking, that no more events are expected, so&#010;&gt; the state is no longer needed and can be cleared. But with maxBy, the state&#010;&gt; for each key (each hour) is kept forever. This is why this is not a good&#010;&gt; approach: the keyspace is unbounded, and we can't intervene to clean up&#010;&gt; stale state.&#010;&gt;&#010;&gt; Regards,&#010;&gt; David&#010;&gt;&#010;&gt; On Wed, Jul 1, 2020 at 2:26 AM Eleanore Jin &lt;eleanore.jin@gmail.com&gt;&#010;&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi experts,&#010;&gt;&gt;&#010;&gt;&gt; I am going through Ververica flink training, and when doing the lab with&#010;&gt;&gt; window (https://training.ververica.com/exercises/windows), basically it&#010;&gt;&gt; requires to compute within an hour which driver earns the most tip.&#010;&gt;&gt;&#010;&gt;&gt; The logic is to&#010;&gt;&gt; 0. keyBy driverId&#010;&gt;&gt; 1. create 1 hour window based on eventTime&#010;&gt;&gt; 2. sum up all the tips for this driver within this 1 hour window&#010;&gt;&gt; 3. create an 1 hour globalWindow for all drivers&#010;&gt;&gt; 4. find the max tips&#010;&gt;&gt;&#010;&gt;&gt; sample code shown as below.&#010;&gt;&gt;&#010;&gt;&gt; SingleOutputStreamOperator&lt;Tuple3&lt;Long, Long, Float&gt;&gt; aggregatedTipsPerDriver&#010;= fares.keyBy(rides -&gt; rides.driverId)&#010;&gt;&gt;  .window(TumblingEventTimeWindows.of(Time.hours(1)))&#010;&gt;&gt;  .process(new SumTipsFunction());&#010;&gt;&gt;&#010;&gt;&gt; // Tuple3: reporting the timestamp for the end of the hour, the driverId, and the&#010;total of that driver's tips for that hour&#010;&gt;&gt; SingleOutputStreamOperator&lt;Tuple3&lt;Long, Long, Float&gt;&gt; hourlyMax =&#010;&gt;&gt;  aggregatedTipsPerDriver.windowAll(TumblingEventTimeWindows.of(Time.hours(1)))&#010;&gt;&gt;  .maxBy(2);&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; The question is shown as 4th slide: why we cannot keyed by the hour?&#010;&gt;&gt;&#010;&gt;&gt; If I change the implementation to keyBy hour and run the HourlyTipsTest,&#010;&gt;&gt;&#010;&gt;&gt; the test of testMaxAcrossDrivers will fail:&#010;&gt;&gt;&#010;&gt;&gt; // (946688400000,1,6.0) -&gt; for timestamp window: 946688400000, driverId: 1, earns&#010;most tip: 6.0&#010;&gt;&gt;&#010;&gt;&gt; Expected :[(946688400000,1,6.0), (946692000000,2,20.0)]&#010;&gt;&gt; Actual   :[(946688400000,1,6.0), (946692000000,2,20.0), (946692000000,2,20.0)]&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; [image: image.png]&#010;&gt;&gt;&#010;&gt;&gt; Thanks a lot!&#010;&gt;&gt; Eleanore&#010;&gt;&gt;&#010;&gt;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CANWTSLAcGZAQYO5NXOo2cnDiGBjm5NAFkEz7WSzKFyf1p6OohA@mail.gmail.com>"
    },
    {
        "id": "<tencent_1585B1668EAE8FB889EC0EBA@qq.com>",
        "from": "&quot;17626017841&quot;&lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 02:53:22 GMT",
        "subject": "回复：【Flink的transformations】",
        "content": "hi，&#010;除了source、sink、union之类有特有的Transformation，大部分算子都属于OneInputTransformation&#010; 原始邮件 &#010;发件人: 忝忝向仧&lt;153488125@qq.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年6月29日(周一) 22:29&#010;主题: 【Flink的transformations】&#010;&#010;&#010;Hi,all: 请教下，Flink的应用程序首先都会转为逻辑映射也就是transformations，我看org.apache.flink.streaming.api.transformations包下面目前有17种Transformation类(SourceTransformation,SplitTransformation,TwoInputTransformation等)，有没有一个映射关系列表，也就是说应用程序里面哪些算子或者操作(比如map,flatmap,filter,connect,select等)会对应到哪一个Transformation类.&#010;谢谢.",
        "depth": "0",
        "reply": "<tencent_1585B1668EAE8FB889EC0EBA@qq.com>"
    },
    {
        "id": "<tencent_55D81C2A62AF0BC0012EF22BBFDD3E3E4709@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 03:02:53 GMT",
        "subject": "flink sql  if 函数使用问题 ",
        "content": "flink-1.10.1 blink_planner&#013;&#010;if使用时候限制了返回的数据类型吗？ &#013;&#010;Cannot apply 'IF' to arguments of type 'IF(&lt;BOOLEAN&amp;gt;, &lt;TIMESTAMP(0)&amp;gt;,&#010;&lt;VARCHAR(2147483647)&amp;gt;)'. Supported form(s): 'IF(&lt;ANY_TYPE&amp;gt;, &lt;NUMERIC_TYPE&amp;gt;,&#010;&lt;NUMERIC_TYPE&amp;gt;)'&#013;&#010;我想创建DDL时候，因为字段可能有空，所以如果为空了我想设置一个默认值，但是报错提示是只支持返回数据类型。",
        "depth": "0",
        "reply": "<tencent_55D81C2A62AF0BC0012EF22BBFDD3E3E4709@qq.com>"
    },
    {
        "id": "<CABKuJ_SijSeH-pnKSYMCXd3yoSNqAacsqpCCNpHYpQwPirDNrw@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 05:15:15 GMT",
        "subject": "Re: flink sql if 函数使用问题",
        "content": "看报错，应该是你的IF的后面两个参数的类型不同吧。这里应该让后面两个参数的类型也相同的，要不然IF函数的返回值类型就不好确定了。&#013;&#010;&#013;&#010;kcz &lt;573693104@qq.com&gt; 于2020年7月1日周三 上午11:03写道：&#013;&#010;&#013;&#010;&gt; flink-1.10.1 blink_planner&#013;&#010;&gt; if使用时候限制了返回的数据类型吗？&#013;&#010;&gt; Cannot apply 'IF' to arguments of type 'IF(&lt;BOOLEAN&amp;gt;,&#013;&#010;&gt; &lt;TIMESTAMP(0)&amp;gt;, &lt;VARCHAR(2147483647)&amp;gt;)'. Supported form(s):&#013;&#010;&gt; 'IF(&lt;ANY_TYPE&amp;gt;, &lt;NUMERIC_TYPE&amp;gt;, &lt;NUMERIC_TYPE&amp;gt;)'&#013;&#010;&gt; 我想创建DDL时候，因为字段可能有空，所以如果为空了我想设置一个默认值，但是报错提示是只支持返回数据类型。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_55D81C2A62AF0BC0012EF22BBFDD3E3E4709@qq.com>"
    },
    {
        "id": "<tencent_721F20236A0E33B17705E20052F95DDF1208@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 05:57:36 GMT",
        "subject": "回复： flink sql if 函数使用问题",
        "content": "tks&amp;nbsp;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月1日(星期三) 中午1:15&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink sql if 函数使用问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;看报错，应该是你的IF的后面两个参数的类型不同吧。这里应该让后面两个参数的类型也相同的，要不然IF函数的返回值类型就不好确定了。&#013;&#010;&#013;&#010;kcz &lt;573693104@qq.com&amp;gt; 于2020年7月1日周三 上午11:03写道：&#013;&#010;&#013;&#010;&amp;gt; flink-1.10.1 blink_planner&#013;&#010;&amp;gt; if使用时候限制了返回的数据类型吗？&#013;&#010;&amp;gt; Cannot apply 'IF' to arguments of type 'IF(&lt;BOOLEAN&amp;amp;gt;,&#013;&#010;&amp;gt; &lt;TIMESTAMP(0)&amp;amp;gt;, &lt;VARCHAR(2147483647)&amp;amp;gt;)'. Supported form(s):&#013;&#010;&amp;gt; 'IF(&lt;ANY_TYPE&amp;amp;gt;, &lt;NUMERIC_TYPE&amp;amp;gt;, &lt;NUMERIC_TYPE&amp;amp;gt;)'&#013;&#010;&amp;gt; 我想创建DDL时候，因为字段可能有空，所以如果为空了我想设置一个默认值，但是报错提示是只支持返回数据类型。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "2",
        "reply": "<tencent_55D81C2A62AF0BC0012EF22BBFDD3E3E4709@qq.com>"
    },
    {
        "id": "<CAFcoNBTU=COf6vR4-GKBHA1hXZk6+g+6q64xkK-6D32-q6QL6w@mail.gmail.com>",
        "from": "徐骁 &lt;ffxrqy...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 03:15:31 GMT",
        "subject": "Re: 作业因为异常restart后，频繁OOM",
        "content": "很早以前遇到这个问题, standalone 模式下 metaspace 释放不掉, 感觉是一个比较严重的&#010;bug&#013;&#010;https://issues.apache.org/jira/browse/FLINK-11205 这边有过讨论&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年6月30日周二 下午11:45写道：&#013;&#010;&#013;&#010;&gt; 作业如果正常运行，堆外内存是足够的。在restart后才会出现频繁重启的情况，重构集群才能恢复正常&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年06月30日 23:39，LakeShen 写道：&#013;&#010;&gt; 我在较低版本，Flink on k8s ，也遇到 OOM 被 kill 了。&#013;&#010;&gt;&#013;&#010;&gt; 我感觉可能是 TaskManager 堆外内存不足了，我目前是 Flink 1.6 版本，Flink&#010;on k8s , standalone per&#013;&#010;&gt; job 模式，堆外内存默认没有限制~。&#013;&#010;&gt;&#013;&#010;&gt; 我的解决方法增加了一个参数：taskmanager.memory.off-heap: true.&#013;&#010;&gt;&#013;&#010;&gt; 目前来看，OOM被 kill 掉的问题没有在出现了。希望能帮到你。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; LakeShen&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年6月30日周二 下午11:19写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 补充一下，内核版本为 3.10.x，是否会是堆外内存cache没被回收而导致的内存超用？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在2020年06月30日 23:00，GuoSmileSmil 写道：&#013;&#010;&gt; &gt; hi all，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我使用的Flink版本为1.10.1，使用的backend是rocksdb，没有开启checkpoint，运行在kubernetes平台上，模式是standalone。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 目前遇到的问题是作业如果因为网络抖动或者硬件故障导致的pod被失联而fail，在pod重生后，作业自动restart，作业运行一段时间（半小时到1小时不等）很容易出现其他pod因为oom被os&#013;&#010;&gt; &gt; kill的现象，然后反复循环，pod 被kill越来越频繁。目前的解决方法是手动销毁这个集群，重新构建一个集群后重启作业，就恢复正常。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如果单纯heap的状态后台，作业restart不会出现这样的问题。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 有一些不成熟的猜测，作业在fail后，native memory没有释放干净，pod的limit假设为10G，那么job&#013;&#010;&gt; &gt; restart后只有8G，TM还是按照10G的标准运行，pod使用的内存就会超过10G而被os&#010;kill（纯属猜测）。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 请问大家是否有什么好的提议或者解决方法？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 其中一次系统内核日志如下：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: memory: usage 28672000kB, limit&#013;&#010;&gt; &gt; 28672000kB, failcnt 11225&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: memory+swap: usage 28672000kB, limit&#013;&#010;&gt; &gt; 9007199254740988kB, failcnt 0&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: kmem: usage 0kB, limit&#013;&#010;&gt; &gt; 9007199254740988kB, failcnt 0&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&gt; &gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice:&#013;&#010;&gt; &gt; cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0K&#013;&#010;&gt; &gt; B inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB&#013;&#010;&gt; &gt; unevictable:0KB&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&gt; &gt;&#013;&#010;&gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-fe101418a3b2a7c534e89b4ac73d29b04070eb923220a5b1&#013;&#010;&gt; &gt; 7338850bbdb3817a.scope: cache:0KB rss:44KB rss_huge:0KB mapped_file:0KB&#013;&#010;&gt; &gt; swap:0KB inactive_anon:0KB active_anon:44KB inactive_file:0KB&#013;&#010;&gt; &gt; active_file:0KB unevictable:0KB&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&gt; &gt;&#013;&#010;&gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-a2295e812a828738810a8f1ae69cd48e99ef98b9e1038158a6e33f81524cc02a.scope:&#013;&#010;&gt; &gt; cache:180KB rss:28671776KB rss_huge:26437632KB mapped_file:144KB swap:0KB&#013;&#010;&gt; &gt; inactive_anon:0KB active_anon:28671760KB inactive_file:4KB&#013;&#010;&gt; active_file:4KB&#013;&#010;&gt; &gt; unevictable:0KB&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: [ pid ]   uid  tgid total_vm      rss&#013;&#010;&gt; &gt; nr_ptes swapents oom_score_adj name&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: [16875]     0 16875      253        1&#013;&#010;&gt; &gt;      4        0          -998 pause&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: [17274]     0 17274     1369      421&#013;&#010;&gt; &gt;      7        0          -998 bash&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: [18089]     0 18089 10824832  7174316&#013;&#010;&gt; &gt;  14500        0          -998 java&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: [18348]     0 18348     1017      196&#013;&#010;&gt; &gt;      6        0          -998 tail&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup out of memory: Kill&#013;&#010;&gt; &gt; process 26824 (Window(Tumbling) score 4 or sacrifice child&#013;&#010;&gt; &gt; Jun 30 21:59:15 flink-tm-1 kernel: Killed process 18089 (java)&#013;&#010;&gt; &gt; total-vm:43299328kB, anon-rss:28669084kB, file-rss:28180kB, shmem-rss:0kB&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Looking forward to your reply and help.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAFcoNBTU=COf6vR4-GKBHA1hXZk6+g+6q64xkK-6D32-q6QL6w@mail.gmail.com>"
    },
    {
        "id": "<tencent_662C38D8C5E6168C03E4E4DB7FC77F808F0A@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 03:32:48 GMT",
        "subject": "回复： 作业因为异常restart后，频繁OOM",
        "content": "1.10.0我也与遇到过，我看1.11.0介绍，会复用classloader，不知道是不是就把这个解决了。&#013;&#010;我的情况是第一次运行OK，之后停止，再次启动，就遇到了OOM，调大了metaspace又可以跑，但是重复停止再次启动，还是OOM。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"徐骁\"&lt;ffxrqyzby@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月1日(星期三) 中午11:15&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 作业因为异常restart后，频繁OOM&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;很早以前遇到这个问题, standalone 模式下 metaspace 释放不掉, 感觉是一个比较严重的&#010;bug&#013;&#010;https://issues.apache.org/jira/browse/FLINK-11205 这边有过讨论&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&amp;gt; 于2020年6月30日周二 下午11:45写道：&#013;&#010;&#013;&#010;&amp;gt; 作业如果正常运行，堆外内存是足够的。在restart后才会出现频繁重启的情况，重构集群才能恢复正常&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; | |&#013;&#010;&amp;gt; a511955993&#013;&#010;&amp;gt; |&#013;&#010;&amp;gt; |&#013;&#010;&amp;gt; 邮箱：a511955993@163.com&#013;&#010;&amp;gt; |&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 签名由 网易邮箱大师 定制&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 在2020年06月30日 23:39，LakeShen 写道：&#013;&#010;&amp;gt; 我在较低版本，Flink on k8s ，也遇到 OOM 被 kill 了。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我感觉可能是 TaskManager 堆外内存不足了，我目前是 Flink 1.6 版本，Flink&#010;on k8s , standalone per&#013;&#010;&amp;gt; job 模式，堆外内存默认没有限制~。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我的解决方法增加了一个参数：taskmanager.memory.off-heap: true.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 目前来看，OOM被 kill 掉的问题没有在出现了。希望能帮到你。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; LakeShen&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; SmileSmile &lt;a511955993@163.com&amp;gt; 于2020年6月30日周二 下午11:19写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 补充一下，内核版本为 3.10.x，是否会是堆外内存cache没被回收而导致的内存超用？&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; | |&#013;&#010;&amp;gt; &amp;gt; a511955993&#013;&#010;&amp;gt; &amp;gt; |&#013;&#010;&amp;gt; &amp;gt; |&#013;&#010;&amp;gt; &amp;gt; 邮箱：a511955993@163.com&#013;&#010;&amp;gt; &amp;gt; |&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 签名由 网易邮箱大师 定制&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 在2020年06月30日 23:00，GuoSmileSmil 写道：&#013;&#010;&amp;gt; &amp;gt; hi all，&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; 我使用的Flink版本为1.10.1，使用的backend是rocksdb，没有开启checkpoint，运行在kubernetes平台上，模式是standalone。&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; 目前遇到的问题是作业如果因为网络抖动或者硬件故障导致的pod被失联而fail，在pod重生后，作业自动restart，作业运行一段时间（半小时到1小时不等）很容易出现其他pod因为oom被os&#013;&#010;&amp;gt; &amp;gt; kill的现象，然后反复循环，pod 被kill越来越频繁。目前的解决方法是手动销毁这个集群，重新构建一个集群后重启作业，就恢复正常。&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 如果单纯heap的状态后台，作业restart不会出现这样的问题。&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 有一些不成熟的猜测，作业在fail后，native memory没有释放干净，pod的limit假设为10G，那么job&#013;&#010;&amp;gt; &amp;gt; restart后只有8G，TM还是按照10G的标准运行，pod使用的内存就会超过10G而被os&#010;kill（纯属猜测）。&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 请问大家是否有什么好的提议或者解决方法？&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 其中一次系统内核日志如下：&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: memory: usage 28672000kB, limit&#013;&#010;&amp;gt; &amp;gt; 28672000kB, failcnt 11225&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: memory+swap: usage 28672000kB, limit&#013;&#010;&amp;gt; &amp;gt; 9007199254740988kB, failcnt 0&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: kmem: usage 0kB, limit&#013;&#010;&amp;gt; &amp;gt; 9007199254740988kB, failcnt 0&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&amp;gt; &amp;gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice:&#013;&#010;&amp;gt; &amp;gt; cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0K&#013;&#010;&amp;gt; &amp;gt; B inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB&#013;&#010;&amp;gt; &amp;gt; unevictable:0KB&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-fe101418a3b2a7c534e89b4ac73d29b04070eb923220a5b1&#013;&#010;&amp;gt; &amp;gt; 7338850bbdb3817a.scope: cache:0KB rss:44KB rss_huge:0KB mapped_file:0KB&#013;&#010;&amp;gt; &amp;gt; swap:0KB inactive_anon:0KB active_anon:44KB inactive_file:0KB&#013;&#010;&amp;gt; &amp;gt; active_file:0KB unevictable:0KB&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-a2295e812a828738810a8f1ae69cd48e99ef98b9e1038158a6e33f81524cc02a.scope:&#013;&#010;&amp;gt; &amp;gt; cache:180KB rss:28671776KB rss_huge:26437632KB mapped_file:144KB swap:0KB&#013;&#010;&amp;gt; &amp;gt; inactive_anon:0KB active_anon:28671760KB inactive_file:4KB&#013;&#010;&amp;gt; active_file:4KB&#013;&#010;&amp;gt; &amp;gt; unevictable:0KB&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [ pid ]&amp;nbsp;&amp;nbsp; uid&amp;nbsp;&#010;tgid total_vm&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; rss&#013;&#010;&amp;gt; &amp;gt; nr_ptes swapents oom_score_adj name&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [16875]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 16875&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 253&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;1&#013;&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 4&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 pause&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [17274]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 17274&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1369&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;421&#013;&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 7&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 bash&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [18089]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 18089 10824832&amp;nbsp; 7174316&#013;&#010;&amp;gt; &amp;gt;&amp;nbsp; 14500&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 java&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [18348]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 18348&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1017&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;196&#013;&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 6&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 tail&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup out of memory: Kill&#013;&#010;&amp;gt; &amp;gt; process 26824 (Window(Tumbling) score 4 or sacrifice child&#013;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Killed process 18089 (java)&#013;&#010;&amp;gt; &amp;gt; total-vm:43299328kB, anon-rss:28669084kB, file-rss:28180kB, shmem-rss:0kB&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; Looking forward to your reply and help.&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; Best&#013;&#010;&amp;gt;",
        "depth": "2",
        "reply": "<CAFcoNBTU=COf6vR4-GKBHA1hXZk6+g+6q64xkK-6D32-q6QL6w@mail.gmail.com>"
    },
    {
        "id": "<1c4afbb8.2318.17308b51959.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 04:49:13 GMT",
        "subject": "回复：作业因为异常restart后，频繁OOM",
        "content": "你的oom的详细报错是metaspace 不足还是被os kill？&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月01日 11:32，kcz 写道：&#010;1.10.0我也与遇到过，我看1.11.0介绍，会复用classloader，不知道是不是就把这个解决了。&#010;我的情况是第一次运行OK，之后停止，再次启动，就遇到了OOM，调大了metaspace又可以跑，但是重复停止再次启动，还是OOM。&#010;&#010;&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:&amp;nbsp;\"徐骁\"&lt;ffxrqyzby@gmail.com&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月1日(星期三) 中午11:15&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Re: 作业因为异常restart后，频繁OOM&#010;&#010;&#010;&#010;很早以前遇到这个问题, standalone 模式下 metaspace 释放不掉, 感觉是一个比较严重的&#010;bug&#010;https://issues.apache.org/jira/browse/FLINK-11205 这边有过讨论&#010;&#010;SmileSmile &lt;a511955993@163.com&amp;gt; 于2020年6月30日周二 下午11:45写道：&#010;&#010;&amp;gt; 作业如果正常运行，堆外内存是足够的。在restart后才会出现频繁重启的情况，重构集群才能恢复正常&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; | |&#010;&amp;gt; a511955993&#010;&amp;gt; |&#010;&amp;gt; |&#010;&amp;gt; 邮箱：a511955993@163.com&#010;&amp;gt; |&#010;&amp;gt;&#010;&amp;gt; 签名由 网易邮箱大师 定制&#010;&amp;gt;&#010;&amp;gt; 在2020年06月30日 23:39，LakeShen 写道：&#010;&amp;gt; 我在较低版本，Flink on k8s ，也遇到 OOM 被 kill 了。&#010;&amp;gt;&#010;&amp;gt; 我感觉可能是 TaskManager 堆外内存不足了，我目前是 Flink 1.6 版本，Flink&#010;on k8s , standalone per&#010;&amp;gt; job 模式，堆外内存默认没有限制~。&#010;&amp;gt;&#010;&amp;gt; 我的解决方法增加了一个参数：taskmanager.memory.off-heap: true.&#010;&amp;gt;&#010;&amp;gt; 目前来看，OOM被 kill 掉的问题没有在出现了。希望能帮到你。&#010;&amp;gt;&#010;&amp;gt; Best,&#010;&amp;gt; LakeShen&#010;&amp;gt;&#010;&amp;gt; SmileSmile &lt;a511955993@163.com&amp;gt; 于2020年6月30日周二 下午11:19写道：&#010;&amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; 补充一下，内核版本为 3.10.x，是否会是堆外内存cache没被回收而导致的内存超用？&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; | |&#010;&amp;gt; &amp;gt; a511955993&#010;&amp;gt; &amp;gt; |&#010;&amp;gt; &amp;gt; |&#010;&amp;gt; &amp;gt; 邮箱：a511955993@163.com&#010;&amp;gt; &amp;gt; |&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; 签名由 网易邮箱大师 定制&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; 在2020年06月30日 23:00，GuoSmileSmil 写道：&#010;&amp;gt; &amp;gt; hi all，&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; 我使用的Flink版本为1.10.1，使用的backend是rocksdb，没有开启checkpoint，运行在kubernetes平台上，模式是standalone。&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; 目前遇到的问题是作业如果因为网络抖动或者硬件故障导致的pod被失联而fail，在pod重生后，作业自动restart，作业运行一段时间（半小时到1小时不等）很容易出现其他pod因为oom被os&#010;&amp;gt; &amp;gt; kill的现象，然后反复循环，pod 被kill越来越频繁。目前的解决方法是手动销毁这个集群，重新构建一个集群后重启作业，就恢复正常。&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; 如果单纯heap的状态后台，作业restart不会出现这样的问题。&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; 有一些不成熟的猜测，作业在fail后，native memory没有释放干净，pod的limit假设为10G，那么job&#010;&amp;gt; &amp;gt; restart后只有8G，TM还是按照10G的标准运行，pod使用的内存就会超过10G而被os&#010;kill（纯属猜测）。&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; 请问大家是否有什么好的提议或者解决方法？&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; 其中一次系统内核日志如下：&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: memory: usage 28672000kB, limit&#010;&amp;gt; &amp;gt; 28672000kB, failcnt 11225&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: memory+swap: usage 28672000kB, limit&#010;&amp;gt; &amp;gt; 9007199254740988kB, failcnt 0&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: kmem: usage 0kB, limit&#010;&amp;gt; &amp;gt; 9007199254740988kB, failcnt 0&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#010;&amp;gt; &amp;gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice:&#010;&amp;gt; &amp;gt; cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0K&#010;&amp;gt; &amp;gt; B inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB&#010;&amp;gt; &amp;gt; unevictable:0KB&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#010;&amp;gt; &amp;gt;&#010;&amp;gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-fe101418a3b2a7c534e89b4ac73d29b04070eb923220a5b1&#010;&amp;gt; &amp;gt; 7338850bbdb3817a.scope: cache:0KB rss:44KB rss_huge:0KB mapped_file:0KB&#010;&amp;gt; &amp;gt; swap:0KB inactive_anon:0KB active_anon:44KB inactive_file:0KB&#010;&amp;gt; &amp;gt; active_file:0KB unevictable:0KB&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#010;&amp;gt; &amp;gt;&#010;&amp;gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-a2295e812a828738810a8f1ae69cd48e99ef98b9e1038158a6e33f81524cc02a.scope:&#010;&amp;gt; &amp;gt; cache:180KB rss:28671776KB rss_huge:26437632KB mapped_file:144KB swap:0KB&#010;&amp;gt; &amp;gt; inactive_anon:0KB active_anon:28671760KB inactive_file:4KB&#010;&amp;gt; active_file:4KB&#010;&amp;gt; &amp;gt; unevictable:0KB&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [ pid ]&amp;nbsp;&amp;nbsp; uid&amp;nbsp;&#010;tgid total_vm&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; rss&#010;&amp;gt; &amp;gt; nr_ptes swapents oom_score_adj name&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [16875]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 16875&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 253&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;1&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 4&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 pause&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [17274]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 17274&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1369&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;421&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 7&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 bash&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [18089]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 18089 10824832&amp;nbsp; 7174316&#010;&amp;gt; &amp;gt;&amp;nbsp; 14500&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 java&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [18348]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0 18348&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1017&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;196&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 6&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 tail&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup out of memory: Kill&#010;&amp;gt; &amp;gt; process 26824 (Window(Tumbling) score 4 or sacrifice child&#010;&amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Killed process 18089 (java)&#010;&amp;gt; &amp;gt; total-vm:43299328kB, anon-rss:28669084kB, file-rss:28180kB, shmem-rss:0kB&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; Looking forward to your reply and help.&#010;&amp;gt; &amp;gt;&#010;&amp;gt; &amp;gt; Best&#010;&amp;gt;",
        "depth": "3",
        "reply": "<CAFcoNBTU=COf6vR4-GKBHA1hXZk6+g+6q64xkK-6D32-q6QL6w@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvvSx8ETUT_qsvPfSN3fy87=b2R8uQw-+T5u5EVUA_vsLA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 04:05:40 GMT",
        "subject": "Re: 作业因为异常restart后，频繁OOM",
        "content": "如果可以的话，在 OOM 的时候把整个进程的 memory dump 一份，然后分析看下是什么内存用的比预期多。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月1日周三 下午12:49写道：&#013;&#010;&#013;&#010;&gt; 你的oom的详细报错是metaspace 不足还是被os kill？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月01日 11:32，kcz 写道：&#013;&#010;&gt; 1.10.0我也与遇到过，我看1.11.0介绍，会复用classloader，不知道是不是就把这个解决了。&#013;&#010;&gt; 我的情况是第一次运行OK，之后停止，再次启动，就遇到了OOM，调大了metaspace又可以跑，但是重复停止再次启动，还是OOM。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"徐骁\"&lt;ffxrqyzby@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月1日(星期三) 中午11:15&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: 作业因为异常restart后，频繁OOM&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 很早以前遇到这个问题, standalone 模式下 metaspace 释放不掉, 感觉是一个比较严重的&#010;bug&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-11205 这边有过讨论&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&amp;gt; 于2020年6月30日周二 下午11:45写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 作业如果正常运行，堆外内存是足够的。在restart后才会出现频繁重启的情况，重构集群才能恢复正常&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; | |&#013;&#010;&gt; &amp;gt; a511955993&#013;&#010;&gt; &amp;gt; |&#013;&#010;&gt; &amp;gt; |&#013;&#010;&gt; &amp;gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &amp;gt; |&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 在2020年06月30日 23:39，LakeShen 写道：&#013;&#010;&gt; &amp;gt; 我在较低版本，Flink on k8s ，也遇到 OOM 被 kill 了。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 我感觉可能是 TaskManager 堆外内存不足了，我目前是 Flink 1.6&#010;版本，Flink on k8s ,&#013;&#010;&gt; standalone per&#013;&#010;&gt; &amp;gt; job 模式，堆外内存默认没有限制~。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 我的解决方法增加了一个参数：taskmanager.memory.off-heap: true.&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 目前来看，OOM被 kill 掉的问题没有在出现了。希望能帮到你。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; Best,&#013;&#010;&gt; &amp;gt; LakeShen&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; SmileSmile &lt;a511955993@163.com&amp;gt; 于2020年6月30日周二 下午11:19写道：&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; 补充一下，内核版本为 3.10.x，是否会是堆外内存cache没被回收而导致的内存超用？&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; | |&#013;&#010;&gt; &amp;gt; &amp;gt; a511955993&#013;&#010;&gt; &amp;gt; &amp;gt; |&#013;&#010;&gt; &amp;gt; &amp;gt; |&#013;&#010;&gt; &amp;gt; &amp;gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &amp;gt; &amp;gt; |&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; 在2020年06月30日 23:00，GuoSmileSmil 写道：&#013;&#010;&gt; &amp;gt; &amp;gt; hi all，&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; 我使用的Flink版本为1.10.1，使用的backend是rocksdb，没有开启checkpoint，运行在kubernetes平台上，模式是standalone。&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; 目前遇到的问题是作业如果因为网络抖动或者硬件故障导致的pod被失联而fail，在pod重生后，作业自动restart，作业运行一段时间（半小时到1小时不等）很容易出现其他pod因为oom被os&#013;&#010;&gt; &amp;gt; &amp;gt; kill的现象，然后反复循环，pod&#013;&#010;&gt; 被kill越来越频繁。目前的解决方法是手动销毁这个集群，重新构建一个集群后重启作业，就恢复正常。&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; 如果单纯heap的状态后台，作业restart不会出现这样的问题。&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; 有一些不成熟的猜测，作业在fail后，native memory没有释放干净，pod的limit假设为10G，那么job&#013;&#010;&gt; &amp;gt; &amp;gt; restart后只有8G，TM还是按照10G的标准运行，pod使用的内存就会超过10G而被os&#010;kill（纯属猜测）。&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; 请问大家是否有什么好的提议或者解决方法？&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; 其中一次系统内核日志如下：&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: memory: usage 28672000kB,&#013;&#010;&gt; limit&#013;&#010;&gt; &amp;gt; &amp;gt; 28672000kB, failcnt 11225&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: memory+swap: usage&#013;&#010;&gt; 28672000kB, limit&#013;&#010;&gt; &amp;gt; &amp;gt; 9007199254740988kB, failcnt 0&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: kmem: usage 0kB, limit&#013;&#010;&gt; &amp;gt; &amp;gt; 9007199254740988kB, failcnt 0&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice:&#013;&#010;&gt; &amp;gt; &amp;gt; cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0K&#013;&#010;&gt; &amp;gt; &amp;gt; B inactive_anon:0KB active_anon:0KB inactive_file:0KB&#013;&#010;&gt; active_file:0KB&#013;&#010;&gt; &amp;gt; &amp;gt; unevictable:0KB&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-fe101418a3b2a7c534e89b4ac73d29b04070eb923220a5b1&#013;&#010;&gt; &amp;gt; &amp;gt; 7338850bbdb3817a.scope: cache:0KB rss:44KB rss_huge:0KB&#013;&#010;&gt; mapped_file:0KB&#013;&#010;&gt; &amp;gt; &amp;gt; swap:0KB inactive_anon:0KB active_anon:44KB inactive_file:0KB&#013;&#010;&gt; &amp;gt; &amp;gt; active_file:0KB unevictable:0KB&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup stats for&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; /kubepods.slice/kubepods-pod5ad5d2ea_5faa_4a11_96b4_39271ab76e99.slice/docker-a2295e812a828738810a8f1ae69cd48e99ef98b9e1038158a6e33f81524cc02a.scope:&#013;&#010;&gt; &amp;gt; &amp;gt; cache:180KB rss:28671776KB rss_huge:26437632KB mapped_file:144KB&#013;&#010;&gt; swap:0KB&#013;&#010;&gt; &amp;gt; &amp;gt; inactive_anon:0KB active_anon:28671760KB inactive_file:4KB&#013;&#010;&gt; &amp;gt; active_file:4KB&#013;&#010;&gt; &amp;gt; &amp;gt; unevictable:0KB&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: [ pid ]&amp;nbsp;&amp;nbsp; uid&amp;nbsp;&#013;&#010;&gt; tgid total_vm&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; rss&#013;&#010;&gt; &amp;gt; &amp;gt; nr_ptes swapents oom_score_adj name&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel:&#013;&#010;&gt; [16875]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0 16875&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 253&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1&#013;&#010;&gt; &amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 4&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 pause&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel:&#013;&#010;&gt; [17274]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0 17274&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 1369&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 421&#013;&#010;&gt; &amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 7&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 bash&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel:&#013;&#010;&gt; [18089]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0 18089 10824832&amp;nbsp; 7174316&#013;&#010;&gt; &amp;gt; &amp;gt;&amp;nbsp; 14500&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 java&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel:&#013;&#010;&gt; [18348]&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0 18348&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 1017&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 196&#013;&#010;&gt; &amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 6&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; 0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;-998 tail&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Memory cgroup out of memory:&#013;&#010;&gt; Kill&#013;&#010;&gt; &amp;gt; &amp;gt; process 26824 (Window(Tumbling) score 4 or sacrifice child&#013;&#010;&gt; &amp;gt; &amp;gt; Jun 30 21:59:15 flink-tm-1 kernel: Killed process 18089 (java)&#013;&#010;&gt; &amp;gt; &amp;gt; total-vm:43299328kB, anon-rss:28669084kB, file-rss:28180kB,&#013;&#010;&gt; shmem-rss:0kB&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; Looking forward to your reply and help.&#013;&#010;&gt; &amp;gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;gt; Best&#013;&#010;&gt; &amp;gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAFcoNBTU=COf6vR4-GKBHA1hXZk6+g+6q64xkK-6D32-q6QL6w@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_Re6ktnuPm4h067E6ehkaqOGCpLeeUuZ6FJPpBjx4zUtg@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 05:22:06 GMT",
        "subject": "Re: 关于flink sql问题",
        "content": "我理解你只需要把这同一个Mysql表再做一个维表即可。可以写两次DDL，一个给维表用，一个给sink用。&#013;&#010;如果你就觉得它是实时变化的，你可以把维表的cache关掉，保证每次都是获取Mysql中最新的数据就可以了吧？&#013;&#010;&#013;&#010;当然了，在DDL的时候并没有区分这个表是维表还是sink表，具体它是什么类型，只是根据你在SQL里面怎么使用来决定的。&#013;&#010;理论上来讲，你一个DDL可以同时做维表也可以做Sink。（只是它们可能有些配置会不同，分开写两个DDL应该是更清晰一些）&#013;&#010;&#013;&#010;zya &lt;z_yuang@foxmail.com&gt; 于2020年6月30日周二 下午11:26写道：&#013;&#010;&#013;&#010;&gt; 请问下，sink写出的表能做维表吗，因为sink会一直写入，做维表的话会一直动态变化&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &amp;nbsp;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年6月30日(星期二) 晚上11:14&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: 关于flink sql问题&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 应该做一个维表Join就可以了。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; zya &lt;z_yuang@foxmail.com&amp;gt; 于2020年6月30日周二 下午9:02写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; Hi 各位，有个问题想请教一下：&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 目前我有一个功能想使用flink sql来完成，source是kafka，sink是mysql，&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;在写入mysql的时候，我希望能先根据key获取mysql中的数据进行判断，然后决定如何写入数据，请问flink1.10目前能实现这种功能吗？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<CABKuJ_Re6ktnuPm4h067E6ehkaqOGCpLeeUuZ6FJPpBjx4zUtg@mail.gmail.com>"
    },
    {
        "id": "<tencent_14D623349A11A028A84F8892C65E9A5EA209@qq.com>",
        "from": "&quot;zya&quot; &lt;z_yu...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 03:30:18 GMT",
        "subject": "回复： 关于flink sql问题",
        "content": "十分感谢，目前我的解决方法是使用blink-planner，通过temporal table的方式实时加载数据&#013;&#010;&#013;&#010;&#013;&#010;sql语句为：insert into mysql_sink select C.log_id, C.vic from (select A.log_id, case when&#010;B.cnt&amp;gt;0 and A.server&amp;gt;0 then B.cnt+1 else A.server end as vic from (select log_id,&#010;server, PROCTIME() as proctime from kafka_source) A left join mysql_source for SYSTEM_TIME&#010;AS OF A.proctime AS B on A.log_id=B.log_id ) C group by log_id,vic&amp;nbsp;&#013;&#010;&#013;&#010;&#013;&#010;参考文档如下：https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/streaming/temporal_tables.html&#013;&#010;&#013;&#010;&#013;&#010;同时我实现了一个redis 的&amp;nbsp;temporal table，支持source和sink，感兴趣的同学可以一起交流下&#013;&#010;&#013;&#010;祝好~&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月1日(星期三) 中午1:22&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 关于flink sql问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我理解你只需要把这同一个Mysql表再做一个维表即可。可以写两次DDL，一个给维表用，一个给sink用。&#013;&#010;如果你就觉得它是实时变化的，你可以把维表的cache关掉，保证每次都是获取Mysql中最新的数据就可以了吧？&#013;&#010;&#013;&#010;当然了，在DDL的时候并没有区分这个表是维表还是sink表，具体它是什么类型，只是根据你在SQL里面怎么使用来决定的。&#013;&#010;理论上来讲，你一个DDL可以同时做维表也可以做Sink。（只是它们可能有些配置会不同，分开写两个DDL应该是更清晰一些）&#013;&#010;&#013;&#010;zya &lt;z_yuang@foxmail.com&amp;gt; 于2020年6月30日周二 下午11:26写道：&#013;&#010;&#013;&#010;&amp;gt; 请问下，sink写出的表能做维表吗，因为sink会一直写入，做维表的话会一直动态变化&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年6月30日(星期二) 晚上11:14&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: 关于flink sql问题&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 应该做一个维表Join就可以了。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; zya &lt;z_yuang@foxmail.com&amp;amp;gt; 于2020年6月30日周二 下午9:02写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Hi 各位，有个问题想请教一下：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; 目前我有一个功能想使用flink&#010;sql来完成，source是kafka，sink是mysql，&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;在写入mysql的时候，我希望能先根据key获取mysql中的数据进行判断，然后决定如何写入数据，请问flink1.10目前能实现这种功能吗？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; --&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Benchao Li&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "2",
        "reply": "<CABKuJ_Re6ktnuPm4h067E6ehkaqOGCpLeeUuZ6FJPpBjx4zUtg@mail.gmail.com>"
    },
    {
        "id": "<202007011351128475178@163.com>",
        "from": "&quot;tiantingting5435@163.com&quot; &lt;tiantingting5...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 05:51:14 GMT",
        "subject": "flink1.10 用flinksql 写hbase，报错：UpsertStreamTableSink requires that Table has a full primary keys if it is updated.",
        "content": "你好，&#013;&#010;flink1.10，用flinkSQL写hbase，报错：UpsertStreamTableSink requires that Table has a&#010;full primary keys if it is updated.&#013;&#010;看到网上的资料说是，upsertSink的primary key是通过query来推断的，而我的query无法推断出PK，所以报错。说是需要1.10的临时解决方法是加一层group&#010;by，使得query可以推断出 primary key。&#013;&#010;但是，我添加group by以后还是报错，这个问题该怎么解决呢？？到底query是如何推断PK的？？&#013;&#010;&#013;&#010;以下是我的sql语句：&#013;&#010;创建表的语句（字段有点多，还请见谅）&#013;&#010;CREATE TABLE `AssetInfoRiskResultSinkTable` (&#013;&#010;  rowkey string,&#013;&#010;  d ROW(`id` bigint,&#013;&#010;  `user_id` string,&#013;&#010;  `income_no` string ,&#013;&#010;  `nation` string,&#013;&#010;  `card_auth_expiry_time` string ,&#013;&#010;  `user_name` string ,&#013;&#010;  `login_phone` string ,&#013;&#010;  `card_no` string ,&#013;&#010;  `address` string ,&#013;&#010;  `highest_eduction` string ,&#013;&#010;  `is_married` string ,&#013;&#010;  `resident_address` string ,&#013;&#010;  `resident_province` string ,&#013;&#010;  `resident_city` string ,&#013;&#010;  `resident_town` string ,&#013;&#010;  `profession` string ,&#013;&#010;  `job_salary` string ,&#013;&#010;  `company_name` string ,&#013;&#010;  `company_province` string ,&#013;&#010;  `company_city` string ,&#013;&#010;  `company_town` string ,&#013;&#010;  `company_address` string ,&#013;&#010;  `family_name` string ,&#013;&#010;  `family_phone` string ,&#013;&#010;  `workmate_name` string ,&#013;&#010;  `workmate_phone` string ,&#013;&#010;  `bank_card_no` string ,&#013;&#010;  `bank_phone` string ,&#013;&#010;  `device_address` string ,&#013;&#010;  `device_ip` string ,&#013;&#010;  `contacts` string ,&#013;&#010;  `create_user` string ,&#013;&#010;  `create_time` string ,&#013;&#010;  `update_user` string ,&#013;&#010;  `update_time` string,&#013;&#010;  `scene_id` string  ,&#013;&#010;  `access_type` string  ,&#013;&#010;  `status` string  ,&#013;&#010;  `label_id` string  ,&#013;&#010;  `label_name` string  ,&#013;&#010;  `ocr_real_name` string ,&#013;&#010;  `ocr_id_card` string ,&#013;&#010;  `ocr_id_card_address` string,&#013;&#010;  `longitude` string ,&#013;&#010;  `latitude` string ,&#013;&#010;  `imei` string ,&#013;&#010;  `imsi` string ,&#013;&#010;  `mac` string ,&#013;&#010;  `resident_province2` string ,&#013;&#010;  `loan_use` string ,&#013;&#010;  `channel_code` string  ,&#013;&#010;  `channel_name` string  ,&#013;&#010;  `credit_card_number` string  ,&#013;&#010;  `product_type_code` string  ,&#013;&#010;  `product_type_name` string  ,&#013;&#010;  `apply_amount` string  ,&#013;&#010;  `income_time` string  ,&#013;&#010;  `credit_card_phone` string  ,&#013;&#010;  `credit_card_amount` string  ,&#013;&#010;  `period` string  ,&#013;&#010;  `start_work_time` string  ,&#013;&#010;  `register_channel` string  ,&#013;&#010;  `register_channel_name` string  ,&#013;&#010;  `bank_name` string  ,&#013;&#010;  `company_call` string  ,&#013;&#010;  `system_tag` string  ,&#013;&#010;  `is_trans` string  ,&#013;&#010;  `is_dial_confirm` string  ,&#013;&#010;  `is_dial_type` string  ,&#013;&#010;  `is_dial_typeM` string  ,&#013;&#010;  `is_dial_typeHuman` string  ,&#013;&#010;  `user_risk_score` string  ,&#013;&#010;  `upload_imgs` string ,&#013;&#010;  `upload_status` string ,&#013;&#010;  `famliy_relationship` string  ,&#013;&#010;  `work_relationship` string  ,&#013;&#010;  `request_time` string  ,&#013;&#010;  `ac_record` string ,&#013;&#010;  `profession_station` string  ,&#013;&#010;  `mobile_os` string  ,&#013;&#010;  `mobile_type` string  ,&#013;&#010;  `mobile_brand` string  ,&#013;&#010;  `networktype` string  ,&#013;&#010;  `jail_break` string  ,&#013;&#010;  `open_udid` string  ,&#013;&#010;  `simulator` string  ,&#013;&#010;  `idfa` string  ,&#013;&#010;  `idfv` string  ,&#013;&#010;  `device_type` string  ,&#013;&#010;  `credit_card_id_card_no` string  ,&#013;&#010;  `credit_card_username` string  ,&#013;&#010;  `sign_issue_org` string  ,&#013;&#010;  `user_level` string  ,&#013;&#010;  `channel_request_time` string  ,&#013;&#010;  `real_income_no` string ,&#013;&#010;  `org_channel_code` string  ,&#013;&#010;  `qq` string  ,&#013;&#010;  `mail` string  ,&#013;&#010;  `pre_grant_credit_amount` string  ,&#013;&#010;  `pre_grant_credit_term` string  ,&#013;&#010;  `pre_grant_credit_term_unit` string  ,&#013;&#010;  `monthly_repay_amount` string  ,&#013;&#010;  `total_repay_amount` string  ,&#013;&#010;  `xhd_white_list_flag` string  ,&#013;&#010;  `white_list_flag` string  ,&#013;&#010;  `white_list_level` string ,&#013;&#010;  `white_list_type` string ,&#013;&#010;  `bus_type` string  ,&#013;&#010;  `housing_fund_status` string  ,&#013;&#010;  `operator_auth_status` string ,&#013;&#010;  `credit_card_status` string  ,&#013;&#010;  `pboc_credit_status` string ,&#013;&#010;  `bh_url_flag` string,&#013;&#010;  `zmxy_auth_expiry_time` string  ,&#013;&#010;  `operator_auth_expiry_time` string  ,&#013;&#010;  `housing_fund_status_time` string  ,&#013;&#010;  `credit_card_expiry_time` string  ,&#013;&#010;  `pboc_credit_status_time` string  ,&#013;&#010;  `credit_card_bank_name` string  ,&#013;&#010;  `due_limit_unit` string  ,&#013;&#010;  `due_limit` string  ,&#013;&#010;  `loan_amount` string  ,&#013;&#010;  `risk_lead_flag` string ,&#013;&#010;  `period_unit` string  ,&#013;&#010;  `face_score` string  ,&#013;&#010;  `notify_url` string  ,&#013;&#010;  `org_id` string  ,&#013;&#010;  `contract_id` string  ,&#013;&#010;  `birthday` string  ,&#013;&#010;  `zmxy_status` string  ,&#013;&#010;  `is_root` string  ,&#013;&#010;  `is_virtualmachine` string  ,&#013;&#010;  `appnum` string  ,&#013;&#010;  `wifi_ip` string ,&#013;&#010;  `blue_mac` string  ,&#013;&#010;  `wifi_mac` string ,&#013;&#010;  `vpn_ip` string  ,&#013;&#010;  `cell_ip` string ,&#013;&#010;  `true_ip` string  ,&#013;&#010;  `is_helical_accelerator` string  ,&#013;&#010;  `bussiness` string  ,&#013;&#010;  `manual_check` string  ,&#013;&#010;  `has_contacts` string  ,&#013;&#010;  `length_of_residence_year` string  ,&#013;&#010;  `length_of_residence_month` string  ,&#013;&#010;  `gps_province` string  ,&#013;&#010;  `gps_city` string  ,&#013;&#010;  `gps_area` string  ,&#013;&#010;  `gps_detail_address` string  ,&#013;&#010;  `positional_titles` string  ,&#013;&#010;  `work_years` string  ,&#013;&#010;  `work_months` string  ,&#013;&#010;  `max_acceptable_monthly_payment` string  ,&#013;&#010;  `profession_code` string  ,&#013;&#010;  `occupation` string  ,&#013;&#010;  `career_status` string  ,&#013;&#010;  `work_position` string  ,&#013;&#010;  `work_time` string  ,&#013;&#010;  `end_result` string  ) &#013;&#010;  ) with (&#013;&#010;   'connector.type' = 'hbase',&#013;&#010;   'connector.version' = '1.4.3', &#013;&#010;   'connector.table-name' = 'rtest:borrower_related_asset_info_real_time', &#013;&#010;   'connector.zookeeper.quorum' = 'fdw6.fengjr.inc,fdw4.fengjr.inc,fdw5.fengjr.inc,fjr-yz-204-11,fjr-yz-204-13',&#013;&#010;   'connector.zookeeper.znode.parent' = 'hbase_test',&#013;&#010;   'connector.write.buffer-flush.max-size' = '10mb',&#013;&#010;   'connector.write.buffer-flush.max-rows' = '1000', &#013;&#010;   'connector.write.buffer-flush.interval' = '2s'&#013;&#010;  )&#013;&#010;&#013;&#010;插入语句：&#013;&#010;INSERT INTO AssetInfoRiskResultSinkTable&#013;&#010;select&#013;&#010;MD5(real_income_no) as rowkey,&#013;&#010;Row(id,&#013;&#010;        user_id,&#013;&#010;        income_no,&#013;&#010;        nation,&#013;&#010;        card_auth_expiry_time,&#013;&#010;        user_name,&#013;&#010;        phone,&#013;&#010;        id_card,&#013;&#010;        address,&#013;&#010;        highest_eduction,&#013;&#010;        is_married,&#013;&#010;        resident_address,&#013;&#010;        resident_province,&#013;&#010;        resident_city,&#013;&#010;        resident_town,&#013;&#010;        profession,&#013;&#010;        job_salary,&#013;&#010;        company_name,&#013;&#010;        company_province,&#013;&#010;        company_city,&#013;&#010;        company_town,&#013;&#010;        company_address,&#013;&#010;        family_name,&#013;&#010;        family_phone,&#013;&#010;        workmate_name,&#013;&#010;        workmate_phone,&#013;&#010;        bank_card_no,&#013;&#010;        bank_phone,&#013;&#010;        device_address,&#013;&#010;        device_ip,&#013;&#010;        contacts,&#013;&#010;        create_user,&#013;&#010;        create_time,&#013;&#010;        update_user,&#013;&#010;        update_time,&#013;&#010;        scene_id,&#013;&#010;        access_type,&#013;&#010;        status,&#013;&#010;        label_id,&#013;&#010;        label_name,&#013;&#010;        ocr_real_name,&#013;&#010;        ocr_id_card,&#013;&#010;        ocr_id_card_address,&#013;&#010;        longitude,&#013;&#010;        latitude,&#013;&#010;        imei,&#013;&#010;        imsi,&#013;&#010;        mac,&#013;&#010;        resident_province2,&#013;&#010;        loan_use,&#013;&#010;        channel_code,&#013;&#010;        channel_name,&#013;&#010;        credit_card_number,&#013;&#010;        product_type_code,&#013;&#010;        product_type_name,&#013;&#010;        apply_amount,&#013;&#010;        income_time,&#013;&#010;        credit_card_phone,&#013;&#010;        credit_card_amount,&#013;&#010;        `period`,&#013;&#010;        start_work_time,&#013;&#010;        register_channel,&#013;&#010;        register_channel_name,&#013;&#010;        bank_name,&#013;&#010;        company_call,&#013;&#010;        system_tag,&#013;&#010;        is_trans,&#013;&#010;        is_dial_confirm,&#013;&#010;        is_dial_type,&#013;&#010;        is_dial_typem,&#013;&#010;        is_dial_typeHuman,&#013;&#010;        user_risk_score,&#013;&#010;        upload_imgs,&#013;&#010;        upload_status,&#013;&#010;        famliy_relationship,&#013;&#010;        work_relationship,&#013;&#010;        request_time,&#013;&#010;        ac_record,&#013;&#010;        profession_station,&#013;&#010;        mobile_os,&#013;&#010;        mobile_type,&#013;&#010;        mobile_brand,&#013;&#010;        networktype,&#013;&#010;        jail_break,&#013;&#010;        open_udid,&#013;&#010;        simulator,&#013;&#010;        idfa,&#013;&#010;        idfv,&#013;&#010;        device_type,&#013;&#010;        credit_card_id_card_no,&#013;&#010;        credit_card_username,&#013;&#010;        sign_issue_org,&#013;&#010;        user_level,&#013;&#010;        channel_request_time,&#013;&#010;        real_income_no,&#013;&#010;        org_channel_code,&#013;&#010;        qq,&#013;&#010;        mail,&#013;&#010;        pre_grant_credit_amount,&#013;&#010;        pre_grant_credit_term,&#013;&#010;        pre_grant_credit_term_unit,&#013;&#010;        monthly_repay_amount,&#013;&#010;        total_repay_amount,&#013;&#010;        xhd_white_list_flag,&#013;&#010;        white_list_flag,&#013;&#010;        white_list_level,&#013;&#010;        white_list_type,&#013;&#010;        bus_type,&#013;&#010;        housing_fund_status,&#013;&#010;        operator_auth_status,&#013;&#010;        credit_card_status,&#013;&#010;        pboc_credit_status,&#013;&#010;        bh_url_flag,&#013;&#010;        zmxy_auth_expiry_time,&#013;&#010;        operator_auth_expiry_time,&#013;&#010;        housing_fund_status_time,&#013;&#010;        credit_card_expiry_time,&#013;&#010;        pboc_credit_status_time,&#013;&#010;        credit_card_bank_name,&#013;&#010;        due_limit_unit,&#013;&#010;        due_limit,&#013;&#010;        loan_amount,&#013;&#010;        risk_lead_flag,&#013;&#010;        period_unit,&#013;&#010;        face_score,&#013;&#010;        notify_url,&#013;&#010;        org_id,&#013;&#010;        contract_id,&#013;&#010;        birthday,&#013;&#010;        zmxy_status,&#013;&#010;        is_root,&#013;&#010;        is_virtualmachine,&#013;&#010;        appnum,&#013;&#010;        wifi_ip,&#013;&#010;        blue_mac,&#013;&#010;        wifi_mac,&#013;&#010;        vpn_ip,&#013;&#010;        cell_ip,&#013;&#010;        true_ip,&#013;&#010;        is_helical_accelerator,&#013;&#010;        bussiness,&#013;&#010;        manual_check,&#013;&#010;        has_contacts,&#013;&#010;        length_of_residence_year,&#013;&#010;        length_of_residence_month,&#013;&#010;        gps_province,&#013;&#010;        gps_city,&#013;&#010;        gps_area,&#013;&#010;        gps_detail_address,&#013;&#010;        positional_titles,&#013;&#010;        work_years,&#013;&#010;        work_months,&#013;&#010;        max_acceptable_monthly_payment,&#013;&#010;        profession_code,&#013;&#010;        occupation,&#013;&#010;        career_status,&#013;&#010;        work_position,&#013;&#010;        work_time,&#013;&#010;        end_result) as d&#013;&#010;from (&#013;&#010;SELECT&#013;&#010;        real_income_no,&#013;&#010;        id,&#013;&#010;        user_id,&#013;&#010;        income_no,&#013;&#010;        nation,&#013;&#010;        card_auth_expiry_time,&#013;&#010;        if (user_name is not null and user_name &lt; '',unifedEncryption(user_name),user_name)&#010;as user_name,&#013;&#010;        if(login_phone is not null and login_phone &lt; '',unifedEncryption(login_phone),login_phone)&#010;as phone,&#013;&#010;        if(card_no is not null and card_no &lt; '',unifedEncryption(card_no),card_no) as id_card,&#013;&#010;        address,&#013;&#010;        highest_eduction,&#013;&#010;        is_married,&#013;&#010;        resident_address,&#013;&#010;        resident_province,&#013;&#010;        resident_city,&#013;&#010;        resident_town,&#013;&#010;        profession,&#013;&#010;        job_salary,&#013;&#010;        company_name,&#013;&#010;        company_province,&#013;&#010;        company_city,&#013;&#010;        company_town,&#013;&#010;        company_address,&#013;&#010;        if(family_name is not null and family_name &lt; '',unifedEncryption(family_name),family_name)&#010;as family_name,&#013;&#010;        if(family_phone is not null and family_phone &lt; '',unifedEncryption(family_phone),family_phone)&#010;as family_phone,&#013;&#010;        if(workmate_name is not null and workmate_name &lt; '',unifedEncryption(workmate_name),workmate_name)&#010;as workmate_name,&#013;&#010;        if(workmate_phone is not null and workmate_phone &lt; '',unifedEncryption(workmate_phone),workmate_phone)&#010;as workmate_phone,&#013;&#010;        if(bank_card_no is not null and bank_card_no &lt; '',unifedEncryption(bank_card_no),bank_card_no)&#010;as bank_card_no,&#013;&#010;        if(bank_phone is not null and bank_phone &lt; '',unifedEncryption(bank_phone),bank_phone)&#010;as bank_phone,&#013;&#010;        device_address,&#013;&#010;        device_ip,&#013;&#010;        contacts,&#013;&#010;        create_user,&#013;&#010;        create_time,&#013;&#010;        update_user,&#013;&#010;        update_time,&#013;&#010;        scene_id,&#013;&#010;        access_type,&#013;&#010;        status,&#013;&#010;        label_id,&#013;&#010;        label_name,&#013;&#010;        if(ocr_real_name is not null and ocr_real_name &lt; '',unifedEncryption(ocr_real_name),ocr_real_name)&#010;as ocr_real_name,&#013;&#010;        if(ocr_id_card is not null and ocr_id_card &lt; '',unifedEncryption(ocr_id_card),ocr_id_card)&#010;as ocr_id_card,&#013;&#010;        ocr_id_card_address,&#013;&#010;        longitude,&#013;&#010;        latitude,&#013;&#010;        imei,&#013;&#010;        imsi,&#013;&#010;        mac,&#013;&#010;        resident_province2,&#013;&#010;        loan_use,&#013;&#010;        channel_code,&#013;&#010;        channel_name,&#013;&#010;        if(credit_card_number is not null and credit_card_number &lt; '',unifedEncryption(credit_card_number),credit_card_number)&#010;as credit_card_number,&#013;&#010;        product_type_code,&#013;&#010;        product_type_name,&#013;&#010;        apply_amount,&#013;&#010;        income_time,&#013;&#010;        if(credit_card_phone is not null and credit_card_phone &lt; '',unifedEncryption(credit_card_phone),credit_card_phone)&#010;as credit_card_phone,&#013;&#010;        credit_card_amount,&#013;&#010;        `period`,&#013;&#010;        start_work_time,&#013;&#010;        register_channel,&#013;&#010;        register_channel_name,&#013;&#010;        bank_name,&#013;&#010;        if(company_call is not null and company_call &lt; '',unifedEncryption(company_call),company_call)&#010;as company_call,&#013;&#010;        system_tag,&#013;&#010;        is_trans,&#013;&#010;        is_dial_confirm,&#013;&#010;        is_dial_type,&#013;&#010;        is_dial_typeM as is_dial_typem,&#013;&#010;        is_dial_typeHuman,&#013;&#010;        user_risk_score,&#013;&#010;        upload_imgs,&#013;&#010;        upload_status,&#013;&#010;        famliy_relationship,&#013;&#010;        work_relationship,&#013;&#010;        request_time,&#013;&#010;        ac_record,&#013;&#010;        profession_station,&#013;&#010;        mobile_os,&#013;&#010;        mobile_type,&#013;&#010;        mobile_brand,&#013;&#010;        networktype,&#013;&#010;        jail_break,&#013;&#010;        open_udid,&#013;&#010;        simulator,&#013;&#010;        idfa,&#013;&#010;        idfv,&#013;&#010;        device_type,&#013;&#010;        credit_card_id_card_no,&#013;&#010;        credit_card_username,&#013;&#010;        sign_issue_org,&#013;&#010;        user_level,&#013;&#010;        channel_request_time,&#013;&#010;        org_channel_code,&#013;&#010;        qq,&#013;&#010;        mail,&#013;&#010;        pre_grant_credit_amount,&#013;&#010;        pre_grant_credit_term,&#013;&#010;        pre_grant_credit_term_unit,&#013;&#010;        monthly_repay_amount,&#013;&#010;        total_repay_amount,&#013;&#010;        xhd_white_list_flag,&#013;&#010;        white_list_flag,&#013;&#010;        white_list_level,&#013;&#010;        white_list_type,&#013;&#010;        bus_type,&#013;&#010;        housing_fund_status,&#013;&#010;        operator_auth_status,&#013;&#010;        credit_card_status,&#013;&#010;        pboc_credit_status,&#013;&#010;        bh_url_flag,&#013;&#010;        zmxy_auth_expiry_time,&#013;&#010;        operator_auth_expiry_time,&#013;&#010;        housing_fund_status_time,&#013;&#010;        credit_card_expiry_time,&#013;&#010;        pboc_credit_status_time,&#013;&#010;        credit_card_bank_name,&#013;&#010;        due_limit_unit,&#013;&#010;        due_limit,&#013;&#010;        loan_amount,&#013;&#010;        risk_lead_flag,&#013;&#010;        period_unit,&#013;&#010;        face_score,&#013;&#010;        notify_url,&#013;&#010;        org_id,&#013;&#010;        contract_id,&#013;&#010;        birthday,&#013;&#010;        zmxy_status,&#013;&#010;        is_root,&#013;&#010;        is_virtualmachine,&#013;&#010;        appnum,&#013;&#010;        wifi_ip,&#013;&#010;        blue_mac,&#013;&#010;        wifi_mac,&#013;&#010;        vpn_ip,&#013;&#010;        cell_ip,&#013;&#010;        true_ip,&#013;&#010;        is_helical_accelerator,&#013;&#010;        bussiness,&#013;&#010;        manual_check,&#013;&#010;        has_contacts,&#013;&#010;        length_of_residence_year,&#013;&#010;        length_of_residence_month,&#013;&#010;        gps_province,&#013;&#010;        gps_city,&#013;&#010;        gps_area,&#013;&#010;        gps_detail_address,&#013;&#010;        positional_titles,&#013;&#010;        work_years,&#013;&#010;        work_months,&#013;&#010;        max_acceptable_monthly_payment,&#013;&#010;        profession_code,&#013;&#010;        occupation,&#013;&#010;        career_status,&#013;&#010;        work_position,&#013;&#010;        work_time,&#013;&#010;        end_result&#013;&#010;FROM&#013;&#010;        (&#013;&#010;                SELECT&#013;&#010;                        *, ROW_NUMBER () OVER (&#013;&#010;                                PARTITION BY id&#013;&#010;                                ORDER BY&#013;&#010;                                        update_time DESC&#013;&#010;                        ) AS rowNum&#013;&#010;                FROM&#013;&#010;                        AssetInfoRiskResultTable&#013;&#010;        )&#013;&#010;WHERE&#013;&#010;        rowNum = 1)&#013;&#010;&#013;&#010;&#013;&#010;tiantingting5435@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007011351128475178@163.com>"
    },
    {
        "id": "<5FB249EC-4470-4510-BD41-EA51A7CEA642@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 10:28:10 GMT",
        "subject": "Re: flink1.10 用flinksql 写hbase，报错：UpsertStreamTableSink requires that Table has a full primary keys if it is updated.",
        "content": "Hello,&#010;&#010;你试下 MD5(real_income_no) as rowkey 放在query的里层，最外层的group by直接用&#010;rowkey试下， Flink 1.11 之后支持在 table里声明 PK, 1.11后就不用推导了。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月1日，13:51，tiantingting5435@163.com 写道：&#010;&gt; &#010;&gt; MD5(real_income_no) as rowkey,&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<202007011351128475178@163.com>"
    },
    {
        "id": "<202007031442016674780@163.com>",
        "from": "&quot;tiantingting5435@163.com&quot; &lt;tiantingting5...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:42:03 GMT",
        "subject": "回复: Re: flink1.10 用flinksql 写hbase，报错：UpsertStreamTableSink requires that Table has a full primary keys if it is updated.",
        "content": "Hello,&#013;&#010;&#013;&#010;你试下 MD5(real_income_no) as rowkey 放在query的里层，最外层的group by直接用&#010;rowkey试下， Flink 1.11 之后支持在 table里声明 PK, 1.11后就不用推导了。&#013;&#010;&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;&gt; 在 2020年7月1日，13:51，tiantingting5435@163.com 写道：&#013;&#010;&gt; &#013;&#010;&gt; MD5(real_income_no) as rowkey,&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<202007011351128475178@163.com>"
    },
    {
        "id": "<CAP+gf35cq6QGAa9AKt_u-n0R1wn7B29NDBjBuHpcRnAPd0L=EQ@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 06:19:24 GMT",
        "subject": "Re: 关于local cluster的问题",
        "content": "start-cluster.sh每次就是会启动一个Standalone集群的，由于都是一个flink conf，所以新起的JM&#013;&#010;肯定会因为端口冲突起不来，TM会注册在之前已经running的JM上。&#013;&#010;如果你只是测试，用完以后，需要stop-cluster.sh停掉&#013;&#010;&#013;&#010;如果是想在一个JVM里面进行测试，那可以用MiniCluster，所有的组件都会以线程模式启动&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;naisili Yuan &lt;yuanlong1990@gmail.com&gt; 于2020年6月30日周二 下午7:09写道：&#013;&#010;&#013;&#010;&gt; 不好意思没说清楚，跟提交任务没关系，只是执行start-cluster.sh后taskmanager就自动加一&#013;&#010;&gt;&#013;&#010;&gt; 发自我的iPhone&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年6月30日，18:54，\"17610775726@163.com\" &lt;17610775726@163.com&gt;&#010;写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ﻿hi&#013;&#010;&gt; &gt; 你这个问题没有描述清楚啊 是提交一个任务jm就会自动启动一个?&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best&#013;&#010;&gt; &gt; JasonLee&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 发件人： naisili Yuan&#013;&#010;&gt; &gt; 发送时间： 2020-06-30 18:29&#013;&#010;&gt; &gt; 收件人： user-zh&#013;&#010;&gt; &gt; 主题： 关于local cluster的问题&#013;&#010;&gt; &gt; Hi all&#013;&#010;&gt; &gt; 我这边有写一个java服务去自动拉起本地flink&#013;&#010;&gt; cluster（单机模式）用来调试使用。我是直接调用的bin/start-cluster.sh脚本。&#013;&#010;&gt; &gt;&#013;&#010;&gt; 现在问题是每次重新发布服务之后，发现这个启动的会话jobmanager会自动增加一个，导致slots总数越来越高。研究半天始终没找到原因，希望获得帮助！&#013;&#010;&gt; &gt; flink版本1.10.0&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAP+gf35cq6QGAa9AKt_u-n0R1wn7B29NDBjBuHpcRnAPd0L=EQ@mail.gmail.com>"
    },
    {
        "id": "<tencent_8129B800292B7F21C408AD07AECEAD244905@qq.com>",
        "from": "&quot;静谧雨寒&quot; &lt;freedom0...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 06:33:28 GMT",
        "subject": "flink sql ddl  CREATE TABLE kafka011 sink 如何开启事务exactly-once？",
        "content": "&amp;nbsp;flink sql CREATE TABLE kafka sink表，开启checkpoint后，如何配置sql sink表使用两阶事务提交，exactly-once一致性保证&#010;？&#013;&#010;官档说法：&#013;&#010;Consistency guarantees: By default, a Kafka sink ingests data with at-least-once guarantees&#010;into a Kafka topic if the query is executed with checkpointing enabled.，&amp;nbsp;&amp;nbsp;&#013;&#010;CREATE TABLE 默认是 at-least-once",
        "depth": "0",
        "reply": "<tencent_8129B800292B7F21C408AD07AECEAD244905@qq.com>"
    },
    {
        "id": "<3f7ce5dc-b565-44cd-aa2f-443a62cf1716.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 07:13:21 GMT",
        "subject": "回复：flink sql ddl  CREATE TABLE kafka011 sink 如何开启事务exactly-once？",
        "content": "你好,可以尝试自定义实现Kafka011TableSourceSinkFactory和Kafka011TableSink来实现exactly-once&#010;&#010;Kafka011TableSink&#010;&#010;&#010;@Override&#010;protected SinkFunction&lt;Row&gt; createKafkaProducer(&#010;      String topic,&#010;      Properties properties,&#010;      SerializationSchema&lt;Row&gt; serializationSchema,&#010;      Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner) {&#010;   return new FlinkKafkaProducer011&lt;&gt;(&#010;      topic,&#010;      new KeyedSerializationSchemaWrapper&lt;&gt;(serializationSchema),&#010;      properties,&#010;      partitioner,&#010;      FlinkKafkaProducer011.Semantic.EXACTLY_ONCE,&#010;      5);&#010;}&#010;如果想要修改配置的话,具体可以参考KafkaTableSourceSinkFactoryBase&#010;&#010;参考: https://jxeditor.github.io/2020/06/11/FlinkSQL%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%88%9B%E5%BB%BA%E8%A1%A8%E8%AF%AD%E5%8F%A5%E6%97%B6%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/&#010;------------------------------------------------------------------&#010;发件人：静谧雨寒 &lt;freedom0083@vip.qq.com&gt;&#010;发送时间：2020年7月1日(星期三) 14:33&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：flink sql ddl CREATE TABLE kafka011 sink 如何开启事务exactly-once？&#010;&#010;&amp;nbsp;flink sql CREATE TABLE kafka sink表，开启checkpoint后，如何配置sql sink表使用两阶事务提交，exactly-once一致性保证&#010;？&#010;官档说法：&#010;Consistency guarantees: By default, a Kafka sink ingests data with at-least-once guarantees&#010;into a Kafka topic if the query is executed with checkpointing enabled.，&amp;nbsp;&amp;nbsp;&#010;CREATE TABLE 默认是 at-least-once&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_8129B800292B7F21C408AD07AECEAD244905@qq.com>"
    },
    {
        "id": "<CA+bvdVB-bE6FGu9ocwYTKJvnNwZyN0jWQRtFm7MEUyqOa4Y_KQ@mail.gmail.com>",
        "from": "方盛凯 &lt;fskm...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 07:31:15 GMT",
        "subject": "Re: flink sql ddl CREATE TABLE kafka011 sink 如何开启事务exactly-once？",
        "content": "我们正准备开发这个功能，详情可以参考：https://issues.apache.org/jira/browse/FLINK-15221&#013;&#010;&#013;&#010;夏帅 &lt;jkillers@dingtalk.com.invalid&gt; 于2020年7月1日周三 下午3:13写道：&#013;&#010;&#013;&#010;&gt; 你好,可以尝试自定义实现Kafka011TableSourceSinkFactory和Kafka011TableSink来实现exactly-once&#013;&#010;&gt;&#013;&#010;&gt; Kafka011TableSink&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; @Override&#013;&#010;&gt; protected SinkFunction&lt;Row&gt; createKafkaProducer(&#013;&#010;&gt;       String topic,&#013;&#010;&gt;       Properties properties,&#013;&#010;&gt;       SerializationSchema&lt;Row&gt; serializationSchema,&#013;&#010;&gt;       Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner) {&#013;&#010;&gt;    return new FlinkKafkaProducer011&lt;&gt;(&#013;&#010;&gt;       topic,&#013;&#010;&gt;       new KeyedSerializationSchemaWrapper&lt;&gt;(serializationSchema),&#013;&#010;&gt;       properties,&#013;&#010;&gt;       partitioner,&#013;&#010;&gt;       FlinkKafkaProducer011.Semantic.EXACTLY_ONCE,&#013;&#010;&gt;       5);&#013;&#010;&gt; }&#013;&#010;&gt; 如果想要修改配置的话,具体可以参考KafkaTableSourceSinkFactoryBase&#013;&#010;&gt;&#013;&#010;&gt; 参考:&#013;&#010;&gt; https://jxeditor.github.io/2020/06/11/FlinkSQL%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%88%9B%E5%BB%BA%E8%A1%A8%E8%AF%AD%E5%8F%A5%E6%97%B6%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/&#013;&#010;&gt; ------------------------------------------------------------------&#013;&#010;&gt; 发件人：静谧雨寒 &lt;freedom0083@vip.qq.com&gt;&#013;&#010;&gt; 发送时间：2020年7月1日(星期三) 14:33&#013;&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主 题：flink sql ddl CREATE TABLE kafka011 sink 如何开启事务exactly-once？&#013;&#010;&gt;&#013;&#010;&gt; &amp;nbsp;flink sql CREATE TABLE kafka sink表，开启checkpoint后，如何配置sql&#013;&#010;&gt; sink表使用两阶事务提交，exactly-once一致性保证 ？&#013;&#010;&gt; 官档说法：&#013;&#010;&gt; Consistency guarantees: By default, a Kafka sink ingests data with&#013;&#010;&gt; at-least-once guarantees into a Kafka topic if the query is executed with&#013;&#010;&gt; checkpointing enabled.，&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; CREATE TABLE 默认是 at-least-once&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_8129B800292B7F21C408AD07AECEAD244905@qq.com>"
    },
    {
        "id": "<tencent_137C96D302B7D146CF645D4449A252564106@qq.com>",
        "from": "&quot;静谧雨寒&quot; &lt;freedom0...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 07:41:12 GMT",
        "subject": "回复： flink sql ddl CREATE TABLE kafka011 sink 如何开启事务exactly-once？",
        "content": "感谢，看了下issues, Fix Version/s:None ，不知何时才能加上，还是老老实实用dataStream&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"方盛凯\"&lt;fskmine@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月1日(星期三) 下午3:31&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;\"夏帅\"&lt;jkillers@dingtalk.com&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink sql ddl CREATE TABLE kafka011 sink 如何开启事务exactly-once？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我们正准备开发这个功能，详情可以参考：https://issues.apache.org/jira/browse/FLINK-15221&#013;&#010;&#013;&#010;夏帅 &lt;jkillers@dingtalk.com.invalid&amp;gt; 于2020年7月1日周三 下午3:13写道：&#013;&#010;&#013;&#010;&amp;gt; 你好,可以尝试自定义实现Kafka011TableSourceSinkFactory和Kafka011TableSink来实现exactly-once&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Kafka011TableSink&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; @Override&#013;&#010;&amp;gt; protected SinkFunction&lt;Row&amp;gt; createKafkaProducer(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; String topic,&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Properties properties,&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SerializationSchema&lt;Row&amp;gt;&#010;serializationSchema,&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Optional&lt;FlinkKafkaPartitioner&lt;Row&amp;gt;&amp;gt;&#010;partitioner) {&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; return new FlinkKafkaProducer011&lt;&amp;gt;(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; topic,&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; new KeyedSerializationSchemaWrapper&lt;&amp;gt;(serializationSchema),&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; properties,&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; partitioner,&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FlinkKafkaProducer011.Semantic.EXACTLY_ONCE,&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 5);&#013;&#010;&amp;gt; }&#013;&#010;&amp;gt; 如果想要修改配置的话,具体可以参考KafkaTableSourceSinkFactoryBase&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 参考:&#013;&#010;&amp;gt; https://jxeditor.github.io/2020/06/11/FlinkSQL%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%88%9B%E5%BB%BA%E8%A1%A8%E8%AF%AD%E5%8F%A5%E6%97%B6%E7%9A%84%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/&#013;&#010;&amp;gt; ------------------------------------------------------------------&#013;&#010;&amp;gt; 发件人：静谧雨寒 &lt;freedom0083@vip.qq.com&amp;gt;&#013;&#010;&amp;gt; 发送时间：2020年7月1日(星期三) 14:33&#013;&#010;&amp;gt; 收件人：user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;&amp;gt; 主 题：flink sql ddl CREATE TABLE kafka011 sink 如何开启事务exactly-once？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;nbsp;flink sql CREATE TABLE kafka sink表，开启checkpoint后，如何配置sql&#013;&#010;&amp;gt; sink表使用两阶事务提交，exactly-once一致性保证 ？&#013;&#010;&amp;gt; 官档说法：&#013;&#010;&amp;gt; Consistency guarantees: By default, a Kafka sink ingests data with&#013;&#010;&amp;gt; at-least-once guarantees into a Kafka topic if the query is executed with&#013;&#010;&amp;gt; checkpointing enabled.，&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; CREATE TABLE 默认是 at-least-once&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;",
        "depth": "3",
        "reply": "<tencent_8129B800292B7F21C408AD07AECEAD244905@qq.com>"
    },
    {
        "id": "<13cae5b5.8040.17309bb9b09.Coremail.iam545@163.com>",
        "from": "boss_大数据开发_史文龙 &lt;iam...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 09:35:56 GMT",
        "subject": "来自boss_大数据开发_史文龙的邮件",
        "content": "xxx-unsubscribe@flink.apache.org ",
        "depth": "0",
        "reply": "<13cae5b5.8040.17309bb9b09.Coremail.iam545@163.com>"
    },
    {
        "id": "<9FD58FA6-172D-4912-BB5A-92789180DA2D@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 10:15:27 GMT",
        "subject": "Re: 来自boss_大数据开发_史文龙的邮件",
        "content": "Hello&#010;&#010;如果要取消订阅 FLink中文社区的邮件，直接发送任意内容的邮件到 user-zh&#010;&lt;mailto:user-zh@flink.apache.org&gt;@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt;&#010;即可，邮件的取消和订阅可以参考[1]&#010;&#010;祝好&#010;&#010;[1] https://flink.apache.org/community.html#mailing-lists &lt;https://flink.apache.org/community.html#mailing-lists&gt;&#010;&#010;&gt; 在 2020年7月1日，17:35，boss_大数据开发_史文龙 &lt;iam545@163.com&gt; 写道：&#010;&gt; &#010;&gt; xxx-unsubscribe@flink.apache.org &lt;mailto:xxx-unsubscribe@flink.apache.org&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<13cae5b5.8040.17309bb9b09.Coremail.iam545@163.com>"
    },
    {
        "id": "<tencent_39A18FD248F33BF93EBFE746CE4627D4FD09@qq.com>",
        "from": "&quot;MuChen&quot; &lt;9329...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 11:50:14 GMT",
        "subject": "flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state",
        "content": "hi，all：&#013;&#010;&#013;&#010;我根据这篇博客https://blog.csdn.net/cndotaci/article/details/106870413的介绍，配置了flink基于yarn的高可用，测试时发现配置的任务失败重试2次没有生效，我测试到第6次时，任务仍然能够被yarn拉起。&#013;&#010;&#013;&#010;请问各位大佬&#013;&#010;&#013;&#010;1. 下面配置中的重试次数为什么没有生效？&#013;&#010;&#013;&#010;2. 通过HA拉起的任务，是否可以重用上次任务失败时的state？&#013;&#010;&#013;&#010;flink版本：1.10.0&#013;&#010;&#013;&#010;flink-conf.yaml配置：&#013;&#010;$ grep -v ^# flink-conf.yaml |grep -v ^$ jobmanager.rpc.address: localhost jobmanager.rpc.port:&#010;6123 jobmanager.heap.size: 1024m taskmanager.memory.process.size: 1568m taskmanager.numberOfTaskSlots:&#010;1 parallelism.default: 1 high-availability: zookeeper high-availability.storageDir: hdfs:///flink/ha/&#010;high-availability.zookeeper.quorum: uhadoop-op3raf-master1,uhadoop-op3raf-master2,uhadoop-op3raf-core1&#010;state.checkpoints.dir: hdfs:///flink/checkpoint state.savepoints.dir: hdfs:///flink/flink-savepoints&#010;state.checkpoints.num-retained:60 state.backend.incremental: true jobmanager.execution.failover-strategy:&#010;region jobmanager.archive.fs.dir: hdfs:///flink/flink-jobs/ historyserver.web.port: 8082 historyserver.archive.fs.dir:&#010;hdfs:///flink/flink-jobs/ historyserver.archive.fs.refresh-interval: 10000 # HA重试次数&#010;yarn.application-attempts: 2 &#013;&#010;ssh到jm节点，手动kill任务的操作日志：&#013;&#010;[root@uhadoop-op3raf-task48 ~]# jps 34785 YarnTaskExecutorRunner 16853 YarnTaskExecutorRunner&#010;17527 PrestoServer 33289 YarnTaskExecutorRunner 18026 YarnJobClusterEntrypoint 20283 Jps 39599&#010;NodeManager [root@uhadoop-op3raf-task48 ~]# kill -9 18026 [root@uhadoop-op3raf-task48 ~]#&#010;jps 34785 YarnTaskExecutorRunner 16853 -- process information unavailable 17527 PrestoServer&#010;21383 Jps 33289 YarnTaskExecutorRunner 20412 YarnJobClusterEntrypoint 39599 NodeManager [root@uhadoop-op3raf-task48&#010;~]# kill -9 20412 [root@uhadoop-op3raf-task48 ~]# jps 34785 YarnTaskExecutorRunner 21926 YarnJobClusterEntrypoint&#010;23207 Jps 17527 PrestoServer 33289 YarnTaskExecutorRunner 39599 NodeManager [root@uhadoop-op3raf-task48&#010;~]# kill -9 21926 [root@uhadoop-op3raf-task48 ~]# jps 34785 YarnTaskExecutorRunner 23318 YarnJobClusterEntrypoint&#010;26279 Jps 17527 PrestoServer 33289 YarnTaskExecutorRunner 39599 NodeManager [root@uhadoop-op3raf-task48&#010;~]# kill -9 23318",
        "depth": "0",
        "reply": "<tencent_39A18FD248F33BF93EBFE746CE4627D4FD09@qq.com>"
    },
    {
        "id": "<CABFgzE6PLL71LU2_0ejKFLqbx_yFx-cP5LZ1wFKDACeTf_tYWQ@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 12:17:31 GMT",
        "subject": "Re: flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state",
        "content": "hi, muchen&#013;&#010;1. yarn.application-attempts&#013;&#010;这个参数与另外一个参数有关系：yarn.application-attempt-failures-validity-interval，大概意思是需要在设置的这个interval内失败重试多少次，才认为flink&#013;&#010;job是失败的，如果超过这个interval，就会重新开始计数。打个比方，yarn.application-attempts:&#013;&#010;2，yarn.application-attempt-failures-validity-interval =&#013;&#010;10000（默认值，10s），只有在10s内 flink job 失败重启2次才会真正的失败。&#013;&#010;2. 如果配置了checkpoint是会重用上次任务失败的state。&#013;&#010;&#013;&#010;这是我个人的理解，有疑问大家一起讨论&#013;&#010;&#013;&#010;MuChen &lt;9329748@qq.com&gt; 于2020年7月1日周三 下午7:50写道：&#013;&#010;&#013;&#010;&gt; hi，all：&#013;&#010;&gt;&#013;&#010;&gt; 我根据这篇博客https://blog.csdn.net/cndotaci/article/details/106870413&#013;&#010;&gt; 的介绍，配置了flink基于yarn的高可用，测试时发现配置的任务失败重试2次没有生效，我测试到第6次时，任务仍然能够被yarn拉起。&#013;&#010;&gt;&#013;&#010;&gt; 请问各位大佬&#013;&#010;&gt;&#013;&#010;&gt; 1. 下面配置中的重试次数为什么没有生效？&#013;&#010;&gt;&#013;&#010;&gt; 2. 通过HA拉起的任务，是否可以重用上次任务失败时的state？&#013;&#010;&gt;&#013;&#010;&gt; flink版本：1.10.0&#013;&#010;&gt;&#013;&#010;&gt; flink-conf.yaml配置：&#013;&#010;&gt; $ grep -v ^# flink-conf.yaml |grep -v ^$ jobmanager.rpc.address: localhost&#013;&#010;&gt; jobmanager.rpc.port: 6123 jobmanager.heap.size: 1024m&#013;&#010;&gt; taskmanager.memory.process.size: 1568m taskmanager.numberOfTaskSlots: 1&#013;&#010;&gt; parallelism.default: 1 high-availability: zookeeper&#013;&#010;&gt; high-availability.storageDir: hdfs:///flink/ha/&#013;&#010;&gt; high-availability.zookeeper.quorum:&#013;&#010;&gt; uhadoop-op3raf-master1,uhadoop-op3raf-master2,uhadoop-op3raf-core1&#013;&#010;&gt; state.checkpoints.dir: hdfs:///flink/checkpoint state.savepoints.dir:&#013;&#010;&gt; hdfs:///flink/flink-savepoints state.checkpoints.num-retained:60&#013;&#010;&gt; state.backend.incremental: true jobmanager.execution.failover-strategy:&#013;&#010;&gt; region jobmanager.archive.fs.dir: hdfs:///flink/flink-jobs/&#013;&#010;&gt; historyserver.web.port: 8082 historyserver.archive.fs.dir:&#013;&#010;&gt; hdfs:///flink/flink-jobs/ historyserver.archive.fs.refresh-interval: 10000&#013;&#010;&gt; # HA重试次数 yarn.application-attempts: 2&#013;&#010;&gt; ssh到jm节点，手动kill任务的操作日志：&#013;&#010;&gt; [root@uhadoop-op3raf-task48 ~]# jps 34785 YarnTaskExecutorRunner 16853&#013;&#010;&gt; YarnTaskExecutorRunner 17527 PrestoServer 33289 YarnTaskExecutorRunner&#013;&#010;&gt; 18026 YarnJobClusterEntrypoint 20283 Jps 39599 NodeManager&#013;&#010;&gt; [root@uhadoop-op3raf-task48 ~]# kill -9 18026 [root@uhadoop-op3raf-task48&#013;&#010;&gt; ~]# jps 34785 YarnTaskExecutorRunner 16853 -- process information&#013;&#010;&gt; unavailable 17527 PrestoServer 21383 Jps 33289 YarnTaskExecutorRunner 20412&#013;&#010;&gt; YarnJobClusterEntrypoint 39599 NodeManager [root@uhadoop-op3raf-task48&#013;&#010;&gt; ~]# kill -9 20412 [root@uhadoop-op3raf-task48 ~]# jps 34785&#013;&#010;&gt; YarnTaskExecutorRunner 21926 YarnJobClusterEntrypoint 23207 Jps 17527&#013;&#010;&gt; PrestoServer 33289 YarnTaskExecutorRunner 39599 NodeManager&#013;&#010;&gt; [root@uhadoop-op3raf-task48 ~]# kill -9 21926 [root@uhadoop-op3raf-task48&#013;&#010;&gt; ~]# jps 34785 YarnTaskExecutorRunner 23318 YarnJobClusterEntrypoint 26279&#013;&#010;&gt; Jps 17527 PrestoServer 33289 YarnTaskExecutorRunner 39599 NodeManager&#013;&#010;&gt; [root@uhadoop-op3raf-task48 ~]# kill -9 23318&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_39A18FD248F33BF93EBFE746CE4627D4FD09@qq.com>"
    },
    {
        "id": "<tencent_E19FC173D367601311FE8147ACA44E9D7E08@qq.com>",
        "from": "&quot;MuChen&quot; &lt;9329...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 14:45:52 GMT",
        "subject": "回复： flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state",
        "content": "hi，王松：&#013;&#010;受教了，多谢指点！&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;MuChen.&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"王松\"&lt;sdlcwangsong11@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月1日(星期三) 晚上8:17&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hi, muchen&#013;&#010;1. yarn.application-attempts&#013;&#010;这个参数与另外一个参数有关系：yarn.application-attempt-failures-validity-interval，大概意思是需要在设置的这个interval内失败重试多少次，才认为flink&#013;&#010;job是失败的，如果超过这个interval，就会重新开始计数。打个比方，yarn.application-attempts:&#013;&#010;2，yarn.application-attempt-failures-validity-interval =&#013;&#010;10000（默认值，10s），只有在10s内 flink job 失败重启2次才会真正的失败。&#013;&#010;2. 如果配置了checkpoint是会重用上次任务失败的state。&#013;&#010;&#013;&#010;这是我个人的理解，有疑问大家一起讨论&#013;&#010;&#013;&#010;MuChen &lt;9329748@qq.com&amp;gt; 于2020年7月1日周三 下午7:50写道：&#013;&#010;&#013;&#010;&amp;gt; hi，all：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我根据这篇博客https://blog.csdn.net/cndotaci/article/details/106870413&#013;&#010;&amp;gt; 的介绍，配置了flink基于yarn的高可用，测试时发现配置的任务失败重试2次没有生效，我测试到第6次时，任务仍然能够被yarn拉起。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 请问各位大佬&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 1. 下面配置中的重试次数为什么没有生效？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 2. 通过HA拉起的任务，是否可以重用上次任务失败时的state？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; flink版本：1.10.0&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; flink-conf.yaml配置：&#013;&#010;&amp;gt; $ grep -v ^# flink-conf.yaml |grep -v ^$ jobmanager.rpc.address: localhost&#013;&#010;&amp;gt; jobmanager.rpc.port: 6123 jobmanager.heap.size: 1024m&#013;&#010;&amp;gt; taskmanager.memory.process.size: 1568m taskmanager.numberOfTaskSlots: 1&#013;&#010;&amp;gt; parallelism.default: 1 high-availability: zookeeper&#013;&#010;&amp;gt; high-availability.storageDir: hdfs:///flink/ha/&#013;&#010;&amp;gt; high-availability.zookeeper.quorum:&#013;&#010;&amp;gt; uhadoop-op3raf-master1,uhadoop-op3raf-master2,uhadoop-op3raf-core1&#013;&#010;&amp;gt; state.checkpoints.dir: hdfs:///flink/checkpoint state.savepoints.dir:&#013;&#010;&amp;gt; hdfs:///flink/flink-savepoints state.checkpoints.num-retained:60&#013;&#010;&amp;gt; state.backend.incremental: true jobmanager.execution.failover-strategy:&#013;&#010;&amp;gt; region jobmanager.archive.fs.dir: hdfs:///flink/flink-jobs/&#013;&#010;&amp;gt; historyserver.web.port: 8082 historyserver.archive.fs.dir:&#013;&#010;&amp;gt; hdfs:///flink/flink-jobs/ historyserver.archive.fs.refresh-interval: 10000&#013;&#010;&amp;gt; # HA重试次数 yarn.application-attempts: 2&#013;&#010;&amp;gt; ssh到jm节点，手动kill任务的操作日志：&#013;&#010;&amp;gt; [root@uhadoop-op3raf-task48 ~]# jps 34785 YarnTaskExecutorRunner 16853&#013;&#010;&amp;gt; YarnTaskExecutorRunner 17527 PrestoServer 33289 YarnTaskExecutorRunner&#013;&#010;&amp;gt; 18026 YarnJobClusterEntrypoint 20283 Jps 39599 NodeManager&#013;&#010;&amp;gt; [root@uhadoop-op3raf-task48 ~]# kill -9 18026 [root@uhadoop-op3raf-task48&#013;&#010;&amp;gt; ~]# jps 34785 YarnTaskExecutorRunner 16853 -- process information&#013;&#010;&amp;gt; unavailable 17527 PrestoServer 21383 Jps 33289 YarnTaskExecutorRunner 20412&#013;&#010;&amp;gt; YarnJobClusterEntrypoint 39599 NodeManager [root@uhadoop-op3raf-task48&#013;&#010;&amp;gt; ~]# kill -9 20412 [root@uhadoop-op3raf-task48 ~]# jps 34785&#013;&#010;&amp;gt; YarnTaskExecutorRunner 21926 YarnJobClusterEntrypoint 23207 Jps 17527&#013;&#010;&amp;gt; PrestoServer 33289 YarnTaskExecutorRunner 39599 NodeManager&#013;&#010;&amp;gt; [root@uhadoop-op3raf-task48 ~]# kill -9 21926 [root@uhadoop-op3raf-task48&#013;&#010;&amp;gt; ~]# jps 34785 YarnTaskExecutorRunner 23318 YarnJobClusterEntrypoint 26279&#013;&#010;&amp;gt; Jps 17527 PrestoServer 33289 YarnTaskExecutorRunner 39599 NodeManager&#013;&#010;&amp;gt; [root@uhadoop-op3raf-task48 ~]# kill -9 23318",
        "depth": "2",
        "reply": "<tencent_39A18FD248F33BF93EBFE746CE4627D4FD09@qq.com>"
    },
    {
        "id": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>",
        "from": "程龙 &lt;13162790...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 12:43:20 GMT",
        "subject": "flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "flink1.10上 程序运行几个小时后就会报不能执行checkpoint 空指针异常 具体如下：&#010;&#010;&#010;java.lang.Exception: Could not perform checkpoint 3201 for operator Filter -&gt; Map (2/8).&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;        at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;        at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;        at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;        at java.lang.Thread.run(Thread.java:745)&#010;Caused by: java.lang.NullPointerException&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)",
        "depth": "0",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<f0e29fa.54b6.1730a6eb406.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 12:51:34 GMT",
        "subject": "回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月01日 20:43，程龙 写道：&#010;flink1.10上 程序运行几个小时后就会报不能执行checkpoint 空指针异常 具体如下：&#010;&#010;&#010;java.lang.Exception: Could not perform checkpoint 3201 for operator Filter -&gt; Map (2/8).&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;       at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;       at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;       at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;       at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;       at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;       at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;       at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;       at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;       at java.lang.Thread.run(Thread.java:745)&#010;Caused by: java.lang.NullPointerException&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;       at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)",
        "depth": "1",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<4c3a3d25.a77d.1730a7e6cd2.Coremail.13162790856@163.com>",
        "from": "程龙 &lt;13162790...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 13:08:44 GMT",
        "subject": "Re:回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;都是分配不到资源(slot)的错误，应该还是checkpoint 为空导致的,不知道为啥为空&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#010;&gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt;&#010;&gt;&#010;&gt;| |&#010;&gt;JasonLee&#010;&gt;|&#010;&gt;|&#010;&gt;邮箱：17610775726@163.com&#010;&gt;|&#010;&gt;&#010;&gt;Signature is customized by Netease Mail Master&#010;&gt;&#010;&gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint 空指针异常&#010;具体如下：&#010;&gt;&#010;&gt;&#010;&gt;java.lang.Exception: Could not perform checkpoint 3201 for operator Filter -&gt; Map (2/8).&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt;       at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt;       at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt;       at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt;       at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt;       at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt;       at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt;       at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt;Caused by: java.lang.NullPointerException&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt;       at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;",
        "depth": "2",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAA8tFvs5WcZ56A+qT1tJc+V6yAM35LBLUSQoKz_N4Zfsr-m8yg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:08:16 GMT",
        "subject": "Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#010;[1]  https://issues.apache.org/jira/browse/FLINK-17479&#010;Best,&#010;Congxian&#010;&#010;&#010;程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#010;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 都是分配不到资源(slot)的错误，应该还是checkpoint 为空导致的,不知道为啥为空&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#010;&gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;| |&#010;&gt; &gt;JasonLee&#010;&gt; &gt;|&#010;&gt; &gt;|&#010;&gt; &gt;邮箱：17610775726@163.com&#010;&gt; &gt;|&#010;&gt; &gt;&#010;&gt; &gt;Signature is customized by Netease Mail Master&#010;&gt; &gt;&#010;&gt; &gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint 空指针异常&#010;具体如下：&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;java.lang.Exception: Could not perform checkpoint 3201 for operator&#010;&gt; Filter -&gt; Map (2/8).&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt; &gt;       at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt; &gt;       at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt; &gt;Caused by: java.lang.NullPointerException&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt; &gt;       at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAMhjQvhYHjFrFqzC+xqfvG7ewdqACmPLaPxkKL67TUa6qWrCiA@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 04:27:22 GMT",
        "subject": "Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "我们也有遇到过这个异常，但是不是很常见&#010;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五 下午2:08写道：&#010;&#010;&gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#010;&gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint 为空导致的,不知道为啥为空&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#010;&gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;| |&#010;&gt; &gt; &gt;JasonLee&#010;&gt; &gt; &gt;|&#010;&gt; &gt; &gt;|&#010;&gt; &gt; &gt;邮箱：17610775726@163.com&#010;&gt; &gt; &gt;|&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;Signature is customized by Netease Mail Master&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint 空指针异常&#010;具体如下：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint 3201 for operator&#010;&gt; &gt; Filter -&gt; Map (2/8).&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt;&#010;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt;&#010;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt; &gt; &gt;       at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt; &gt; &gt;       at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt; &gt; &gt;Caused by: java.lang.NullPointerException&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt; &gt; &gt;       at&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAA8tFvvjohvmGKy=krhj=R360_yPBa1FB7=LP7zrvX=YSMUyMg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 07:21:11 GMT",
        "subject": "Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "@zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六 下午12:27写道：&#010;&#010;&gt; 我们也有遇到过这个异常，但是不是很常见&#010;&gt;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五 下午2:08写道：&#010;&gt;&#010;&gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#010;&gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#010;&gt; &gt; Best,&#010;&gt; &gt; Congxian&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint 为空导致的,不知道为啥为空&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt;JasonLee&#010;&gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#010;&gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint 3201 for operator&#010;&gt; &gt; &gt; Filter -&gt; Map (2/8).&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt; &gt; &gt; &gt;       at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAMhjQvgosJPD58yRtiNZBguXH1fBz3DrKvV7ARkiiO9H1dS8+A@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 07:00:43 GMT",
        "subject": "Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#010;&#010;Best！&#010;zhisheng&#010;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六 下午3:21写道：&#010;&#010;&gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六 下午12:27写道：&#010;&gt;&#010;&gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#010;&gt; &gt;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五 下午2:08写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#010;&gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint 为空导致的,不知道为啥为空&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt; &gt;JasonLee&#010;&gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#010;&gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint 3201 for operator&#010;&gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt; &gt; &gt; &gt;&#010;&gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt; &gt; &gt; &gt; &gt;       at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "6",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<80bda6e0e6884fb4a25d6b9e445a5bb6@mbx01-ops-lgy.ESG.360ES.CN>",
        "from": "陈凯 &lt;chen...@qianxin.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 01:53:15 GMT",
        "subject": "答复: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "&#013;&#010;Hi,zhisheng 程龙.我们也遇到这个问题了，jdk版本jdk8_40，低版本 jdk 确实有大概率会NPE。&#013;&#010;我之前提了个jira 描述了这个问题 &#013;&#010;https://issues.apache.org/jira/browse/FLINK-18196&#013;&#010;&#013;&#010;修改了Checkpoint 相关代码后，在低版本 jdk 上也没有再发现过过NPE。如果实在不能升级&#010;jdk 版本，可以参考下面的patch：&#013;&#010;https://github.com/yuchuanchen/flink/commit/e5122d9787be1fee9bce141887e0d70c9b0a4f19&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-----邮件原件-----&#013;&#010;发件人: zhisheng &lt;zhisheng2018@gmail.com&gt; &#013;&#010;发送时间: 2020年7月5日 15:01&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常&#013;&#010;&#013;&#010;生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#013;&#010;&#013;&#010;Best！&#013;&#010;zhisheng&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六 下午3:21写道：&#013;&#010;&#013;&#010;&gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六 下午12:27写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五 下午2:08写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#013;&#010;&gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint 为空导致的,不知道为啥为空&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;| |&#013;&#010;&gt; &gt; &gt; &gt; &gt;JasonLee&#013;&#010;&gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#013;&#010;&gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint 3201 for operator&#013;&#010;&gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#013;&#010;&gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#013;&#010;&gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAA8tFvudP72jqVADaxc-6Nt87qjR8qpmZ-2ktw4TRddYiekS5g@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 06:04:58 GMT",
        "subject": "Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "@陈凯 感谢你分享的这个方法，比较好奇这两个的区别是什么？修改后的&#010;patch 在 closure 中一开始 copy 了一份&#013;&#010;CheckpointMeta，也就是说 845 - 867 行之间，之前的 checkpointMeta 会变为 null，这个比较奇怪。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;陈凯 &lt;chenkai@qianxin.com&gt; 于2020年7月6日周一 上午9:53写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; Hi,zhisheng 程龙.我们也遇到这个问题了，jdk版本jdk8_40，低版本 jdk&#010;确实有大概率会NPE。&#013;&#010;&gt; 我之前提了个jira 描述了这个问题&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18196&#013;&#010;&gt;&#013;&#010;&gt; 修改了Checkpoint 相关代码后，在低版本 jdk 上也没有再发现过过NPE。如果实在不能升级&#010;jdk 版本，可以参考下面的patch：&#013;&#010;&gt;&#013;&#010;&gt; https://github.com/yuchuanchen/flink/commit/e5122d9787be1fee9bce141887e0d70c9b0a4f19&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; -----邮件原件-----&#013;&#010;&gt; 发件人: zhisheng &lt;zhisheng2018@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月5日 15:01&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常&#013;&#010;&gt;&#013;&#010;&gt; 生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#013;&#010;&gt;&#013;&#010;&gt; Best！&#013;&#010;&gt; zhisheng&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六 下午3:21写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六 下午12:27写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五 下午2:08写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#013;&#010;&gt; &gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#013;&#010;&gt; &gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint&#010;为空导致的,不知道为啥为空&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;| |&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;JasonLee&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint 3201 for&#013;&#010;&gt; operator&#013;&#010;&gt; &gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<4a474c0f.7199.173231c936d.Coremail.chenkaibit@163.com>",
        "from": "chenkaibit  &lt;chenkai...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 07:52:22 GMT",
        "subject": "Re:Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "hi，Congxian。我在发现这个问题时也很奇怪，但是在打印了一些日志后，确实验证了我的想法。因为&#010;&lt;低版本jdk+flink1.9&gt; 和 &lt;高版本jdk+1.10&gt; 都不会抛 NPE(见 FLINK-17479)，我猜测和&#010;lambda 表达式中外部变量的垃圾回收机制以及 1.10 引入的 MailBox 模型有关，外部&#010;checkpointMetaData 实例被意外回收了。所以在修复的 patch 中我在 lambda 表达式内部实例化了一个新的&#010;checkpointMetaData，目前看这个方法是有效的，没有再发现过 NPE。这是个临时的修复方法，根本原因可能还需要进一步分析。&#010;&#010;&#010;&#010;&#010;--&#010;&#010;Best, yuchuan&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-06 14:04:58，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;@陈凯 感谢你分享的这个方法，比较好奇这两个的区别是什么？修改后的&#010;patch 在 closure 中一开始 copy 了一份&#010;&gt;CheckpointMeta，也就是说 845 - 867 行之间，之前的 checkpointMeta 会变为&#010;null，这个比较奇怪。&#010;&gt;&#010;&gt;Best,&#010;&gt;Congxian&#010;&gt;&#010;&gt;&#010;&gt;陈凯 &lt;chenkai@qianxin.com&gt; 于2020年7月6日周一 上午9:53写道：&#010;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Hi,zhisheng 程龙.我们也遇到这个问题了，jdk版本jdk8_40，低版本&#010;jdk 确实有大概率会NPE。&#010;&gt;&gt; 我之前提了个jira 描述了这个问题&#010;&gt;&gt; https://issues.apache.org/jira/browse/FLINK-18196&#010;&gt;&gt;&#010;&gt;&gt; 修改了Checkpoint 相关代码后，在低版本 jdk 上也没有再发现过过NPE。如果实在不能升级&#010;jdk 版本，可以参考下面的patch：&#010;&gt;&gt;&#010;&gt;&gt; https://github.com/yuchuanchen/flink/commit/e5122d9787be1fee9bce141887e0d70c9b0a4f19&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; -----邮件原件-----&#010;&gt;&gt; 发件人: zhisheng &lt;zhisheng2018@gmail.com&gt;&#010;&gt;&gt; 发送时间: 2020年7月5日 15:01&#010;&gt;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; 主题: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常&#010;&gt;&gt;&#010;&gt;&gt; 生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#010;&gt;&gt;&#010;&gt;&gt; Best！&#010;&gt;&gt; zhisheng&#010;&gt;&gt;&#010;&gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六 下午3:21写道：&#010;&gt;&gt;&#010;&gt;&gt; &gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Congxian&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六 下午12:27写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五 下午2:08写道：&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#010;&gt;&gt; &gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#010;&gt;&gt; &gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; &gt; Congxian&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint&#010;为空导致的,不知道为啥为空&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt;&#010;写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;| |&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;JasonLee&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint 3201 for&#010;&gt;&gt; operator&#010;&gt;&gt; &gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; &gt; &gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; &gt; &gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;",
        "depth": "9",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAA8tFvu=AVs-cKZQPKdZa=Xjyj-SdCNowdASpmiTRik97Q1=Tg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 08:11:17 GMT",
        "subject": "Re: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "@chenkaibit 多谢你的回复~&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;chenkaibit &lt;chenkaibit@163.com&gt; 于2020年7月6日周一 下午3:53写道：&#013;&#010;&#013;&#010;&gt; hi，Congxian。我在发现这个问题时也很奇怪，但是在打印了一些日志后，确实验证了我的想法。因为&#010;&lt;低版本jdk+flink1.9&gt; 和&#013;&#010;&gt; &lt;高版本jdk+1.10&gt; 都不会抛 NPE(见 FLINK-17479)，我猜测和 lambda 表达式中外部变量的垃圾回收机制以及&#010;1.10&#013;&#010;&gt; 引入的 MailBox 模型有关，外部 checkpointMetaData 实例被意外回收了。所以在修复的&#010;patch 中我在 lambda&#013;&#010;&gt; 表达式内部实例化了一个新的 checkpointMetaData，目前看这个方法是有效的，没有再发现过&#013;&#010;&gt; NPE。这是个临时的修复方法，根本原因可能还需要进一步分析。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, yuchuan&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-06 14:04:58，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;@陈凯 感谢你分享的这个方法，比较好奇这两个的区别是什么？修改后的&#010;patch 在 closure 中一开始 copy 了一份&#013;&#010;&gt; &gt;CheckpointMeta，也就是说 845 - 867 行之间，之前的 checkpointMeta 会变为&#010;null，这个比较奇怪。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;陈凯 &lt;chenkai@qianxin.com&gt; 于2020年7月6日周一 上午9:53写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Hi,zhisheng 程龙.我们也遇到这个问题了，jdk版本jdk8_40，低版本&#010;jdk 确实有大概率会NPE。&#013;&#010;&gt; &gt;&gt; 我之前提了个jira 描述了这个问题&#013;&#010;&gt; &gt;&gt; https://issues.apache.org/jira/browse/FLINK-18196&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 修改了Checkpoint 相关代码后，在低版本 jdk 上也没有再发现过过NPE。如果实在不能升级&#010;jdk 版本，可以参考下面的patch：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; https://github.com/yuchuanchen/flink/commit/e5122d9787be1fee9bce141887e0d70c9b0a4f19&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; -----邮件原件-----&#013;&#010;&gt; &gt;&gt; 发件人: zhisheng &lt;zhisheng2018@gmail.com&gt;&#013;&#010;&gt; &gt;&gt; 发送时间: 2020年7月5日 15:01&#013;&#010;&gt; &gt;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt; 主题: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best！&#013;&#010;&gt; &gt;&gt; zhisheng&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六 下午3:21写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; Best,&#013;&#010;&gt; &gt;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六 下午12:27写道：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五&#010;下午2:08写道：&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; Best,&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三 下午9:09写道：&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint&#010;为空导致的,不知道为啥为空&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt;&#010;写道：&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;| |&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;JasonLee&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint 3201&#010;for&#013;&#010;&gt; &gt;&gt; operator&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "10",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<4b7c2eac.2eae.173460fcbf5.Coremail.13162790856@163.com>",
        "from": "程龙 &lt;13162790...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 02:45:07 GMT",
        "subject": "Re:Re: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "&#010;&#010;&#010;&#010;&#010;&#010;问题不是很常见 ，但是同一个任务，提交在flink1.10 和 flink1.10.1上都会复现，&#010;准备尝试一下升级一下jdk试试 &#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-06 16:11:17，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;@chenkaibit 多谢你的回复~&#010;&gt;&#010;&gt;Best,&#010;&gt;Congxian&#010;&gt;&#010;&gt;&#010;&gt;chenkaibit &lt;chenkaibit@163.com&gt; 于2020年7月6日周一 下午3:53写道：&#010;&gt;&#010;&gt;&gt; hi，Congxian。我在发现这个问题时也很奇怪，但是在打印了一些日志后，确实验证了我的想法。因为&#010;&lt;低版本jdk+flink1.9&gt; 和&#010;&gt;&gt; &lt;高版本jdk+1.10&gt; 都不会抛 NPE(见 FLINK-17479)，我猜测和 lambda&#010;表达式中外部变量的垃圾回收机制以及 1.10&#010;&gt;&gt; 引入的 MailBox 模型有关，外部 checkpointMetaData 实例被意外回收了。所以在修复的&#010;patch 中我在 lambda&#010;&gt;&gt; 表达式内部实例化了一个新的 checkpointMetaData，目前看这个方法是有效的，没有再发现过&#010;&gt;&gt; NPE。这是个临时的修复方法，根本原因可能还需要进一步分析。&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt; Best, yuchuan&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-06 14:04:58，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;@陈凯 感谢你分享的这个方法，比较好奇这两个的区别是什么？修改后的&#010;patch 在 closure 中一开始 copy 了一份&#010;&gt;&gt; &gt;CheckpointMeta，也就是说 845 - 867 行之间，之前的 checkpointMeta&#010;会变为 null，这个比较奇怪。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Congxian&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;陈凯 &lt;chenkai@qianxin.com&gt; 于2020年7月6日周一 上午9:53写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Hi,zhisheng 程龙.我们也遇到这个问题了，jdk版本jdk8_40，低版本&#010;jdk 确实有大概率会NPE。&#010;&gt;&gt; &gt;&gt; 我之前提了个jira 描述了这个问题&#010;&gt;&gt; &gt;&gt; https://issues.apache.org/jira/browse/FLINK-18196&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 修改了Checkpoint 相关代码后，在低版本 jdk 上也没有再发现过过NPE。如果实在不能升级&#010;jdk 版本，可以参考下面的patch：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; https://github.com/yuchuanchen/flink/commit/e5122d9787be1fee9bce141887e0d70c9b0a4f19&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; -----邮件原件-----&#010;&gt;&gt; &gt;&gt; 发件人: zhisheng &lt;zhisheng2018@gmail.com&gt;&#010;&gt;&gt; &gt;&gt; 发送时间: 2020年7月5日 15:01&#010;&gt;&gt; &gt;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; &gt;&gt; 主题: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Best！&#010;&gt;&gt; &gt;&gt; zhisheng&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六 下午3:21写道：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; Best,&#010;&gt;&gt; &gt;&gt; &gt; Congxian&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六 下午12:27写道：&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五&#010;下午2:08写道：&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; Best,&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; Congxian&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三&#010;下午9:09写道：&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint&#010;为空导致的,不知道为啥为空&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt;&#010;写道：&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;| |&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;JasonLee&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint&#010;3201 for&#010;&gt;&gt; &gt;&gt; operator&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "11",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<CAA8tFvs3aqcBkziEv_pvdJg4ufCaBB7miHgDmzH5NtH8=xQxBg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 12:06:01 GMT",
        "subject": "Re: Re: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "Hi  程龙&#013;&#010;&#013;&#010;如果可以的话，也麻烦使用 1.11.0 测试下看问题是否还存在。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;程龙 &lt;13162790856@163.com&gt; 于2020年7月13日周一 上午10:45写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 问题不是很常见 ，但是同一个任务，提交在flink1.10 和 flink1.10.1上都会复现，&#010;准备尝试一下升级一下jdk试试&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-06 16:11:17，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;@chenkaibit 多谢你的回复~&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;chenkaibit &lt;chenkaibit@163.com&gt; 于2020年7月6日周一 下午3:53写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; hi，Congxian。我在发现这个问题时也很奇怪，但是在打印了一些日志后，确实验证了我的想法。因为&#010;&lt;低版本jdk+flink1.9&gt; 和&#013;&#010;&gt; &gt;&gt; &lt;高版本jdk+1.10&gt; 都不会抛 NPE(见 FLINK-17479)，我猜测和 lambda&#010;表达式中外部变量的垃圾回收机制以及 1.10&#013;&#010;&gt; &gt;&gt; 引入的 MailBox 模型有关，外部 checkpointMetaData 实例被意外回收了。所以在修复的&#010;patch 中我在 lambda&#013;&#010;&gt; &gt;&gt; 表达式内部实例化了一个新的 checkpointMetaData，目前看这个方法是有效的，没有再发现过&#013;&#010;&gt; &gt;&gt; NPE。这是个临时的修复方法，根本原因可能还需要进一步分析。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; --&#013;&#010;&gt; &gt;&gt; Best, yuchuan&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在 2020-07-06 14:04:58，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt; &gt;@陈凯 感谢你分享的这个方法，比较好奇这两个的区别是什么？修改后的&#010;patch 在 closure 中一开始 copy 了一份&#013;&#010;&gt; &gt;&gt; &gt;CheckpointMeta，也就是说 845 - 867 行之间，之前的 checkpointMeta&#010;会变为 null，这个比较奇怪。&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;Best,&#013;&#010;&gt; &gt;&gt; &gt;Congxian&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;陈凯 &lt;chenkai@qianxin.com&gt; 于2020年7月6日周一 上午9:53写道：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; Hi,zhisheng 程龙.我们也遇到这个问题了，jdk版本jdk8_40，低版本&#010;jdk 确实有大概率会NPE。&#013;&#010;&gt; &gt;&gt; &gt;&gt; 我之前提了个jira 描述了这个问题&#013;&#010;&gt; &gt;&gt; &gt;&gt; https://issues.apache.org/jira/browse/FLINK-18196&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; 修改了Checkpoint 相关代码后，在低版本 jdk 上也没有再发现过过NPE。如果实在不能升级&#010;jdk&#013;&#010;&gt; 版本，可以参考下面的patch：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; https://github.com/yuchuanchen/flink/commit/e5122d9787be1fee9bce141887e0d70c9b0a4f19&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; -----邮件原件-----&#013;&#010;&gt; &gt;&gt; &gt;&gt; 发件人: zhisheng &lt;zhisheng2018@gmail.com&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; 发送时间: 2020年7月5日 15:01&#013;&#010;&gt; &gt;&gt; &gt;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; 主题: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; 生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; Best！&#013;&#010;&gt; &gt;&gt; &gt;&gt; zhisheng&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六&#010;下午3:21写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; Best,&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六&#010;下午12:27写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五&#010;下午2:08写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; Best,&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三&#010;下午9:09写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint&#010;为空导致的,不知道为啥为空&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt;&#010;写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;| |&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;JasonLee&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail Master&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform checkpoint&#010;3201 for&#013;&#010;&gt; &gt;&gt; &gt;&gt; operator&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "12",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<1fd1eb5.2a2c.1736a3911dc.Coremail.13162790856@163.com>",
        "from": "程龙 &lt;13162790...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:16:31 GMT",
        "subject": "Re:Re: Re: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常",
        "content": "ok  我过两天试一下 最近有点忙   过两天测试一下1.11  到时候结果同步给大家&#010;，另外之前测试jdk使用jdk1.8.0_231 也是不行的 会报同样的错误&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-13 20:06:01，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;Hi  程龙&#010;&gt;&#010;&gt;如果可以的话，也麻烦使用 1.11.0 测试下看问题是否还存在。&#010;&gt;&#010;&gt;Best,&#010;&gt;Congxian&#010;&gt;&#010;&gt;&#010;&gt;程龙 &lt;13162790856@163.com&gt; 于2020年7月13日周一 上午10:45写道：&#010;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 问题不是很常见 ，但是同一个任务，提交在flink1.10 和 flink1.10.1上都会复现，&#010;准备尝试一下升级一下jdk试试&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-06 16:11:17，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;@chenkaibit 多谢你的回复~&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Congxian&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;chenkaibit &lt;chenkaibit@163.com&gt; 于2020年7月6日周一 下午3:53写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; hi，Congxian。我在发现这个问题时也很奇怪，但是在打印了一些日志后，确实验证了我的想法。因为&#010;&lt;低版本jdk+flink1.9&gt; 和&#010;&gt;&gt; &gt;&gt; &lt;高版本jdk+1.10&gt; 都不会抛 NPE(见 FLINK-17479)，我猜测和&#010;lambda 表达式中外部变量的垃圾回收机制以及 1.10&#010;&gt;&gt; &gt;&gt; 引入的 MailBox 模型有关，外部 checkpointMetaData 实例被意外回收了。所以在修复的&#010;patch 中我在 lambda&#010;&gt;&gt; &gt;&gt; 表达式内部实例化了一个新的 checkpointMetaData，目前看这个方法是有效的，没有再发现过&#010;&gt;&gt; &gt;&gt; NPE。这是个临时的修复方法，根本原因可能还需要进一步分析。&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; --&#010;&gt;&gt; &gt;&gt; Best, yuchuan&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在 2020-07-06 14:04:58，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt;&#010;写道：&#010;&gt;&gt; &gt;&gt; &gt;@陈凯 感谢你分享的这个方法，比较好奇这两个的区别是什么？修改后的&#010;patch 在 closure 中一开始 copy 了一份&#010;&gt;&gt; &gt;&gt; &gt;CheckpointMeta，也就是说 845 - 867 行之间，之前的 checkpointMeta&#010;会变为 null，这个比较奇怪。&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt; &gt;Congxian&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;陈凯 &lt;chenkai@qianxin.com&gt; 于2020年7月6日周一 上午9:53写道：&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Hi,zhisheng 程龙.我们也遇到这个问题了，jdk版本jdk8_40，低版本&#010;jdk 确实有大概率会NPE。&#010;&gt;&gt; &gt;&gt; &gt;&gt; 我之前提了个jira 描述了这个问题&#010;&gt;&gt; &gt;&gt; &gt;&gt; https://issues.apache.org/jira/browse/FLINK-18196&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 修改了Checkpoint 相关代码后，在低版本 jdk 上也没有再发现过过NPE。如果实在不能升级&#010;jdk&#010;&gt;&gt; 版本，可以参考下面的patch：&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; https://github.com/yuchuanchen/flink/commit/e5122d9787be1fee9bce141887e0d70c9b0a4f19&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; -----邮件原件-----&#010;&gt;&gt; &gt;&gt; &gt;&gt; 发件人: zhisheng &lt;zhisheng2018@gmail.com&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 发送时间: 2020年7月5日 15:01&#010;&gt;&gt; &gt;&gt; &gt;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 主题: Re: 回复：flink1.10 checkpoint 运行一段时间空指针异常&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 生产集群 JDK 使用的地方比较多，不敢轻易换版本，后面再观察一下，如果频繁出现这种问题再考虑更换版本，感谢&#010;Congxian&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Best！&#010;&gt;&gt; &gt;&gt; &gt;&gt; zhisheng&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月4日周六&#010;下午3:21写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; @zhisheng 你们有尝试过更换 jdk 版本吗？更换版本是否能解决这个问题呢？&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Best,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; Congxian&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年7月4日周六&#010;下午12:27写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; 我们也有遇到过这个异常，但是不是很常见&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月3日周五&#010;下午2:08写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; 你可以看看是否 FLINK-17479[1] 和你的问题一样，是的话，可以尝试修改一下&#010;jdk 版本试试&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; [1]  https://issues.apache.org/jira/browse/FLINK-17479&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; Best,&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; Congxian&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; 程龙 &lt;13162790856@163.com&gt; 于2020年7月1日周三&#010;下午9:09写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 都是分配不到资源(slot)的错误，应该还是checkpoint&#010;为空导致的,不知道为啥为空&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 在 2020-07-01 20:51:34，\"JasonLee\" &lt;17610775726@163.com&gt;&#010;写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;你到具体的tm上找到相关的operator看看是不是有异常信息&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;| |&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;JasonLee&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：17610775726@163.com&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Signature is customized by Netease Mail&#010;Master&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;在2020年07月01日 20:43，程龙 写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;flink1.10上 程序运行几个小时后就会报不能执行checkpoint&#010;空指针异常 具体如下：&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;java.lang.Exception: Could not perform&#010;checkpoint 3201 for&#010;&gt;&gt; &gt;&gt; &gt;&gt; operator&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Filter -&gt; Map (2/8).&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:816)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; .CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; .CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:177)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; .CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:310)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at java.lang.Thread.run(Thread.java:745)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;Caused by: java.lang.NullPointerException&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1382)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:974)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:870)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:843)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;       at&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:803)&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "13",
        "reply": "<448d779f.a554.1730a672c9f.Coremail.13162790856@163.com>"
    },
    {
        "id": "<tencent_D1F573AB67251361FE6347B4A41F9D5D9F08@qq.com>",
        "from": "&quot;MuChen&quot; &lt;9329...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 01 Jul 2020 15:16:22 GMT",
        "subject": "回复： flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state",
        "content": "hi,jiliang1993:&#013;&#010;&#013;&#010;&#013;&#010;我的理解还是受yarn的yarn.resourcemanager.am.max-attempts还是有效的。&#013;&#010;参数yarn.application-attempt-failures-validity-interval应该只是限制了达成attempts一次计数的条件。以默认值10秒为例，在距离上次启动10秒钟内的失败重试会让attempts计数加1，而10秒后的失败重试attempts的计数是不变的。当attempts计数达到min(yarn中配置的yarn.resourcemanager.am.max-attempts,flink中配置的yarn.application-attempts)时，yarn就不再对该任务进行失败重试了。&#013;&#010;如有理解不当，请大佬们指正！&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;MuChen.&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"jiliang1993\"&lt;jiliang1993@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月1日(星期三) 晚上10:56&#013;&#010;收件人:&amp;nbsp;\"MuChen\"&lt;9329748@qq.com&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复： flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;意思是只要配置了ha就会不受yarn的attempt 控制对吗？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: \"MuChen\" &lt;9329748@qq.com&amp;gt; &lt;\"MuChen\" &lt;9329748@qq.com&amp;gt;&amp;gt;&#013;&#010;发送时间: 2020年7月1日 22:48&#013;&#010;收件人: jiliang1993 &lt;jiliang1993@gmail.com&amp;gt;&#013;&#010;主题: 回复： flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hi，王松： 受教了，多谢指点！ Best, MuChen. ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#010;发件人:&amp;amp;nbsp;\"王松\"&lt;sdlcwangsong11@gmail.com&amp;amp;gt;; 发送时间:&amp;amp;nbsp;2020年7月1日(星期三)&#010;晚上8:17 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;; 主题:&amp;amp;nbsp;Re:&#010;flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state hi, muchen&#010;1. yarn.application-attempts 这个参数与另外一个参数有关系：yarn.application-attempt-failures-validity-interval，大概意思是需要在设置的这个interval内失败重试多少次，才认为flink&#010;job是失败的，如果超过这个interval，就会重新开始计数。打个比方，yarn.application-attempts:&#010;2，yarn.application-attempt-failures-validity-interval = 10000（默认值，10s），只有在10s内&#010;flink job 失败重启2次才会真正的失败。 2. 如果配置了checkpoint是会重用上次任务失败的state。&#010;这是我个人的理解，有疑问大家一起讨论 MuChen &lt;9329748@qq.com&amp;amp;gt;&#010;于2020年7月1日周三 下午7:50写道： &amp;amp;gt; hi，all： &amp;amp;gt; &amp;amp;gt;&#010;我根据这篇博客https://blog.csdn.net/cndotaci/article/details/106870413 &amp;amp;gt;&#010;的介绍，配置了flink基于yarn的高可用，测试时发现配置的任务失败重试2次没有生效，我测试到第6次时，任务仍然能够被yarn拉起。&#010;&amp;amp;gt; &amp;amp;gt; 请问各位大佬 &amp;amp;gt; &amp;amp;gt; 1. 下面配置中的重试次数为什么没有生效？&#010;&amp;amp;gt; &amp;amp;gt; 2. 通过HA拉起的任务，是否可以重用上次任务失败时的state？&#010;&amp;amp;gt; &amp;amp;gt; flink版本：1.10.0 &amp;amp;gt; &amp;amp;gt; flink-conf.yaml配置：&#010;&amp;amp;gt; $ grep -v ^# flink-conf.yaml |grep -v ^$ jobmanager.rpc.address: localhost &amp;amp;gt;&#010;jobmanager.rpc.port: 6123 jobmanager.heap.size: 1024m &amp;amp;gt; taskmanager.memory.process.size:&#010;1568m taskmanager.numberOfTaskSlots: 1 &amp;amp;gt; parallelism.default: 1 high-availability:&#010;zookeeper &amp;amp;gt; high-availability.storageDir: hdfs:///flink/ha/ &amp;amp;gt; high-availability.zookeeper.quorum:&#010;&amp;amp;gt; uhadoop-op3raf-master1,uhadoop-op3raf-master2,uhadoop-op3raf-core1 &amp;amp;gt;&#010;state.checkpoints.dir: hdfs:///flink/checkpoint state.savepoints.dir: &amp;amp;gt; hdfs:///flink/flink-savepoints&#010;state.checkpoints.num-retained:60 &amp;amp;gt; state.backend.incremental: true jobmanager.execution.failover-strategy:&#010;&amp;amp;gt; region jobmanager.archive.fs.dir: hdfs:///flink/flink-jobs/ &amp;amp;gt; historyserver.web.port:&#010;8082 historyserver.archive.fs.dir: &amp;amp;gt; hdfs:///flink/flink-jobs/ historyserver.archive.fs.refresh-interval:&#010;10000 &amp;amp;gt; # HA重试次数 yarn.application-attempts: 2 &amp;amp;gt; ssh到jm节点，手动kill任务的操作日志：&#010;&amp;amp;gt; [root@uhadoop-op3raf-task48 ~]# jps 34785 YarnTaskExecutorRunner 16853 &amp;amp;gt;&#010;YarnTaskExecutorRunner 17527 PrestoServer 33289 YarnTaskExecutorRunner &amp;amp;gt; 18026&#010;YarnJobClusterEntrypoint 20283 Jps 39599 NodeManager &amp;amp;gt; [root@uhadoop-op3raf-task48&#010;~]# kill -9 18026 [root@uhadoop-op3raf-task48 &amp;amp;gt; ~]# jps 34785 YarnTaskExecutorRunner&#010;16853 -- process information &amp;amp;gt; unavailable 17527 PrestoServer 21383 Jps 33289 YarnTaskExecutorRunner&#010;20412 &amp;amp;gt; YarnJobClusterEntrypoint 39599 NodeManager [root@uhadoop-op3raf-task48&#010;&amp;amp;gt; ~]# kill -9 20412 [root@uhadoop-op3raf-task48 ~]# jps 34785 &amp;amp;gt; YarnTaskExecutorRunner&#010;21926 YarnJobClusterEntrypoint 23207 Jps 17527 &amp;amp;gt; PrestoServer 33289 YarnTaskExecutorRunner&#010;39599 NodeManager &amp;amp;gt; [root@uhadoop-op3raf-task48 ~]# kill -9 21926 [root@uhadoop-op3raf-task48&#010;&amp;amp;gt; ~]# jps 34785 YarnTaskExecutorRunner 23318 YarnJobClusterEntrypoint 26279 &amp;amp;gt;&#010;Jps 17527 PrestoServer 33289 YarnTaskExecutorRunner 39599 NodeManager &amp;amp;gt; [root@uhadoop-op3raf-task48&#010;~]# kill -9 23318",
        "depth": "1",
        "reply": "<tencent_D1F573AB67251361FE6347B4A41F9D5D9F08@qq.com>"
    },
    {
        "id": "<1593659347556-0.post@n8.nabble.com>",
        "from": "liangji &lt;jiliang1...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 03:09:07 GMT",
        "subject": "Re: 回复： flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state",
        "content": "我之前配置了HA，也配置了flink中yarn-attempts=2，结果是kill jm进程可以无限重启&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_D1F573AB67251361FE6347B4A41F9D5D9F08@qq.com>"
    },
    {
        "id": "<ACFB0406-7963-4064-A4D3-BE6A399387D9@gmail.com>",
        "from": "Paul Lam &lt;paullin3...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 07:10:57 GMT",
        "subject": "Re: flink基于yarn的HA次数无效，以及HA拉起的任务是否可以重用state",
        "content": "判断 Attempt 失败的标准是 Flink 通过 AMRMClientAsyncImpl 通知 YARN RM Application&#010;失败并注销自己，所以 kill jm 是不算的。&#010;&#010;Best,&#010;Paul Lam&#010;&#010;&gt; 2020年7月2日 11:09，liangji &lt;jiliang1993@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; 我之前配置了HA，也配置了flink中yarn-attempts=2，结果是kill jm进程可以无限重启&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_D1F573AB67251361FE6347B4A41F9D5D9F08@qq.com>"
    },
    {
        "id": "<1593659424303-0.post@n8.nabble.com>",
        "from": "liangji &lt;jiliang1...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 03:10:24 GMT",
        "subject": "UDTAGGs sql的查询怎么写",
        "content": "https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/functions/udfs.html#table-aggregation-functions&#010;请问下UDTAGGs支持sql的写法吗，怎么写？看官档上只有table api的示例。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1593659424303-0.post@n8.nabble.com>"
    },
    {
        "id": "<CABi+2jRQfcCs4VPzpPy8NB2yvZ=H6Gwjt0boBsTj+spWY_Ve-A@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 03:14:53 GMT",
        "subject": "Re: UDTAGGs sql的查询怎么写",
        "content": "Hi,&#013;&#010;&#013;&#010;因为UDTAGGs不属于标准SQL的语法，所以只有TableApi&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Thu, Jul 2, 2020 at 11:10 AM liangji &lt;jiliang1993@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/functions/udfs.html#table-aggregation-functions&#013;&#010;&gt; 请问下UDTAGGs支持sql的写法吗，怎么写？看官档上只有table api的示例。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<1593659424303-0.post@n8.nabble.com>"
    },
    {
        "id": "<1593660975867-0.post@n8.nabble.com>",
        "from": "liangji &lt;jiliang1...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 03:36:15 GMT",
        "subject": "Re: UDTAGGs sql的查询怎么写",
        "content": "好的，谢谢jinsong大佬&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "2",
        "reply": "<1593659424303-0.post@n8.nabble.com>"
    },
    {
        "id": "<202007021117030258115@163.com>",
        "from": "&quot;hdxg1101300123@163.com&quot; &lt;hdxg1101300...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 03:17:04 GMT",
        "subject": "flink的state过期设置",
        "content": "您好：&#013;&#010;    想咨询一下关于state的ttl问题;&#013;&#010;    想问一下 state设置的ttl,如果从checkpoints重启 ttl会不会失效；ttl针对的是process&#010;time，&#013;&#010;    比如我设置的7天过期,重新从checkpoints启动是第一次启动的时间算还是恢复时的新processtime算；他是state的一部分&#010;还是怎么算；&#013;&#010;   或者要注册定时器来实现&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hdxg1101300123@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007021117030258115@163.com>"
    },
    {
        "id": "<CH2PR20MB296693A5F5FA251CE181F57DDA6D0@CH2PR20MB2966.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 06:10:45 GMT",
        "subject": "Re: flink的state过期设置",
        "content": "Hi&#013;&#010;&#013;&#010;TTL的时间戳实际是会存储在 state 里面 [1]，与每个entry在一起，也就是说从Checkpoint恢复的话，数据里面的时间戳是当时插入时候的时间戳。&#013;&#010;&#013;&#010;[1] https://github.com/apache/flink/blob/ba92b3b8b02e099c8aab4b2b23a37dca4558cabd/flink-runtime/src/main/java/org/apache/flink/runtime/state/ttl/TtlValueState.java#L50&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;&#013;&#010;________________________________&#013;&#010;From: hdxg1101300123@163.com &lt;hdxg1101300123@163.com&gt;&#013;&#010;Sent: Thursday, July 2, 2020 11:17&#013;&#010;To: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: flink的state过期设置&#013;&#010;&#013;&#010;您好：&#013;&#010;    想咨询一下关于state的ttl问题;&#013;&#010;    想问一下 state设置的ttl,如果从checkpoints重启 ttl会不会失效；ttl针对的是process&#010;time，&#013;&#010;    比如我设置的7天过期,重新从checkpoints启动是第一次启动的时间算还是恢复时的新processtime算；他是state的一部分&#010;还是怎么算；&#013;&#010;   或者要注册定时器来实现&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hdxg1101300123@163.com&#013;&#010;",
        "depth": "1",
        "reply": "<202007021117030258115@163.com>"
    },
    {
        "id": "<CAEZk040-+H5wxpTJh5OfQWWWvm3MKcEaoQXRpHuAe2NcNtULDQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 04:09:05 GMT",
        "subject": "flink任务提交方式",
        "content": "hi&#013;&#010;请问现在flink有没有像sparklauncher这种任务提交方式，在任务提交成功后返回对应的任务id（不管是onyarn还是standlone），我这面想用java代码提交任务并在提交后获取任务id，请问有没有对应功能或工具&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk040-+H5wxpTJh5OfQWWWvm3MKcEaoQXRpHuAe2NcNtULDQ@mail.gmail.com>"
    },
    {
        "id": "<1cbf8ae6.38e0.1730dd0973d.Coremail.rjianxu@163.com>",
        "from": "jianxu &lt;rjia...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 04:37:20 GMT",
        "subject": "Re:flink任务提交方式",
        "content": "你可以看下这个项目https://github.com/todd5167/clusters-submiter，改造下应该满足你的需求。&#010;在 2020-07-02 12:09:05，\"Dream-底限\" &lt;zhangyu@akulaku.com&gt; 写道：&#010;&gt;hi&#013;&#010;&gt;请问现在flink有没有像sparklauncher这种任务提交方式，在任务提交成功后返回对应的任务id（不管是onyarn还是standlone），我这面想用java代码提交任务并在提交后获取任务id，请问有没有对应功能或工具&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk040-+H5wxpTJh5OfQWWWvm3MKcEaoQXRpHuAe2NcNtULDQ@mail.gmail.com>"
    },
    {
        "id": "<CAEZk040SKOkG_FWNp42rJiKsD5wa=95dh9d0Em1GFUVRKhO=aA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 06:14:07 GMT",
        "subject": "Re: flink任务提交方式",
        "content": "好的，感谢&#013;&#010;&#013;&#010;On Thu, Jul 2, 2020 at 12:37 PM jianxu &lt;rjianxu@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 你可以看下这个项目https://github.com/todd5167/clusters-submiter，改造下应该满足你的需求。&#013;&#010;&gt; 在 2020-07-02 12:09:05，\"Dream-底限\" &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &gt;hi&#013;&#010;&gt;&#013;&#010;&gt; &gt;请问现在flink有没有像sparklauncher这种任务提交方式，在任务提交成功后返回对应的任务id（不管是onyarn还是standlone），我这面想用java代码提交任务并在提交后获取任务id，请问有没有对应功能或工具&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk040-+H5wxpTJh5OfQWWWvm3MKcEaoQXRpHuAe2NcNtULDQ@mail.gmail.com>"
    },
    {
        "id": "<E0651FF7-35A6-4E7D-85AE-880F9DFE9A54@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 02:14:07 GMT",
        "subject": "Re: flink任务提交方式",
        "content": "Hi,&#013;&#010;1.10.x版本以后env.execute()是返回一个JobExecutionResult&#013;&#010;对象的，这里面可以获取到job相关信息,比如你想要的jobid&#013;&#010;&#013;&#010;&gt; 2020年7月2日 下午12:09，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; hi&#013;&#010;&gt; 请问现在flink有没有像sparklauncher这种任务提交方式，在任务提交成功后返回对应的任务id（不管是onyarn还是standlone），我这面想用java代码提交任务并在提交后获取任务id，请问有没有对应功能或工具&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk040-+H5wxpTJh5OfQWWWvm3MKcEaoQXRpHuAe2NcNtULDQ@mail.gmail.com>"
    },
    {
        "id": "<6db10caf.6a42.1730e3b51e6.Coremail.fszwfly@163.com>",
        "from": "forideal  &lt;fszw...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 06:33:55 GMT",
        "subject": "Re: 关于flink sql问题",
        "content": "Hi 本超&#010;&#010;&#010;&#010;  关于Mysql 做维表，关掉cache后的优化手段，有什么建议吗？&#010;&#010;&#010;&#010;比如，20k records per second 的流量，关掉 cache 会对 mysql 产生很大的压力。不知道&#010;MySQL Lookup 做成 async + batch 会不会提升性能或者有副作用。&#010;&#010;&#010;&#010;Best forideal.&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 Benchao Li &lt;libenchao@apache.org&gt;，2020年7月1日 13:22写道：&#010;&#010;&#010;我理解你只需要把这同一个Mysql表再做一个维表即可。可以写两次DDL，一个给维表用，一个给sink用。&#010;如果你就觉得它是实时变化的，你可以把维表的cache关掉，保证每次都是获取Mysql中最新的数据就可以了吧？&#010;&#010;当然了，在DDL的时候并没有区分这个表是维表还是sink表，具体它是什么类型，只是根据你在SQL里面怎么使用来决定的。&#010;理论上来讲，你一个DDL可以同时做维表也可以做Sink。（只是它们可能有些配置会不同，分开写两个DDL应该是更清晰一些）&#010;&#010;zya &lt;z_yuang@foxmail.com&gt; 于2020年6月30日周二 下午11:26写道：&#010;&#010;&gt; 请问下，sink写出的表能做维表吗，因为sink会一直写入，做维表的话会一直动态变化&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年6月30日(星期二) 晚上11:14&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: 关于flink sql问题&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 应该做一个维表Join就可以了。&#010;&gt;&#010;&gt;&#010;&gt; zya &lt;z_yuang@foxmail.com&amp;gt; 于2020年6月30日周二 下午9:02写道：&#010;&gt;&#010;&gt; &amp;gt; Hi 各位，有个问题想请教一下：&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; 目前我有一个功能想使用flink sql来完成，source是kafka，sink是mysql，&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;在写入mysql的时候，我希望能先根据key获取mysql中的数据进行判断，然后决定如何写入数据，请问flink1.10目前能实现这种功能吗？&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&#010;&#010;&#010;--&#010;&#010;Best,&#010;Benchao Li&#010;",
        "depth": "0",
        "reply": "<6db10caf.6a42.1730e3b51e6.Coremail.fszwfly@163.com>"
    },
    {
        "id": "<202007021534287317879@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 07:34:29 GMT",
        "subject": "在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？",
        "content": "&#013;&#010;官网上的例子：&#013;&#010;&#013;&#010;public class MyMapper extends RichMapFunction&lt;String, String&gt; {&#010;  private transient Counter counter;&#010;  @Override&#010;  public void open(Configuration config) {&#010;    this.counter = getRuntimeContext()&#010;      .getMetricGroup()&#010;      .counter(\"myCounter\");&#010;  }&#010;  @Override&#010;  public String map(String value) throws Exception {&#010;    this.counter.inc();&#010;    return value;&#010;  }&#010;}&#013;&#010;&#013;&#010;我想要根据 map 方法中传入的参数生成不同的 监控指标，怎样可以实现呢？&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn&#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<202007021534287317879@geekplus.com.cn>"
    },
    {
        "id": "<58a7c55e.9cbc.1730e995e8c.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 08:16:39 GMT",
        "subject": "回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？",
        "content": "是要生成不同类型的metric吗 比如counter meter ？&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月02日 15:34，wanglei2@geekplus.com.cn 写道：&#010;&#010;官网上的例子：&#010;&#010;public class MyMapper extends RichMapFunction&lt;String, String&gt; {&#010; private transient Counter counter;&#010; @Override&#010; public void open(Configuration config) {&#010;   this.counter = getRuntimeContext()&#010;     .getMetricGroup()&#010;     .counter(\"myCounter\");&#010; }&#010; @Override&#010; public String map(String value) throws Exception {&#010;   this.counter.inc();&#010;   return value;&#010; }&#010;}&#010;&#010;我想要根据 map 方法中传入的参数生成不同的 监控指标，怎样可以实现呢？&#010;&#010;谢谢，&#010;王磊&#010;&#010;&#010;&#010;wanglei2@geekplus.com.cn&#010;&#010;",
        "depth": "1",
        "reply": "<202007021534287317879@geekplus.com.cn>"
    },
    {
        "id": "<2020070216390149096413@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 08:39:02 GMT",
        "subject": "回复: 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？",
        "content": "&#013;&#010;全都是同一种类型的 metrics.&#013;&#010;比如消息中是 mysql binlog 解析结果，我想要根据消息内容拿到 tableName，&#010;按 tableName 生成不同名称的 metrics（但都是 meter 类型） &#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;&#013;&#010;发件人： JasonLee&#013;&#010;发送时间： 2020-07-02 16:16&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;是要生成不同类型的metric吗 比如counter meter ？&#013;&#010; &#013;&#010; &#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010; &#013;&#010;Signature is customized by Netease Mail Master&#013;&#010; &#013;&#010;在2020年07月02日 15:34，wanglei2@geekplus.com.cn 写道：&#013;&#010; &#013;&#010;官网上的例子：&#013;&#010; &#013;&#010;public class MyMapper extends RichMapFunction&lt;String, String&gt; {&#013;&#010;private transient Counter counter;&#013;&#010;@Override&#013;&#010;public void open(Configuration config) {&#013;&#010;   this.counter = getRuntimeContext()&#013;&#010;     .getMetricGroup()&#013;&#010;     .counter(\"myCounter\");&#013;&#010;}&#013;&#010;@Override&#013;&#010;public String map(String value) throws Exception {&#013;&#010;   this.counter.inc();&#013;&#010;   return value;&#013;&#010;}&#013;&#010;}&#013;&#010; &#013;&#010;我想要根据 map 方法中传入的参数生成不同的 监控指标，怎样可以实现呢？&#013;&#010; &#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;wanglei2@geekplus.com.cn&#013;&#010; &#013;&#010;",
        "depth": "1",
        "reply": "<202007021534287317879@geekplus.com.cn>"
    },
    {
        "id": "<13f090f8.b4e2.1730fa7e4d5.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 13:12:08 GMT",
        "subject": "回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？",
        "content": "你把tablename传到下面metric里不就行了吗&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月02日 16:39，wanglei2@geekplus.com.cn 写道：&#010;&#010;全都是同一种类型的 metrics.&#010;比如消息中是 mysql binlog 解析结果，我想要根据消息内容拿到 tableName，&#010;按 tableName 生成不同名称的 metrics（但都是 meter 类型）&#010;&#010;谢谢，&#010;王磊&#010;&#010;&#010;&#010;&#010;wanglei2@geekplus.com.cn&#010;&#010;&#010;发件人： JasonLee&#010;发送时间： 2020-07-02 16:16&#010;收件人： user-zh&#010;主题： 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#010;是要生成不同类型的metric吗 比如counter meter ？&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月02日 15:34，wanglei2@geekplus.com.cn 写道：&#010;&#010;官网上的例子：&#010;&#010;public class MyMapper extends RichMapFunction&lt;String, String&gt; {&#010;private transient Counter counter;&#010;@Override&#010;public void open(Configuration config) {&#010;  this.counter = getRuntimeContext()&#010;    .getMetricGroup()&#010;    .counter(\"myCounter\");&#010;}&#010;@Override&#010;public String map(String value) throws Exception {&#010;  this.counter.inc();&#010;  return value;&#010;}&#010;}&#010;&#010;我想要根据 map 方法中传入的参数生成不同的 监控指标，怎样可以实现呢？&#010;&#010;谢谢，&#010;王磊&#010;&#010;&#010;&#010;wanglei2@geekplus.com.cn&#010;&#010;",
        "depth": "2",
        "reply": "<202007021534287317879@geekplus.com.cn>"
    },
    {
        "id": "<00846877-b8fb-4035-8208-c1124f7a5957.wanglei2@geekplus.com.cn>",
        "from": "&quot;王磊2&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 13:46:11 GMT",
        "subject": "回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？",
        "content": "&#010;没有明白你说的实现方式。&#010;&#010;我最终要得到类似的 Metrics:  myCounter_table1, myCounter_table2, ..., myCounter_tableX&#010;但我看代码中 Metrics 的初始化都是在 open 方法中的，在这个方法中我没法得到&#010;tableName 是什么。&#010;&#010;谢谢，&#010;王磊&#010;&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：JasonLee &lt;17610775726@163.com&gt;&#010;发送时间：2020年7月2日(星期四) 21:12&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#010;&#010;你把tablename传到下面metric里不就行了吗&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月02日 16:39，wanglei2@geekplus.com.cn 写道：&#010;&#010;全都是同一种类型的 metrics.&#010;比如消息中是 mysql binlog 解析结果，我想要根据消息内容拿到 tableName，&#010;按 tableName 生成不同名称的 metrics（但都是 meter 类型）&#010;&#010;谢谢，&#010;王磊&#010;&#010;&#010;&#010;&#010;wanglei2@geekplus.com.cn&#010;&#010;&#010;发件人： JasonLee&#010;发送时间： 2020-07-02 16:16&#010;收件人： user-zh&#010;主题： 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#010;是要生成不同类型的metric吗 比如counter meter ？&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月02日 15:34，wanglei2@geekplus.com.cn 写道：&#010;&#010;官网上的例子：&#010;&#010;public class MyMapper extends RichMapFunction&lt;String, String&gt; {&#010;private transient Counter counter;&#010;@Override&#010;public void open(Configuration config) {&#010;  this.counter = getRuntimeContext()&#010;    .getMetricGroup()&#010;    .counter(\"myCounter\");&#010;}&#010;@Override&#010;public String map(String value) throws Exception {&#010;  this.counter.inc();&#010;  return value;&#010;}&#010;}&#010;&#010;我想要根据 map 方法中传入的参数生成不同的 监控指标，怎样可以实现呢？&#010;&#010;谢谢，&#010;王磊&#010;&#010;&#010;&#010;wanglei2@geekplus.com.cn&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<202007021534287317879@geekplus.com.cn>"
    },
    {
        "id": "<2020070310243339568817@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 02:24:34 GMT",
        "subject": "Re: 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？",
        "content": "public void invoke(ObjectNode node, Context context) throws Exception {&#013;&#010;&#013;&#010;    String tableName = node.get(\"metadata\").get(\"topic\").asText();&#013;&#010;    Meter meter = getRuntimeContext().getMetricGroup().meter(tableName, new MeterView(10));&#013;&#010;    meter.markEvent();&#013;&#010;    log.info(\"### counter: \" + meter.toString() + \"\\t\" +  meter.getCount());&#013;&#010;&#013;&#010;如上面代码所示，在 invoke 方法中解析得到 tableName， 以 tableName 名字作为&#010;metrics.&#013;&#010;但这样写每一消息下来了后相当于重新定义了 这个 metrics ， 又从 0 开始计数了。&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010; &#013;&#010;Sender: kcz&#013;&#010;Send Time: 2020-07-03 09:13&#013;&#010;Receiver: wanglei2&#013;&#010;Subject: 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;按照你的描述 你就是少了tablename，那么你解析log 得到了tablename又做metric就好了吧&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: 王磊2 &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;发送时间: 2020年7月2日 21:46&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;, 17610775726 &lt;17610775726@163.com&gt;&#013;&#010;主题: 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;&#013;&#010;&#013;&#010;没有明白你说的实现方式。&#013;&#010;&#013;&#010;我最终要得到类似的 Metrics:  myCounter_table1, myCounter_table2, ..., myCounter_tableX&#013;&#010;但我看代码中 Metrics 的初始化都是在 open 方法中的，在这个方法中我没法得到&#010;tableName 是什么。&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------------------------------------------------------&#013;&#010;发件人：JasonLee &lt;17610775726@163.com&gt;&#013;&#010;发送时间：2020年7月2日(星期四) 21:12&#013;&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主　题：回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;&#013;&#010;你把tablename传到下面metric里不就行了吗&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010;&#013;&#010;Signature is customized by Netease Mail Master&#013;&#010;&#013;&#010;在2020年07月02日 16:39，wanglei2@geekplus.com.cn 写道：&#013;&#010;&#013;&#010;全都是同一种类型的 metrics.&#013;&#010;比如消息中是 mysql binlog 解析结果，我想要根据消息内容拿到 tableName，&#010;按 tableName 生成不同名称的 metrics（但都是 meter 类型）&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn&#013;&#010;&#013;&#010;&#013;&#010;发件人： JasonLee&#013;&#010;发送时间： 2020-07-02 16:16&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;是要生成不同类型的metric吗 比如counter meter ？&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010;&#013;&#010;Signature is customized by Netease Mail Master&#013;&#010;&#013;&#010;在2020年07月02日 15:34，wanglei2@geekplus.com.cn 写道：&#013;&#010;&#013;&#010;官网上的例子：&#013;&#010;&#013;&#010;public class MyMapper extends RichMapFunction&lt;String, String&gt; {&#013;&#010;private transient Counter counter;&#013;&#010;@Override&#013;&#010;public void open(Configuration config) {&#013;&#010;  this.counter = getRuntimeContext()&#013;&#010;    .getMetricGroup()&#013;&#010;    .counter(\"myCounter\");&#013;&#010;}&#013;&#010;@Override&#013;&#010;public String map(String value) throws Exception {&#013;&#010;  this.counter.inc();&#013;&#010;  return value;&#013;&#010;}&#013;&#010;}&#013;&#010;&#013;&#010;我想要根据 map 方法中传入的参数生成不同的 监控指标，怎样可以实现呢？&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn&#013;&#010;&#013;&#010;&#013;&#010;",
        "depth": "2",
        "reply": "<202007021534287317879@geekplus.com.cn>"
    },
    {
        "id": "<CAMhjQvjQ=uEX+FNSVZfHcOj0zVvHFX8ufcEV9vR-h95tmFrpRA@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 04:35:34 GMT",
        "subject": "Re: 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？",
        "content": "我猜你是想要将 table name 作为一个标签方便后期分组查询过滤？&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月3日周五 上午10:24写道：&#013;&#010;&#013;&#010;&gt; public void invoke(ObjectNode node, Context context) throws Exception {&#013;&#010;&gt;&#013;&#010;&gt;     String tableName = node.get(\"metadata\").get(\"topic\").asText();&#013;&#010;&gt;     Meter meter = getRuntimeContext().getMetricGroup().meter(tableName,&#013;&#010;&gt; new MeterView(10));&#013;&#010;&gt;     meter.markEvent();&#013;&#010;&gt;     log.info(\"### counter: \" + meter.toString() + \"\\t\" +&#013;&#010;&gt; meter.getCount());&#013;&#010;&gt;&#013;&#010;&gt; 如上面代码所示，在 invoke 方法中解析得到 tableName， 以 tableName 名字作为&#010;metrics.&#013;&#010;&gt; 但这样写每一消息下来了后相当于重新定义了 这个 metrics ， 又从&#010;0 开始计数了。&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Sender: kcz&#013;&#010;&gt; Send Time: 2020-07-03 09:13&#013;&#010;&gt; Receiver: wanglei2&#013;&#010;&gt; Subject: 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;&gt; 按照你的描述 你就是少了tablename，那么你解析log 得到了tablename又做metric就好了吧&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------ 原始邮件 ------------------&#013;&#010;&gt; 发件人: 王磊2 &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;&gt; 发送时间: 2020年7月2日 21:46&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;, 17610775726 &lt;17610775726@163.com&gt;&#013;&#010;&gt; 主题: 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 没有明白你说的实现方式。&#013;&#010;&gt;&#013;&#010;&gt; 我最终要得到类似的 Metrics:  myCounter_table1, myCounter_table2, ...,&#013;&#010;&gt; myCounter_tableX&#013;&#010;&gt; 但我看代码中 Metrics 的初始化都是在 open 方法中的，在这个方法中我没法得到&#010;tableName 是什么。&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------------------------------------------------------&#013;&#010;&gt; 发件人：JasonLee &lt;17610775726@163.com&gt;&#013;&#010;&gt; 发送时间：2020年7月2日(星期四) 21:12&#013;&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主 题：回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;&gt;&#013;&#010;&gt; 你把tablename传到下面metric里不就行了吗&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; JasonLee&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：17610775726@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; Signature is customized by Netease Mail Master&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月02日 16:39，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt;&#013;&#010;&gt; 全都是同一种类型的 metrics.&#013;&#010;&gt; 比如消息中是 mysql binlog 解析结果，我想要根据消息内容拿到 tableName，&#010;按 tableName 生成不同名称的&#013;&#010;&gt; metrics（但都是 meter 类型）&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 发件人： JasonLee&#013;&#010;&gt; 发送时间： 2020-07-02 16:16&#013;&#010;&gt; 收件人： user-zh&#013;&#010;&gt; 主题： 回复：在一个 flink operator 中怎样根据消息内容动态生成多个监控指标？&#013;&#010;&gt; 是要生成不同类型的metric吗 比如counter meter ？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; JasonLee&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：17610775726@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; Signature is customized by Netease Mail Master&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月02日 15:34，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt;&#013;&#010;&gt; 官网上的例子：&#013;&#010;&gt;&#013;&#010;&gt; public class MyMapper extends RichMapFunction&lt;String, String&gt; {&#013;&#010;&gt; private transient Counter counter;&#013;&#010;&gt; @Override&#013;&#010;&gt; public void open(Configuration config) {&#013;&#010;&gt;   this.counter = getRuntimeContext()&#013;&#010;&gt;     .getMetricGroup()&#013;&#010;&gt;     .counter(\"myCounter\");&#013;&#010;&gt; }&#013;&#010;&gt; @Override&#013;&#010;&gt; public String map(String value) throws Exception {&#013;&#010;&gt;   this.counter.inc();&#013;&#010;&gt;   return value;&#013;&#010;&gt; }&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt; 我想要根据 map 方法中传入的参数生成不同的 监控指标，怎样可以实现呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<202007021534287317879@geekplus.com.cn>"
    },
    {
        "id": "<cefdba0.5606.1730e872d38.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 07:56:46 GMT",
        "subject": "flink asynctablefunction调用异常",
        "content": "hi，&#010;我在使用flink 1.10.1 blink planner，通过扩展tablesourcesinkfactory和asynctablefunction扩展了一个维表，维表会初始化并调用rpc服务。&#010;遇到一个比较诡异的问题，作业在执行如下join的sql时，没有任何输出，等一段时间后，抛出了异常：Caused&#010;by : java.lang.Exception: Could not complete the stream element: org.apache.flink.table.dataformat.BinaryRow....&#010; caused by : java.util.concurrent.TimeoutException: Async function call has timed out.&#010;&#010;&#010;我开了debug日志，debug时在我的lookupfunction.eval里，可以正常调用rpc接口服务并future.complete，但是并不输出任何结果。不确定可能是啥原因。望指导。谢谢。",
        "depth": "0",
        "reply": "<cefdba0.5606.1730e872d38.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<68E33DD8-CE90-474F-B3A3-1CBDB43D71FD@gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 08:29:14 GMT",
        "subject": "Re: flink asynctablefunction调用异常",
        "content": "可以分享下你的 AsyncTableFunction 的实现吗？&#010;&#010;Best,&#010;Jark&#010;&#010;&gt; 2020年7月2日 15:56，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt; &#010;&gt; hi，&#010;&gt; 我在使用flink 1.10.1 blink planner，通过扩展tablesourcesinkfactory和asynctablefunction扩展了一个维表，维表会初始化并调用rpc服务。&#010;&gt; 遇到一个比较诡异的问题，作业在执行如下join的sql时，没有任何输出，等一段时间后，抛出了异常：Caused&#010;by : java.lang.Exception: Could not complete the stream element: org.apache.flink.table.dataformat.BinaryRow....&#010; caused by : java.util.concurrent.TimeoutException: Async function call has timed out.&#010;&gt; &#010;&gt; &#010;&gt; 我开了debug日志，debug时在我的lookupfunction.eval里，可以正常调用rpc接口服务并future.complete，但是并不输出任何结果。不确定可能是啥原因。望指导。谢谢。&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<cefdba0.5606.1730e872d38.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<657fa5da.e00b.17313e89f91.Coremail.fszwfly@163.com>",
        "from": "forideal  &lt;fszw...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 09:01:19 GMT",
        "subject": "Re:flink asynctablefunction调用异常",
        "content": "Hi sunfulin：&#010;     &#010;      我这么实现是可以的。&#010;public void eval(CompletableFuture&lt;Collection&lt;Row&gt;&gt; result, String key) {&#010;    executorService.submit(() -&gt; {&#010;try {&#010;Row row = fetchdata(key);&#010;            if (row != null) {&#010;result.complete(Collections.singletonList(row));&#010;} else {&#010;result.complete(Collections.singletonList(new Row(this.fieldNames.length)));&#010;}&#010;        } catch (Exception e) {&#010;result.complete(Collections.singletonList(new Row(this.fieldNames.length)));&#010;}&#010;    });&#010;}&#010;&#010;&#010;&#010;&#010;Best forideal.&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-02 15:56:46，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;hi，&#010;&gt;我在使用flink 1.10.1 blink planner，通过扩展tablesourcesinkfactory和asynctablefunction扩展了一个维表，维表会初始化并调用rpc服务。&#010;&gt;遇到一个比较诡异的问题，作业在执行如下join的sql时，没有任何输出，等一段时间后，抛出了异常：Caused&#010;by : java.lang.Exception: Could not complete the stream element: org.apache.flink.table.dataformat.BinaryRow....&#010; caused by : java.util.concurrent.TimeoutException: Async function call has timed out.&#010;&gt;&#010;&gt;&#010;&gt;我开了debug日志，debug时在我的lookupfunction.eval里，可以正常调用rpc接口服务并future.complete，但是并不输出任何结果。不确定可能是啥原因。望指导。谢谢。&#010;",
        "depth": "1",
        "reply": "<cefdba0.5606.1730e872d38.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<6073ce3f.8259.17314a5c894.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 12:27:55 GMT",
        "subject": "Re:Re:flink asynctablefunction调用异常",
        "content": "&#010;&#010;&#010;hi&#010;抱歉忘记回复了。经过进一步调试发现，是因为定义的schema的column类型，与实际获取到的字段类型不一致导致。主要是在调试的过程中，ComplettedFuture.complete会吃掉这种类型不一致的异常，也不下发数据。看源码发现只会在timeout的时候才调用future.completeException。记录下。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-03 17:01:19，\"forideal\" &lt;fszwfly@163.com&gt; 写道：&#010;&gt;Hi sunfulin：&#010;&gt;     &#010;&gt;      我这么实现是可以的。&#010;&gt;public void eval(CompletableFuture&lt;Collection&lt;Row&gt;&gt; result, String key) {&#010;&gt;    executorService.submit(() -&gt; {&#010;&gt;try {&#010;&gt;Row row = fetchdata(key);&#010;&gt;            if (row != null) {&#010;&gt;result.complete(Collections.singletonList(row));&#010;&gt;} else {&#010;&gt;result.complete(Collections.singletonList(new Row(this.fieldNames.length)));&#010;&gt;}&#010;&gt;        } catch (Exception e) {&#010;&gt;result.complete(Collections.singletonList(new Row(this.fieldNames.length)));&#010;&gt;}&#010;&gt;    });&#010;&gt;}&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;Best forideal.&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;在 2020-07-02 15:56:46，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt;hi，&#010;&gt;&gt;我在使用flink 1.10.1 blink planner，通过扩展tablesourcesinkfactory和asynctablefunction扩展了一个维表，维表会初始化并调用rpc服务。&#010;&gt;&gt;遇到一个比较诡异的问题，作业在执行如下join的sql时，没有任何输出，等一段时间后，抛出了异常：Caused&#010;by : java.lang.Exception: Could not complete the stream element: org.apache.flink.table.dataformat.BinaryRow....&#010; caused by : java.util.concurrent.TimeoutException: Async function call has timed out.&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;我开了debug日志，debug时在我的lookupfunction.eval里，可以正常调用rpc接口服务并future.complete，但是并不输出任何结果。不确定可能是啥原因。望指导。谢谢。&#010;",
        "depth": "2",
        "reply": "<cefdba0.5606.1730e872d38.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<74e76f7e.8e5c.1730eeb9325.Coremail.xiaguo98@163.com>",
        "from": "xuhaiLong &lt;xiagu...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 09:46:26 GMT",
        "subject": "Position out of bounds.",
        "content": "flink 1.10  onYarn&#010;job 中 有一个MapState[Long,Bean]&#010; https://www.helloimg.com/image/Pe1QR&#010; 程序启动一段时间（20分钟）后出现了 附件中的异常&#010;查看对应源码也没看懂是什么引起的异常&#010;https://www.helloimg.com/image/Peqc5&#010;&#010;",
        "depth": "0",
        "reply": "<74e76f7e.8e5c.1730eeb9325.Coremail.xiaguo98@163.com>"
    },
    {
        "id": "<2408bb6e-36fa-46b5-adf8-8f7c4795f5bf.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 10:39:35 GMT",
        "subject": "回复：Position out of bounds.",
        "content": "你好,请问解决了么,我看了下源码,好像是一个bug&#010;DataOutputSerializer&#010;&#010;@Override&#010;public void write(int b) throws IOException {&#010;   if (this.position &gt;= this.buffer.length) {&#010;      resize(1);&#010;   }&#010;   this.buffer[this.position++] = (byte) (b &amp; 0xff);&#010;}&#010;此处position应该自增&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：xuhaiLong &lt;xiaguo98@163.com&gt;&#010;发送时间：2020年7月2日(星期四) 17:46&#010;收件人：flink 中文社区 &lt;user-zh@flink.apache.org&gt;&#010;主　题：Position out of bounds.&#010;&#010;  &#010;flink 1.10  onYarn&#010;job 中 有一个MapState[Long,Bean]&#010;https://www.helloimg.com/image/Pe1QR&#010; 程序启动一段时间（20分钟）后出现了 附件中的异常&#010;查看对应源码也没看懂是什么引起的异常&#010;https://www.helloimg.com/image/Peqc5&#010;&#010;",
        "depth": "1",
        "reply": "<74e76f7e.8e5c.1730eeb9325.Coremail.xiaguo98@163.com>"
    },
    {
        "id": "<68d6b948.9280.1730f22ae16.Coremail.xiaguo98@163.com>",
        "from": "xuhaiLong &lt;xiagu...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 10:46:37 GMT",
        "subject": "回复：Position out of bounds.",
        "content": "感谢&#010;没看明白这个bug引起的原因是什么，或者说有什么合适的解决方案？&#010;&#010;&#010;| |&#010;夏*&#010;|&#010;|&#010;邮箱：xiaguo98@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月02日 18:39，夏帅 写道：&#010;你好,请问解决了么,我看了下源码,好像是一个bug&#010;DataOutputSerializer&#010;&#010;@Override&#010;public void write(int b) throws IOException {&#010;  if (this.position &gt;= this.buffer.length) {&#010;     resize(1);&#010;  }&#010;  this.buffer[this.position++] = (byte) (b &amp; 0xff);&#010;}&#010;此处position应该自增&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：xuhaiLong &lt;xiaguo98@163.com&gt;&#010;发送时间：2020年7月2日(星期四) 17:46&#010;收件人：flink 中文社区 &lt;user-zh@flink.apache.org&gt;&#010;主　题：Position out of bounds.&#010;&#010; &#010;flink 1.10  onYarn&#010;job 中 有一个MapState[Long,Bean]&#010;https://www.helloimg.com/image/Pe1QR&#010;程序启动一段时间（20分钟）后出现了 附件中的异常&#010;查看对应源码也没看懂是什么引起的异常&#010;https://www.helloimg.com/image/Peqc5&#010;&#010;",
        "depth": "2",
        "reply": "<74e76f7e.8e5c.1730eeb9325.Coremail.xiaguo98@163.com>"
    },
    {
        "id": "<4053e3fa-e12f-4345-af1f-3750a7cbc33d.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 11:00:31 GMT",
        "subject": "回复：Position out of bounds.",
        "content": "不好意思，看错了，这里是自增了&#010;&#010;&#010;&#010;&#010;&#010;来自钉钉专属商务邮箱------------------------------------------------------------------&#010;发件人：xuhaiLong&lt;xiaguo98@163.com&gt;&#010;日　期：2020年07月02日 18:46:37&#010;收件人：夏帅&lt;jkillers@dingtalk.com.INVALID&gt;&#010;抄　送：user-zh&lt;user-zh@flink.apache.org&gt;&#010;主　题：回复：Position out of bounds.&#010;&#010;感谢&#010;没看明白这个bug引起的原因是什么，或者说有什么合适的解决方案？&#010;&#010;&#010;| |&#010;夏*&#010;|&#010;|&#010;邮箱：xiaguo98@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月02日 18:39，夏帅 写道：&#010;你好,请问解决了么,我看了下源码,好像是一个bug&#010;DataOutputSerializer&#010;&#010;@Override&#010;public void write(int b) throws IOException {&#010;  if (this.position &gt;= this.buffer.length) {&#010;     resize(1);&#010;  }&#010;  this.buffer[this.position++] = (byte) (b &amp; 0xff);&#010;}&#010;此处position应该自增&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：xuhaiLong &lt;xiaguo98@163.com&gt;&#010;发送时间：2020年7月2日(星期四) 17:46&#010;收件人：flink 中文社区 &lt;user-zh@flink.apache.org&gt;&#010;主　题：Position out of bounds.&#010;&#010;&#010;flink 1.10  onYarn&#010;job 中 有一个MapState[Long,Bean]&#010;https://www.helloimg.com/image/Pe1QR&#010;程序启动一段时间（20分钟）后出现了 附件中的异常&#010;查看对应源码也没看懂是什么引起的异常&#010;https://www.helloimg.com/image/Peqc5&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<74e76f7e.8e5c.1730eeb9325.Coremail.xiaguo98@163.com>"
    },
    {
        "id": "<35d16b28.9361.1730f3045db.Coremail.xiaguo98@163.com>",
        "from": "xuhaiLong &lt;xiagu...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 11:01:28 GMT",
        "subject": "Re: Position out of bounds.",
        "content": "感谢&#010;没看明白这个bug引起的原因是什么，或者说有什么合适的解决方案？&#010;&#010;&#010;On 7/2/2020 18:39，夏帅&lt;jkillers@dingtalk.com.INVALID&gt; wrote：&#010;你好,请问解决了么,我看了下源码,好像是一个bug&#010;DataOutputSerializer&#010;&#010;@Override&#010;public void write(int b) throws IOException {&#010;if (this.position &gt;= this.buffer.length) {&#010;resize(1);&#010;}&#010;this.buffer[this.position++] = (byte) (b &amp; 0xff);&#010;}&#010;此处position应该自增&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：xuhaiLong &lt;xiaguo98@163.com&gt;&#010;发送时间：2020年7月2日(星期四) 17:46&#010;收件人：flink 中文社区 &lt;user-zh@flink.apache.org&gt;&#010;主　题：Position out of bounds.&#010;&#010;&#010;flink 1.10  onYarn&#010;job 中 有一个MapState[Long,Bean]&#010;https://www.helloimg.com/image/Pe1QR&#010;程序启动一段时间（20分钟）后出现了 附件中的异常&#010;查看对应源码也没看懂是什么引起的异常&#010;https://www.helloimg.com/image/Peqc5&#010;&#010;",
        "depth": "2",
        "reply": "<74e76f7e.8e5c.1730eeb9325.Coremail.xiaguo98@163.com>"
    },
    {
        "id": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 10:05:27 GMT",
        "subject": "rocksdb的block cache usage应该如何使用",
        "content": "&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#010;&#010;&#010;我们的作业一个TM的内存设置如下：&#010;&#010;taskmanager.memory.process.size: 23000m&#010;taskmanager.memory.managed.fraction: 0.4&#010;&#010;ui上显示的Flink Managed MEM是8.48G。&#010;&#010;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#010;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#010;&#010;&#010;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#010;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#010;&#010;&#010;请问这个指标应该如何使用？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "0",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CH2PR20MB2966EE9DA5D1D4014A3588E3DA6A0@CH2PR20MB2966.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:10:13 GMT",
        "subject": "Re: rocksdb的block cache usage应该如何使用",
        "content": "Hi&#013;&#010;&#013;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Thursday, July 2, 2020 18:05&#013;&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;&#013;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#013;&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#013;&#010;&#013;&#010;&#013;&#010;我们的作业一个TM的内存设置如下：&#013;&#010;&#013;&#010;taskmanager.memory.process.size: 23000m&#013;&#010;taskmanager.memory.managed.fraction: 0.4&#013;&#010;&#013;&#010;ui上显示的Flink Managed MEM是8.48G。&#013;&#010;&#013;&#010;&#013;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#013;&#010;&#013;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#013;&#010;&#013;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#013;&#010;&#013;&#010;&#013;&#010;请问这个指标应该如何使用？&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "1",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<7e831cdb.e8b3.17313552cc1.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:20:15 GMT",
        "subject": "回复：rocksdb的block cache usage应该如何使用",
        "content": "thanks yun tang！&#010;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#010;Hi&#010;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Thursday, July 2, 2020 18:05&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: rocksdb的block cache usage应该如何使用&#010;&#010;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#010;&#010;&#010;我们的作业一个TM的内存设置如下：&#010;&#010;taskmanager.memory.process.size: 23000m&#010;taskmanager.memory.managed.fraction: 0.4&#010;&#010;ui上显示的Flink Managed MEM是8.48G。&#010;&#010;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#010;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#010;&#010;&#010;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#010;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#010;&#010;&#010;请问这个指标应该如何使用？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "2",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CH2PR20MB29665B78B05A76BC1317FF96DA6A0@CH2PR20MB2966.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:59:55 GMT",
        "subject": "Re: 回复：rocksdb的block cache usage应该如何使用",
        "content": "Hi&#013;&#010;&#013;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 14:20&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;thanks yun tang！&#013;&#010;&#013;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Thursday, July 2, 2020 18:05&#013;&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;&#013;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#013;&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#013;&#010;&#013;&#010;&#013;&#010;我们的作业一个TM的内存设置如下：&#013;&#010;&#013;&#010;taskmanager.memory.process.size: 23000m&#013;&#010;taskmanager.memory.managed.fraction: 0.4&#013;&#010;&#013;&#010;ui上显示的Flink Managed MEM是8.48G。&#013;&#010;&#013;&#010;&#013;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#013;&#010;&#013;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#013;&#010;&#013;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#013;&#010;&#013;&#010;&#013;&#010;请问这个指标应该如何使用？&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "2",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<7f7c1161.eeea.173137fff21.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 07:07:02 GMT",
        "subject": "回复：rocksdb的block cache usage应该如何使用",
        "content": "hi yun tang！&#010;&#010;因为网络或者不可抗力导致pod重生，作业会重启，目前作业没有开启checkpoint，恢复等价继续消费最新数据计算，运行一段时间很容易内存超用被os&#010;kill，然后重启，再运行一段时间，间隔变短，死的越来越频繁。&#010;&#010;从现象上看很像是内存没有释放，这种场景下，上一次作业残留的未到水位线还没有被触发计算的数据是否在作业重启过程中被清除了？&#010;&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 14:59，Yun Tang 写道：&#010;Hi&#010;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 14:20&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;thanks yun tang！&#010;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#010;Hi&#010;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Thursday, July 2, 2020 18:05&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: rocksdb的block cache usage应该如何使用&#010;&#010;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#010;&#010;&#010;我们的作业一个TM的内存设置如下：&#010;&#010;taskmanager.memory.process.size: 23000m&#010;taskmanager.memory.managed.fraction: 0.4&#010;&#010;ui上显示的Flink Managed MEM是8.48G。&#010;&#010;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#010;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#010;&#010;&#010;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#010;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#010;&#010;&#010;请问这个指标应该如何使用？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "3",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CH2PR20MB296648FB28496ED339766436DA6A0@CH2PR20MB2966.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 07:13:43 GMT",
        "subject": "Re: 回复：rocksdb的block cache usage应该如何使用",
        "content": "Hi&#013;&#010;&#013;&#010;如果是没有开启Checkpoint，作业是如何做到failover的？failover的时候一定需要从Checkpoint加载数据的。还是说你其实开启了Checkpoint，但是Checkpoint的interval设置的很大，所以每次failover相当于作业重新运行。&#013;&#010;如果都没有从checkpoint加载数据，哪里来的历史数据呢？作业一旦发生failover，state&#010;backend的数据都需要清空然后再启动的时候进行加载。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 15:07&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;hi yun tang！&#013;&#010;&#013;&#010;因为网络或者不可抗力导致pod重生，作业会重启，目前作业没有开启checkpoint，恢复等价继续消费最新数据计算，运行一段时间很容易内存超用被os&#010;kill，然后重启，再运行一段时间，间隔变短，死的越来越频繁。&#013;&#010;&#013;&#010;从现象上看很像是内存没有释放，这种场景下，上一次作业残留的未到水位线还没有被触发计算的数据是否在作业重启过程中被清除了？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&gt;&#013;&#010;[https://mail-online.nosdn.127.net/qiyelogo/defaultAvatar.png]&#013;&#010;a511955993&#013;&#010;邮箱：a511955993@163.com&#013;&#010;&#013;&#010;签名由 网易邮箱大师&lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt; 定制&#013;&#010;&#013;&#010;在2020年07月03日 14:59，Yun Tang&lt;mailto:myasuka@live.com&gt; 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 14:20&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;thanks yun tang！&#013;&#010;&#013;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Thursday, July 2, 2020 18:05&#013;&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;&#013;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#013;&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#013;&#010;&#013;&#010;&#013;&#010;我们的作业一个TM的内存设置如下：&#013;&#010;&#013;&#010;taskmanager.memory.process.size: 23000m&#013;&#010;taskmanager.memory.managed.fraction: 0.4&#013;&#010;&#013;&#010;ui上显示的Flink Managed MEM是8.48G。&#013;&#010;&#013;&#010;&#013;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#013;&#010;&#013;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#013;&#010;&#013;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#013;&#010;&#013;&#010;&#013;&#010;请问这个指标应该如何使用？&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "3",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<127a1ba2.f0e4.173138e0089.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 07:22:20 GMT",
        "subject": "回复：rocksdb的block cache usage应该如何使用",
        "content": "Hi&#010;&#010;作业只配置了重启策略，作业如果fail了，只会重启，没有恢复历史数据。&#010;&#010;【作业一旦发生failover，state backend的数据都需要清空然后再启动的时候进行加载。】&#010;我目前遇到的情况是作业fail重启，pod就很容易被os kill，只能重构集群解决。&#010;&#010;详情可见&#010;http://apache-flink.147419.n8.nabble.com/Checkpoint-td4406.html&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 15:13，Yun Tang 写道：&#010;Hi&#010;&#010;如果是没有开启Checkpoint，作业是如何做到failover的？failover的时候一定需要从Checkpoint加载数据的。还是说你其实开启了Checkpoint，但是Checkpoint的interval设置的很大，所以每次failover相当于作业重新运行。&#010;如果都没有从checkpoint加载数据，哪里来的历史数据呢？作业一旦发生failover，state&#010;backend的数据都需要清空然后再启动的时候进行加载。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 15:07&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;hi yun tang！&#010;&#010;因为网络或者不可抗力导致pod重生，作业会重启，目前作业没有开启checkpoint，恢复等价继续消费最新数据计算，运行一段时间很容易内存超用被os&#010;kill，然后重启，再运行一段时间，间隔变短，死的越来越频繁。&#010;&#010;从现象上看很像是内存没有释放，这种场景下，上一次作业残留的未到水位线还没有被触发计算的数据是否在作业重启过程中被清除了？&#010;&#010;&#010;&#010;&#010;&lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;[https://mail-online.nosdn.127.net/qiyelogo/defaultAvatar.png]&#010;a511955993&#010;邮箱：a511955993@163.com&#010;&#010;签名由 网易邮箱大师&lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt;&#010;定制&#010;&#010;在2020年07月03日 14:59，Yun Tang&lt;mailto:myasuka@live.com&gt; 写道：&#010;Hi&#010;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 14:20&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;thanks yun tang！&#010;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#010;Hi&#010;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Thursday, July 2, 2020 18:05&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: rocksdb的block cache usage应该如何使用&#010;&#010;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#010;&#010;&#010;我们的作业一个TM的内存设置如下：&#010;&#010;taskmanager.memory.process.size: 23000m&#010;taskmanager.memory.managed.fraction: 0.4&#010;&#010;ui上显示的Flink Managed MEM是8.48G。&#010;&#010;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#010;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#010;&#010;&#010;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#010;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#010;&#010;&#010;请问这个指标应该如何使用？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "4",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CH2PR20MB2966AC9E525406A50F0970CADA6A0@CH2PR20MB2966.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 11:13:11 GMT",
        "subject": "Re: 回复：rocksdb的block cache usage应该如何使用",
        "content": "hi&#013;&#010;&#013;&#010;有采集过内存使用情况么，推荐使用jemalloc的预先加载方式[1][2]来sample&#010;JVM的内存使用，观察是否有malloc的内存存在超用的场景。需要配置相关参数&#010;containerized.taskmanager.env.MALLOC_CONF 和 containerized.taskmanager.env.LD_PRELOAD&#013;&#010;&#013;&#010;&#013;&#010;[1] https://github.com/jemalloc/jemalloc/wiki/Use-Case%3A-Heap-Profiling&#013;&#010;[2] https://www.evanjones.ca/java-native-leak-bug.html&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;&#013;&#010;&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 15:22&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;作业只配置了重启策略，作业如果fail了，只会重启，没有恢复历史数据。&#013;&#010;&#013;&#010;【作业一旦发生failover，state backend的数据都需要清空然后再启动的时候进行加载。】&#013;&#010;我目前遇到的情况是作业fail重启，pod就很容易被os kill，只能重构集群解决。&#013;&#010;&#013;&#010;详情可见&#013;&#010;http://apache-flink.147419.n8.nabble.com/Checkpoint-td4406.html&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;在2020年07月03日 15:13，Yun Tang 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;如果是没有开启Checkpoint，作业是如何做到failover的？failover的时候一定需要从Checkpoint加载数据的。还是说你其实开启了Checkpoint，但是Checkpoint的interval设置的很大，所以每次failover相当于作业重新运行。&#013;&#010;如果都没有从checkpoint加载数据，哪里来的历史数据呢？作业一旦发生failover，state&#010;backend的数据都需要清空然后再启动的时候进行加载。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 15:07&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;hi yun tang！&#013;&#010;&#013;&#010;因为网络或者不可抗力导致pod重生，作业会重启，目前作业没有开启checkpoint，恢复等价继续消费最新数据计算，运行一段时间很容易内存超用被os&#010;kill，然后重启，再运行一段时间，间隔变短，死的越来越频繁。&#013;&#010;&#013;&#010;从现象上看很像是内存没有释放，这种场景下，上一次作业残留的未到水位线还没有被触发计算的数据是否在作业重启过程中被清除了？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#013;&#010;[https://mail-online.nosdn.127.net/qiyelogo/defaultAvatar.png]&#013;&#010;a511955993&#013;&#010;邮箱：a511955993@163.com&#013;&#010;&#013;&#010;签名由 网易邮箱大师&lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt;&#010;定制&#013;&#010;&#013;&#010;在2020年07月03日 14:59，Yun Tang&lt;mailto:myasuka@live.com&gt; 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 14:20&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;thanks yun tang！&#013;&#010;&#013;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Thursday, July 2, 2020 18:05&#013;&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;&#013;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#013;&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#013;&#010;&#013;&#010;&#013;&#010;我们的作业一个TM的内存设置如下：&#013;&#010;&#013;&#010;taskmanager.memory.process.size: 23000m&#013;&#010;taskmanager.memory.managed.fraction: 0.4&#013;&#010;&#013;&#010;ui上显示的Flink Managed MEM是8.48G。&#013;&#010;&#013;&#010;&#013;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#013;&#010;&#013;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#013;&#010;&#013;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#013;&#010;&#013;&#010;&#013;&#010;请问这个指标应该如何使用？&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "4",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<504d2ccf.8533.17322c3ffab.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 06:15:37 GMT",
        "subject": "回复：rocksdb的block cache usage应该如何使用",
        "content": "hi yun tang！&#010;&#010;我在容器内加入了libjemalloc.so.2并且在配置中加上了&#010;containerized.master.env.LD_PRELOAD: \"/opt/jemalloc/lib/libjemalloc.so.2\"&#010;containerized.master.env.MALLOC_CONF: \"prof:true,lg_prof_interval:25,lg_prof_sample:17\"&#010;&#010;请问要如何可以得到内存文件？试着kill一个tm，找不到对应的heap文件。求助&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 19:13，Yun Tang 写道：&#010;hi&#010;&#010;有采集过内存使用情况么，推荐使用jemalloc的预先加载方式[1][2]来sample&#010;JVM的内存使用，观察是否有malloc的内存存在超用的场景。需要配置相关参数&#010;containerized.taskmanager.env.MALLOC_CONF 和 containerized.taskmanager.env.LD_PRELOAD&#010;&#010;&#010;[1] https://github.com/jemalloc/jemalloc/wiki/Use-Case%3A-Heap-Profiling&#010;[2] https://www.evanjones.ca/java-native-leak-bug.html&#010;&#010;祝好&#010;唐云&#010;&#010;&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 15:22&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;Hi&#010;&#010;作业只配置了重启策略，作业如果fail了，只会重启，没有恢复历史数据。&#010;&#010;【作业一旦发生failover，state backend的数据都需要清空然后再启动的时候进行加载。】&#010;我目前遇到的情况是作业fail重启，pod就很容易被os kill，只能重构集群解决。&#010;&#010;详情可见&#010;http://apache-flink.147419.n8.nabble.com/Checkpoint-td4406.html&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 15:13，Yun Tang 写道：&#010;Hi&#010;&#010;如果是没有开启Checkpoint，作业是如何做到failover的？failover的时候一定需要从Checkpoint加载数据的。还是说你其实开启了Checkpoint，但是Checkpoint的interval设置的很大，所以每次failover相当于作业重新运行。&#010;如果都没有从checkpoint加载数据，哪里来的历史数据呢？作业一旦发生failover，state&#010;backend的数据都需要清空然后再启动的时候进行加载。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 15:07&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;hi yun tang！&#010;&#010;因为网络或者不可抗力导致pod重生，作业会重启，目前作业没有开启checkpoint，恢复等价继续消费最新数据计算，运行一段时间很容易内存超用被os&#010;kill，然后重启，再运行一段时间，间隔变短，死的越来越频繁。&#010;&#010;从现象上看很像是内存没有释放，这种场景下，上一次作业残留的未到水位线还没有被触发计算的数据是否在作业重启过程中被清除了？&#010;&#010;&#010;&#010;&#010;&lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;[https://mail-online.nosdn.127.net/qiyelogo/defaultAvatar.png]&#010;a511955993&#010;邮箱：a511955993@163.com&#010;&#010;签名由 网易邮箱大师&lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt;&#010;定制&#010;&#010;在2020年07月03日 14:59，Yun Tang&lt;mailto:myasuka@live.com&gt; 写道：&#010;Hi&#010;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 14:20&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;thanks yun tang！&#010;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#010;Hi&#010;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Thursday, July 2, 2020 18:05&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: rocksdb的block cache usage应该如何使用&#010;&#010;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#010;&#010;&#010;我们的作业一个TM的内存设置如下：&#010;&#010;taskmanager.memory.process.size: 23000m&#010;taskmanager.memory.managed.fraction: 0.4&#010;&#010;ui上显示的Flink Managed MEM是8.48G。&#010;&#010;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#010;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#010;&#010;&#010;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#010;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#010;&#010;&#010;请问这个指标应该如何使用？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "5",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CY4PR20MB1221017D3C29963619484019DA660@CY4PR20MB1221.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 09:47:24 GMT",
        "subject": "Re: 回复：rocksdb的block cache usage应该如何使用",
        "content": "Hi&#013;&#010;&#013;&#010;你的jemalloc有带debug的重新编译么? 例如用下面的命令重新编译jemalloc得到相关的so文件&#013;&#010;./configure --enable-prof --enable-stats --enable-debug --enable-fill&#013;&#010;make&#013;&#010;&#013;&#010;其次最好指定dump文件的输出地址，例如在 MALLOC_CONF中加上前缀的配置&#010; prof_prefix:/tmp/jeprof.out ，以确保文件位置可写。&#013;&#010;&#013;&#010;最后，由于你是在容器中跑，在容器退出前要保证相关文件能上传或者退出时候hang住一段时间，否则相关dump的文件无法看到了&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Monday, July 6, 2020 14:15&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;hi yun tang！&#013;&#010;&#013;&#010;我在容器内加入了libjemalloc.so.2并且在配置中加上了&#013;&#010;containerized.master.env.LD_PRELOAD: \"/opt/jemalloc/lib/libjemalloc.so.2\"&#013;&#010;containerized.master.env.MALLOC_CONF: \"prof:true,lg_prof_interval:25,lg_prof_sample:17\"&#013;&#010;&#013;&#010;请问要如何可以得到内存文件？试着kill一个tm，找不到对应的heap文件。求助&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;在2020年07月03日 19:13，Yun Tang 写道：&#013;&#010;hi&#013;&#010;&#013;&#010;有采集过内存使用情况么，推荐使用jemalloc的预先加载方式[1][2]来sample&#010;JVM的内存使用，观察是否有malloc的内存存在超用的场景。需要配置相关参数&#010;containerized.taskmanager.env.MALLOC_CONF 和 containerized.taskmanager.env.LD_PRELOAD&#013;&#010;&#013;&#010;&#013;&#010;[1] https://github.com/jemalloc/jemalloc/wiki/Use-Case%3A-Heap-Profiling&#013;&#010;[2] https://www.evanjones.ca/java-native-leak-bug.html&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;&#013;&#010;&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 15:22&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;作业只配置了重启策略，作业如果fail了，只会重启，没有恢复历史数据。&#013;&#010;&#013;&#010;【作业一旦发生failover，state backend的数据都需要清空然后再启动的时候进行加载。】&#013;&#010;我目前遇到的情况是作业fail重启，pod就很容易被os kill，只能重构集群解决。&#013;&#010;&#013;&#010;详情可见&#013;&#010;http://apache-flink.147419.n8.nabble.com/Checkpoint-td4406.html&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;在2020年07月03日 15:13，Yun Tang 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;如果是没有开启Checkpoint，作业是如何做到failover的？failover的时候一定需要从Checkpoint加载数据的。还是说你其实开启了Checkpoint，但是Checkpoint的interval设置的很大，所以每次failover相当于作业重新运行。&#013;&#010;如果都没有从checkpoint加载数据，哪里来的历史数据呢？作业一旦发生failover，state&#010;backend的数据都需要清空然后再启动的时候进行加载。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 15:07&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;hi yun tang！&#013;&#010;&#013;&#010;因为网络或者不可抗力导致pod重生，作业会重启，目前作业没有开启checkpoint，恢复等价继续消费最新数据计算，运行一段时间很容易内存超用被os&#010;kill，然后重启，再运行一段时间，间隔变短，死的越来越频繁。&#013;&#010;&#013;&#010;从现象上看很像是内存没有释放，这种场景下，上一次作业残留的未到水位线还没有被触发计算的数据是否在作业重启过程中被清除了？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#013;&#010;[https://mail-online.nosdn.127.net/qiyelogo/defaultAvatar.png]&#013;&#010;a511955993&#013;&#010;邮箱：a511955993@163.com&#013;&#010;&#013;&#010;签名由 网易邮箱大师&lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt;&#010;定制&#013;&#010;&#013;&#010;在2020年07月03日 14:59，Yun Tang&lt;mailto:myasuka@live.com&gt; 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Friday, July 3, 2020 14:20&#013;&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#013;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;thanks yun tang！&#013;&#010;&#013;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;&#013;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#013;&#010;Hi&#013;&#010;&#013;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#013;&#010;Sent: Thursday, July 2, 2020 18:05&#013;&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: rocksdb的block cache usage应该如何使用&#013;&#010;&#013;&#010;&#013;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#013;&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#013;&#010;&#013;&#010;&#013;&#010;我们的作业一个TM的内存设置如下：&#013;&#010;&#013;&#010;taskmanager.memory.process.size: 23000m&#013;&#010;taskmanager.memory.managed.fraction: 0.4&#013;&#010;&#013;&#010;ui上显示的Flink Managed MEM是8.48G。&#013;&#010;&#013;&#010;&#013;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#013;&#010;&#013;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#013;&#010;&#013;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#013;&#010;&#013;&#010;&#013;&#010;请问这个指标应该如何使用？&#013;&#010;&#013;&#010;| |&#013;&#010;a511955993&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：a511955993@163.com&#013;&#010;|&#013;&#010;&#013;&#010;签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "5",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<6e3ee54f.4b0d.17328c3d430.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:13:09 GMT",
        "subject": "回复：rocksdb的block cache usage应该如何使用",
        "content": "hi yun tang！&#010;&#010;下午通过配置yaml的方式修改env成功生成内存文件，目前在重新复现和获取文件ing！&#010;tanks！具体内存dump在获取ing&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月07日 17:47，Yun Tang 写道：&#010;Hi&#010;&#010;你的jemalloc有带debug的重新编译么? 例如用下面的命令重新编译jemalloc得到相关的so文件&#010;./configure --enable-prof --enable-stats --enable-debug --enable-fill&#010;make&#010;&#010;其次最好指定dump文件的输出地址，例如在 MALLOC_CONF中加上前缀的配置&#010; prof_prefix:/tmp/jeprof.out ，以确保文件位置可写。&#010;&#010;最后，由于你是在容器中跑，在容器退出前要保证相关文件能上传或者退出时候hang住一段时间，否则相关dump的文件无法看到了&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Monday, July 6, 2020 14:15&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;hi yun tang！&#010;&#010;我在容器内加入了libjemalloc.so.2并且在配置中加上了&#010;containerized.master.env.LD_PRELOAD: \"/opt/jemalloc/lib/libjemalloc.so.2\"&#010;containerized.master.env.MALLOC_CONF: \"prof:true,lg_prof_interval:25,lg_prof_sample:17\"&#010;&#010;请问要如何可以得到内存文件？试着kill一个tm，找不到对应的heap文件。求助&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 19:13，Yun Tang 写道：&#010;hi&#010;&#010;有采集过内存使用情况么，推荐使用jemalloc的预先加载方式[1][2]来sample&#010;JVM的内存使用，观察是否有malloc的内存存在超用的场景。需要配置相关参数&#010;containerized.taskmanager.env.MALLOC_CONF 和 containerized.taskmanager.env.LD_PRELOAD&#010;&#010;&#010;[1] https://github.com/jemalloc/jemalloc/wiki/Use-Case%3A-Heap-Profiling&#010;[2] https://www.evanjones.ca/java-native-leak-bug.html&#010;&#010;祝好&#010;唐云&#010;&#010;&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 15:22&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;Hi&#010;&#010;作业只配置了重启策略，作业如果fail了，只会重启，没有恢复历史数据。&#010;&#010;【作业一旦发生failover，state backend的数据都需要清空然后再启动的时候进行加载。】&#010;我目前遇到的情况是作业fail重启，pod就很容易被os kill，只能重构集群解决。&#010;&#010;详情可见&#010;http://apache-flink.147419.n8.nabble.com/Checkpoint-td4406.html&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 15:13，Yun Tang 写道：&#010;Hi&#010;&#010;如果是没有开启Checkpoint，作业是如何做到failover的？failover的时候一定需要从Checkpoint加载数据的。还是说你其实开启了Checkpoint，但是Checkpoint的interval设置的很大，所以每次failover相当于作业重新运行。&#010;如果都没有从checkpoint加载数据，哪里来的历史数据呢？作业一旦发生failover，state&#010;backend的数据都需要清空然后再启动的时候进行加载。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 15:07&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;hi yun tang！&#010;&#010;因为网络或者不可抗力导致pod重生，作业会重启，目前作业没有开启checkpoint，恢复等价继续消费最新数据计算，运行一段时间很容易内存超用被os&#010;kill，然后重启，再运行一段时间，间隔变短，死的越来越频繁。&#010;&#010;从现象上看很像是内存没有释放，这种场景下，上一次作业残留的未到水位线还没有被触发计算的数据是否在作业重启过程中被清除了？&#010;&#010;&#010;&#010;&#010;&lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;[https://mail-online.nosdn.127.net/qiyelogo/defaultAvatar.png]&#010;a511955993&#010;邮箱：a511955993@163.com&#010;&#010;签名由 网易邮箱大师&lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt;&#010;定制&#010;&#010;在2020年07月03日 14:59，Yun Tang&lt;mailto:myasuka@live.com&gt; 写道：&#010;Hi&#010;&#010;观察block cache usage的显示数值是否超过你的单个slot的managed memory，计算方法是&#010;managed memory / slot数目，得到一个slot的managed memory，将该数值与block cache&#010;usage比较，看内存是否超用。重启之后容易被os kill，使用的是从savepoint恢复数据么？&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Friday, July 3, 2020 14:20&#010;To: Yun Tang &lt;myasuka@live.com&gt;&#010;Cc: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: 回复：rocksdb的block cache usage应该如何使用&#010;&#010;thanks yun tang！&#010;&#010;那如果想通过block cache usage判断是否超过managed memory，该如何配置呢？&#010;最近遇到作业只要重启后很容易被os kill的情况，想对比下&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 14:10，Yun Tang 写道：&#010;Hi&#010;&#010;默认Flink启用了rocksDB 的managed memory，这里涉及到这个功能的实现原理，简单来说，一个slot里面的所有rocksDB实例底层“托管”内存的LRU&#010;block cache均是一个，这样你可以根据taskmanager和subtask_index 作为tag来区分，你会发现在同一个TM里面的某个subtask对应的不同column_family&#010;的block cache的数值均是完全相同的。所以不需要将这个数值进行求和统计。&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: SmileSmile &lt;a511955993@163.com&gt;&#010;Sent: Thursday, July 2, 2020 18:05&#010;To: Flink user-zh mailing list &lt;user-zh@flink.apache.org&gt;&#010;Subject: rocksdb的block cache usage应该如何使用&#010;&#010;&#010;通过 state.backend.rocksdb.metrics.block-cache-usage: true开启 rocksdb_block_cache_usage监控，上报到prometheus，对应的指标名称是&#010;flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage。&#010;&#010;&#010;我们的作业一个TM的内存设置如下：&#010;&#010;taskmanager.memory.process.size: 23000m&#010;taskmanager.memory.managed.fraction: 0.4&#010;&#010;ui上显示的Flink Managed MEM是8.48G。&#010;&#010;&#010;通过grafana配置出来的图，如果group by的维度是host，得出来的每个TM在作业稳定后是45G，超过8.48G了。&#010;&#010;sum﻿(﻿flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage﻿{﻿reportName﻿=~﻿\"$reportName\"﻿}﻿)﻿&#010;﻿by﻿ ﻿(﻿host﻿)&#010;&#010;&#010;&#010;如果维度是host，operator_name，每个operator_name维度是22G。&#010;&#010;sum(flink_taskmanager_job_task_operator_window_contents_rocksdb_block_cache_usage{reportName=~\"$reportName\"})&#010;by (host,operator_name)&#010;&#010;&#010;请问这个指标应该如何使用？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "6",
        "reply": "<21b9b0f9.aac8.1730efcfc3c.Coremail.a511955993@163.com>"
    },
    {
        "id": "<1593684748654-0.post@n8.nabble.com>",
        "from": "liangji &lt;jiliang1...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 10:12:28 GMT",
        "subject": "table execution-options 能否通过 -yd 生效",
        "content": "https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html#execution-options&#010;// instantiate table environment&#010;TableEnvironment tEnv = ...&#010;&#010;// access flink configuration&#010;Configuration configuration = tEnv.getConfig().getConfiguration();&#010;// set low-level key-value options&#010;configuration.setString(\"table.exec.mini-batch.enabled\", \"true\");&#010;configuration.setString(\"table.exec.mini-batch.allow-latency\", \"5 s\");&#010;configuration.setString(\"table.exec.mini-batch.size\", \"5000\");&#010;&#010;请问下，table的这些参数是不是只能在代码里面设置，通过 -yd 传入可否生效呢？&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1593684748654-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAP+gf34rB2g_5ZWHYRuD6u44tBGdS_f_cfpbGXBHqiDa4_y1bA@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 07:06:47 GMT",
        "subject": "Re: table execution-options 能否通过 -yd 生效",
        "content": "我理解在Yarn上运行，通过-yD传入和写在flink-conf.yaml里面都是可以生效的&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;liangji &lt;jiliang1993@gmail.com&gt; 于2020年7月2日周四 下午6:12写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html#execution-options&#013;&#010;&gt; // instantiate table environment&#013;&#010;&gt; TableEnvironment tEnv = ...&#013;&#010;&gt;&#013;&#010;&gt; // access flink configuration&#013;&#010;&gt; Configuration configuration = tEnv.getConfig().getConfiguration();&#013;&#010;&gt; // set low-level key-value options&#013;&#010;&gt; configuration.setString(\"table.exec.mini-batch.enabled\", \"true\");&#013;&#010;&gt; configuration.setString(\"table.exec.mini-batch.allow-latency\", \"5 s\");&#013;&#010;&gt; configuration.setString(\"table.exec.mini-batch.size\", \"5000\");&#013;&#010;&gt;&#013;&#010;&gt; 请问下，table的这些参数是不是只能在代码里面设置，通过 -yd 传入可否生效呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<1593684748654-0.post@n8.nabble.com>"
    },
    {
        "id": "<CABi+2jTBsN0QcD5nog8PfqVSocwEamR0S8RHYnyR8VSUaF5Jbw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 07:19:03 GMT",
        "subject": "Re: table execution-options 能否通过 -yd 生效",
        "content": "Hi,&#013;&#010;&#013;&#010;如果你是写代码来使用TableEnvironment的，&#013;&#010;你要显示的在代码中塞进TableConfig中：&#013;&#010;&#013;&#010;Configuration configuration = tEnv.getConfig().getConfiguration();&#013;&#010;configuration.addAll(GlobalConfiguration.loadConfiguration());&#013;&#010;&#013;&#010;CC: @Yang Wang &lt;danrtsey.wy@gmail.com&gt;&#013;&#010;GlobalConfiguration是个internal的类，有没有public&#013;&#010;API获取对应的Configuration？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 3, 2020 at 3:07 PM Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 我理解在Yarn上运行，通过-yD传入和写在flink-conf.yaml里面都是可以生效的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt; liangji &lt;jiliang1993@gmail.com&gt; 于2020年7月2日周四 下午6:12写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html#execution-options&#013;&#010;&gt; &gt; // instantiate table environment&#013;&#010;&gt; &gt; TableEnvironment tEnv = ...&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; // access flink configuration&#013;&#010;&gt; &gt; Configuration configuration = tEnv.getConfig().getConfiguration();&#013;&#010;&gt; &gt; // set low-level key-value options&#013;&#010;&gt; &gt; configuration.setString(\"table.exec.mini-batch.enabled\", \"true\");&#013;&#010;&gt; &gt; configuration.setString(\"table.exec.mini-batch.allow-latency\", \"5 s\");&#013;&#010;&gt; &gt; configuration.setString(\"table.exec.mini-batch.size\", \"5000\");&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 请问下，table的这些参数是不是只能在代码里面设置，通过 -yd&#010;传入可否生效呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "2",
        "reply": "<1593684748654-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAP+gf35uryH7sRSwOQPawdQLYfaBPE9-nzM6D_YrnRnvY4SF-A@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 09:10:26 GMT",
        "subject": "Re: table execution-options 能否通过 -yd 生效",
        "content": "其实是没有Public的API去从文件load Configuration的，因为我理解这是个Client端的内部逻辑&#013;&#010;&#013;&#010;在用户调用了flink run以后，client会把conf/flink-conf.yaml加载，并apply上dynamic&#010;options，&#013;&#010;然后会把这个Configuration传给各个Environment去使用&#013;&#010;&#013;&#010;如果TableEnvironment在构建的时候没有使用传过来的Configuration，那-yD就没有办法生效了&#013;&#010;只能用户在代码里面再设置一次&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月3日周五 下午3:19写道：&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 如果你是写代码来使用TableEnvironment的，&#013;&#010;&gt; 你要显示的在代码中塞进TableConfig中：&#013;&#010;&gt;&#013;&#010;&gt; Configuration configuration = tEnv.getConfig().getConfiguration();&#013;&#010;&gt; configuration.addAll(GlobalConfiguration.loadConfiguration());&#013;&#010;&gt;&#013;&#010;&gt; CC: @Yang Wang &lt;danrtsey.wy@gmail.com&gt; GlobalConfiguration是个internal的类，有没有public&#013;&#010;&gt; API获取对应的Configuration？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Fri, Jul 3, 2020 at 3:07 PM Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt;&gt; 我理解在Yarn上运行，通过-yD传入和写在flink-conf.yaml里面都是可以生效的&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best,&#013;&#010;&gt;&gt; Yang&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; liangji &lt;jiliang1993@gmail.com&gt; 于2020年7月2日周四 下午6:12写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html#execution-options&#013;&#010;&gt;&gt; &gt; // instantiate table environment&#013;&#010;&gt;&gt; &gt; TableEnvironment tEnv = ...&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; // access flink configuration&#013;&#010;&gt;&gt; &gt; Configuration configuration = tEnv.getConfig().getConfiguration();&#013;&#010;&gt;&gt; &gt; // set low-level key-value options&#013;&#010;&gt;&gt; &gt; configuration.setString(\"table.exec.mini-batch.enabled\", \"true\");&#013;&#010;&gt;&gt; &gt; configuration.setString(\"table.exec.mini-batch.allow-latency\", \"5 s\");&#013;&#010;&gt;&gt; &gt; configuration.setString(\"table.exec.mini-batch.size\", \"5000\");&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 请问下，table的这些参数是不是只能在代码里面设置，通过&#010;-yd 传入可否生效呢？&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; --&#013;&#010;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<1593684748654-0.post@n8.nabble.com>"
    },
    {
        "id": "<CABKuJ_Tj6dy-3a64hpnAu338egwoG0Pb1QTZnkY_KFyK1EBvJA@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 13:51:25 GMT",
        "subject": "Re: table execution-options 能否通过 -yd 生效",
        "content": "这个应该可以生效的，我们就是这样用的。&#013;&#010;&#013;&#010;如果没理解错，在`PlannerBase#mergeParameters`会把ExecutionEnvironment中的参数和TableConfig的参数合并的。&#013;&#010;&#013;&#010;Yang Wang &lt;danrtsey.wy@gmail.com&gt; 于2020年7月3日周五 下午5:10写道：&#013;&#010;&#013;&#010;&gt; 其实是没有Public的API去从文件load Configuration的，因为我理解这是个Client端的内部逻辑&#013;&#010;&gt;&#013;&#010;&gt; 在用户调用了flink run以后，client会把conf/flink-conf.yaml加载，并apply上dynamic&#010;options，&#013;&#010;&gt; 然后会把这个Configuration传给各个Environment去使用&#013;&#010;&gt;&#013;&#010;&gt; 如果TableEnvironment在构建的时候没有使用传过来的Configuration，那-yD就没有办法生效了&#013;&#010;&gt; 只能用户在代码里面再设置一次&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt; Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月3日周五 下午3:19写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如果你是写代码来使用TableEnvironment的，&#013;&#010;&gt; &gt; 你要显示的在代码中塞进TableConfig中：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Configuration configuration = tEnv.getConfig().getConfiguration();&#013;&#010;&gt; &gt; configuration.addAll(GlobalConfiguration.loadConfiguration());&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; CC: @Yang Wang &lt;danrtsey.wy@gmail.com&gt;&#013;&#010;&gt; GlobalConfiguration是个internal的类，有没有public&#013;&#010;&gt; &gt; API获取对应的Configuration？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Jingsong&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Fri, Jul 3, 2020 at 3:07 PM Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; 我理解在Yarn上运行，通过-yD传入和写在flink-conf.yaml里面都是可以生效的&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best,&#013;&#010;&gt; &gt;&gt; Yang&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; liangji &lt;jiliang1993@gmail.com&gt; 于2020年7月2日周四 下午6:12写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html#execution-options&#013;&#010;&gt; &gt;&gt; &gt; // instantiate table environment&#013;&#010;&gt; &gt;&gt; &gt; TableEnvironment tEnv = ...&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; // access flink configuration&#013;&#010;&gt; &gt;&gt; &gt; Configuration configuration = tEnv.getConfig().getConfiguration();&#013;&#010;&gt; &gt;&gt; &gt; // set low-level key-value options&#013;&#010;&gt; &gt;&gt; &gt; configuration.setString(\"table.exec.mini-batch.enabled\", \"true\");&#013;&#010;&gt; &gt;&gt; &gt; configuration.setString(\"table.exec.mini-batch.allow-latency\", \"5 s\");&#013;&#010;&gt; &gt;&gt; &gt; configuration.setString(\"table.exec.mini-batch.size\", \"5000\");&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 请问下，table的这些参数是不是只能在代码里面设置，通过&#010;-yd 传入可否生效呢？&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; --&#013;&#010;&gt; &gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best, Jingsong Lee&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "4",
        "reply": "<1593684748654-0.post@n8.nabble.com>"
    },
    {
        "id": "<1a4acd3.93a8.1730f1c9a2a.Coremail.wangfei23_job@163.com>",
        "from": "air23 &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 10:39:59 GMT",
        "subject": "做实时数仓，sql怎么保证分topic区有序",
        "content": "hi&#010;就是我用&#010;       flink sql 通过ddl读取和写入kafka怎么设置并行度呢？&#010;       flink sql 通过ddl写入kafka怎么自定义分区呢？&#010;&#010;&#010;这样才能保证提高消费能力。和保证数据有序。 但是好像没有发现再table模式&#010;或者sql 语句上设置  或者做自定义分区。&#010;&#010;&#010;&#010;&#010;&#010; ",
        "depth": "0",
        "reply": "<1a4acd3.93a8.1730f1c9a2a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<CAOMLN=Y34r0JnvDjFJ=tXtxwybTxAC3=FUB1TMkrNTOXdPPN6Q@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 02 Jul 2020 11:46:29 GMT",
        "subject": "Re: 做实时数仓，sql怎么保证分topic区有序",
        "content": "Hi air23,&#013;&#010;&#013;&#010;sql似乎不支持相关的设置，可以通过env或配置文件设置所有蒜子的并行度。&#013;&#010;你可以试试流转表，可以做到细粒度的控制。&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;air23 &lt;wangfei23_job@163.com&gt; 于2020年7月2日周四 下午6:40写道：&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt; 就是我用&#013;&#010;&gt;        flink sql 通过ddl读取和写入kafka怎么设置并行度呢？&#013;&#010;&gt;        flink sql 通过ddl写入kafka怎么自定义分区呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 这样才能保证提高消费能力。和保证数据有序。 但是好像没有发现再table模式&#010;或者sql 语句上设置  或者做自定义分区。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<1a4acd3.93a8.1730f1c9a2a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<CACaQKu5UtAkNoEOoz7B198m8NSBSuszez1rNmx_7Y3bYkRsrpQ@mail.gmail.com>",
        "from": "LakeShen &lt;shenleifight...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 02:05:36 GMT",
        "subject": "Re: 做实时数仓，sql怎么保证分topic区有序",
        "content": "Hi air23,&#013;&#010;&#013;&#010;  &gt; flink sql 通过ddl读取和写入kafka怎么设置并行度呢？&#013;&#010; 你可以为你的程序设置默认的并发度，代码或者命令行参数，配置文件都可以。&#013;&#010;&#013;&#010;&gt;  flink sql 通过ddl写入kafka怎么自定义分区呢？&#013;&#010;kafka sink 自定义分区器：&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/connect.html#kafka-connector&#013;&#010;，&#013;&#010;将 'connector.sink-partitioner'设置为 'custom', 然后设置 '&#013;&#010;connector.sink-partitioner-class'. Best,&#013;&#010;LakeShen&#013;&#010;&#013;&#010;shizk233 &lt;wangwangdaxian233@gmail.com&gt; 于2020年7月2日周四 下午7:46写道：&#013;&#010;&#013;&#010;&gt; Hi air23,&#013;&#010;&gt;&#013;&#010;&gt; sql似乎不支持相关的设置，可以通过env或配置文件设置所有蒜子的并行度。&#013;&#010;&gt; 你可以试试流转表，可以做到细粒度的控制。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; air23 &lt;wangfei23_job@163.com&gt; 于2020年7月2日周四 下午6:40写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi&#013;&#010;&gt; &gt; 就是我用&#013;&#010;&gt; &gt;        flink sql 通过ddl读取和写入kafka怎么设置并行度呢？&#013;&#010;&gt; &gt;        flink sql 通过ddl写入kafka怎么自定义分区呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 这样才能保证提高消费能力。和保证数据有序。 但是好像没有发现再table模式&#010;或者sql 语句上设置  或者做自定义分区。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<1a4acd3.93a8.1730f1c9a2a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<7646392E-8B19-494A-BDEE-C57C050020F9@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 02:24:50 GMT",
        "subject": "Re: 做实时数仓，sql怎么保证分topic区有序",
        "content": "kafka默认分区有序，所以source的并发一般小于等于kafka的partition数，理想状态是1：1&#013;&#010;sink的并发一般也是也是和输出topic相关，如果要保证有序，可以按key进行分区，&#013;&#010;保证数据均匀可以自定义分区策略，比如roundrobin、shuffle等&#013;&#010;&#013;&#010;&gt; 2020年7月2日 下午6:39，air23 &lt;wangfei23_job@163.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; hi&#013;&#010;&gt; 就是我用&#013;&#010;&gt;       flink sql 通过ddl读取和写入kafka怎么设置并行度呢？&#013;&#010;&gt;       flink sql 通过ddl写入kafka怎么自定义分区呢？&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 这样才能保证提高消费能力。和保证数据有序。 但是好像没有发现再table模式&#010;或者sql 语句上设置  或者做自定义分区。&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<1a4acd3.93a8.1730f1c9a2a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<CAH+dVif4WUurWh7JYYFEx-qXD3gOAX8VzXX2wgV35pSP4sF7YQ@mail.gmail.com>",
        "from": "noon cjihg &lt;ccyjs...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 02:48:20 GMT",
        "subject": "Flink job不定期就会重启，版本是1.9",
        "content": "Hi,大佬们&#010;&#010;Flink job经常不定期重启，看了异常日志基本都是下面这种，可以帮忙解释下什么原因吗？&#010;&#010;2020-07-01 20:20:43.875 [flink-akka.actor.default-dispatcher-27] INFO&#010;akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;flink-akka.remote.default-remote-dispatcher-22 - Remoting shut down.&#010;2020-07-01 20:20:43.875 [flink-akka.actor.default-dispatcher-27] INFO&#010;akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;flink-akka.remote.default-remote-dispatcher-22 - Remoting shut down.&#010;2020-07-01 20:20:43.875 [flink-metrics-16] INFO&#010;akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;flink-metrics-akka.remote.default-remote-dispatcher-14 - Remoting shut&#010;down.&#010;2020-07-01 20:20:43.875 [flink-metrics-16] INFO&#010;akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;flink-metrics-akka.remote.default-remote-dispatcher-14 - Remoting shut&#010;down.&#010;2020-07-01 20:20:43.891 [flink-metrics-16] INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcService  - Stopped Akka RPC&#010;service.&#010;2020-07-01 20:20:43.895 [flink-akka.actor.default-dispatcher-15] INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint  - Terminating&#010;cluster entrypoint process YarnJobClusterEntrypoint with exit code 2.&#010;java.util.concurrent.CompletionException:&#010;akka.pattern.AskTimeoutException: Ask timed out on&#010;[Actor[akka://flink/user/resourcemanager#-781959047",
        "depth": "0",
        "reply": "<CAH+dVif4WUurWh7JYYFEx-qXD3gOAX8VzXX2wgV35pSP4sF7YQ@mail.gmail.com>"
    },
    {
        "id": "<CAHsnkPvvBq6FKMnEafs-u8VrB-Vjjx2ZfqXSkg950_BS0O_soA@mail.gmail.com>",
        "from": "Xintong Song &lt;tonysong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 03:05:46 GMT",
        "subject": "Re: Flink job不定期就会重启，版本是1.9",
        "content": "从报错信息看是 Akka 的 RPC 调用超时，因为是 LocalFencedMessage 所以基本上可以排除网络问题。&#010;建议看一下 JM 进程的 GC 压力以及线程数量，是否存在压力过大 RPC 来不及响应的情况。&#010;&#010;Thank you~&#010;&#010;Xintong Song&#010;&#010;&#010;&#010;On Fri, Jul 3, 2020 at 10:48 AM noon cjihg &lt;ccyjshnb@gmail.com&gt; wrote:&#010;&#010;&gt; Hi,大佬们&#010;&gt;&#010;&gt; Flink job经常不定期重启，看了异常日志基本都是下面这种，可以帮忙解释下什么原因吗？&#010;&gt;&#010;&gt; 2020-07-01 20:20:43.875 [flink-akka.actor.default-dispatcher-27] INFO&#010;&gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; flink-akka.remote.default-remote-dispatcher-22 - Remoting shut down.&#010;&gt; 2020-07-01 20:20:43.875 [flink-akka.actor.default-dispatcher-27] INFO&#010;&gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; flink-akka.remote.default-remote-dispatcher-22 - Remoting shut down.&#010;&gt; 2020-07-01 20:20:43.875 [flink-metrics-16] INFO&#010;&gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; flink-metrics-akka.remote.default-remote-dispatcher-14 - Remoting shut&#010;&gt; down.&#010;&gt; 2020-07-01 20:20:43.875 [flink-metrics-16] INFO&#010;&gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; flink-metrics-akka.remote.default-remote-dispatcher-14 - Remoting shut&#010;&gt; down.&#010;&gt; 2020-07-01 20:20:43.891 [flink-metrics-16] INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcService  - Stopped Akka RPC&#010;&gt; service.&#010;&gt; 2020-07-01 20:20:43.895 [flink-akka.actor.default-dispatcher-15] INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint  - Terminating&#010;&gt; cluster entrypoint process YarnJobClusterEntrypoint with exit code 2.&#010;&gt; java.util.concurrent.CompletionException:&#010;&gt; akka.pattern.AskTimeoutException: Ask timed out on&#010;&gt; [Actor[akka://flink/user/resourcemanager#-781959047",
        "depth": "1",
        "reply": "<CAH+dVif4WUurWh7JYYFEx-qXD3gOAX8VzXX2wgV35pSP4sF7YQ@mail.gmail.com>"
    },
    {
        "id": "<CAMhjQvh6DNTYBxj9fAFZby0ZVwK2WVq9DSvs_R-b+En0z3E1pQ@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 04:32:48 GMT",
        "subject": "Re: Flink job不定期就会重启，版本是1.9",
        "content": "我们集群一般出现这种异常大都是因为 Full GC 次数比较多，然后最后伴随着就是&#010;TaskManager 挂掉的异常&#010;&#010;Xintong Song &lt;tonysong820@gmail.com&gt; 于2020年7月3日周五 上午11:06写道：&#010;&#010;&gt; 从报错信息看是 Akka 的 RPC 调用超时，因为是 LocalFencedMessage 所以基本上可以排除网络问题。&#010;&gt; 建议看一下 JM 进程的 GC 压力以及线程数量，是否存在压力过大 RPC&#010;来不及响应的情况。&#010;&gt;&#010;&gt; Thank you~&#010;&gt;&#010;&gt; Xintong Song&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; On Fri, Jul 3, 2020 at 10:48 AM noon cjihg &lt;ccyjshnb@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; Hi,大佬们&#010;&gt; &gt;&#010;&gt; &gt; Flink job经常不定期重启，看了异常日志基本都是下面这种，可以帮忙解释下什么原因吗？&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-01 20:20:43.875 [flink-akka.actor.default-dispatcher-27] INFO&#010;&gt; &gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; &gt; flink-akka.remote.default-remote-dispatcher-22 - Remoting shut down.&#010;&gt; &gt; 2020-07-01 20:20:43.875 [flink-akka.actor.default-dispatcher-27] INFO&#010;&gt; &gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; &gt; flink-akka.remote.default-remote-dispatcher-22 - Remoting shut down.&#010;&gt; &gt; 2020-07-01 20:20:43.875 [flink-metrics-16] INFO&#010;&gt; &gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; &gt; flink-metrics-akka.remote.default-remote-dispatcher-14 - Remoting shut&#010;&gt; &gt; down.&#010;&gt; &gt; 2020-07-01 20:20:43.875 [flink-metrics-16] INFO&#010;&gt; &gt; akka.remote.RemoteActorRefProvider$RemotingTerminator&#010;&gt; &gt; flink-metrics-akka.remote.default-remote-dispatcher-14 - Remoting shut&#010;&gt; &gt; down.&#010;&gt; &gt; 2020-07-01 20:20:43.891 [flink-metrics-16] INFO&#010;&gt; &gt; org.apache.flink.runtime.rpc.akka.AkkaRpcService  - Stopped Akka RPC&#010;&gt; &gt; service.&#010;&gt; &gt; 2020-07-01 20:20:43.895 [flink-akka.actor.default-dispatcher-15] INFO&#010;&gt; &gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint  - Terminating&#010;&gt; &gt; cluster entrypoint process YarnJobClusterEntrypoint with exit code 2.&#010;&gt; &gt; java.util.concurrent.CompletionException:&#010;&gt; &gt; akka.pattern.AskTimeoutException: Ask timed out on&#010;&gt; &gt; [Actor[akka://flink/user/resourcemanager#-781959047",
        "depth": "2",
        "reply": "<CAH+dVif4WUurWh7JYYFEx-qXD3gOAX8VzXX2wgV35pSP4sF7YQ@mail.gmail.com>"
    },
    {
        "id": "<2020070311114448266917@163.com>",
        "from": "&quot;liuhy_email@163.com&quot; &lt;liuhy_em...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 03:11:56 GMT",
        "subject": "flink对task分配slot问题",
        "content": "Dear，&#013;&#010;    请教一个问题，当前同一个job下的多个task（不在一个算子链）中，都会存在某一个subTask任务过重，这些subTask会分配到同一个slot下吗？&#013;&#010;    flink在对subTask分配slot时候，会先判断slot当前存在的任务数，磁盘IO之类的吗？&#013;&#010;&#013;&#010;Thanks,&#013;&#010;Hongyang&#013;&#010;",
        "depth": "0",
        "reply": "<2020070311114448266917@163.com>"
    },
    {
        "id": "<CAHsnkPv8W0QGgVF05r9hM5t_p+gg5-nrTXeVxrxt4LwFB6A2qw@mail.gmail.com>",
        "from": "Xintong Song &lt;tonysong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 05:02:09 GMT",
        "subject": "Re: flink对task分配slot问题",
        "content": "Flink 在进行 slot sharing 的时候，不会考虑当前 slot 的任务数、磁盘 IO 这些，而是会遵循“相同&#010;task 的多个&#013;&#010;subtask 不能分配到同一个 slot 中”这样的一个规则。&#013;&#010;&#013;&#010;举个例子：&#013;&#010;如果作业中有 A, B 两个 vertex，并发为 2，那就有 A1, A2, B1, B2 这 4 个 subtask。&#013;&#010;那么 A1 和 A2 不能放到一个 slot 中，B1 和 B2 不能够放到一个 slot 中。&#013;&#010;所以，slot sharing 的结果只能是 (A1, B1), (A2, B2) 或 (A1, B2), (A2, B1) 这两种情况。&#013;&#010;通常情况下，A 和 B 之间的负载可能存在较大差异，而 A1 和 A2、B1 和&#010;B2 之间通常不会有太大差异。&#013;&#010;因此，slot sharing 的规则使得每个 slot 中都分配了一个 A 和一个 B，各个&#010;slot 之间的负载大体上是均衡的。&#013;&#010;&#013;&#010;Thank you~&#013;&#010;&#013;&#010;Xintong Song&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;On Fri, Jul 3, 2020 at 11:12 AM liuhy_email@163.com &lt;liuhy_email@163.com&gt;&#013;&#010;wrote:&#013;&#010;&#013;&#010;&gt; Dear，&#013;&#010;&gt;&#013;&#010;&gt; 请教一个问题，当前同一个job下的多个task（不在一个算子链）中，都会存在某一个subTask任务过重，这些subTask会分配到同一个slot下吗？&#013;&#010;&gt;     flink在对subTask分配slot时候，会先判断slot当前存在的任务数，磁盘IO之类的吗？&#013;&#010;&gt;&#013;&#010;&gt; Thanks,&#013;&#010;&gt; Hongyang&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<2020070311114448266917@163.com>"
    },
    {
        "id": "<2020070313525671500618@163.com>",
        "from": "&quot;liuhy_email@163.com&quot; &lt;liuhy_em...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 05:53:21 GMT",
        "subject": "Re: Re: flink对task分配slot问题",
        "content": "明白了，非常感谢您的回复！&#013;&#010;&#013;&#010;Thanks,&#013;&#010;Hongyang&#013;&#010;&#013;&#010;&#013;&#010;liuhy_email@163.com&#013;&#010; &#013;&#010;发件人： Xintong Song&#013;&#010;发送时间： 2020-07-03 13:02&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: flink对task分配slot问题&#013;&#010;Flink 在进行 slot sharing 的时候，不会考虑当前 slot 的任务数、磁盘 IO&#010;这些，而是会遵循“相同 task 的多个&#013;&#010;subtask 不能分配到同一个 slot 中”这样的一个规则。&#013;&#010; &#013;&#010;举个例子：&#013;&#010;如果作业中有 A, B 两个 vertex，并发为 2，那就有 A1, A2, B1, B2 这 4 个 subtask。&#013;&#010;那么 A1 和 A2 不能放到一个 slot 中，B1 和 B2 不能够放到一个 slot 中。&#013;&#010;所以，slot sharing 的结果只能是 (A1, B1), (A2, B2) 或 (A1, B2), (A2, B1) 这两种情况。&#013;&#010;通常情况下，A 和 B 之间的负载可能存在较大差异，而 A1 和 A2、B1 和&#010;B2 之间通常不会有太大差异。&#013;&#010;因此，slot sharing 的规则使得每个 slot 中都分配了一个 A 和一个 B，各个&#010;slot 之间的负载大体上是均衡的。&#013;&#010; &#013;&#010;Thank you~&#013;&#010; &#013;&#010;Xintong Song&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;On Fri, Jul 3, 2020 at 11:12 AM liuhy_email@163.com &lt;liuhy_email@163.com&gt;&#013;&#010;wrote:&#013;&#010; &#013;&#010;&gt; Dear，&#013;&#010;&gt;&#013;&#010;&gt; 请教一个问题，当前同一个job下的多个task（不在一个算子链）中，都会存在某一个subTask任务过重，这些subTask会分配到同一个slot下吗？&#013;&#010;&gt;     flink在对subTask分配slot时候，会先判断slot当前存在的任务数，磁盘IO之类的吗？&#013;&#010;&gt;&#013;&#010;&gt; Thanks,&#013;&#010;&gt; Hongyang&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<2020070311114448266917@163.com>"
    },
    {
        "id": "<66bd19b0.d904.17312b1e2c3.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 03:21:54 GMT",
        "subject": " 在没有开启Checkpoint的情况下，当作业重启时，历史数据是否会残留在内存中？ ",
        "content": "&#010;Hi&#010;&#010;我的作业是运行在1.10.1, 使用的是event time ，没有开启checkPoint。每当作业重启一次，container&#010;memory usage会上涨2G，每重启一次就会上涨一些内存直到被OS kill。&#010;&#010;&#010;历史数据的清理是在新event time到达之后调用 WindowOperator#onEventTime() 的clearAllState实现清理，如果作业重启，又没有开启checkpoint，尚未被处理的历史数据是否一直残留在内存中无法清理？&#010;&#010;&#010;是否有哪位大佬可以帮忙解惑？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "0",
        "reply": "<66bd19b0.d904.17312b1e2c3.Coremail.a511955993@163.com>"
    },
    {
        "id": "<1288a597.dee0.17312e99a31.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 04:22:46 GMT",
        "subject": "回复：在没有开启Checkpoint的情况下，当作业重启时，历史数据是否会残留在内存中？",
        "content": "这种现象只会出现在on rocksdb中。&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 11:21，SmileSmile 写道：&#010;&#010;Hi&#010;&#010;我的作业是运行在1.10.1, 使用的是event time ，没有开启checkPoint。每当作业重启一次，container&#010;memory usage会上涨2G，每重启一次就会上涨一些内存直到被OS kill。&#010;&#010;&#010;历史数据的清理是在新event time到达之后调用 WindowOperator#onEventTime() 的clearAllState实现清理，如果作业重启，又没有开启checkpoint，尚未被处理的历史数据是否一直残留在内存中无法清理？&#010;&#010;&#010;是否有哪位大佬可以帮忙解惑？&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "1",
        "reply": "<66bd19b0.d904.17312b1e2c3.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAA8tFvuHGVFCOqM3YoqGRRbL0g30hgjPqfVcCTtSA4kuxS2snA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:02:36 GMT",
        "subject": "Re: 在没有开启Checkpoint的情况下，当作业重启时，历史数据是否会残留在内存中？",
        "content": "理论上作业重启后，会释放内存，这里的问题从描述看，重启后有内存没有释放。能否在重启后&#010;dump 一下内存看看呢？&#013;&#010;或者你这个问题能够完全重现吗？可否告知一下如何复现这个问题呢&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月3日周五 下午12:23写道：&#013;&#010;&#013;&#010;&gt; 这种现象只会出现在on rocksdb中。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月03日 11:21，SmileSmile 写道：&#013;&#010;&gt;&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 我的作业是运行在1.10.1, 使用的是event time ，没有开启checkPoint。每当作业重启一次，container&#010;memory&#013;&#010;&gt; usage会上涨2G，每重启一次就会上涨一些内存直到被OS kill。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 历史数据的清理是在新event time到达之后调用 WindowOperator#onEventTime()&#013;&#010;&gt; 的clearAllState实现清理，如果作业重启，又没有开启checkpoint，尚未被处理的历史数据是否一直残留在内存中无法清理？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 是否有哪位大佬可以帮忙解惑？&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "2",
        "reply": "<66bd19b0.d904.17312b1e2c3.Coremail.a511955993@163.com>"
    },
    {
        "id": "<59f97850.e822.1731350c684.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:15:27 GMT",
        "subject": "回复：在没有开启Checkpoint的情况下，当作业重启时，历史数据是否会残留在内存中？",
        "content": "作业运行在k8s上，这个现象可以重现，目前我这边有多份数据join的作业基本都会有这个问题。步骤如下：&#010;1. 使用eventtime，水位线设置为数据时间-3分钟，状态使用rocksdb，不开启checkpoint，设置内存limit&#010;2. 作业运行一段时间。&#010;3. kill 其中一个pod，作业fail&#010;4. k8s自动拉起该pod，观察其他pod的内存使用，会上涨。运行一段时间然后很容易超过limit被os&#010;kill&#010;5. 陷入被重复kill的死循环。&#010;&#010;解决方法：销毁集群，重构即可。&#010;&#010;观察过heap的内存，没有问题。 被os kill怀疑是offheap超用，offheap没有正常释放。&#010;&#010;有一个疑问，如果没有开启ck，作业恢复后是重新开始的，重启前的旧数据在rocksdb中是在如何清理的呢？&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月03日 14:02，Congxian Qiu 写道：&#010;理论上作业重启后，会释放内存，这里的问题从描述看，重启后有内存没有释放。能否在重启后&#010;dump 一下内存看看呢？&#010;或者你这个问题能够完全重现吗？可否告知一下如何复现这个问题呢&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月3日周五 下午12:23写道：&#010;&#010;&gt; 这种现象只会出现在on rocksdb中。&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; 在2020年07月03日 11:21，SmileSmile 写道：&#010;&gt;&#010;&gt; Hi&#010;&gt;&#010;&gt; 我的作业是运行在1.10.1, 使用的是event time ，没有开启checkPoint。每当作业重启一次，container&#010;memory&#010;&gt; usage会上涨2G，每重启一次就会上涨一些内存直到被OS kill。&#010;&gt;&#010;&gt;&#010;&gt; 历史数据的清理是在新event time到达之后调用 WindowOperator#onEventTime()&#010;&gt; 的clearAllState实现清理，如果作业重启，又没有开启checkpoint，尚未被处理的历史数据是否一直残留在内存中无法清理？&#010;&gt;&#010;&gt;&#010;&gt; 是否有哪位大佬可以帮忙解惑？&#010;&gt;&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;",
        "depth": "3",
        "reply": "<66bd19b0.d904.17312b1e2c3.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAA8tFvsOBGWzg6SprfLHXcwKttCO7x1hZ8eMBWL9eCwd4NDyag@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 04:01:10 GMT",
        "subject": "Re: 在没有开启Checkpoint的情况下，当作业重启时，历史数据是否会残留在内存中？",
        "content": "从现象看，应该是有内存泄漏，你需要看一下这些内存都是啥，然后才好定位是哪里的问题&#013;&#010;&#013;&#010;checkpoint 是指 state 的一个快照，rocksdb 中存的是 state。理论上来说，作业&#010;fail 了，之前 rocksdb&#013;&#010;中的数据就没有了。新的作业是会使用新的 RocksDB&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月3日周五 下午2:15写道：&#013;&#010;&#013;&#010;&gt; 作业运行在k8s上，这个现象可以重现，目前我这边有多份数据join的作业基本都会有这个问题。步骤如下：&#013;&#010;&gt; 1. 使用eventtime，水位线设置为数据时间-3分钟，状态使用rocksdb，不开启checkpoint，设置内存limit&#013;&#010;&gt; 2. 作业运行一段时间。&#013;&#010;&gt; 3. kill 其中一个pod，作业fail&#013;&#010;&gt; 4. k8s自动拉起该pod，观察其他pod的内存使用，会上涨。运行一段时间然后很容易超过limit被os&#010;kill&#013;&#010;&gt; 5. 陷入被重复kill的死循环。&#013;&#010;&gt;&#013;&#010;&gt; 解决方法：销毁集群，重构即可。&#013;&#010;&gt;&#013;&#010;&gt; 观察过heap的内存，没有问题。 被os kill怀疑是offheap超用，offheap没有正常释放。&#013;&#010;&gt;&#013;&#010;&gt; 有一个疑问，如果没有开启ck，作业恢复后是重新开始的，重启前的旧数据在rocksdb中是在如何清理的呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月03日 14:02，Congxian Qiu 写道：&#013;&#010;&gt; 理论上作业重启后，会释放内存，这里的问题从描述看，重启后有内存没有释放。能否在重启后&#010;dump 一下内存看看呢？&#013;&#010;&gt; 或者你这个问题能够完全重现吗？可否告知一下如何复现这个问题呢&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月3日周五 下午12:23写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 这种现象只会出现在on rocksdb中。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在2020年07月03日 11:21，SmileSmile 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我的作业是运行在1.10.1, 使用的是event time ，没有开启checkPoint。每当作业重启一次，container&#010;memory&#013;&#010;&gt; &gt; usage会上涨2G，每重启一次就会上涨一些内存直到被OS kill。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 历史数据的清理是在新event time到达之后调用 WindowOperator#onEventTime()&#013;&#010;&gt; &gt; 的clearAllState实现清理，如果作业重启，又没有开启checkpoint，尚未被处理的历史数据是否一直残留在内存中无法清理？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 是否有哪位大佬可以帮忙解惑？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<66bd19b0.d904.17312b1e2c3.Coremail.a511955993@163.com>"
    },
    {
        "id": "<b7286175-3f05-477b-815d-bbb34aa26535.zhengbinbin@heint.cn>",
        "from": "&quot;郑斌斌&quot; &lt;zhengbin...@heint.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 05:38:10 GMT",
        "subject": "kafkaf To mysql 写入问题",
        "content": "dear:&#010;   请教两个问题&#010;1)  用下面的代码消费kafka 发生序列化异常时，会发生JOB反复重试，重启后也是这样，&#010;改用FlinkKafkaConsumer010类的话，有相关的解决方法，参照https://stackoverflow.com/questions/51301549/how-to-handle-exception-while-parsing-json-in-flink/51302225&#010;不知道，用Kafka类的话，如何解决&#010;.connect(&#010;    new Kafka()&#010;      .version(\"0.10\")&#010;      .topic(\"test-input\")&#010;2)  对于timestamp类型字段，用JDBCAppendTableSink 把DataStream&lt;Row&gt;写入到mysql时，会发下面的错误LocalTimeStamp到Timestamp的转型错误&#010;    kafka消息是avro格式，字段类型设置为timestamp（3），我是把System.currentTimeMillis()写入到kafka中的&#010;    jdbc参数类型设置为Types.SQL_TIMESTAMP&#010;thanks&#010;",
        "depth": "0",
        "reply": "<b7286175-3f05-477b-815d-bbb34aa26535.zhengbinbin@heint.cn>"
    },
    {
        "id": "<CABi+2jSOp_97vHfV_LUBeRZ9SsD=1Q+40g1VCfceX7FKV4kFGw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:29:15 GMT",
        "subject": "Re: kafkaf To mysql 写入问题",
        "content": "Hi,&#013;&#010;&#013;&#010;估计需要使用Flink 1.11。&#013;&#010;&#013;&#010;1.JSON Format有参数控制 [1]&#013;&#010;2.是之前的bug，Flink 1.11应该是不会存在了，不确定1.10.1有没有修。&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/json.html#json-ignore-parse-errors&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 3, 2020 at 1:38 PM 郑斌斌 &lt;zhengbinbin@heint.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; dear:&#013;&#010;&gt;    请教两个问题&#013;&#010;&gt; 1)  用下面的代码消费kafka 发生序列化异常时，会发生JOB反复重试，重启后也是这样，&#013;&#010;&gt; 改用FlinkKafkaConsumer010类的话，有相关的解决方法，参照&#013;&#010;&gt; https://stackoverflow.com/questions/51301549/how-to-handle-exception-while-parsing-json-in-flink/51302225&#013;&#010;&gt; 不知道，用Kafka类的话，如何解决&#013;&#010;&gt; .connect(&#013;&#010;&gt;     new Kafka()&#013;&#010;&gt;       .version(\"0.10\")&#013;&#010;&gt;       .topic(\"test-input\")&#013;&#010;&gt; 2)  对于timestamp类型字段，用JDBCAppendTableSink&#013;&#010;&gt; 把DataStream&lt;Row&gt;写入到mysql时，会发下面的错误LocalTimeStamp到Timestamp的转型错误&#013;&#010;&gt;&#013;&#010;&gt; kafka消息是avro格式，字段类型设置为timestamp（3），我是把System.currentTimeMillis()写入到kafka中的&#013;&#010;&gt;     jdbc参数类型设置为Types.SQL_TIMESTAMP&#013;&#010;&gt; thanks&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<b7286175-3f05-477b-815d-bbb34aa26535.zhengbinbin@heint.cn>"
    },
    {
        "id": "<179f3209-fd31-4b56-acea-d4d498d45c00.zhengbinbin@heint.cn>",
        "from": "&quot;郑斌斌&quot; &lt;zhengbin...@heint.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:56:03 GMT",
        "subject": "回复：kafkaf To mysql 写入问题",
        "content": "&#010;谢谢了，查了下jira, No.1在1.11中才用修复, 另外，目前我用的版本就是1.10&#010;https://issues.apache.org/jira/browse/FLINK-15396&#010;------------------------------------------------------------------&#010;发件人：Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#010;发送时间：2020年7月3日(星期五) 14:29&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;; 郑斌斌 &lt;zhengbinbin@heint.cn&gt;&#010;主　题：Re: kafkaf To mysql 写入问题&#010;&#010;Hi,&#010;&#010;估计需要使用Flink 1.11。&#010;&#010;1.JSON Format有参数控制 [1]&#010;2.是之前的bug，Flink 1.11应该是不会存在了，不确定1.10.1有没有修。&#010;&#010;[1]https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/json.html#json-ignore-parse-errors&#010;&#010;Best,&#010;Jingsong&#010;On Fri, Jul 3, 2020 at 1:38 PM 郑斌斌 &lt;zhengbinbin@heint.cn&gt; wrote:&#010;dear:&#010;    请教两个问题&#010; 1)  用下面的代码消费kafka 发生序列化异常时，会发生JOB反复重试，重启后也是这样，&#010; 改用FlinkKafkaConsumer010类的话，有相关的解决方法，参照https://stackoverflow.com/questions/51301549/how-to-handle-exception-while-parsing-json-in-flink/51302225&#010; 不知道，用Kafka类的话，如何解决&#010; .connect(&#010;     new Kafka()&#010;       .version(\"0.10\")&#010;       .topic(\"test-input\")&#010; 2)  对于timestamp类型字段，用JDBCAppendTableSink 把DataStream&lt;Row&gt;写入到mysql时，会发下面的错误LocalTimeStamp到Timestamp的转型错误&#010;     kafka消息是avro格式，字段类型设置为timestamp（3），我是把System.currentTimeMillis()写入到kafka中的&#010;     jdbc参数类型设置为Types.SQL_TIMESTAMP&#010; thanks&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "1",
        "reply": "<b7286175-3f05-477b-815d-bbb34aa26535.zhengbinbin@heint.cn>"
    },
    {
        "id": "<202007031402115215597@163.com>",
        "from": "&quot;18579099920@163.com&quot; &lt;18579099...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 06:02:11 GMT",
        "subject": "如何在窗口关闭的时候清除状态",
        "content": "大家好，我有一个需求，我在ProcessWindowFunction算子中定义了一个valueState，我希望在窗口关闭的时候能够将状态清理。我应该在哪里清理呢？&#013;&#010;1.刚开始我选择在ProcessWindowFunction算子的process方法中清理，但是这里会有一个问题，我事件时间窗口开1天，我写了一个trigger，每隔一个小时输出一次结果。&#013;&#010;如果我在process方法中清理，每隔一个小时就会被清理，而valueState中存的是我的中间结果，应该在窗口关闭的时候被清理（即一天之后）。这应该怎么办呢？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;18579099920@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007031402115215597@163.com>"
    },
    {
        "id": "<CAMhjQvi7ys=iQztADuikcO4EmpG-9teuDydcDe-fazCrKivmug@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 04:30:29 GMT",
        "subject": "Re: 如何在窗口关闭的时候清除状态",
        "content": "你试试在 clear 方法中清理&#013;&#010;&#013;&#010;18579099920@163.com &lt;18579099920@163.com&gt; 于2020年7月3日周五 下午2:02写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 大家好，我有一个需求，我在ProcessWindowFunction算子中定义了一个valueState，我希望在窗口关闭的时候能够将状态清理。我应该在哪里清理呢？&#013;&#010;&gt;&#013;&#010;&gt; 1.刚开始我选择在ProcessWindowFunction算子的process方法中清理，但是这里会有一个问题，我事件时间窗口开1天，我写了一个trigger，每隔一个小时输出一次结果。&#013;&#010;&gt;&#013;&#010;&gt; 如果我在process方法中清理，每隔一个小时就会被清理，而valueState中存的是我的中间结果，应该在窗口关闭的时候被清理（即一天之后）。这应该怎么办呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 18579099920@163.com&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<202007031402115215597@163.com>"
    },
    {
        "id": "<1a12d271.283c.17319cd2392.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 12:29:01 GMT",
        "subject": "回复：如何在窗口关闭的时候清除状态",
        "content": "设置一下状态过期的时间呢&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月03日 14:02，18579099920@163.com 写道：&#010;大家好，我有一个需求，我在ProcessWindowFunction算子中定义了一个valueState，我希望在窗口关闭的时候能够将状态清理。我应该在哪里清理呢？&#010;1.刚开始我选择在ProcessWindowFunction算子的process方法中清理，但是这里会有一个问题，我事件时间窗口开1天，我写了一个trigger，每隔一个小时输出一次结果。&#010;如果我在process方法中清理，每隔一个小时就会被清理，而valueState中存的是我的中间结果，应该在窗口关闭的时候被清理（即一天之后）。这应该怎么办呢？&#010;&#010;&#010;&#010;18579099920@163.com&#010;",
        "depth": "1",
        "reply": "<202007031402115215597@163.com>"
    },
    {
        "id": "<CAA8tFvs9-3w+4poiany34r7wz_cSFxGir4ALe_FkaUEeMiAagA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 03:56:03 GMT",
        "subject": "Re: 如何在窗口关闭的时候清除状态",
        "content": "看上去这个需求是 一天的窗口，每个小时都 trigger 一次，希望 state 在&#010;1 天之后进行清理。&#013;&#010;你可以尝试一下 TTL[1] State&#013;&#010;另外想问一下，你自己写 ProcessWindowFunction 的话，为什么不考虑 KeyedProcessFunction[2]&#010;呢&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;[2]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/operators/process_function.html#the-keyedprocessfunction&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;JasonLee &lt;17610775726@163.com&gt; 于2020年7月4日周六 下午8:29写道：&#013;&#010;&#013;&#010;&gt; 设置一下状态过期的时间呢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; JasonLee&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：17610775726@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; Signature is customized by Netease Mail Master&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月03日 14:02，18579099920@163.com 写道：&#013;&#010;&gt;&#013;&#010;&gt; 大家好，我有一个需求，我在ProcessWindowFunction算子中定义了一个valueState，我希望在窗口关闭的时候能够将状态清理。我应该在哪里清理呢？&#013;&#010;&gt;&#013;&#010;&gt; 1.刚开始我选择在ProcessWindowFunction算子的process方法中清理，但是这里会有一个问题，我事件时间窗口开1天，我写了一个trigger，每隔一个小时输出一次结果。&#013;&#010;&gt;&#013;&#010;&gt; 如果我在process方法中清理，每隔一个小时就会被清理，而valueState中存的是我的中间结果，应该在窗口关闭的时候被清理（即一天之后）。这应该怎么办呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 18579099920@163.com&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<202007031402115215597@163.com>"
    },
    {
        "id": "<tencent_AA42050A0005F9A6DF3B1F8A23060E3BB108@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 06:15:14 GMT",
        "subject": "回复： 如何在窗口关闭的时候清除状态",
        "content": "用了window一天的窗口，这个开窗flink会自己将一天的数据自动清除的吧&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Congxian Qiu\"&lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月5日(星期天) 中午11:56&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 如何在窗口关闭的时候清除状态&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;看上去这个需求是 一天的窗口，每个小时都 trigger 一次，希望 state 在&#010;1 天之后进行清理。&#013;&#010;你可以尝试一下 TTL[1] State&#013;&#010;另外想问一下，你自己写 ProcessWindowFunction 的话，为什么不考虑 KeyedProcessFunction[2]&#010;呢&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;[2]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/operators/process_function.html#the-keyedprocessfunction&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;JasonLee &lt;17610775726@163.com&amp;gt; 于2020年7月4日周六 下午8:29写道：&#013;&#010;&#013;&#010;&amp;gt; 设置一下状态过期的时间呢&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; | |&#013;&#010;&amp;gt; JasonLee&#013;&#010;&amp;gt; |&#013;&#010;&amp;gt; |&#013;&#010;&amp;gt; 邮箱：17610775726@163.com&#013;&#010;&amp;gt; |&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Signature is customized by Netease Mail Master&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 在2020年07月03日 14:02，18579099920@163.com 写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 大家好，我有一个需求，我在ProcessWindowFunction算子中定义了一个valueState，我希望在窗口关闭的时候能够将状态清理。我应该在哪里清理呢？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 1.刚开始我选择在ProcessWindowFunction算子的process方法中清理，但是这里会有一个问题，我事件时间窗口开1天，我写了一个trigger，每隔一个小时输出一次结果。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 如果我在process方法中清理，每隔一个小时就会被清理，而valueState中存的是我的中间结果，应该在窗口关闭的时候被清理（即一天之后）。这应该怎么办呢？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 18579099920@163.com&#013;&#010;&amp;gt;",
        "depth": "3",
        "reply": "<202007031402115215597@163.com>"
    },
    {
        "id": "<e99496.2747.173272927e8.Coremail.18579099920@163.com>",
        "from": "flink小猪 &lt;18579099...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 02:44:35 GMT",
        "subject": "Re:Re: 如何在窗口关闭的时候清除状态",
        "content": "&#010;&#010;&#010;[1].设置TTL应该也能达到相同的效果，我还是希望在窗口关闭的时候能够做一些自定义的操作（比如这里的清除状态，也许之后会有其他的操作TTL就不一样好用了）&#010;[2].KeyedProcessFunction，应该自己注册定时器把，在我的代码里面是timeWIndow().trigger().process(),&#010;ProcessWindowFunction方法我只需要处理逻辑即可，不需要管定时的窗口。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-05 11:56:03，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#010;&gt;看上去这个需求是 一天的窗口，每个小时都 trigger 一次，希望 state&#010;在 1 天之后进行清理。&#010;&gt;你可以尝试一下 TTL[1] State&#010;&gt;另外想问一下，你自己写 ProcessWindowFunction 的话，为什么不考虑 KeyedProcessFunction[2]&#010;呢&#010;&gt;&#010;&gt;[1]&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#010;&gt;[2]&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/operators/process_function.html#the-keyedprocessfunction&#010;&gt;Best,&#010;&gt;Congxian&#010;&gt;&#010;&gt;&#010;&gt;JasonLee &lt;17610775726@163.com&gt; 于2020年7月4日周六 下午8:29写道：&#010;&gt;&#010;&gt;&gt; 设置一下状态过期的时间呢&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; | |&#010;&gt;&gt; JasonLee&#010;&gt;&gt; |&#010;&gt;&gt; |&#010;&gt;&gt; 邮箱：17610775726@163.com&#010;&gt;&gt; |&#010;&gt;&gt;&#010;&gt;&gt; Signature is customized by Netease Mail Master&#010;&gt;&gt;&#010;&gt;&gt; 在2020年07月03日 14:02，18579099920@163.com 写道：&#010;&gt;&gt;&#010;&gt;&gt; 大家好，我有一个需求，我在ProcessWindowFunction算子中定义了一个valueState，我希望在窗口关闭的时候能够将状态清理。我应该在哪里清理呢？&#010;&gt;&gt;&#010;&gt;&gt; 1.刚开始我选择在ProcessWindowFunction算子的process方法中清理，但是这里会有一个问题，我事件时间窗口开1天，我写了一个trigger，每隔一个小时输出一次结果。&#010;&gt;&gt;&#010;&gt;&gt; 如果我在process方法中清理，每隔一个小时就会被清理，而valueState中存的是我的中间结果，应该在窗口关闭的时候被清理（即一天之后）。这应该怎么办呢？&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 18579099920@163.com&#010;&gt;&gt;&#010;",
        "depth": "3",
        "reply": "<202007031402115215597@163.com>"
    },
    {
        "id": "<CY4PR20MB1221452D927B4D70A3E0D95FDA670@CY4PR20MB1221.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 08:22:12 GMT",
        "subject": "Re: Re:Re: 如何在窗口关闭的时候清除状态",
        "content": "Hi&#013;&#010;&#013;&#010;TTL需要state descriptor明确声明enableTimeToLive[1]，而一旦使用window，window内使用的timer和window&#010;state实际上不暴露给用户 的，没法开启TTL，二者在使用方式上存在一定互斥。从语义上来说TTL可以清理过期数据，而默认的window实现都会清理已经trigger过的window内的state，所以二者在语义上其实也是有一定互斥的。&#013;&#010;&#013;&#010;从性能角度考虑，一天的窗口显得有点大了，往往性能不好，如果能把类似逻辑迁移到TTL上实现会对性能更友好。&#013;&#010;&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/state.html#state-time-to-live-ttl&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;&#013;&#010;________________________________&#013;&#010;From: flink小猪 &lt;18579099920@163.com&gt;&#013;&#010;Sent: Tuesday, July 7, 2020 10:44&#013;&#010;To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: Re:Re: 如何在窗口关闭的时候清除状态&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;[1].设置TTL应该也能达到相同的效果，我还是希望在窗口关闭的时候能够做一些自定义的操作（比如这里的清除状态，也许之后会有其他的操作TTL就不一样好用了）&#013;&#010;[2].KeyedProcessFunction，应该自己注册定时器把，在我的代码里面是timeWIndow().trigger().process(),&#010;ProcessWindowFunction方法我只需要处理逻辑即可，不需要管定时的窗口。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;在 2020-07-05 11:56:03，\"Congxian Qiu\" &lt;qcx978132955@gmail.com&gt; 写道：&#013;&#010;&gt;看上去这个需求是 一天的窗口，每个小时都 trigger 一次，希望 state&#010;在 1 天之后进行清理。&#013;&#010;&gt;你可以尝试一下 TTL[1] State&#013;&#010;&gt;另外想问一下，你自己写 ProcessWindowFunction 的话，为什么不考虑 KeyedProcessFunction[2]&#010;呢&#013;&#010;&gt;&#013;&#010;&gt;[1]&#013;&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/state/state.html#%E7%8A%B6%E6%80%81%E6%9C%89%E6%95%88%E6%9C%9F-ttl&#013;&#010;&gt;[2]&#013;&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.10/zh/dev/stream/operators/process_function.html#the-keyedprocessfunction&#013;&#010;&gt;Best,&#013;&#010;&gt;Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;JasonLee &lt;17610775726@163.com&gt; 于2020年7月4日周六 下午8:29写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 设置一下状态过期的时间呢&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; | |&#013;&#010;&gt;&gt; JasonLee&#013;&#010;&gt;&gt; |&#013;&#010;&gt;&gt; |&#013;&#010;&gt;&gt; 邮箱：17610775726@163.com&#013;&#010;&gt;&gt; |&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Signature is customized by Netease Mail Master&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 在2020年07月03日 14:02，18579099920@163.com 写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 大家好，我有一个需求，我在ProcessWindowFunction算子中定义了一个valueState，我希望在窗口关闭的时候能够将状态清理。我应该在哪里清理呢？&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 1.刚开始我选择在ProcessWindowFunction算子的process方法中清理，但是这里会有一个问题，我事件时间窗口开1天，我写了一个trigger，每隔一个小时输出一次结果。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 如果我在process方法中清理，每隔一个小时就会被清理，而valueState中存的是我的中间结果，应该在窗口关闭的时候被清理（即一天之后）。这应该怎么办呢？&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 18579099920@163.com&#013;&#010;&gt;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<202007031402115215597@163.com>"
    },
    {
        "id": "<CANYrj=+YQYyptsT-Mr_+aa4xHRPDMRPvLhwYZi6aVK1iWcFrjw@mail.gmail.com>",
        "from": "Jun Zou &lt;nianjun...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 07:43:22 GMT",
        "subject": "flink 1.9 中 StreamTableEnvironment 注册 registerDataStream处理嵌套别名",
        "content": "Hi,&#010;我在使用flink 1.9版本的 StreamTableEnvironment 注册 table 时，想指定一个嵌套字段的&#010;cloumns&#010;alianame，&#010;例如：&#010;String fieldExprsStr = \"modbus.parsedResponse,timestamp\";&#010;tableEnv.registerDataStream(src.getName(), srcStream, fieldExprsStr);&#010;在对 modbus.parsedResponse 进行校验的时候&#010;抛出了如下错误：&#010;org.apache.flink.table.api.ValidationException: Field reference expression&#010;or alias on field expression expected.&#010;at&#010;org.apache.flink.table.typeutils.FieldInfoUtils$IndexedExprToFieldInfo.defaultMethod(FieldInfoUtils.java:543)&#010;at&#010;org.apache.flink.table.typeutils.FieldInfoUtils$IndexedExprToFieldInfo.defaultMethod(FieldInfoUtils.java:470)&#010;at&#010;org.apache.flink.table.expressions.utils.ApiExpressionDefaultVisitor.visit(ApiExpressionDefaultVisitor.java:92)&#010;at&#010;org.apache.flink.table.expressions.ApiExpressionVisitor.visit(ApiExpressionVisitor.java:37)&#010;at&#010;org.apache.flink.table.expressions.LookupCallExpression.accept(LookupCallExpression.java:67)&#010;&#010;请问是否有方法来指定这种cloumns 别名呢？&#010;&#010;",
        "depth": "0",
        "reply": "<CANYrj=+YQYyptsT-Mr_+aa4xHRPDMRPvLhwYZi6aVK1iWcFrjw@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpb6rPbi0gW-jR1ztc3wiE-o30WU5w7o4rGWfp0j8D3Z1A@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 08:02:17 GMT",
        "subject": "flink 1.10 kafka collector topic 配置pattern",
        "content": "hello&#013;&#010;&#013;&#010;      请教大家，flink 1.10里面kafka connector 不能配置topic pattern，后续会支持吗？&#013;&#010;&#013;&#010;best wishes&#013;&#010;",
        "depth": "0",
        "reply": "<CAGR9zpb6rPbi0gW-jR1ztc3wiE-o30WU5w7o4rGWfp0j8D3Z1A@mail.gmail.com>"
    },
    {
        "id": "<0AC15E9F-8405-43D9-B6F6-FCB8213AAA1F@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 08:06:48 GMT",
        "subject": "Re: flink 1.10 kafka collector topic 配置pattern",
        "content": "Hello&#010;&#010;我了解到社区有人在做了，1.12 应该会支持&#010;&#010;祝好&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月3日，16:02，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; hello&#010;&gt; &#010;&gt;      请教大家，flink 1.10里面kafka connector 不能配置topic pattern，后续会支持吗？&#010;&gt; &#010;&gt; best wishes&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<CAGR9zpb6rPbi0gW-jR1ztc3wiE-o30WU5w7o4rGWfp0j8D3Z1A@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpa6mSKqQ_evuc45n7_=PkVytLYRxg+C9pUTAyscwJ54cA@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 08:26:52 GMT",
        "subject": "Re: flink 1.10 kafka collector topic 配置pattern",
        "content": "好的，感谢🤗&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月3日周五 下午4:07写道：&#013;&#010;&#013;&#010;&gt; Hello&#013;&#010;&gt;&#013;&#010;&gt; 我了解到社区有人在做了，1.12 应该会支持&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月3日，16:02，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hello&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;      请教大家，flink 1.10里面kafka connector 不能配置topic pattern，后续会支持吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; best wishes&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAGR9zpb6rPbi0gW-jR1ztc3wiE-o30WU5w7o4rGWfp0j8D3Z1A@mail.gmail.com>"
    },
    {
        "id": "<CAELO930oXR94k7EhdcpLTLzCmEhBpMu_syM-pmA-xQOA+hminQ@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 08:34:01 GMT",
        "subject": "Re: flink 1.10 kafka collector topic 配置pattern",
        "content": "可以关注下：https://issues.apache.org/jira/browse/FLINK-18449&#013;&#010;&#013;&#010;预计1.12会支持。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Fri, 3 Jul 2020 at 16:27, Peihui He &lt;peihuihe@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 好的，感谢🤗&#013;&#010;&gt;&#013;&#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月3日周五 下午4:07写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hello&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我了解到社区有人在做了，1.12 应该会支持&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 祝好&#013;&#010;&gt; &gt; Leonard Xu&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 在 2020年7月3日，16:02，Peihui He &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; hello&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;      请教大家，flink 1.10里面kafka connector 不能配置topic pattern，后续会支持吗？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; best wishes&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CAGR9zpb6rPbi0gW-jR1ztc3wiE-o30WU5w7o4rGWfp0j8D3Z1A@mail.gmail.com>"
    },
    {
        "id": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>",
        "from": "&quot;x&quot; &lt;35907...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 08:34:12 GMT",
        "subject": "回复： 求助：FLINKSQL1.10实时统计累计UV",
        "content": "您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#013;&#010;我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年6月18日(星期四) 中午12:16&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;是的，我觉得这样子是能绕过的。&#013;&#010;&#013;&#010;On Thu, 18 Jun 2020 at 10:34, x &lt;35907418@qq.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#013;&#010;&amp;gt; val resTmpTab: Table = tabEnv.sqlQuery(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd HH:mm:00'))&#013;&#010;&amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM user_behavior&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;GROUP BY DATE_FORMAT(ts, 'yyyy-MM-dd')&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; val resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp; .filter(line=&amp;amp;gt;line._1==true).map(line=&amp;amp;gt;line._2)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#013;&#010;&amp;gt; tabEnv.sqlUpdate(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp; s\"\"\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INSERT INTO rt_totaluv&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT _1,MAX(_2)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM $res&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; GROUP BY _1&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年6月17日(星期三) 中午1:55&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; CREATE TABLE mysql (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; time_str STRING,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; uv BIGINT,&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; PRIMARY KEY (ts) NOT ENFORCED&#013;&#010;&amp;gt; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; 'url' = 'jdbc:mysql://localhost:3306/mydatabase',&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; 'table-name' = 'myuv'&#013;&#010;&amp;gt; );&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; INSERT INTO mysql&#013;&#010;&amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd HH:mm:00')), COUNT(DISTINCT&amp;amp;nbsp;&#013;&#010;&amp;gt; user_id)&#013;&#010;&amp;gt; FROM user_behavior;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; On Wed, 17 Jun 2020 at 13:49, x &lt;35907418@qq.com&amp;amp;gt; wrote:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#013;&#010;&amp;gt; &amp;amp;gt; sink表这个样式&#013;&#010;&amp;gt; &amp;amp;gt; tm uv&#013;&#010;&amp;gt; &amp;amp;gt; 2020/06/17 13:46:00 10000&#013;&#010;&amp;gt; &amp;amp;gt; 2020/06/17 13:47:00 20000&#013;&#010;&amp;gt; &amp;amp;gt; 2020/06/17 13:48:00 30000&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; group by 日期的话，分钟如何获取&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年6月17日(星期三) 中午11:46&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Hi，&#013;&#010;&amp;gt; &amp;amp;gt; 我感觉这种场景可以有两种方式，&#013;&#010;&amp;gt; &amp;amp;gt; 1. 可以直接用group by + mini batch&#013;&#010;&amp;gt; &amp;amp;gt; 2. window聚合 + fast emit&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 对于#1，group by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#010;'yyyy-MM-dd')。&#013;&#010;&amp;gt; &amp;amp;gt; 这种情况下的状态清理，需要配置state retention时间，配置方法可以参考[1]&#010;。同时，mini batch的开启也需要&#013;&#010;&amp;gt; &amp;amp;gt; 用参数[2] 来打开。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#013;&#010;&amp;gt; &amp;amp;gt; fast emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#013;&#010;&amp;gt; &amp;amp;gt; table.exec.emit.early-fire.enabled = true&#013;&#010;&amp;gt; &amp;amp;gt; table.exec.emit.early-fire.delay = 60 s&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; [1]&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#013;&#010;&amp;gt; &amp;amp;gt; [2]&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; x &lt;35907418@qq.com&amp;amp;amp;gt; 于2020年6月17日周三 上午11:14写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; CREATE VIEW uv_per_10min AS&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; SELECT&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; MAX(DATE_FORMAT(proctime&amp;amp;amp;amp;nbsp;,&#013;&#010;&amp;gt; 'yyyy-MM-dd&#013;&#010;&amp;gt; &amp;amp;gt; HH:mm:00'))&amp;amp;amp;amp;nbsp;OVER w&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; AS time_str,&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; COUNT(DISTINCT user_id) OVER&#010;w AS uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; FROM user_behavior&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; WINDOW w AS (ORDER BY proctime ROWS BETWEEN UNBOUNDED&#013;&#010;&amp;gt; PRECEDING AND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; CURRENT ROW);&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 想请教一下，应该如何处理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; PARTITION BY DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&amp;gt; 这样可以吗，另外状态应该如何清理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; PS：1.10貌似不支持DDL貌似不支持CREATE VIEW吧&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 多谢",
        "depth": "1",
        "reply": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>"
    },
    {
        "id": "<CABKuJ_Q2UN0nxk98hsQ_r75e-iTMUhPUtBYRVxMEKSd8b7vVog@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 13:47:51 GMT",
        "subject": "Re: 求助：FLINKSQL1.10实时统计累计UV",
        "content": "你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题，&#013;&#010;这个已经在1.11中修复了。&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-17942&#013;&#010;&#013;&#010;x &lt;35907418@qq.com&gt; 于2020年7月3日周五 下午4:34写道：&#013;&#010;&#013;&#010;&gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#013;&#010;&gt;&#013;&#010;&gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年6月18日(星期四) 中午12:16&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 是的，我觉得这样子是能绕过的。&#013;&#010;&gt;&#013;&#010;&gt; On Thu, 18 Jun 2020 at 10:34, x &lt;35907418@qq.com&amp;gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#013;&#010;&gt; &amp;gt; val resTmpTab: Table = tabEnv.sqlQuery(&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#013;&#010;&gt; HH:mm:00'))&#013;&#010;&gt; &amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM user_behavior&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;GROUP BY&#013;&#010;&gt; DATE_FORMAT(ts, 'yyyy-MM-dd')&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; val resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab)&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&#013;&#010;&gt; .filter(line=&amp;amp;gt;line._1==true).map(line=&amp;amp;gt;line._2)&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#013;&#010;&gt; &amp;gt; tabEnv.sqlUpdate(&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp; s\"\"\"&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INSERT INTO rt_totaluv&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT _1,MAX(_2)&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM $res&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; GROUP BY _1&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; 发件人:&amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;gt;;&#013;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年6月17日(星期三) 中午1:55&#013;&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; CREATE TABLE mysql (&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; time_str STRING,&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; uv BIGINT,&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; PRIMARY KEY (ts) NOT ENFORCED&#013;&#010;&gt; &amp;gt; ) WITH (&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; 'url' = 'jdbc:mysql://localhost:3306/mydatabase',&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp; 'table-name' = 'myuv'&#013;&#010;&gt; &amp;gt; );&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; INSERT INTO mysql&#013;&#010;&gt; &amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd HH:mm:00')),&#013;&#010;&gt; COUNT(DISTINCT&amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; user_id)&#013;&#010;&gt; &amp;gt; FROM user_behavior;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; On Wed, 17 Jun 2020 at 13:49, x &lt;35907418@qq.com&amp;amp;gt; wrote:&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; sink表这个样式&#013;&#010;&gt; &amp;gt; &amp;amp;gt; tm uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 2020/06/17 13:46:00 10000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 2020/06/17 13:47:00 20000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 2020/06/17 13:48:00 30000&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; group by 日期的话，分钟如何获取&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&#013;&#010;&gt; &amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年6月17日(星期三) 中午11:46&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&gt; &amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; Hi，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 我感觉这种场景可以有两种方式，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 1. 可以直接用group by + mini batch&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 2. window聚合 + fast emit&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 对于#1，group by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#013;&#010;&gt; 'yyyy-MM-dd')。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 这种情况下的状态清理，需要配置state retention时间，配置方法可以参考[1]&#010;。同时，mini&#013;&#010;&gt; batch的开启也需要&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 用参数[2] 来打开。&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; fast&#013;&#010;&gt; emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#013;&#010;&gt; &amp;gt; &amp;amp;gt; table.exec.emit.early-fire.enabled = true&#013;&#010;&gt; &amp;gt; &amp;amp;gt; table.exec.emit.early-fire.delay = 60 s&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; [1]&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#013;&#010;&gt; &amp;gt; &amp;amp;gt; [2]&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; x &lt;35907418@qq.com&amp;amp;amp;gt; 于2020年6月17日周三&#010;上午11:14写道：&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; 需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; CREATE VIEW uv_per_10min AS&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; SELECT&amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&#013;&#010;&gt; MAX(DATE_FORMAT(proctime&amp;amp;amp;amp;nbsp;,&#013;&#010;&gt; &amp;gt; 'yyyy-MM-dd&#013;&#010;&gt; &amp;gt; &amp;amp;gt; HH:mm:00'))&amp;amp;amp;amp;nbsp;OVER w&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; AS time_str,&amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; COUNT(DISTINCT user_id)&#010;OVER&#013;&#010;&gt; w AS uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; FROM user_behavior&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; WINDOW w AS (ORDER BY proctime ROWS BETWEEN&#013;&#010;&gt; UNBOUNDED&#013;&#010;&gt; &amp;gt; PRECEDING AND&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; CURRENT ROW);&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 想请教一下，应该如何处理？&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; PARTITION BY DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&gt; &amp;gt; 这样可以吗，另外状态应该如何清理？&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#010;VIEW吧&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 多谢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>"
    },
    {
        "id": "<tencent_0FEE10B9C122211928D9A9297F5D9F3BAB0A@qq.com>",
        "from": "&quot;x&quot; &lt;35907...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 03:15:21 GMT",
        "subject": "回复： 求助：FLINKSQL1.10实时统计累计UV",
        "content": "版本是1.10.1，最后sink的时候确实是一个window里面做count distinct操作。请问是只要计算过程中含有一个window里面做count&#010;distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，group&amp;nbsp;DATE_FORMAT(rowtm,&#010;'yyyy-MM-dd') 这个sql对应的状态很大。代码如下：&#013;&#010;val rt_totaluv_view : Table = tabEnv.sqlQuery(&#013;&#010;  \"\"\"&#013;&#010;    SELECT MAX(DATE_FORMAT(rowtm, 'yyyy-MM-dd HH:mm:00')) time_str,COUNT(DISTINCT userkey)&#010;uv&#013;&#010;    FROM source&#013;&#010;    GROUP BY DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;    \"\"\")&#013;&#010;tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view)&#013;&#010;&#013;&#010;val totaluvTmp = tabEnv.toRetractStream[(String,Long)](rt_totaluv_view)&#013;&#010;  .filter( line =&amp;gt; line._1 == true ).map( line =&amp;gt; line._2 )&#013;&#010;&#013;&#010;val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#013;&#010;&#013;&#010;tabEnv.sqlUpdate(&#013;&#010;  s\"\"\"&#013;&#010;    INSERT INTO mysql_totaluv&#013;&#010;    SELECT _1,MAX(_2)&#013;&#010;    FROM $totaluvTabTmp&#013;&#010;    GROUP BY _1&#013;&#010;    \"\"\")&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月3日(星期五) 晚上9:47&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题，&#013;&#010;这个已经在1.11中修复了。&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-17942&#013;&#010;&#013;&#010;x &lt;35907418@qq.com&amp;gt; 于2020年7月3日周五 下午4:34写道：&#013;&#010;&#013;&#010;&amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年6月18日(星期四) 中午12:16&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 是的，我觉得这样子是能绕过的。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; On Thu, 18 Jun 2020 at 10:34, x &lt;35907418@qq.com&amp;amp;gt; wrote:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#013;&#010;&amp;gt; &amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery(&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; SELECT MAX(DATE_FORMAT(ts,&#010;'yyyy-MM-dd&#013;&#010;&amp;gt; HH:mm:00'))&#013;&#010;&amp;gt; &amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; FROM user_behavior&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;GROUP BY&#013;&#010;&amp;gt; DATE_FORMAT(ts, 'yyyy-MM-dd')&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\")&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; val resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; .filter(line=&amp;amp;amp;gt;line._1==true).map(line=&amp;amp;amp;gt;line._2)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#013;&#010;&amp;gt; &amp;amp;gt; tabEnv.sqlUpdate(&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; s\"\"\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; INSERT INTO&#010;rt_totaluv&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; SELECT _1,MAX(_2)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; FROM $res&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; GROUP BY _1&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\")&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年6月17日(星期三) 中午1:55&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; CREATE TABLE mysql (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; time_str STRING,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; uv BIGINT,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; PRIMARY KEY (ts) NOT ENFORCED&#013;&#010;&amp;gt; &amp;amp;gt; ) WITH (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 'url' = 'jdbc:mysql://localhost:3306/mydatabase',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 'table-name' = 'myuv'&#013;&#010;&amp;gt; &amp;amp;gt; );&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; INSERT INTO mysql&#013;&#010;&amp;gt; &amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd HH:mm:00')),&#013;&#010;&amp;gt; COUNT(DISTINCT&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; user_id)&#013;&#010;&amp;gt; &amp;amp;gt; FROM user_behavior;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; On Wed, 17 Jun 2020 at 13:49, x &lt;35907418@qq.com&amp;amp;amp;gt;&#010;wrote:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; sink表这个样式&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; tm uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020/06/17 13:46:00 10000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020/06/17 13:47:00 20000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020/06/17 13:48:00 30000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; group by 日期的话，分钟如何获取&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午11:46&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Hi，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 我感觉这种场景可以有两种方式，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 1. 可以直接用group by + mini batch&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2. window聚合 + fast emit&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 对于#1，group by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#013;&#010;&amp;gt; 'yyyy-MM-dd')。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 这种情况下的状态清理，需要配置state&#010;retention时间，配置方法可以参考[1] 。同时，mini&#013;&#010;&amp;gt; batch的开启也需要&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 用参数[2] 来打开。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; fast&#013;&#010;&amp;gt; emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; table.exec.emit.early-fire.enabled = true&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; table.exec.emit.early-fire.delay = 60 s&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; [1]&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; [2]&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; x &lt;35907418@qq.com&amp;amp;amp;amp;gt; 于2020年6月17日周三&#010;上午11:14写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; CREATE VIEW uv_per_10min AS&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; SELECT&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; MAX(DATE_FORMAT(proctime&amp;amp;amp;amp;amp;nbsp;,&#013;&#010;&amp;gt; &amp;amp;gt; 'yyyy-MM-dd&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; HH:mm:00'))&amp;amp;amp;amp;amp;nbsp;OVER w&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; AS time_str,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp; COUNT(DISTINCT&#010;user_id) OVER&#013;&#010;&amp;gt; w AS uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; FROM user_behavior&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; WINDOW w AS (ORDER BY proctime&#010;ROWS BETWEEN&#013;&#010;&amp;gt; UNBOUNDED&#013;&#010;&amp;gt; &amp;amp;gt; PRECEDING AND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; CURRENT ROW);&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 想请教一下，应该如何处理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; PARTITION BY DATE_FORMAT(rowtm,&#010;'yyyy-MM-dd')&#013;&#010;&amp;gt; &amp;amp;gt; 这样可以吗，另外状态应该如何清理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#010;VIEW吧&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 多谢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "1",
        "reply": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>"
    },
    {
        "id": "<CABKuJ_T0B5xhym44usDNETeve8WpZL079KRn9fs4W2cQ7KxS-A@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 04:52:56 GMT",
        "subject": "Re: 求助：FLINKSQL1.10实时统计累计UV",
        "content": "我看你的SQL里面并没有用到窗口呀，只是一个普通的聚合。&#013;&#010;这种聚合需要设置合理的state retention[1]时间的，要不然状态默认是永远不清理的。&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html#idle-state-retention-time&#013;&#010;&#013;&#010;x &lt;35907418@qq.com&gt; 于2020年7月6日周一 上午11:15写道：&#013;&#010;&#013;&#010;&gt; 版本是1.10.1，最后sink的时候确实是一个window里面做count&#013;&#010;&gt; distinct操作。请问是只要计算过程中含有一个window里面做count&#013;&#010;&gt; distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，group&amp;nbsp;DATE_FORMAT(rowtm,&#013;&#010;&gt; 'yyyy-MM-dd') 这个sql对应的状态很大。代码如下：&#013;&#010;&gt; val rt_totaluv_view : Table = tabEnv.sqlQuery(&#013;&#010;&gt;   \"\"\"&#013;&#010;&gt;     SELECT MAX(DATE_FORMAT(rowtm, 'yyyy-MM-dd HH:mm:00'))&#013;&#010;&gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&gt;     FROM source&#013;&#010;&gt;     GROUP BY DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&gt;     \"\"\")&#013;&#010;&gt; tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view)&#013;&#010;&gt;&#013;&#010;&gt; val totaluvTmp = tabEnv.toRetractStream[(String,Long)](rt_totaluv_view)&#013;&#010;&gt;   .filter( line =&amp;gt; line._1 == true ).map( line =&amp;gt; line._2 )&#013;&#010;&gt;&#013;&#010;&gt; val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#013;&#010;&gt;&#013;&#010;&gt; tabEnv.sqlUpdate(&#013;&#010;&gt;   s\"\"\"&#013;&#010;&gt;     INSERT INTO mysql_totaluv&#013;&#010;&gt;     SELECT _1,MAX(_2)&#013;&#010;&gt;     FROM $totaluvTabTmp&#013;&#010;&gt;     GROUP BY _1&#013;&#010;&gt;     \"\"\")&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月3日(星期五) 晚上9:47&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题，&#013;&#010;&gt; 这个已经在1.11中修复了。&#013;&#010;&gt;&#013;&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-17942&#013;&#010;&gt;&#013;&#010;&gt; x &lt;35907418@qq.com&amp;gt; 于2020年7月3日周五 下午4:34写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; 发件人:&amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;gt;;&#013;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年6月18日(星期四) 中午12:16&#013;&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 是的，我觉得这样子是能绕过的。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; On Thu, 18 Jun 2020 at 10:34, x &lt;35907418@qq.com&amp;amp;gt; wrote:&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#013;&#010;&gt; &amp;gt; &amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery(&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\"&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; SELECT&#013;&#010;&gt; MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#013;&#010;&gt; &amp;gt; HH:mm:00'))&#013;&#010;&gt; &amp;gt; &amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; FROM&#013;&#010;&gt; user_behavior&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; GROUP BY&#013;&#010;&gt; &amp;gt; DATE_FORMAT(ts, 'yyyy-MM-dd')&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\")&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; val&#013;&#010;&gt; resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab)&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; .filter(line=&amp;amp;amp;gt;line._1==true).map(line=&amp;amp;amp;gt;line._2)&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#013;&#010;&gt; &amp;gt; &amp;amp;gt; tabEnv.sqlUpdate(&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; s\"\"\"&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; INSERT&#010;INTO&#013;&#010;&gt; rt_totaluv&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; SELECT&#010;_1,MAX(_2)&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; FROM $res&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; GROUP BY&#010;_1&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\")&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年6月17日(星期三) 中午1:55&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&gt; &amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; CREATE TABLE mysql (&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; time_str STRING,&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; uv BIGINT,&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; PRIMARY KEY (ts) NOT ENFORCED&#013;&#010;&gt; &amp;gt; &amp;amp;gt; ) WITH (&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 'url' =&#013;&#010;&gt; 'jdbc:mysql://localhost:3306/mydatabase',&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp;&amp;amp;amp;nbsp; 'table-name' = 'myuv'&#013;&#010;&gt; &amp;gt; &amp;amp;gt; );&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; INSERT INTO mysql&#013;&#010;&gt; &amp;gt; &amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd HH:mm:00')),&#013;&#010;&gt; &amp;gt; COUNT(DISTINCT&amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; user_id)&#013;&#010;&gt; &amp;gt; &amp;amp;gt; FROM user_behavior;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; On Wed, 17 Jun 2020 at 13:49, x &lt;35907418@qq.com&amp;amp;amp;gt;&#013;&#010;&gt; wrote:&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; sink表这个样式&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; tm uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020/06/17 13:46:00 10000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020/06/17 13:47:00 20000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020/06/17 13:48:00 30000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; group by 日期的话，分钟如何获取&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Benchao Li\"&lt;&#013;&#010;&gt; libenchao@apache.org&#013;&#010;&gt; &amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午11:46&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&gt; user-zh@flink.apache.org&#013;&#010;&gt; &amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Hi，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 我感觉这种场景可以有两种方式，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 1. 可以直接用group by + mini batch&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2. window聚合 + fast emit&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 对于#1，group&#013;&#010;&gt; by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#013;&#010;&gt; &amp;gt; 'yyyy-MM-dd')。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 这种情况下的状态清理，需要配置state&#010;retention时间，配置方法可以参考[1]&#013;&#010;&gt; 。同时，mini&#013;&#010;&gt; &amp;gt; batch的开启也需要&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 用参数[2] 来打开。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; fast&#013;&#010;&gt; &amp;gt; emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; table.exec.emit.early-fire.enabled = true&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; table.exec.emit.early-fire.delay = 60 s&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; [1]&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; [2]&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; x &lt;35907418@qq.com&amp;amp;amp;amp;gt;&#013;&#010;&gt; 于2020年6月17日周三 上午11:14写道：&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; 需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; CREATE VIEW uv_per_10min&#010;AS&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; SELECT&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; MAX(DATE_FORMAT(proctime&amp;amp;amp;amp;amp;nbsp;,&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 'yyyy-MM-dd&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; HH:mm:00'))&amp;amp;amp;amp;amp;nbsp;OVER w&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; AS&#013;&#010;&gt; time_str,&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; COUNT(DISTINCT user_id) OVER&#013;&#010;&gt; &amp;gt; w AS uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; FROM user_behavior&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; WINDOW w AS (ORDER BY proctime&#013;&#010;&gt; ROWS BETWEEN&#013;&#010;&gt; &amp;gt; UNBOUNDED&#013;&#010;&gt; &amp;gt; &amp;amp;gt; PRECEDING AND&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; CURRENT ROW);&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 想请教一下，应该如何处理？&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; PARTITION BY&#013;&#010;&gt; DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 这样可以吗，另外状态应该如何清理？&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#013;&#010;&gt; VIEW吧&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 多谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>"
    },
    {
        "id": "<tencent_220E49DE80CECE5439E95ADA196AE7D38D05@qq.com>",
        "from": "&quot;x&quot; &lt;35907...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 05:23:45 GMT",
        "subject": "回复： 求助：FLINKSQL1.10实时统计累计UV",
        "content": "sorry,我说错了，确实没有，都是group agg.&#013;&#010;我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7))，但是状态还是越来越大，没有按既定配置自动清理.&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 中午12:52&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我看你的SQL里面并没有用到窗口呀，只是一个普通的聚合。&#013;&#010;这种聚合需要设置合理的state retention[1]时间的，要不然状态默认是永远不清理的。&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html#idle-state-retention-time&#013;&#010;&#013;&#010;x &lt;35907418@qq.com&amp;gt; 于2020年7月6日周一 上午11:15写道：&#013;&#010;&#013;&#010;&amp;gt; 版本是1.10.1，最后sink的时候确实是一个window里面做count&#013;&#010;&amp;gt; distinct操作。请问是只要计算过程中含有一个window里面做count&#013;&#010;&amp;gt; distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，group&amp;amp;nbsp;DATE_FORMAT(rowtm,&#013;&#010;&amp;gt; 'yyyy-MM-dd') 这个sql对应的状态很大。代码如下：&#013;&#010;&amp;gt; val rt_totaluv_view : Table = tabEnv.sqlQuery(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT MAX(DATE_FORMAT(rowtm, 'yyyy-MM-dd&#010;HH:mm:00'))&#013;&#010;&amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM source&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; GROUP BY DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&amp;gt; tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; val totaluvTmp = tabEnv.toRetractStream[(String,Long)](rt_totaluv_view)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp; .filter( line =&amp;amp;gt; line._1 == true ).map( line =&amp;amp;gt;&#010;line._2 )&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; tabEnv.sqlUpdate(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp; s\"\"\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INSERT INTO mysql_totaluv&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT _1,MAX(_2)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM $totaluvTabTmp&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; GROUP BY _1&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月3日(星期五) 晚上9:47&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题，&#013;&#010;&amp;gt; 这个已经在1.11中修复了。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; [1] https://issues.apache.org/jira/browse/FLINK-17942&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; x &lt;35907418@qq.com&amp;amp;gt; 于2020年7月3日周五 下午4:34写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年6月18日(星期四) 中午12:16&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 是的，我觉得这样子是能绕过的。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; On Thu, 18 Jun 2020 at 10:34, x &lt;35907418@qq.com&amp;amp;amp;gt;&#010;wrote:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;SELECT&#013;&#010;&amp;gt; MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#013;&#010;&amp;gt; &amp;amp;gt; HH:mm:00'))&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;FROM&#013;&#010;&amp;gt; user_behavior&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; GROUP BY&#013;&#010;&amp;gt; &amp;amp;gt; DATE_FORMAT(ts, 'yyyy-MM-dd')&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;\"\"\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; val&#013;&#010;&amp;gt; resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; .filter(line=&amp;amp;amp;amp;gt;line._1==true).map(line=&amp;amp;amp;amp;gt;line._2)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; tabEnv.sqlUpdate(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; s\"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;INSERT INTO&#013;&#010;&amp;gt; rt_totaluv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;SELECT _1,MAX(_2)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;FROM $res&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;GROUP BY _1&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;\"\"\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午1:55&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; CREATE TABLE mysql (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; time_str&#010;STRING,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; uv BIGINT,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; PRIMARY&#010;KEY (ts) NOT ENFORCED&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ) WITH (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; 'connector'&#010;= 'jdbc',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; 'url'&#010;=&#013;&#010;&amp;gt; 'jdbc:mysql://localhost:3306/mydatabase',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; 'table-name'&#010;= 'myuv'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; );&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INSERT INTO mysql&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd HH:mm:00')),&#013;&#010;&amp;gt; &amp;amp;gt; COUNT(DISTINCT&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; user_id)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; FROM user_behavior;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; On Wed, 17 Jun 2020 at 13:49, x &lt;35907418@qq.com&amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; wrote:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; sink表这个样式&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; tm uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2020/06/17 13:46:00 10000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2020/06/17 13:47:00 20000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2020/06/17 13:48:00 30000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; group by 日期的话，分钟如何获取&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;amp;nbsp;\"Benchao&#010;Li\"&lt;&#013;&#010;&amp;gt; libenchao@apache.org&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午11:46&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&amp;gt; user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;Re:&#010;求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; Hi，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 我感觉这种场景可以有两种方式，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 1. 可以直接用group by + mini&#010;batch&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2. window聚合 + fast emit&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 对于#1，group&#013;&#010;&amp;gt; by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#013;&#010;&amp;gt; &amp;amp;gt; 'yyyy-MM-dd')。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 这种情况下的状态清理，需要配置state&#010;retention时间，配置方法可以参考[1]&#013;&#010;&amp;gt; 。同时，mini&#013;&#010;&amp;gt; &amp;amp;gt; batch的开启也需要&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 用参数[2] 来打开。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; fast&#013;&#010;&amp;gt; &amp;amp;gt; emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; table.exec.emit.early-fire.enabled&#010;= true&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; table.exec.emit.early-fire.delay&#010;= 60 s&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; [1]&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; [2]&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; x &lt;35907418@qq.com&amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 于2020年6月17日周三 上午11:14写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; CREATE&#010;VIEW uv_per_10min AS&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; SELECT&amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; MAX(DATE_FORMAT(proctime&amp;amp;amp;amp;amp;amp;nbsp;,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'yyyy-MM-dd&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; HH:mm:00'))&amp;amp;amp;amp;amp;amp;nbsp;OVER&#010;w&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; AS&#013;&#010;&amp;gt; time_str,&amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; COUNT(DISTINCT user_id) OVER&#013;&#010;&amp;gt; &amp;amp;gt; w AS uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; FROM&#010;user_behavior&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; WINDOW&#010;w AS (ORDER BY proctime&#013;&#010;&amp;gt; ROWS BETWEEN&#013;&#010;&amp;gt; &amp;amp;gt; UNBOUNDED&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; PRECEDING AND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; CURRENT&#010;ROW);&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 想请教一下，应该如何处理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; PARTITION&#010;BY&#013;&#010;&amp;gt; DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 这样可以吗，另外状态应该如何清理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#013;&#010;&amp;gt; VIEW吧&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 多谢&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; --&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Benchao Li&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "3",
        "reply": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>"
    },
    {
        "id": "<CABKuJ_TAigw68+ttT7BF4EHUFqi+pmrN=J=iocA6atg-0J43sg@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 15:11:09 GMT",
        "subject": "Re: 求助：FLINKSQL1.10实时统计累计UV",
        "content": "感觉不太应该有这种情况，你用的是blink planner么？&#013;&#010;&#013;&#010;x &lt;35907418@qq.com&gt; 于2020年7月6日周一 下午1:24写道：&#013;&#010;&#013;&#010;&gt; sorry,我说错了，确实没有，都是group agg.&#013;&#010;&gt;&#013;&#010;&gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7))，但是状态还是越来越大，没有按既定配置自动清理.&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月6日(星期一) 中午12:52&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我看你的SQL里面并没有用到窗口呀，只是一个普通的聚合。&#013;&#010;&gt; 这种聚合需要设置合理的state retention[1]时间的，要不然状态默认是永远不清理的。&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html#idle-state-retention-time&#013;&#010;&gt;&#013;&#010;&gt; x &lt;35907418@qq.com&amp;gt; 于2020年7月6日周一 上午11:15写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 版本是1.10.1，最后sink的时候确实是一个window里面做count&#013;&#010;&gt; &amp;gt; distinct操作。请问是只要计算过程中含有一个window里面做count&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，group&amp;amp;nbsp;DATE_FORMAT(rowtm,&#013;&#010;&gt; &amp;gt; 'yyyy-MM-dd') 这个sql对应的状态很大。代码如下：&#013;&#010;&gt; &amp;gt; val rt_totaluv_view : Table = tabEnv.sqlQuery(&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT MAX(DATE_FORMAT(rowtm, 'yyyy-MM-dd&#013;&#010;&gt; HH:mm:00'))&#013;&#010;&gt; &amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM source&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; GROUP BY DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&gt; &amp;gt; tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view)&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; val totaluvTmp =&#013;&#010;&gt; tabEnv.toRetractStream[(String,Long)](rt_totaluv_view)&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp; .filter( line =&amp;amp;gt; line._1 == true ).map( line&#013;&#010;&gt; =&amp;amp;gt; line._2 )&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; tabEnv.sqlUpdate(&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp; s\"\"\"&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; INSERT INTO mysql_totaluv&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; SELECT _1,MAX(_2)&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; FROM $totaluvTabTmp&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; GROUP BY _1&#013;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\")&#013;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; 发件人:&amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;amp;gt;;&#013;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月3日(星期五) 晚上9:47&#013;&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题，&#013;&#010;&gt; &amp;gt; 这个已经在1.11中修复了。&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; [1] https://issues.apache.org/jira/browse/FLINK-17942&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; x &lt;35907418@qq.com&amp;amp;gt; 于2020年7月3日周五 下午4:34写道：&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年6月18日(星期四) 中午12:16&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&gt; &amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 是的，我觉得这样子是能绕过的。&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; On Thu, 18 Jun 2020 at 10:34, x &lt;35907418@qq.com&amp;amp;amp;gt;&#013;&#010;&gt; wrote:&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery(&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; \"\"\"&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;SELECT&#013;&#010;&gt; &amp;gt; MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#013;&#010;&gt; &amp;gt; &amp;amp;gt; HH:mm:00'))&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;FROM&#013;&#010;&gt; &amp;gt; user_behavior&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; GROUP BY&#013;&#010;&gt; &amp;gt; &amp;amp;gt; DATE_FORMAT(ts,&#013;&#010;&gt; 'yyyy-MM-dd')&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; \"\"\")&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; val&#013;&#010;&gt; &amp;gt; resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab)&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; .filter(line=&amp;amp;amp;amp;gt;line._1==true).map(line=&amp;amp;amp;amp;gt;line._2)&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; tabEnv.sqlUpdate(&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp; s\"\"\"&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;INSERT&#013;&#010;&gt; INTO&#013;&#010;&gt; &amp;gt; rt_totaluv&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;SELECT&#013;&#010;&gt; _1,MAX(_2)&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;FROM&#013;&#010;&gt; $res&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;GROUP&#013;&#010;&gt; BY _1&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;\"\"\")&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Jark Wu\"&lt;&#013;&#010;&gt; imjark@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午1:55&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&gt; user-zh@flink.apache.org&#013;&#010;&gt; &amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; CREATE TABLE mysql (&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; time_str&#013;&#010;&gt; STRING,&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; uv&#010;BIGINT,&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; PRIMARY&#013;&#010;&gt; KEY (ts) NOT ENFORCED&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ) WITH (&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&gt; 'connector' = 'jdbc',&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; 'url'&#010;=&#013;&#010;&gt; &amp;gt; 'jdbc:mysql://localhost:3306/mydatabase',&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&gt; 'table-name' = 'myuv'&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; );&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; INSERT INTO mysql&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#013;&#010;&gt; HH:mm:00')),&#013;&#010;&gt; &amp;gt; &amp;amp;gt; COUNT(DISTINCT&amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; user_id)&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; FROM user_behavior;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; On Wed, 17 Jun 2020 at 13:49, x &lt;&#013;&#010;&gt; 35907418@qq.com&amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; wrote:&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; sink表这个样式&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; tm uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2020/06/17 13:46:00 10000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2020/06/17 13:47:00 20000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2020/06/17 13:48:00 30000&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; group by 日期的话，分钟如何获取&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; 发件人:&amp;amp;amp;amp;amp;nbsp;\"Benchao Li\"&lt;&#013;&#010;&gt; &amp;gt; libenchao@apache.org&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年6月17日(星期三) 中午11:46&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&gt; &amp;gt; user-zh@flink.apache.org&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;Re:&#013;&#010;&gt; 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; Hi，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 我感觉这种场景可以有两种方式，&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 1. 可以直接用group by&#010;+ mini batch&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 2. window聚合 + fast emit&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 对于#1，group&#013;&#010;&gt; &amp;gt; by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 'yyyy-MM-dd')。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 这种情况下的状态清理，需要配置state&#013;&#010;&gt; retention时间，配置方法可以参考[1]&#013;&#010;&gt; &amp;gt; 。同时，mini&#013;&#010;&gt; &amp;gt; &amp;amp;gt; batch的开启也需要&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 用参数[2] 来打开。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; fast&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; table.exec.emit.early-fire.enabled = true&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; table.exec.emit.early-fire.delay = 60 s&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; [1]&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; [2]&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; x &lt;35907418@qq.com&#013;&#010;&gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; 于2020年6月17日周三 上午11:14写道：&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; 需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#010;CREATE&#013;&#010;&gt; VIEW uv_per_10min AS&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; SELECT&amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; MAX(DATE_FORMAT(proctime&amp;amp;amp;amp;amp;amp;nbsp;,&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'yyyy-MM-dd&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&gt; HH:mm:00'))&amp;amp;amp;amp;amp;amp;nbsp;OVER w&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#010;AS&#013;&#010;&gt; &amp;gt; time_str,&amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; COUNT(DISTINCT user_id) OVER&#013;&#010;&gt; &amp;gt; &amp;amp;gt; w AS uv&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#010;FROM&#013;&#010;&gt; user_behavior&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#010;WINDOW w&#013;&#010;&gt; AS (ORDER BY proctime&#013;&#010;&gt; &amp;gt; ROWS BETWEEN&#013;&#010;&gt; &amp;gt; &amp;amp;gt; UNBOUNDED&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; PRECEDING AND&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#010;CURRENT&#013;&#010;&gt; ROW);&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; 想请教一下，应该如何处理？&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#010;PARTITION&#013;&#010;&gt; BY&#013;&#010;&gt; &amp;gt; DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 这样可以吗，另外状态应该如何清理？&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#013;&#010;&gt; &amp;gt; VIEW吧&#013;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#010;多谢&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; --&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; Best,&#013;&#010;&gt; &amp;gt; Benchao Li&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "4",
        "reply": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>"
    },
    {
        "id": "<tencent_98D9EB3ADF796AC7C8ED0709D100D02E5F05@qq.com>",
        "from": "&quot;x&quot; &lt;35907...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 02:46:55 GMT",
        "subject": "回复： 求助：FLINKSQL1.10实时统计累计UV",
        "content": "是blinkval setttings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 晚上11:11&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;感觉不太应该有这种情况，你用的是blink planner么？&#013;&#010;&#013;&#010;x &lt;35907418@qq.com&amp;gt; 于2020年7月6日周一 下午1:24写道：&#013;&#010;&#013;&#010;&amp;gt; sorry,我说错了，确实没有，都是group agg.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7))，但是状态还是越来越大，没有按既定配置自动清理.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月6日(星期一) 中午12:52&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我看你的SQL里面并没有用到窗口呀，只是一个普通的聚合。&#013;&#010;&amp;gt; 这种聚合需要设置合理的state retention[1]时间的，要不然状态默认是永远不清理的。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; [1]&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/streaming/query_configuration.html#idle-state-retention-time&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; x &lt;35907418@qq.com&amp;amp;gt; 于2020年7月6日周一 上午11:15写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 版本是1.10.1，最后sink的时候确实是一个window里面做count&#013;&#010;&amp;gt; &amp;amp;gt; distinct操作。请问是只要计算过程中含有一个window里面做count&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; distinct操作，就会造成所有状态过期不自动清理吗？实际我window这步的状态很小，group&amp;amp;amp;nbsp;DATE_FORMAT(rowtm,&#013;&#010;&amp;gt; &amp;amp;gt; 'yyyy-MM-dd') 这个sql对应的状态很大。代码如下：&#013;&#010;&amp;gt; &amp;amp;gt; val rt_totaluv_view : Table = tabEnv.sqlQuery(&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; SELECT MAX(DATE_FORMAT(rowtm,&#010;'yyyy-MM-dd&#013;&#010;&amp;gt; HH:mm:00'))&#013;&#010;&amp;gt; &amp;amp;gt; time_str,COUNT(DISTINCT userkey) uv&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; FROM source&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; GROUP BY DATE_FORMAT(rowtm,&#010;'yyyy-MM-dd')&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\")&#013;&#010;&amp;gt; &amp;amp;gt; tabEnv.createTemporaryView(\"rt_totaluv_view\",rt_totaluv_view)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; val totaluvTmp =&#013;&#010;&amp;gt; tabEnv.toRetractStream[(String,Long)](rt_totaluv_view)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; .filter( line =&amp;amp;amp;gt; line._1&#010;== true ).map( line&#013;&#010;&amp;gt; =&amp;amp;amp;gt; line._2 )&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; val totaluvTabTmp = tabEnv.fromDataStream( totaluvTmp )&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; tabEnv.sqlUpdate(&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; s\"\"\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; INSERT INTO&#010;mysql_totaluv&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; SELECT _1,MAX(_2)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; FROM $totaluvTabTmp&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; GROUP BY _1&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; \"\"\")&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&amp;amp;amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月3日(星期五) 晚上9:47&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 你用的是哪个版本？之前是存在一个类似问题的[1]，是在window里面做count&#010;distinct会有这个问题，&#013;&#010;&amp;gt; &amp;amp;gt; 这个已经在1.11中修复了。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; [1] https://issues.apache.org/jira/browse/FLINK-17942&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; x &lt;35907418@qq.com&amp;amp;amp;gt; 于2020年7月3日周五 下午4:34写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 您好，我程序运行一段时间后，发现checkpoint文件总在增长，应该是状态没有过期，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 我配置了tableConfig.setIdleStateRetentionTime(Time.minutes(2),Time.minutes(7)),按理说，日期是前一天的key对应的状态会在第二天过期的。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;nbsp;\"Jark Wu\"&lt;imjark@gmail.com&amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;nbsp;2020年6月18日(星期四)&#010;中午12:16&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 主题:&amp;amp;amp;amp;nbsp;Re: 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 是的，我觉得这样子是能绕过的。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; On Thu, 18 Jun 2020 at 10:34, x &lt;35907418@qq.com&amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; wrote:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 如果是1.10的话，我通过表转流,再转表的方式实现了，您看合理吗?&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; val resTmpTab: Table = tabEnv.sqlQuery(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;\"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;SELECT&#013;&#010;&amp;gt; &amp;amp;gt; MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; HH:mm:00'))&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; time_str,COUNT(DISTINCT userkey)&#010;uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;FROM&#013;&#010;&amp;gt; &amp;amp;gt; user_behavior&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;GROUP BY&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; DATE_FORMAT(ts,&#013;&#010;&amp;gt; 'yyyy-MM-dd')&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp; \"\"\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; val&#013;&#010;&amp;gt; &amp;amp;gt; resTmpStream=tabEnv.toRetractStream[(String,Long)](resTmpTab)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; .filter(line=&amp;amp;amp;amp;amp;gt;line._1==true).map(line=&amp;amp;amp;amp;amp;gt;line._2)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; val res= tabEnv.fromDataStream(resTmpStream)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; tabEnv.sqlUpdate(&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;s\"\"\"&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;INSERT&#013;&#010;&amp;gt; INTO&#013;&#010;&amp;gt; &amp;amp;gt; rt_totaluv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;SELECT&#013;&#010;&amp;gt; _1,MAX(_2)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;FROM&#013;&#010;&amp;gt; $res&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;GROUP&#013;&#010;&amp;gt; BY _1&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;gt;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;nbsp;&#010;\"\"\")&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发件人:&amp;amp;amp;amp;amp;nbsp;\"Jark&#010;Wu\"&lt;&#013;&#010;&amp;gt; imjark@gmail.com&amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 发送时间:&amp;amp;amp;amp;amp;nbsp;2020年6月17日(星期三)&#010;中午1:55&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 收件人:&amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&amp;gt; user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;nbsp;Re:&#010;求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 在 Flink 1.11 中，你可以尝试这样：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; CREATE TABLE mysql (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#010;time_str&#013;&#010;&amp;gt; STRING,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#010;uv BIGINT,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#010;PRIMARY&#013;&#010;&amp;gt; KEY (ts) NOT ENFORCED&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; ) WITH (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'connector' = 'jdbc',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#010;'url' =&#013;&#010;&amp;gt; &amp;amp;gt; 'jdbc:mysql://localhost:3306/mydatabase',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; 'table-name' = 'myuv'&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; );&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; INSERT INTO mysql&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; SELECT MAX(DATE_FORMAT(ts, 'yyyy-MM-dd&#013;&#010;&amp;gt; HH:mm:00')),&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; COUNT(DISTINCT&amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; user_id)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; FROM user_behavior;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; On Wed, 17 Jun 2020 at 13:49,&#010;x &lt;&#013;&#010;&amp;gt; 35907418@qq.com&amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; wrote:&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 感谢您的回复，您提到的\"方法一\"我还是有点不太理解，我需要每分钟输出一个累计UV，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; sink表这个样式&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; tm uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 2020/06/17&#010;13:46:00 10000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 2020/06/17&#010;13:47:00 20000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 2020/06/17&#010;13:48:00 30000&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; group&#010;by 日期的话，分钟如何获取&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;amp;amp;amp;amp;nbsp;原始邮件&amp;amp;amp;amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 发件人:&amp;amp;amp;amp;amp;amp;nbsp;\"Benchao Li\"&lt;&#013;&#010;&amp;gt; &amp;amp;gt; libenchao@apache.org&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 发送时间:&amp;amp;amp;amp;amp;amp;nbsp;2020年6月17日(星期三) 中午11:46&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 收件人:&amp;amp;amp;amp;amp;amp;nbsp;\"user-zh\"&lt;&#013;&#010;&amp;gt; &amp;amp;gt; user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 主题:&amp;amp;amp;amp;amp;amp;nbsp;Re:&#013;&#010;&amp;gt; 求助：FLINKSQL1.10实时统计累计UV&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; Hi，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 我感觉这种场景可以有两种方式，&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 1. 可以直接用group&#010;by + mini batch&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 2. window聚合&#010;+ fast emit&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 对于#1，group&#013;&#010;&amp;gt; &amp;amp;gt; by的字段里面可以有一个日期的字段，例如你上面提到的DATE_FORMAT(rowtm,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 'yyyy-MM-dd')。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 这种情况下的状态清理，需要配置state&#013;&#010;&amp;gt; retention时间，配置方法可以参考[1]&#013;&#010;&amp;gt; &amp;amp;gt; 。同时，mini&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; batch的开启也需要&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; 用参数[2]&#010;来打开。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 对于#2，这种直接开一个天级别的tumble窗口就行。然后状态清理不用特殊配置，默认就可以清理。&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; fast&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; emit这个配置现在还是一个experimental的feature，所以没有在文档中列出来，我把配置贴到这里，你可以参考一下：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; table.exec.emit.early-fire.enabled = true&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; table.exec.emit.early-fire.delay = 60 s&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; [1]&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/streaming/query_configuration.html&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; [2]&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/dev/table/config.html&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; x &lt;35907418@qq.com&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 于2020年6月17日周三 上午11:14写道：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 需求是，每分钟统计当日0时到当前的累计UV数，用下面的这种方式，跨天的时候，累计UV不会清零，还会一直累计&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#010;CREATE&#013;&#010;&amp;gt; VIEW uv_per_10min AS&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; SELECT&amp;amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; MAX(DATE_FORMAT(proctime&amp;amp;amp;amp;amp;amp;amp;nbsp;,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 'yyyy-MM-dd&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; HH:mm:00'))&amp;amp;amp;amp;amp;amp;amp;nbsp;OVER w&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#010;AS&#013;&#010;&amp;gt; &amp;amp;gt; time_str,&amp;amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; COUNT(DISTINCT user_id) OVER&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; w AS uv&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#010;FROM&#013;&#010;&amp;gt; user_behavior&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#010;WINDOW w&#013;&#010;&amp;gt; AS (ORDER BY proctime&#013;&#010;&amp;gt; &amp;amp;gt; ROWS BETWEEN&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; UNBOUNDED&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; PRECEDING AND&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#010;CURRENT&#013;&#010;&amp;gt; ROW);&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; 想请教一下，应该如何处理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#010;PARTITION&#013;&#010;&amp;gt; BY&#013;&#010;&amp;gt; &amp;amp;gt; DATE_FORMAT(rowtm, 'yyyy-MM-dd')&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; 这样可以吗，另外状态应该如何清理？&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; PS：1.10貌似不支持DDL貌似不支持CREATE&#013;&#010;&amp;gt; &amp;amp;gt; VIEW吧&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;gt; &amp;amp;amp;amp;amp;amp;gt;&#010;多谢&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; --&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; Benchao Li&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; --&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Benchao Li&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "5",
        "reply": "<tencent_CE4C3B0A6E7F485D067E76A6CAFAD720CE08@qq.com>"
    },
    {
        "id": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 09:53:43 GMT",
        "subject": "Flink sql 主动使数据延时一段时间有什么方案",
        "content": "Hi，all&#013;&#010;我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#013;&#010;FLink sql有什么方案实现吗？&#013;&#010;&#013;&#010;感谢您的回复",
        "depth": "0",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<13990240-1F74-4BFF-9E2C-49EEF1AA4C89@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 10:01:23 GMT",
        "subject": "Re: Flink sql 主动使数据延时一段时间有什么方案",
        "content": "补充一下：明确的说是维表的join，A表关联B表（维表），想让A表延迟一会再关联B表&#013;&#010;&#013;&#010;&gt; 2020年7月3日 下午5:53，admin &lt;17626017841@163.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; Hi，all&#013;&#010;&gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#013;&#010;&gt; FLink sql有什么方案实现吗？&#013;&#010;&gt; &#013;&#010;&gt; 感谢您的回复&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<tencent_CE6BFED6F7EEC7D0888783466C7D01020006@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 11:11:37 GMT",
        "subject": "回复：Flink sql 主动使数据延时一段时间有什么方案",
        "content": "设置一个窗口时间，如果有需要取最新的，可以再做一下处理。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: admin &lt;17626017841@163.com&amp;gt;&#013;&#010;发送时间: 2020年7月3日 18:01&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：Flink sql 主动使数据延时一段时间有什么方案&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;补充一下：明确的说是维表的join，A表关联B表（维表），想让A表延迟一会再关联B表&#013;&#010;&#013;&#010;&amp;gt; 2020年7月3日 下午5:53，admin &lt;17626017841@163.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; Hi，all&#013;&#010;&amp;gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#013;&#010;&amp;gt; FLink sql有什么方案实现吗？&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 感谢您的回复",
        "depth": "2",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<3ba2a5b4.100ac.17314e17866.Coremail.17626017841@163.com>",
        "from": "&quot;Sun.Zhu&quot; &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 13:33:07 GMT",
        "subject": "回复：Flink sql 主动使数据延时一段时间有什么方案",
        "content": "&#010;&#010;窗口得用group by，字段会丢失&#010;在2020年07月03日 19:11，kcz 写道：&#010;设置一个窗口时间，如果有需要取最新的，可以再做一下处理。&#010;&#010;&#010;&#010;&#010;&#010;------------------ 原始邮件 ------------------&#010;发件人: admin &lt;17626017841@163.com&amp;gt;&#010;发送时间: 2020年7月3日 18:01&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#010;主题: 回复：Flink sql 主动使数据延时一段时间有什么方案&#010;&#010;&#010;&#010;补充一下：明确的说是维表的join，A表关联B表（维表），想让A表延迟一会再关联B表&#010;&#010;&amp;gt; 2020年7月3日 下午5:53，admin &lt;17626017841@163.com&amp;gt; 写道：&#010;&amp;gt;&#010;&amp;gt; Hi，all&#010;&amp;gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#010;&amp;gt; FLink sql有什么方案实现吗？&#010;&amp;gt;&#010;&amp;gt; 感谢您的回复",
        "depth": "3",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<CABKuJ_TCsdSxaPo2ghM8VVDUAvLkZpbt99HHPo-J6Q-J2_WxFg@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 14:59:56 GMT",
        "subject": "Re: Flink sql 主动使数据延时一段时间有什么方案",
        "content": "我们也遇到过类似场景。&#013;&#010;如果你的数据里面有事件时间，可以写个udf来判断下，如果事件时间-当前时间&#010;小于某个阈值，可以sleep一下。&#013;&#010;如果没有事件时间，那就不太好直接搞了，我们是自己搞了一个延迟维表，就是保证每条数据进到维表join算子后等固定时间后再去join。&#013;&#010;&#013;&#010;admin &lt;17626017841@163.com&gt; 于2020年7月3日周五 下午5:54写道：&#013;&#010;&#013;&#010;&gt; Hi，all&#013;&#010;&gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#013;&#010;&gt; FLink sql有什么方案实现吗？&#013;&#010;&gt;&#013;&#010;&gt; 感谢您的回复&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<CABKuJ_QQzsZdBg=ug9qH+-JTomBSHT9NdQ3cw5ic48Syvm8iDw@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 15:01:10 GMT",
        "subject": "Re: Flink sql 主动使数据延时一段时间有什么方案",
        "content": "还有一种很有意思的思路。&#013;&#010;如果你不考虑数据是否会有乱序，而且保证维表中一定能join到结果，那就可以正常join，如果join不到，就把这条数据再发送到source的topic里，实现了一种类似于for循环的能力。。&#013;&#010;&#013;&#010;admin &lt;17626017841@163.com&gt; 于2020年7月3日周五 下午5:54写道：&#013;&#010;&#013;&#010;&gt; Hi，all&#013;&#010;&gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#013;&#010;&gt; FLink sql有什么方案实现吗？&#013;&#010;&gt;&#013;&#010;&gt; 感谢您的回复&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<CABKuJ_QvcfY=3siiZPex4toQJ=KW47--t137_ToNbNr18K+kYQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 15:05:06 GMT",
        "subject": "Re: Flink sql 主动使数据延时一段时间有什么方案",
        "content": "奥，对，还有一种思路。如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了。&#013;&#010;&#013;&#010;admin &lt;17626017841@163.com&gt; 于2020年7月3日周五 下午5:54写道：&#013;&#010;&#013;&#010;&gt; Hi，all&#013;&#010;&gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#013;&#010;&gt; FLink sql有什么方案实现吗？&#013;&#010;&gt;&#013;&#010;&gt; 感谢您的回复&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<3a13ce04.11ad9.1731548f5f5.Coremail.fszwfly@163.com>",
        "from": "forideal  &lt;fszw...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 15:26:09 GMT",
        "subject": "Re:Re: Flink sql 主动使数据延时一段时间有什么方案",
        "content": "Hi &#010;&#010;&#010;&#010;&#010;刚刚本超说了四种方法，&#010;&#010;方法1.使用udf，查不到 sleep 等一下在查&#010;&#010;方法2.在 join operator处数据等一会再去查&#010;&#010;方法3.如果没有 join 上，就把数据发到source，循环join。&#010;&#010;方法4.如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了&#010;&#010;&#010;&#010;&#010;上述方法应该都能实现相同的效果。&#010;&#010;&#010;&#010;&#010;我们也实现了一种方法。这种方法是扩展了下 Flink 的 Source。比如在&#010;kafka connector 中加了一个 time.wait 的属性，当用户设置了这个属性，就让source&#010;的数据等一会儿发到下游。起到等一会的效果。&#010;&#010;&#010;&#010;&#010;Best forideal&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-03 23:05:06，\"Benchao Li\" &lt;libenchao@apache.org&gt; 写道：&#010;&gt;奥，对，还有一种思路。如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了。&#010;&gt;&#010;&gt;admin &lt;17626017841@163.com&gt; 于2020年7月3日周五 下午5:54写道：&#010;&gt;&#010;&gt;&gt; Hi，all&#010;&gt;&gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#010;&gt;&gt; FLink sql有什么方案实现吗？&#010;&gt;&gt;&#010;&gt;&gt; 感谢您的回复&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;-- &#010;&gt;&#010;&gt;Best,&#010;&gt;Benchao Li&#010;",
        "depth": "2",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<8dcaf31.105c0.173157ced1f.Coremail.17626017841@163.com>",
        "from": "&quot;Sun.Zhu&quot; &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 16:22:55 GMT",
        "subject": "回复： Flink sql 主动使数据延时一段时间有什么方案",
        "content": "感谢benchao和forideal的方案，&#010;方法1.使用udf，查不到 sleep 等一下在查&#010;--这个可以尝试&#010;方法2.在 join operator处数据等一会再去查&#010;—我们使用的是flink sql，不是streaming，所以该方案可能行不通&#010;方法3.如果没有 join 上，就把数据发到source，循环join。&#010;--我们这个维表join的场景类似filter的功能，如果关联上则主流数据就不处理了，所以不一定非要join上，只是想延迟一会提升准确率&#010;方法4.如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了&#010;—我们的source是kafka，好像不支持kafka的功能&#010;方法5.扩展了下 Flink 的 Source。比如在 kafka connector 中加了一个 time.wait&#010;的属性，当用户设置了这个属性，就让source 的数据等一会儿发到下游。起到等一会的效果。&#010;--这个方案需要修改源码，也可以试一下&#010;&#010;&#010;Best&#010;Sun.Zhu&#010;| |&#010;Sun.Zhu&#010;|&#010;|&#010;17626017841@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;&#010;&#010;在2020年07月3日 23:26，forideal&lt;fszwfly@163.com&gt; 写道：&#010;Hi&#010;&#010;&#010;&#010;&#010;刚刚本超说了四种方法，&#010;&#010;方法1.使用udf，查不到 sleep 等一下在查&#010;&#010;方法2.在 join operator处数据等一会再去查&#010;&#010;方法3.如果没有 join 上，就把数据发到source，循环join。&#010;&#010;方法4.如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了&#010;&#010;&#010;&#010;&#010;上述方法应该都能实现相同的效果。&#010;&#010;&#010;&#010;&#010;我们也实现了一种方法。这种方法是扩展了下 Flink 的 Source。比如在&#010;kafka connector 中加了一个 time.wait 的属性，当用户设置了这个属性，就让source&#010;的数据等一会儿发到下游。起到等一会的效果。&#010;&#010;&#010;&#010;&#010;Best forideal&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-03 23:05:06，\"Benchao Li\" &lt;libenchao@apache.org&gt; 写道：&#010;奥，对，还有一种思路。如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了。&#010;&#010;admin &lt;17626017841@163.com&gt; 于2020年7月3日周五 下午5:54写道：&#010;&#010;Hi，all&#010;我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#010;FLink sql有什么方案实现吗？&#010;&#010;感谢您的回复&#010;&#010;&#010;&#010;--&#010;&#010;Best,&#010;Benchao Li&#010;",
        "depth": "3",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<CAOMLN=YWMFwo_scYU9QXvrM5SR9Rzoi5=OjST-4XWBCghiRg0A@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 10:47:39 GMT",
        "subject": "Re: Flink sql 主动使数据延时一段时间有什么方案",
        "content": "Hi Sun ZHu,&#013;&#010;&#013;&#010;关于方法4，我记得kafka有时间轮功能，可以做到延迟消息的，可以了解一下。&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;Sun.Zhu &lt;17626017841@163.com&gt; 于2020年7月4日周六 上午12:23写道：&#013;&#010;&#013;&#010;&gt; 感谢benchao和forideal的方案，&#013;&#010;&gt; 方法1.使用udf，查不到 sleep 等一下在查&#013;&#010;&gt; --这个可以尝试&#013;&#010;&gt; 方法2.在 join operator处数据等一会再去查&#013;&#010;&gt; —我们使用的是flink sql，不是streaming，所以该方案可能行不通&#013;&#010;&gt; 方法3.如果没有 join 上，就把数据发到source，循环join。&#013;&#010;&gt; --我们这个维表join的场景类似filter的功能，如果关联上则主流数据就不处理了，所以不一定非要join上，只是想延迟一会提升准确率&#013;&#010;&gt; 方法4.如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了&#013;&#010;&gt; —我们的source是kafka，好像不支持kafka的功能&#013;&#010;&gt; 方法5.扩展了下 Flink 的 Source。比如在 kafka connector 中加了一个 time.wait&#013;&#010;&gt; 的属性，当用户设置了这个属性，就让source 的数据等一会儿发到下游。起到等一会的效果。&#013;&#010;&gt; --这个方案需要修改源码，也可以试一下&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Sun.Zhu&#013;&#010;&gt; | |&#013;&#010;&gt; Sun.Zhu&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 17626017841@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月3日 23:26，forideal&lt;fszwfly@163.com&gt; 写道：&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 刚刚本超说了四种方法，&#013;&#010;&gt;&#013;&#010;&gt; 方法1.使用udf，查不到 sleep 等一下在查&#013;&#010;&gt;&#013;&#010;&gt; 方法2.在 join operator处数据等一会再去查&#013;&#010;&gt;&#013;&#010;&gt; 方法3.如果没有 join 上，就把数据发到source，循环join。&#013;&#010;&gt;&#013;&#010;&gt; 方法4.如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 上述方法应该都能实现相同的效果。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我们也实现了一种方法。这种方法是扩展了下 Flink 的 Source。比如在&#010;kafka connector 中加了一个 time.wait&#013;&#010;&gt; 的属性，当用户设置了这个属性，就让source 的数据等一会儿发到下游。起到等一会的效果。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best forideal&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-03 23:05:06，\"Benchao Li\" &lt;libenchao@apache.org&gt; 写道：&#013;&#010;&gt; 奥，对，还有一种思路。如果你的source的mq支持延迟消息，这个应该就不需要Flink做什么了，直接用mq的延迟消息就可以了。&#013;&#010;&gt;&#013;&#010;&gt; admin &lt;17626017841@163.com&gt; 于2020年7月3日周五 下午5:54写道：&#013;&#010;&gt;&#013;&#010;&gt; Hi，all&#013;&#010;&gt; 我们有这样一个场景，双流join，一个快流，一个慢流，想让快流等一段时间，目的是能提高join的命中率。&#013;&#010;&gt; FLink sql有什么方案实现吗？&#013;&#010;&gt;&#013;&#010;&gt; 感谢您的回复&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<82355378-0DBD-4CEC-9047-1091C0654F7D@163.com>"
    },
    {
        "id": "<344ae797.a964.17314c39dcf.Coremail.dlguanyq@163.com>",
        "from": "guanyq  &lt;dlgua...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 13:00:31 GMT",
        "subject": "flink1.9自定义实现source的问题",
        "content": "附件图片，想把listener出来的数据，传给ctx。&#010;如何实现这个数据的传递。&#010;public class RMQRichParallelSource extends RichParallelSourceFunction&lt;String&gt; implements&#010;MessageOrderListener {&#010;&#010;@Override&#010;public void open(Configuration parameters) throws Exception {&#010;super.open(parameters);&#010;Properties properties = new Properties();&#010;&#010;// 在订阅消息前，必须调用 start 方法来启动 Consumer，只需调用一次即可。&#010;OrderConsumer consumer = ONSFactory.createOrderedConsumer(properties);&#010;&#010;consumer.subscribe(&#010;\"PRODNOC_KB_SYNC_CUST_ORDER\",&#010;\"*\",&#010;                this);&#010;consumer.start();&#010;}&#010;&#010;@Override&#010;public void run(SourceContext&lt;String&gt; ctx) {&#010;&#010;&#010;    }&#010;&#010;@Override&#010;public OrderAction consume(Message message, ConsumeOrderContext consumeOrderContext) {&#010;try {&#010;            System.out.println(new String(message.getBody(), \"UTF-8\"));&#010;} catch (UnsupportedEncodingException e) {&#010;            e.printStackTrace();&#010;}&#010;return OrderAction.Success;&#010;}&#010;&#010;@Override&#010;public void cancel() {&#010;    }&#010;&#010;@Override&#010;public void close() throws Exception {&#010;super.close();&#010;}&#010;}",
        "depth": "1",
        "reply": "<344ae797.a964.17314c39dcf.Coremail.dlguanyq@163.com>"
    },
    {
        "id": "<1d7ef0ef.b199.17315596d95.Coremail.dlguanyq@163.com>",
        "from": "guanyq  &lt;dlgua...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 03 Jul 2020 15:44:09 GMT",
        "subject": "flink1.9读取阿里Mq问题",
        "content": "flink1.9读取阿里RocketMQ&#010;如何设置AccessKey，SecretKey 参数&#010;&#010;&#010;finalRMQConnectionConfigconnectionConfig=newRMQConnectionConfig.Builder().setHost(\"localhost\").setPort(5000)....build();",
        "depth": "2",
        "reply": "<344ae797.a964.17314c39dcf.Coremail.dlguanyq@163.com>"
    },
    {
        "id": "<CAMhjQvgUhNNpYc3ppjGMfu8bayjHfTW_rCJx5Jbm2WcgMVDyrg@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 03:55:06 GMT",
        "subject": "Re: flink1.9读取阿里Mq问题",
        "content": "hi，guanyq&#013;&#010;&#013;&#010;社区版本的 Flink 应该默认没有和 RocketMQ 连接的 Connector，在 RocketMQ 的社区项目中看到和&#010;Flink 整合的模块：&#013;&#010;https://github.com/apache/rocketmq-externals/tree/master/rocketmq-flink&#013;&#010;&#013;&#010;你说的 AccessKey，SecretKey 参数应该是 ACL 权限校验，看了代码应该是不支持的，不过可以自己去进行扩展。&#013;&#010;&#013;&#010;Best！&#013;&#010;zhisheng&#013;&#010;&#013;&#010;guanyq &lt;dlguanyq@163.com&gt; 于2020年7月3日周五 下午11:44写道：&#013;&#010;&#013;&#010;&gt; flink1.9读取阿里RocketMQ&#013;&#010;&gt; 如何设置AccessKey，SecretKey 参数&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; finalRMQConnectionConfigconnectionConfig=newRMQConnectionConfig.Builder().setHost(\"localhost\").setPort(5000)....build();&#013;&#010;",
        "depth": "3",
        "reply": "<344ae797.a964.17314c39dcf.Coremail.dlguanyq@163.com>"
    },
    {
        "id": "<24407ca0.13f7.1731806b0e8.Coremail.hold_lijun@163.com>",
        "from": "李军 &lt;hold_li...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 04 Jul 2020 04:12:38 GMT",
        "subject": "回复：flink1.9读取阿里Mq问题",
        "content": "        您好！&#010;        自定义source继承RichSourceFuntion.open() 里去构建Conumer 可以设置AccessKey，SecretKey&#010;参数；&#010;        &#010;&#010;&#010;2020-7-4&#010;| |&#010;李军&#010;|&#010;|&#010;hold_lijun@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月3日 23:44，guanyq&lt;dlguanyq@163.com&gt; 写道：&#010;flink1.9读取阿里RocketMQ&#010;如何设置AccessKey，SecretKey 参数&#010;&#010;&#010;finalRMQConnectionConfigconnectionConfig=newRMQConnectionConfig.Builder().setHost(\"localhost\").setPort(5000)....build();",
        "depth": "3",
        "reply": "<344ae797.a964.17314c39dcf.Coremail.dlguanyq@163.com>"
    },
    {
        "id": "<CAA8tFvshuGe4zk6hAW9rOwwm1vzF5j7C9sJX-c4TcCx8=y-U_Q@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 04:12:58 GMT",
        "subject": "Re: flink1.9 on yarn 运行二个多月之后出现错误",
        "content": "你给的日志不像第一次失败的作业的日志，你可能需要看一下之前那个&#010;job 的 jm log 看看是啥原因导致的失败。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年6月23日周二 下午10:07写道：&#013;&#010;&#013;&#010;&gt; Hi guanyq,&#013;&#010;&gt;&#013;&#010;&gt; 从日志中，我看到 TaskManager 所在机器的本地存储几乎快用完了。&#013;&#010;&gt;&#013;&#010;&gt; 看下是否因为 TaskManager 所在机器的存储不够导致&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; LakeShen&#013;&#010;&gt;&#013;&#010;&gt; xueaohui_com@163.com &lt;xueaohui_com@163.com&gt; 于2020年6月20日周六 上午9:57写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 不知道有没有yarn上面的详细日志。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hdfs是否有权限问题&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; xueaohui_com@163.com&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 发件人： guanyq&#013;&#010;&gt; &gt; 发送时间： 2020-06-20 08:48&#013;&#010;&gt; &gt; 收件人： user-zh&#013;&#010;&gt; &gt; 主题： flink1.9 on yarn 运行二个多月之后出现错误&#013;&#010;&gt; &gt; 附件为错误日志。哪位大佬帮忙分析下。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<344ae797.a964.17314c39dcf.Coremail.dlguanyq@163.com>"
    },
    {
        "id": "<CABKuJ_QhDfELhD1Stw55xhFt-RHjD9WiTS7TDVuraJEfyTkxrA@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 03:58:54 GMT",
        "subject": "Re: flink interval join后按窗口聚组问题",
        "content": "回到你的问题，我觉得你的观察是正确的。Time interval join产生的结果的确是会有这种情况。&#010;所以如果用事件时间的time interval join，后面再接一个事件时间的window（或者其他的使用事件时间的算子，比如CEP等）&#010;就会有些问题，很多数据被作为late数据直接丢掉了。&#010;&#010;元始(Bob Hu) &lt;657390448@qq.com&gt; 于2020年7月3日周五 下午3:29写道：&#010;&#010;&gt; 您好，我想请教一个问题：&#010;&gt; flink双流表 interval join后再做window group是不是有问题呢，有些left join关联不上的数据会被丢掉。&#010;&gt; 比如关联条件是select * from a,b where a.id=b.id and b.rowtime between a.rowtime&#010;&gt; and a.rowtime + INTERVAL '1' HOUR&#010;&gt; ，看源码leftRelativeSize=1小时，rightRelativeSize=0，左流cleanUpTime = rowTime&#010;+&#010;&gt; leftRelativeSize + (leftRelativeSize + rightRelativeSize) / 2 +&#010;&gt; allowedLateness +&#010;&gt; 1，左表关联不上的数据会在1.5小时后输出（右表为null），而watermark的调整值是Math.max(leftRelativeSize,&#010;&gt; rightRelativeSize) +&#010;&gt; allowedLateness，也就是1小时，那这样等数据输出的时候watermark不是比左表rowtime还大0.5小时了吗,后面再有对连接流做group&#010;&gt; by的时候这种右表数据为空的数据就丢掉了啊。&#010;&gt; flink版本 1.10.0。&#010;&gt;&#010;&gt; 下面是我的一段测试代码：&#010;&gt;&#010;&gt; import org.apache.commons.net.ntp.TimeStamp;&#010;&gt; import org.apache.flink.api.common.typeinfo.TypeInformation;&#010;&gt; import org.apache.flink.api.common.typeinfo.Types;&#010;&gt; import org.apache.flink.api.java.typeutils.RowTypeInfo;&#010;&gt; import org.apache.flink.streaming.api.TimeCharacteristic;&#010;&gt; import org.apache.flink.streaming.api.datastream.DataStream;&#010;&gt; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt; import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;&#010;&gt; import org.apache.flink.streaming.api.functions.ProcessFunction;&#010;&gt; import org.apache.flink.streaming.api.functions.source.SourceFunction;&#010;&gt; import org.apache.flink.streaming.api.watermark.Watermark;&#010;&gt; import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt; import org.apache.flink.table.api.Table;&#010;&gt; import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;&gt; import org.apache.flink.table.functions.ScalarFunction;&#010;&gt; import org.apache.flink.types.Row;&#010;&gt; import org.apache.flink.util.Collector;&#010;&gt; import org.apache.flink.util.IOUtils;&#010;&gt;&#010;&gt; import java.io.BufferedReader;&#010;&gt; import java.io.InputStreamReader;&#010;&gt; import java.io.Serializable;&#010;&gt; import java.net.InetSocketAddress;&#010;&gt; import java.net.Socket;&#010;&gt; import java.sql.Timestamp;&#010;&gt; import java.text.SimpleDateFormat;&#010;&gt; import java.util.ArrayList;&#010;&gt; import java.util.Date;&#010;&gt; import java.util.List;&#010;&gt;&#010;&gt; public class TimeBoundedJoin {&#010;&gt;&#010;&gt;     public static AssignerWithPeriodicWatermarks&lt;Row&gt; getWatermark(Integer maxIdleTime,&#010;long finalMaxOutOfOrderness) {&#010;&gt;         AssignerWithPeriodicWatermarks&lt;Row&gt; timestampExtractor = new AssignerWithPeriodicWatermarks&lt;Row&gt;()&#010;{&#010;&gt;             private long currentMaxTimestamp = 0;&#010;&gt;             private long lastMaxTimestamp = 0;&#010;&gt;             private long lastUpdateTime = 0;&#010;&gt;             boolean firstWatermark = true;&#010;&gt; //            Integer maxIdleTime = 30;&#010;&gt;&#010;&gt;             @Override&#010;&gt;             public Watermark getCurrentWatermark() {&#010;&gt;                 if(firstWatermark) {&#010;&gt;                     lastUpdateTime = System.currentTimeMillis();&#010;&gt;                     firstWatermark = false;&#010;&gt;                 }&#010;&gt;                 if(currentMaxTimestamp != lastMaxTimestamp) {&#010;&gt;                     lastMaxTimestamp = currentMaxTimestamp;&#010;&gt;                     lastUpdateTime = System.currentTimeMillis();&#010;&gt;                 }&#010;&gt;                 if(maxIdleTime != null &amp;&amp; System.currentTimeMillis() - lastUpdateTime&#010;&gt; maxIdleTime * 1000) {&#010;&gt;                     return new Watermark(new Date().getTime() - finalMaxOutOfOrderness&#010;* 1000);&#010;&gt;                 }&#010;&gt;                 return new Watermark(currentMaxTimestamp - finalMaxOutOfOrderness * 1000);&#010;&gt;&#010;&gt;             }&#010;&gt;&#010;&gt;             @Override&#010;&gt;             public long extractTimestamp(Row row, long previousElementTimestamp) {&#010;&gt;                 Object value = row.getField(1);&#010;&gt;                 long timestamp;&#010;&gt;                 try {&#010;&gt;                     timestamp = (long)value;&#010;&gt;                 } catch (Exception e) {&#010;&gt;                     timestamp = ((Timestamp)value).getTime();&#010;&gt;                 }&#010;&gt;                 if(timestamp &gt; currentMaxTimestamp) {&#010;&gt;                     currentMaxTimestamp = timestamp;&#010;&gt;                 }&#010;&gt;                 return timestamp;&#010;&gt;             }&#010;&gt;         };&#010;&gt;         return timestampExtractor;&#010;&gt;     }&#010;&gt;&#010;&gt;     public static void main(String[] args) throws Exception {&#010;&gt;         StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;         EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;         StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;         bsEnv.setParallelism(1);&#010;&gt;         bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&gt;&#010;&gt;&#010;&gt; //        DataStream&lt;Row&gt; ds1 = bsEnv.addSource(sourceFunction(9000));&#010;&gt;         SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");&#010;&gt;         List&lt;Row&gt; list = new ArrayList&lt;&gt;();&#010;&gt;         list.add(Row.of(\"001\",new Timestamp(sdf.parse(\"2020-05-13 00:00:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 00:20:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 00:40:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"002\",new Timestamp(sdf.parse(\"2020-05-13 01:00:01\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:30:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"003\",new Timestamp(sdf.parse(\"2020-05-13 02:00:02\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:40:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"004\",new Timestamp(sdf.parse(\"2020-05-13 03:00:03\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 03:20:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 03:40:00\").getTime()),&#010;100));&#010;&gt;         list.add(Row.of(\"005\",new Timestamp(sdf.parse(\"2020-05-13 04:00:04\").getTime()),&#010;100));&#010;&gt;         DataStream&lt;Row&gt; ds1 = bsEnv.addSource(new SourceFunction&lt;Row&gt;() {&#010;&gt;             @Override&#010;&gt;             public void run(SourceContext&lt;Row&gt; ctx) throws Exception {&#010;&gt;                 for(Row row : list) {&#010;&gt;                     ctx.collect(row);&#010;&gt;                     Thread.sleep(1000);&#010;&gt;                 }&#010;&gt;&#010;&gt;             }&#010;&gt;&#010;&gt;             @Override&#010;&gt;             public void cancel() {&#010;&gt;&#010;&gt;             }&#010;&gt;         });&#010;&gt;         ds1 = ds1.assignTimestampsAndWatermarks(getWatermark(null, 0));&#010;&gt;         ds1.getTransformation().setOutputType((new RowTypeInfo(Types.STRING, Types.SQL_TIMESTAMP,&#010;Types.INT)));&#010;&gt;         bsTableEnv.createTemporaryView(\"order_info\", ds1, \"order_id, order_time, fee,&#010;rowtime.rowtime\");&#010;&gt;&#010;&gt;         List&lt;Row&gt; list2 = new ArrayList&lt;&gt;();&#010;&gt;         list2.add(Row.of(\"001\",new Timestamp(sdf.parse(\"2020-05-13 01:00:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 01:20:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 01:30:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"002\",new Timestamp(sdf.parse(\"2020-05-13 02:00:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 02:40:00\").getTime())));&#010;&gt; //        list2.add(Row.of(\"003\",new Timestamp(sdf.parse(\"2020-05-13 03:00:03\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 03:20:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 03:40:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"004\",new Timestamp(sdf.parse(\"2020-05-13 04:00:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 04:20:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 04:40:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"005\",new Timestamp(sdf.parse(\"2020-05-13 05:00:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 05:20:00\").getTime())));&#010;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 05:40:00\").getTime())));&#010;&gt;         DataStream&lt;Row&gt; ds2 = bsEnv.addSource(new SourceFunction&lt;Row&gt;() {&#010;&gt;             @Override&#010;&gt;             public void run(SourceContext&lt;Row&gt; ctx) throws Exception {&#010;&gt;                 for(Row row : list2) {&#010;&gt;                     ctx.collect(row);&#010;&gt;                     Thread.sleep(1000);&#010;&gt;                 }&#010;&gt;&#010;&gt;             }&#010;&gt;&#010;&gt;             @Override&#010;&gt;             public void cancel() {&#010;&gt;&#010;&gt;             }&#010;&gt;         });&#010;&gt;         ds2 = ds2.assignTimestampsAndWatermarks(getWatermark(null, 0));&#010;&gt;         ds2.getTransformation().setOutputType((new RowTypeInfo(Types.STRING, Types.SQL_TIMESTAMP)));&#010;&gt;         bsTableEnv.createTemporaryView(\"pay\", ds2, \"order_id, pay_time, rowtime.rowtime\");&#010;&gt;&#010;&gt;         Table joinTable =  bsTableEnv.sqlQuery(\"SELECT a.*,b.order_id from order_info&#010;a left join pay b on a.order_id=b.order_id and b.rowtime between a.rowtime and a.rowtime +&#010;INTERVAL '1' HOUR where a.order_id &lt;&gt;'000' \");&#010;&gt;&#010;&gt;         bsTableEnv.toAppendStream(joinTable, Row.class).process(new ProcessFunction&lt;Row,&#010;Object&gt;() {&#010;&gt;             @Override&#010;&gt;             public void processElement(Row value, Context ctx, Collector&lt;Object&gt;&#010;out) throws Exception {&#010;&gt;                 SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");&#010;&gt;                 System.err.println(\"row:\" + value + \",rowtime:\" + value.getField(3) +&#010;\",watermark:\" + sdf.format(ctx.timerService().currentWatermark()));&#010;&gt;             }&#010;&gt;         });&#010;&gt;&#010;&gt;         bsTableEnv.execute(\"job\");&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&gt;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "1",
        "reply": "<CABKuJ_QhDfELhD1Stw55xhFt-RHjD9WiTS7TDVuraJEfyTkxrA@mail.gmail.com>"
    },
    {
        "id": "<tencent_0A1586278C72017AF8008E2632724AE6C209@qq.com>",
        "from": "&quot;元始(Bob Hu)&quot; &lt;657390...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 12:48:31 GMT",
        "subject": "回复： flink interval join后按窗口聚组问题",
        "content": "谢谢您的解答。感觉flink这个机制有点奇怪呢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Benchao Li\"&lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月5日(星期天) 中午11:58&#013;&#010;收件人:&amp;nbsp;\"元始(Bob Hu)\"&lt;657390448@qq.com&amp;gt;;&#013;&#010;抄送:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;主题:&amp;nbsp;Re: flink interval join后按窗口聚组问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;回到你的问题，我觉得你的观察是正确的。Time interval join产生的结果的确是会有这种情况。所以如果用事件时间的time&#010;interval join，后面再接一个事件时间的window（或者其他的使用事件时间的算子，比如CEP等）&#013;&#010;就会有些问题，很多数据被作为late数据直接丢掉了。&#013;&#010;&#013;&#010;&#013;&#010;元始(Bob Hu) &lt;657390448@qq.com&amp;gt; 于2020年7月3日周五 下午3:29写道：&#013;&#010;&#013;&#010;您好，我想请教一个问题：&#013;&#010;flink双流表 interval join后再做window group是不是有问题呢，有些left join关联不上的数据会被丢掉。&#013;&#010;比如关联条件是select * from a,b where a.id=b.id and b.rowtime between a.rowtime and&#010;a.rowtime + INTERVAL '1' HOUR ，看源码leftRelativeSize=1小时，rightRelativeSize=0，左流cleanUpTime&#010;= rowTime + leftRelativeSize + (leftRelativeSize + rightRelativeSize) / 2 + allowedLateness&#010;+ 1，左表关联不上的数据会在1.5小时后输出（右表为null），而watermark的调整值是Math.max(leftRelativeSize,&#010;rightRelativeSize) + allowedLateness，也就是1小时，那这样等数据输出的时候watermark不是比左表rowtime还大0.5小时了吗,后面再有对连接流做group&#010;by的时候这种右表数据为空的数据就丢掉了啊。&#013;&#010;&#013;&#010;flink版本 1.10.0。&#013;&#010;&#013;&#010;&#013;&#010;下面是我的一段测试代码：&#013;&#010;import org.apache.commons.net.ntp.TimeStamp;&#013;&#010;import org.apache.flink.api.common.typeinfo.TypeInformation;&#013;&#010;import org.apache.flink.api.common.typeinfo.Types;&#013;&#010;import org.apache.flink.api.java.typeutils.RowTypeInfo;&#013;&#010;import org.apache.flink.streaming.api.TimeCharacteristic;&#013;&#010;import org.apache.flink.streaming.api.datastream.DataStream;&#013;&#010;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#013;&#010;import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;&#013;&#010;import org.apache.flink.streaming.api.functions.ProcessFunction;&#013;&#010;import org.apache.flink.streaming.api.functions.source.SourceFunction;&#013;&#010;import org.apache.flink.streaming.api.watermark.Watermark;&#013;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#013;&#010;import org.apache.flink.table.api.Table;&#013;&#010;import org.apache.flink.table.api.java.StreamTableEnvironment;&#013;&#010;import org.apache.flink.table.functions.ScalarFunction;&#013;&#010;import org.apache.flink.types.Row;&#013;&#010;import org.apache.flink.util.Collector;&#013;&#010;import org.apache.flink.util.IOUtils;&#013;&#010;&#013;&#010;import java.io.BufferedReader;&#013;&#010;import java.io.InputStreamReader;&#013;&#010;import java.io.Serializable;&#013;&#010;import java.net.InetSocketAddress;&#013;&#010;import java.net.Socket;&#013;&#010;import java.sql.Timestamp;&#013;&#010;import java.text.SimpleDateFormat;&#013;&#010;import java.util.ArrayList;&#013;&#010;import java.util.Date;&#013;&#010;import java.util.List;&#013;&#010;&#013;&#010;public class TimeBoundedJoin {&#013;&#010;&#013;&#010;    public static AssignerWithPeriodicWatermarks&lt;Row&amp;gt; getWatermark(Integer maxIdleTime,&#010;long finalMaxOutOfOrderness) {&#013;&#010;        AssignerWithPeriodicWatermarks&lt;Row&amp;gt; timestampExtractor = new AssignerWithPeriodicWatermarks&lt;Row&amp;gt;()&#010;{&#013;&#010;            private long currentMaxTimestamp = 0;&#013;&#010;            private long lastMaxTimestamp = 0;&#013;&#010;            private long lastUpdateTime = 0;&#013;&#010;            boolean firstWatermark = true;&#013;&#010;//            Integer maxIdleTime = 30;&#013;&#010;&#013;&#010;            @Override&#013;&#010;            public Watermark getCurrentWatermark() {&#013;&#010;                if(firstWatermark) {&#013;&#010;                    lastUpdateTime = System.currentTimeMillis();&#013;&#010;                    firstWatermark = false;&#013;&#010;                }&#013;&#010;                if(currentMaxTimestamp != lastMaxTimestamp) {&#013;&#010;                    lastMaxTimestamp = currentMaxTimestamp;&#013;&#010;                    lastUpdateTime = System.currentTimeMillis();&#013;&#010;                }&#013;&#010;                if(maxIdleTime != null &amp;amp;&amp;amp; System.currentTimeMillis() - lastUpdateTime&#010;&amp;gt; maxIdleTime * 1000) {&#013;&#010;                    return new Watermark(new Date().getTime() - finalMaxOutOfOrderness * 1000);&#013;&#010;                }&#013;&#010;                return new Watermark(currentMaxTimestamp - finalMaxOutOfOrderness * 1000);&#013;&#010;&#013;&#010;            }&#013;&#010;&#013;&#010;            @Override&#013;&#010;            public long extractTimestamp(Row row, long previousElementTimestamp) {&#013;&#010;                Object value = row.getField(1);&#013;&#010;                long timestamp;&#013;&#010;                try {&#013;&#010;                    timestamp = (long)value;&#013;&#010;                } catch (Exception e) {&#013;&#010;                    timestamp = ((Timestamp)value).getTime();&#013;&#010;                }&#013;&#010;                if(timestamp &amp;gt; currentMaxTimestamp) {&#013;&#010;                    currentMaxTimestamp = timestamp;&#013;&#010;                }&#013;&#010;                return timestamp;&#013;&#010;            }&#013;&#010;        };&#013;&#010;        return timestampExtractor;&#013;&#010;    }&#013;&#010;&#013;&#010;    public static void main(String[] args) throws Exception {&#013;&#010;        StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;        EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;        StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#013;&#010;        bsEnv.setParallelism(1);&#013;&#010;        bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;&#013;&#010;&#013;&#010;//        DataStream&lt;Row&amp;gt; ds1 = bsEnv.addSource(sourceFunction(9000));&#013;&#010;        SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");&#013;&#010;        List&lt;Row&amp;gt; list = new ArrayList&lt;&amp;gt;();&#013;&#010;        list.add(Row.of(\"001\",new Timestamp(sdf.parse(\"2020-05-13 00:00:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 00:20:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 00:40:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"002\",new Timestamp(sdf.parse(\"2020-05-13 01:00:01\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:30:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"003\",new Timestamp(sdf.parse(\"2020-05-13 02:00:02\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:40:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"004\",new Timestamp(sdf.parse(\"2020-05-13 03:00:03\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 03:20:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 03:40:00\").getTime()), 100));&#013;&#010;        list.add(Row.of(\"005\",new Timestamp(sdf.parse(\"2020-05-13 04:00:04\").getTime()), 100));&#013;&#010;        DataStream&lt;Row&amp;gt; ds1 = bsEnv.addSource(new SourceFunction&lt;Row&amp;gt;()&#010;{&#013;&#010;            @Override&#013;&#010;            public void run(SourceContext&lt;Row&amp;gt; ctx) throws Exception {&#013;&#010;                for(Row row : list) {&#013;&#010;                    ctx.collect(row);&#013;&#010;                    Thread.sleep(1000);&#013;&#010;                }&#013;&#010;&#013;&#010;            }&#013;&#010;&#013;&#010;            @Override&#013;&#010;            public void cancel() {&#013;&#010;&#013;&#010;            }&#013;&#010;        });&#013;&#010;        ds1 = ds1.assignTimestampsAndWatermarks(getWatermark(null, 0));&#013;&#010;        ds1.getTransformation().setOutputType((new RowTypeInfo(Types.STRING, Types.SQL_TIMESTAMP,&#010;Types.INT)));&#013;&#010;        bsTableEnv.createTemporaryView(\"order_info\", ds1, \"order_id, order_time, fee, rowtime.rowtime\");&#013;&#010;&#013;&#010;        List&lt;Row&amp;gt; list2 = new ArrayList&lt;&amp;gt;();&#013;&#010;        list2.add(Row.of(\"001\",new Timestamp(sdf.parse(\"2020-05-13 01:00:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 01:20:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 01:30:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"002\",new Timestamp(sdf.parse(\"2020-05-13 02:00:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 02:40:00\").getTime())));&#013;&#010;//        list2.add(Row.of(\"003\",new Timestamp(sdf.parse(\"2020-05-13 03:00:03\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 03:20:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 03:40:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"004\",new Timestamp(sdf.parse(\"2020-05-13 04:00:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 04:20:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 04:40:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"005\",new Timestamp(sdf.parse(\"2020-05-13 05:00:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 05:20:00\").getTime())));&#013;&#010;        list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 05:40:00\").getTime())));&#013;&#010;        DataStream&lt;Row&amp;gt; ds2 = bsEnv.addSource(new SourceFunction&lt;Row&amp;gt;()&#010;{&#013;&#010;            @Override&#013;&#010;            public void run(SourceContext&lt;Row&amp;gt; ctx) throws Exception {&#013;&#010;                for(Row row : list2) {&#013;&#010;                    ctx.collect(row);&#013;&#010;                    Thread.sleep(1000);&#013;&#010;                }&#013;&#010;&#013;&#010;            }&#013;&#010;&#013;&#010;            @Override&#013;&#010;            public void cancel() {&#013;&#010;&#013;&#010;            }&#013;&#010;        });&#013;&#010;        ds2 = ds2.assignTimestampsAndWatermarks(getWatermark(null, 0));&#013;&#010;        ds2.getTransformation().setOutputType((new RowTypeInfo(Types.STRING, Types.SQL_TIMESTAMP)));&#013;&#010;        bsTableEnv.createTemporaryView(\"pay\", ds2, \"order_id, pay_time, rowtime.rowtime\");&#013;&#010;&#013;&#010;        Table joinTable =  bsTableEnv.sqlQuery(\"SELECT a.*,b.order_id from order_info a left&#010;join pay b on a.order_id=b.order_id and b.rowtime between a.rowtime and a.rowtime + INTERVAL&#010;'1' HOUR where a.order_id &lt;&amp;gt;'000' \");&#013;&#010;&#013;&#010;        bsTableEnv.toAppendStream(joinTable, Row.class).process(new ProcessFunction&lt;Row,&#010;Object&amp;gt;() {&#013;&#010;            @Override&#013;&#010;            public void processElement(Row value, Context ctx, Collector&lt;Object&amp;gt;&#010;out) throws Exception {&#013;&#010;                SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");&#013;&#010;                System.err.println(\"row:\" + value + \",rowtime:\" + value.getField(3) + \",watermark:\"&#010;+ sdf.format(ctx.timerService().currentWatermark()));&#013;&#010;            }&#013;&#010;        });&#013;&#010;&#013;&#010;        bsTableEnv.execute(\"job\");&#013;&#010;    }&#013;&#010;}&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "2",
        "reply": "<CABKuJ_QhDfELhD1Stw55xhFt-RHjD9WiTS7TDVuraJEfyTkxrA@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_Rn+DLxJ3s4THLibxoHb7zXwqUxhLBgc3ju_ttxzY3udg@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 15:07:49 GMT",
        "subject": "Re: flink interval join后按窗口聚组问题",
        "content": "我们最开始发现这个现象的时候也有些惊讶，不过后来想了一下感觉也是合理的。&#010;&#010;因为双流Join的时间范围有可能会比较大，比如 A流 在 B流的[-10min, +10min]，那这样的话，&#010;A流来一条数据，可能会join到几分钟之前的数据，而此时的watermark其实已经大于了那条数据的事件时间。&#010;&#010;我个人感觉，这应该就是在更实时的产生Join结果和导致数据时间晚于watermark之间，需要有一个balance。&#010;现在默认实现是选择了更加实时的产生结果。当然还有另外一种实现思路，就是保证watermark不会超过数据时间，&#010;那样的话，Join结果的产生就会delay，或者需要修改watermark逻辑，让watermark一定要小于当前能join到的数据&#010;的时间最早的那个。&#010;&#010;元始(Bob Hu) &lt;657390448@qq.com&gt; 于2020年7月5日周日 下午8:48写道：&#010;&#010;&gt; 谢谢您的解答。感觉flink这个机制有点奇怪呢&#010;&gt;&#010;&gt;&#010;&gt; ------------------ 原始邮件 ------------------&#010;&gt; *发件人:* \"Benchao Li\"&lt;libenchao@apache.org&gt;;&#010;&gt; *发送时间:* 2020年7月5日(星期天) 中午11:58&#010;&gt; *收件人:* \"元始(Bob Hu)\"&lt;657390448@qq.com&gt;;&#010;&gt; *抄送:* \"user-zh\"&lt;user-zh@flink.apache.org&gt;;&#010;&gt; *主题:* Re: flink interval join后按窗口聚组问题&#010;&gt;&#010;&gt; 回到你的问题，我觉得你的观察是正确的。Time interval join产生的结果的确是会有这种情况。&#010;&gt; 所以如果用事件时间的time interval join，后面再接一个事件时间的window（或者其他的使用事件时间的算子，比如CEP等）&#010;&gt; 就会有些问题，很多数据被作为late数据直接丢掉了。&#010;&gt;&#010;&gt; 元始(Bob Hu) &lt;657390448@qq.com&gt; 于2020年7月3日周五 下午3:29写道：&#010;&gt;&#010;&gt;&gt; 您好，我想请教一个问题：&#010;&gt;&gt; flink双流表 interval join后再做window group是不是有问题呢，有些left&#010;join关联不上的数据会被丢掉。&#010;&gt;&gt; 比如关联条件是select * from a,b where a.id=b.id and b.rowtime between a.rowtime&#010;&gt;&gt; and a.rowtime + INTERVAL '1' HOUR&#010;&gt;&gt; ，看源码leftRelativeSize=1小时，rightRelativeSize=0，左流cleanUpTime =&#010;rowTime +&#010;&gt;&gt; leftRelativeSize + (leftRelativeSize + rightRelativeSize) / 2 +&#010;&gt;&gt; allowedLateness +&#010;&gt;&gt; 1，左表关联不上的数据会在1.5小时后输出（右表为null），而watermark的调整值是Math.max(leftRelativeSize,&#010;&gt;&gt; rightRelativeSize) +&#010;&gt;&gt; allowedLateness，也就是1小时，那这样等数据输出的时候watermark不是比左表rowtime还大0.5小时了吗,后面再有对连接流做group&#010;&gt;&gt; by的时候这种右表数据为空的数据就丢掉了啊。&#010;&gt;&gt; flink版本 1.10.0。&#010;&gt;&gt;&#010;&gt;&gt; 下面是我的一段测试代码：&#010;&gt;&gt;&#010;&gt;&gt; import org.apache.commons.net.ntp.TimeStamp;&#010;&gt;&gt; import org.apache.flink.api.common.typeinfo.TypeInformation;&#010;&gt;&gt; import org.apache.flink.api.common.typeinfo.Types;&#010;&gt;&gt; import org.apache.flink.api.java.typeutils.RowTypeInfo;&#010;&gt;&gt; import org.apache.flink.streaming.api.TimeCharacteristic;&#010;&gt;&gt; import org.apache.flink.streaming.api.datastream.DataStream;&#010;&gt;&gt; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt;&gt; import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks;&#010;&gt;&gt; import org.apache.flink.streaming.api.functions.ProcessFunction;&#010;&gt;&gt; import org.apache.flink.streaming.api.functions.source.SourceFunction;&#010;&gt;&gt; import org.apache.flink.streaming.api.watermark.Watermark;&#010;&gt;&gt; import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt;&gt; import org.apache.flink.table.api.Table;&#010;&gt;&gt; import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;&gt;&gt; import org.apache.flink.table.functions.ScalarFunction;&#010;&gt;&gt; import org.apache.flink.types.Row;&#010;&gt;&gt; import org.apache.flink.util.Collector;&#010;&gt;&gt; import org.apache.flink.util.IOUtils;&#010;&gt;&gt;&#010;&gt;&gt; import java.io.BufferedReader;&#010;&gt;&gt; import java.io.InputStreamReader;&#010;&gt;&gt; import java.io.Serializable;&#010;&gt;&gt; import java.net.InetSocketAddress;&#010;&gt;&gt; import java.net.Socket;&#010;&gt;&gt; import java.sql.Timestamp;&#010;&gt;&gt; import java.text.SimpleDateFormat;&#010;&gt;&gt; import java.util.ArrayList;&#010;&gt;&gt; import java.util.Date;&#010;&gt;&gt; import java.util.List;&#010;&gt;&gt;&#010;&gt;&gt; public class TimeBoundedJoin {&#010;&gt;&gt;&#010;&gt;&gt;     public static AssignerWithPeriodicWatermarks&lt;Row&gt; getWatermark(Integer&#010;maxIdleTime, long finalMaxOutOfOrderness) {&#010;&gt;&gt;         AssignerWithPeriodicWatermarks&lt;Row&gt; timestampExtractor = new AssignerWithPeriodicWatermarks&lt;Row&gt;()&#010;{&#010;&gt;&gt;             private long currentMaxTimestamp = 0;&#010;&gt;&gt;             private long lastMaxTimestamp = 0;&#010;&gt;&gt;             private long lastUpdateTime = 0;&#010;&gt;&gt;             boolean firstWatermark = true;&#010;&gt;&gt; //            Integer maxIdleTime = 30;&#010;&gt;&gt;&#010;&gt;&gt;             @Override&#010;&gt;&gt;             public Watermark getCurrentWatermark() {&#010;&gt;&gt;                 if(firstWatermark) {&#010;&gt;&gt;                     lastUpdateTime = System.currentTimeMillis();&#010;&gt;&gt;                     firstWatermark = false;&#010;&gt;&gt;                 }&#010;&gt;&gt;                 if(currentMaxTimestamp != lastMaxTimestamp) {&#010;&gt;&gt;                     lastMaxTimestamp = currentMaxTimestamp;&#010;&gt;&gt;                     lastUpdateTime = System.currentTimeMillis();&#010;&gt;&gt;                 }&#010;&gt;&gt;                 if(maxIdleTime != null &amp;&amp; System.currentTimeMillis() - lastUpdateTime&#010;&gt; maxIdleTime * 1000) {&#010;&gt;&gt;                     return new Watermark(new Date().getTime() - finalMaxOutOfOrderness&#010;* 1000);&#010;&gt;&gt;                 }&#010;&gt;&gt;                 return new Watermark(currentMaxTimestamp - finalMaxOutOfOrderness&#010;* 1000);&#010;&gt;&gt;&#010;&gt;&gt;             }&#010;&gt;&gt;&#010;&gt;&gt;             @Override&#010;&gt;&gt;             public long extractTimestamp(Row row, long previousElementTimestamp)&#010;{&#010;&gt;&gt;                 Object value = row.getField(1);&#010;&gt;&gt;                 long timestamp;&#010;&gt;&gt;                 try {&#010;&gt;&gt;                     timestamp = (long)value;&#010;&gt;&gt;                 } catch (Exception e) {&#010;&gt;&gt;                     timestamp = ((Timestamp)value).getTime();&#010;&gt;&gt;                 }&#010;&gt;&gt;                 if(timestamp &gt; currentMaxTimestamp) {&#010;&gt;&gt;                     currentMaxTimestamp = timestamp;&#010;&gt;&gt;                 }&#010;&gt;&gt;                 return timestamp;&#010;&gt;&gt;             }&#010;&gt;&gt;         };&#010;&gt;&gt;         return timestampExtractor;&#010;&gt;&gt;     }&#010;&gt;&gt;&#010;&gt;&gt;     public static void main(String[] args) throws Exception {&#010;&gt;&gt;         StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;         EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt;         StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv,&#010;bsSettings);&#010;&gt;&gt;         bsEnv.setParallelism(1);&#010;&gt;&gt;         bsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; //        DataStream&lt;Row&gt; ds1 = bsEnv.addSource(sourceFunction(9000));&#010;&gt;&gt;         SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");&#010;&gt;&gt;         List&lt;Row&gt; list = new ArrayList&lt;&gt;();&#010;&gt;&gt;         list.add(Row.of(\"001\",new Timestamp(sdf.parse(\"2020-05-13 00:00:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 00:20:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 00:40:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"002\",new Timestamp(sdf.parse(\"2020-05-13 01:00:01\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:30:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"003\",new Timestamp(sdf.parse(\"2020-05-13 02:00:02\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 02:40:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"004\",new Timestamp(sdf.parse(\"2020-05-13 03:00:03\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 03:20:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"000\",new Timestamp(sdf.parse(\"2020-05-13 03:40:00\").getTime()),&#010;100));&#010;&gt;&gt;         list.add(Row.of(\"005\",new Timestamp(sdf.parse(\"2020-05-13 04:00:04\").getTime()),&#010;100));&#010;&gt;&gt;         DataStream&lt;Row&gt; ds1 = bsEnv.addSource(new SourceFunction&lt;Row&gt;()&#010;{&#010;&gt;&gt;             @Override&#010;&gt;&gt;             public void run(SourceContext&lt;Row&gt; ctx) throws Exception {&#010;&gt;&gt;                 for(Row row : list) {&#010;&gt;&gt;                     ctx.collect(row);&#010;&gt;&gt;                     Thread.sleep(1000);&#010;&gt;&gt;                 }&#010;&gt;&gt;&#010;&gt;&gt;             }&#010;&gt;&gt;&#010;&gt;&gt;             @Override&#010;&gt;&gt;             public void cancel() {&#010;&gt;&gt;&#010;&gt;&gt;             }&#010;&gt;&gt;         });&#010;&gt;&gt;         ds1 = ds1.assignTimestampsAndWatermarks(getWatermark(null, 0));&#010;&gt;&gt;         ds1.getTransformation().setOutputType((new RowTypeInfo(Types.STRING, Types.SQL_TIMESTAMP,&#010;Types.INT)));&#010;&gt;&gt;         bsTableEnv.createTemporaryView(\"order_info\", ds1, \"order_id, order_time,&#010;fee, rowtime.rowtime\");&#010;&gt;&gt;&#010;&gt;&gt;         List&lt;Row&gt; list2 = new ArrayList&lt;&gt;();&#010;&gt;&gt;         list2.add(Row.of(\"001\",new Timestamp(sdf.parse(\"2020-05-13 01:00:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 01:20:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 01:30:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"002\",new Timestamp(sdf.parse(\"2020-05-13 02:00:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 02:20:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 02:40:00\").getTime())));&#010;&gt;&gt; //        list2.add(Row.of(\"003\",new Timestamp(sdf.parse(\"2020-05-13 03:00:03\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 03:20:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 03:40:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"004\",new Timestamp(sdf.parse(\"2020-05-13 04:00:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 04:20:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 04:40:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"005\",new Timestamp(sdf.parse(\"2020-05-13 05:00:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 05:20:00\").getTime())));&#010;&gt;&gt;         list2.add(Row.of(\"111\",new Timestamp(sdf.parse(\"2020-05-13 05:40:00\").getTime())));&#010;&gt;&gt;         DataStream&lt;Row&gt; ds2 = bsEnv.addSource(new SourceFunction&lt;Row&gt;()&#010;{&#010;&gt;&gt;             @Override&#010;&gt;&gt;             public void run(SourceContext&lt;Row&gt; ctx) throws Exception {&#010;&gt;&gt;                 for(Row row : list2) {&#010;&gt;&gt;                     ctx.collect(row);&#010;&gt;&gt;                     Thread.sleep(1000);&#010;&gt;&gt;                 }&#010;&gt;&gt;&#010;&gt;&gt;             }&#010;&gt;&gt;&#010;&gt;&gt;             @Override&#010;&gt;&gt;             public void cancel() {&#010;&gt;&gt;&#010;&gt;&gt;             }&#010;&gt;&gt;         });&#010;&gt;&gt;         ds2 = ds2.assignTimestampsAndWatermarks(getWatermark(null, 0));&#010;&gt;&gt;         ds2.getTransformation().setOutputType((new RowTypeInfo(Types.STRING, Types.SQL_TIMESTAMP)));&#010;&gt;&gt;         bsTableEnv.createTemporaryView(\"pay\", ds2, \"order_id, pay_time, rowtime.rowtime\");&#010;&gt;&gt;&#010;&gt;&gt;         Table joinTable =  bsTableEnv.sqlQuery(\"SELECT a.*,b.order_id from order_info&#010;a left join pay b on a.order_id=b.order_id and b.rowtime between a.rowtime and a.rowtime +&#010;INTERVAL '1' HOUR where a.order_id &lt;&gt;'000' \");&#010;&gt;&gt;&#010;&gt;&gt;         bsTableEnv.toAppendStream(joinTable, Row.class).process(new ProcessFunction&lt;Row,&#010;Object&gt;() {&#010;&gt;&gt;             @Override&#010;&gt;&gt;             public void processElement(Row value, Context ctx, Collector&lt;Object&gt;&#010;out) throws Exception {&#010;&gt;&gt;                 SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");&#010;&gt;&gt;                 System.err.println(\"row:\" + value + \",rowtime:\" + value.getField(3)&#010;+ \",watermark:\" + sdf.format(ctx.timerService().currentWatermark()));&#010;&gt;&gt;             }&#010;&gt;&gt;         });&#010;&gt;&gt;&#010;&gt;&gt;         bsTableEnv.execute(\"job\");&#010;&gt;&gt;     }&#010;&gt;&gt; }&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "3",
        "reply": "<CABKuJ_QhDfELhD1Stw55xhFt-RHjD9WiTS7TDVuraJEfyTkxrA@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvt8QPqeW_szXw+tU2nHks3OE6NcJNQjRNfmDCZTmDMTxg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 04:09:13 GMT",
        "subject": "Re: Re: 为什么 flink checkpoint Checkpoint Duration (Async) 阶段耗时很慢",
        "content": "你好&#013;&#010;&#013;&#010;对于 Checkpoint Async 阶段比较慢的情况，你可以看一下 网络的情况，以及&#010;HDFS 的读写情况，-- 包括 NN 的压力等。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zhisheng &lt;zhisheng2018@gmail.com&gt; 于2020年6月28日周日 上午10:31写道：&#013;&#010;&#013;&#010;&gt; hi，立志：&#013;&#010;&gt;&#013;&#010;&gt; 从你的描述（能跑 10 几天且使用的是 FsStateBackend），可以提供一下&#010;JobManager 和 TaskManager 的 GC&#013;&#010;&gt; 时间和次数的监控信息吗？怀疑是不是因为 Full GC 导致的问题。&#013;&#010;&gt;&#013;&#010;&gt; Best！&#013;&#010;&gt; zhisheng&#013;&#010;&gt;&#013;&#010;&gt; 张立志 &lt;sparrowzoo@163.com&gt; 于2020年6月28日周日 上午10:13写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 从监控后台看back presure 是正常的，flatMap 这个任务是存在的，但只是连了一下mysql，没有别的任何操作，而且另一个job&#013;&#010;&gt; &gt; 没有flatmap ,单纯的map reduce&#013;&#010;&gt; &gt; 统计，能跑10几天，到1个多G的时侯就明显变慢，然后超时10分钟就报错了，从后台的错误日志里，没有明显的异常信息，都是checkpoint&#013;&#010;&gt; &gt; 超时后的信息.&#013;&#010;&gt; &gt; 在 2020-06-28 09:58:00，\"LakeShen\" &lt;shenleifighting@gmail.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;Hi 张立志,&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;一般 Checkpoint 超时，可以先看看你的任务中，是否存在反压，比如&#010;Sink 阶段，又或者是某个地方有 flatMap操作导致。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;然后看下自己任务中，是否存在热点问题等。如果一切都是正常的话，可以尝试使用&#010;RocksDB 的增量 Checkpoint ，具体参考[1]。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;[1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.10/ops/state/state_backends.html#rocksdb-state-backend-details&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;Best,&#013;&#010;&gt; &gt; &gt;LakeShen&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;张立志 &lt;sparrowzoo@163.com&gt; 于2020年6月28日周日 上午9:52写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; flink 版本1.8&#013;&#010;&gt; &gt; &gt;&gt; 部署集群yarn&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; 配置代码:&#013;&#010;&gt; &gt; &gt;&gt; StreamExecutionEnvironment.stateBackend(new&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; FsStateBackend(\"hdfs://nsstreaming/streaming/flink_checkpoint/state\").checkpointingInterval(1000*60*10).checkpointTimeout(1000*60*10).timeCharacteristic(TimeCharacteristic.IngestionTime).build();&#013;&#010;&gt; &gt; &gt;&gt; 业务代码相对比较简单，内存占用较大&#013;&#010;&gt; &gt; &gt;&gt; 超过10分钟后开始报错，state 大概在1.5G时，开始耗时开始变长&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAA8tFvt8QPqeW_szXw+tU2nHks3OE6NcJNQjRNfmDCZTmDMTxg@mail.gmail.com>"
    },
    {
        "id": "<tencent_CBBD616ADC5F3B74629B3FE14BCD1B372708@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 07:50:05 GMT",
        "subject": "【Flink Join内存问题】",
        "content": "Hi,all:&#013;&#010;&#013;&#010;我看源码里写到JoinedStreams:&#013;&#010;也就是说join时候都是走内存计算的，那么如果某个stream的key值过多，会导致oom&#013;&#010;那么有什么预防措施呢?&#013;&#010;将key值多的一边进行打散？&#013;&#010;&#013;&#010;&#013;&#010;Right now, the join is being evaluated in memory so you need to ensure that the number&#013;&#010;* of elements per key does not get too high. Otherwise the JVM might crash.",
        "depth": "0",
        "reply": "<tencent_CBBD616ADC5F3B74629B3FE14BCD1B372708@qq.com>"
    },
    {
        "id": "<A8FC4E4C-F321-47CA-A242-024BC5014D95@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 03:12:10 GMT",
        "subject": "Re: 【Flink Join内存问题】",
        "content": "regular join确实是这样，所以量大的话可以用interval join 、temporal join&#010;&#010;&gt; 2020年7月5日 下午3:50，忝忝向仧 &lt;153488125@qq.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,all:&#010;&gt; &#010;&gt; 我看源码里写到JoinedStreams:&#010;&gt; 也就是说join时候都是走内存计算的，那么如果某个stream的key值过多，会导致oom&#010;&gt; 那么有什么预防措施呢?&#010;&gt; 将key值多的一边进行打散？&#010;&gt; &#010;&gt; &#010;&gt; Right now, the join is being evaluated in memory so you need to ensure that the number&#010;&gt; * of elements per key does not get too high. Otherwise the JVM might crash.&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_CBBD616ADC5F3B74629B3FE14BCD1B372708@qq.com>"
    },
    {
        "id": "<tencent_789BB3DE4435E82F0A3A944DC0F08566B607@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 13 Jul 2020 14:30:18 GMT",
        "subject": "回复： 【Flink Join内存问题】",
        "content": "Hi:&#013;&#010;&#013;&#010;&#013;&#010;interval join可以缓解key值过多问题么?&#013;&#010;interval join不也是计算某段时间范围内的join么，跟regular join相比，如何做到避免某个stream的key过多问题?&#013;&#010;谢谢.&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;17626017841@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 中午11:12&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 【Flink Join内存问题】&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;regular join确实是这样，所以量大的话可以用interval join 、temporal join&#013;&#010;&#013;&#010;&amp;gt; 2020年7月5日 下午3:50，忝忝向仧 &lt;153488125@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; Hi,all:&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 我看源码里写到JoinedStreams:&#013;&#010;&amp;gt; 也就是说join时候都是走内存计算的，那么如果某个stream的key值过多，会导致oom&#013;&#010;&amp;gt; 那么有什么预防措施呢?&#013;&#010;&amp;gt; 将key值多的一边进行打散？&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; Right now, the join is being evaluated in memory so you need to ensure that the number&#013;&#010;&amp;gt; * of elements per key does not get too high. Otherwise the JVM might crash.",
        "depth": "2",
        "reply": "<tencent_CBBD616ADC5F3B74629B3FE14BCD1B372708@qq.com>"
    },
    {
        "id": "<AC4DBEE0-0FF2-44E6-A524-41E3C86B72F0@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:55:14 GMT",
        "subject": "Re: 【Flink Join内存问题】",
        "content": "regular join会缓存两边流的所有数据，interval join只存一段时间内的，相比当然节省很大的状态存储&#010;&#010;&gt; 2020年7月13日 下午10:30，忝忝向仧 &lt;153488125@qq.com&gt; 写道：&#010;&gt; &#010;&gt; Hi:&#010;&gt; &#010;&gt; &#010;&gt; interval join可以缓解key值过多问题么?&#010;&gt; interval join不也是计算某段时间范围内的join么，跟regular join相比，如何做到避免某个stream的key过多问题?&#010;&gt; 谢谢.&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:                                                                          &#010;                                             \"user-zh\"                                   &#010;                                                &lt;17626017841@163.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月6日(星期一) 中午11:12&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt; &#010;&gt; 主题:&amp;nbsp;Re: 【Flink Join内存问题】&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; regular join确实是这样，所以量大的话可以用interval join 、temporal join&#010;&gt; &#010;&gt; &amp;gt; 2020年7月5日 下午3:50，忝忝向仧 &lt;153488125@qq.com&amp;gt; 写道：&#010;&gt; &amp;gt; &#010;&gt; &amp;gt; Hi,all:&#010;&gt; &amp;gt; &#010;&gt; &amp;gt; 我看源码里写到JoinedStreams:&#010;&gt; &amp;gt; 也就是说join时候都是走内存计算的，那么如果某个stream的key值过多，会导致oom&#010;&gt; &amp;gt; 那么有什么预防措施呢?&#010;&gt; &amp;gt; 将key值多的一边进行打散？&#010;&gt; &amp;gt; &#010;&gt; &amp;gt; &#010;&gt; &amp;gt; Right now, the join is being evaluated in memory so you need to ensure that&#010;the number&#010;&gt; &amp;gt; * of elements per key does not get too high. Otherwise the JVM might crash.&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_CBBD616ADC5F3B74629B3FE14BCD1B372708@qq.com>"
    },
    {
        "id": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 05 Jul 2020 14:47:40 GMT",
        "subject": "【Flink的shuffle mode】",
        "content": "Hi,all:&#013;&#010;&#013;&#010;&#013;&#010;看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle mode看是UNDEFINED的。&#013;&#010;那么，shuffle mode有哪些方式?在应用里面可以设置么?&#013;&#010;&#013;&#010;&#013;&#010;谢谢.",
        "depth": "0",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<CABi+2jRjKFdYra8Poh9Q-QFf9U4e70bofdWiR3btJCVbXSKyfg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 03:03:18 GMT",
        "subject": "Re: 【Flink的shuffle mode】",
        "content": "Hi,&#013;&#010;&#013;&#010;现在就两种：pipeline和batch&#013;&#010;&#013;&#010;batch的话是block住，直到执行完毕才发给下游的，所以这个shuffle mode一般只对批作业有用。&#013;&#010;&#013;&#010;理论上可以per transformation的来设置，see PartitionTransformation.&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Sun, Jul 5, 2020 at 10:48 PM 忝忝向仧 &lt;153488125@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi,all:&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle&#013;&#010;&gt; mode看是UNDEFINED的。&#013;&#010;&gt; 那么，shuffle mode有哪些方式?在应用里面可以设置么?&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 谢谢.&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<tencent_9F79D1D96C3E84773747DC3CF5E6A9CD7F0A@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 04:16:36 GMT",
        "subject": "回复：【Flink的shuffle mode】",
        "content": "那就是说datasream默认模式就是pipeline,而批模式是batch，批的模式是存在shuffle情况下，需要等shuffle操作造成，才能发送到下游.那如果批应用有shuffle操作和没有shuffle的，是都要等这个shuffle操作完成了才能一起发给下游，还是说其他非shuffle操作完成了可以先发给下游，不用等shuffle操作完成一起再发送？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;发自我的iPhone&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: Jingsong Li &lt;jingsonglee0@gmail.com&amp;gt;&#013;&#010;发送时间: 2020年7月6日 11:03&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：【Flink的shuffle mode】&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi,&#013;&#010;&#013;&#010;现在就两种：pipeline和batch&#013;&#010;&#013;&#010;batch的话是block住，直到执行完毕才发给下游的，所以这个shuffle mode一般只对批作业有用。&#013;&#010;&#013;&#010;理论上可以per transformation的来设置，see PartitionTransformation.&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Sun, Jul 5, 2020 at 10:48 PM 忝忝向仧 &lt;153488125@qq.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; Hi,all:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle&#013;&#010;&amp;gt; mode看是UNDEFINED的。&#013;&#010;&amp;gt; 那么，shuffle mode有哪些方式?在应用里面可以设置么?&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 谢谢.&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee",
        "depth": "2",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<CABi+2jTZYfQeAskK+JGdtgR5-Ek=tGgg2pD4F_LsbNa=9wTFpg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 04:19:58 GMT",
        "subject": "Re: 【Flink的shuffle mode】",
        "content": "pipeline：直接走网络传输，不buffer所有数据&#013;&#010;batch：buffer所有数据，结束后一起发送&#013;&#010;&#013;&#010;流一定是pipeline&#013;&#010;批可以是pipeline(更好的性能)，也可以是batch(更好的容错和更简单的资源申请)&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Mon, Jul 6, 2020 at 12:16 PM 忝忝向仧 &lt;153488125@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 那就是说datasream默认模式就是pipeline,而批模式是batch，批的模式是存在shuffle情况下，需要等shuffle操作造成，才能发送到下游.那如果批应用有shuffle操作和没有shuffle的，是都要等这个shuffle操作完成了才能一起发给下游，还是说其他非shuffle操作完成了可以先发给下游，不用等shuffle操作完成一起再发送？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 发自我的iPhone&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------ 原始邮件 ------------------&#013;&#010;&gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&amp;gt;&#013;&#010;&gt; 发送时间: 2020年7月6日 11:03&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;&gt; 主题: 回复：【Flink的shuffle mode】&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 现在就两种：pipeline和batch&#013;&#010;&gt;&#013;&#010;&gt; batch的话是block住，直到执行完毕才发给下游的，所以这个shuffle mode一般只对批作业有用。&#013;&#010;&gt;&#013;&#010;&gt; 理论上可以per transformation的来设置，see PartitionTransformation.&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Sun, Jul 5, 2020 at 10:48 PM 忝忝向仧 &lt;153488125@qq.com&amp;gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; Hi,all:&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle&#013;&#010;&gt; &amp;gt; mode看是UNDEFINED的。&#013;&#010;&gt; &amp;gt; 那么，shuffle mode有哪些方式?在应用里面可以设置么?&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 谢谢.&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<tencent_3CA8317CD5D2916E8A854F3777DEACF5B60A@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 15:35:12 GMT",
        "subject": "回复： 【Flink的shuffle mode】",
        "content": "如果是批的模式，怎么在应用程序里面指定shuffle_mode呢?&#013;&#010;另外，下面提到如果是流的计算，一定是pipeline模式.&#013;&#010;那为什么我使用datastream做keyby流操作后，跟踪源码它的mode是UNDEFINED呢?&#013;&#010;谢谢.&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:&amp;nbsp;\"Jingsong Li\"&lt;jingsonglee0@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 中午12:19&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: 【Flink的shuffle mode】&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;pipeline：直接走网络传输，不buffer所有数据&#013;&#010;batch：buffer所有数据，结束后一起发送&#013;&#010;&#013;&#010;流一定是pipeline&#013;&#010;批可以是pipeline(更好的性能)，也可以是batch(更好的容错和更简单的资源申请)&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Mon, Jul 6, 2020 at 12:16 PM 忝忝向仧 &lt;153488125@qq.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 那就是说datasream默认模式就是pipeline,而批模式是batch，批的模式是存在shuffle情况下，需要等shuffle操作造成，才能发送到下游.那如果批应用有shuffle操作和没有shuffle的，是都要等这个shuffle操作完成了才能一起发给下游，还是说其他非shuffle操作完成了可以先发给下游，不用等shuffle操作完成一起再发送？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 发自我的iPhone&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&amp;amp;gt;&#013;&#010;&amp;gt; 发送时间: 2020年7月6日 11:03&#013;&#010;&amp;gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;amp;gt;&#013;&#010;&amp;gt; 主题: 回复：【Flink的shuffle mode】&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Hi,&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 现在就两种：pipeline和batch&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; batch的话是block住，直到执行完毕才发给下游的，所以这个shuffle&#010;mode一般只对批作业有用。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 理论上可以per transformation的来设置，see PartitionTransformation.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Jingsong&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; On Sun, Jul 5, 2020 at 10:48 PM 忝忝向仧 &lt;153488125@qq.com&amp;amp;gt; wrote:&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Hi,all:&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle&#013;&#010;&amp;gt; &amp;amp;gt; mode看是UNDEFINED的。&#013;&#010;&amp;gt; &amp;amp;gt; 那么，shuffle mode有哪些方式?在应用里面可以设置么?&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 谢谢.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; --&#013;&#010;&amp;gt; Best, Jingsong Lee&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee",
        "depth": "4",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<8fd3be6e-c143-498a-a8a8-849b078b583c.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 01:25:56 GMT",
        "subject": "回复：【Flink的shuffle mode】",
        "content": "你好:&#010;问题1,指定shuffle_mode&#010;tEnv.getConfig.getConfiguration.setString(ExecutionConfigOptions.TABLE_EXEC_SHUFFLE_MODE,&#010;\"pipeline\")&#010;问题2,mode是UNDEFINED的概念&#010;使用UNDEFINED并不是说模式没有定义,而是由框架自己决定&#010;The shuffle mode is undefined. It leaves it up to the framework to decide the shuffle mode.&#010;&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：忝忝向仧 &lt;153488125@qq.com&gt;&#010;发送时间：2020年7月7日(星期二) 23:37&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：回复： 【Flink的shuffle mode】&#010;&#010;如果是批的模式，怎么在应用程序里面指定shuffle_mode呢?&#010;另外，下面提到如果是流的计算，一定是pipeline模式.&#010;那为什么我使用datastream做keyby流操作后，跟踪源码它的mode是UNDEFINED呢?&#010;谢谢.&#010;&#010;&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:&amp;nbsp;\"Jingsong Li\"&lt;jingsonglee0@gmail.com&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 中午12:19&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Re: 【Flink的shuffle mode】&#010;&#010;&#010;&#010;pipeline：直接走网络传输，不buffer所有数据&#010;batch：buffer所有数据，结束后一起发送&#010;&#010;流一定是pipeline&#010;批可以是pipeline(更好的性能)，也可以是batch(更好的容错和更简单的资源申请)&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 6, 2020 at 12:16 PM 忝忝向仧 &lt;153488125@qq.com&amp;gt; wrote:&#010;&#010;&amp;gt;&#010;&amp;gt; 那就是说datasream默认模式就是pipeline,而批模式是batch，批的模式是存在shuffle情况下，需要等shuffle操作造成，才能发送到下游.那如果批应用有shuffle操作和没有shuffle的，是都要等这个shuffle操作完成了才能一起发给下游，还是说其他非shuffle操作完成了可以先发给下游，不用等shuffle操作完成一起再发送？&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; 发自我的iPhone&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#010;&amp;gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&amp;amp;gt;&#010;&amp;gt; 发送时间: 2020年7月6日 11:03&#010;&amp;gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;amp;gt;&#010;&amp;gt; 主题: 回复：【Flink的shuffle mode】&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; Hi,&#010;&amp;gt;&#010;&amp;gt; 现在就两种：pipeline和batch&#010;&amp;gt;&#010;&amp;gt; batch的话是block住，直到执行完毕才发给下游的，所以这个shuffle&#010;mode一般只对批作业有用。&#010;&amp;gt;&#010;&amp;gt; 理论上可以per transformation的来设置，see PartitionTransformation.&#010;&amp;gt;&#010;&amp;gt; Best,&#010;&amp;gt; Jingsong&#010;&amp;gt;&#010;&amp;gt; On Sun, Jul 5, 2020 at 10:48 PM 忝忝向仧 &lt;153488125@qq.com&amp;amp;gt; wrote:&#010;&amp;gt;&#010;&amp;gt; &amp;amp;gt; Hi,all:&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt; 看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle&#010;&amp;gt; &amp;amp;gt; mode看是UNDEFINED的。&#010;&amp;gt; &amp;amp;gt; 那么，shuffle mode有哪些方式?在应用里面可以设置么?&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt; 谢谢.&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; --&#010;&amp;gt; Best, Jingsong Lee&#010;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee",
        "depth": "4",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<b4e16796-ed95-4742-86bc-844397cfef45.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 08 Jul 2020 01:33:33 GMT",
        "subject": "回复：【Flink的shuffle mode】",
        "content": "补充: 1.11的shuffle-mode配置的默认值为ALL_EDGES_BLOCKING&#010;共有&#010;ALL_EDGES_BLOCKING(等同于batch)&#010;FORWARD_EDGES_PIPELINEDPOINTWISE_EDGES_PIPELINED&#010;ALL_EDGES_PIPELINED(等同于pipelined)对于pipelined多出了两种选择&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：忝忝向仧 &lt;153488125@qq.com&gt;&#010;发送时间：2020年7月7日(星期二) 23:37&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：回复： 【Flink的shuffle mode】&#010;&#010;如果是批的模式，怎么在应用程序里面指定shuffle_mode呢?&#010;另外，下面提到如果是流的计算，一定是pipeline模式.&#010;那为什么我使用datastream做keyby流操作后，跟踪源码它的mode是UNDEFINED呢?&#010;谢谢.&#010;&#010;&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:&amp;nbsp;\"Jingsong Li\"&lt;jingsonglee0@gmail.com&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月6日(星期一) 中午12:19&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Re: 【Flink的shuffle mode】&#010;&#010;&#010;&#010;pipeline：直接走网络传输，不buffer所有数据&#010;batch：buffer所有数据，结束后一起发送&#010;&#010;流一定是pipeline&#010;批可以是pipeline(更好的性能)，也可以是batch(更好的容错和更简单的资源申请)&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 6, 2020 at 12:16 PM 忝忝向仧 &lt;153488125@qq.com&amp;gt; wrote:&#010;&#010;&amp;gt;&#010;&amp;gt; 那就是说datasream默认模式就是pipeline,而批模式是batch，批的模式是存在shuffle情况下，需要等shuffle操作造成，才能发送到下游.那如果批应用有shuffle操作和没有shuffle的，是都要等这个shuffle操作完成了才能一起发给下游，还是说其他非shuffle操作完成了可以先发给下游，不用等shuffle操作完成一起再发送？&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; 发自我的iPhone&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#010;&amp;gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&amp;amp;gt;&#010;&amp;gt; 发送时间: 2020年7月6日 11:03&#010;&amp;gt; 收件人: user-zh &lt;user-zh@flink.apache.org&amp;amp;gt;&#010;&amp;gt; 主题: 回复：【Flink的shuffle mode】&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; Hi,&#010;&amp;gt;&#010;&amp;gt; 现在就两种：pipeline和batch&#010;&amp;gt;&#010;&amp;gt; batch的话是block住，直到执行完毕才发给下游的，所以这个shuffle&#010;mode一般只对批作业有用。&#010;&amp;gt;&#010;&amp;gt; 理论上可以per transformation的来设置，see PartitionTransformation.&#010;&amp;gt;&#010;&amp;gt; Best,&#010;&amp;gt; Jingsong&#010;&amp;gt;&#010;&amp;gt; On Sun, Jul 5, 2020 at 10:48 PM 忝忝向仧 &lt;153488125@qq.com&amp;amp;gt; wrote:&#010;&amp;gt;&#010;&amp;gt; &amp;amp;gt; Hi,all:&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt; 看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle&#010;&amp;gt; &amp;amp;gt; mode看是UNDEFINED的。&#010;&amp;gt; &amp;amp;gt; 那么，shuffle mode有哪些方式?在应用里面可以设置么?&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt;&#010;&amp;gt; &amp;amp;gt; 谢谢.&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; --&#010;&amp;gt; Best, Jingsong Lee&#010;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee",
        "depth": "4",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<742cc92e-4f80-42bf-83be-278c517b11db.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 04:39:16 GMT",
        "subject": "回复：【Flink的shuffle mode】",
        "content": "你好,可以参考下ExecutionConfigOptions,OptimizerConfigOptions和GlobalConfiguration,里面有比较清楚地介绍&#010;&#010;&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：忝忝向仧 &lt;153488125@qq.com&gt;&#010;发送时间：2020年7月6日(星期一) 12:16&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：回复：【Flink的shuffle mode】&#010;&#010;那就是说datasream默认模式就是pipeline,而批模式是batch，批的模式是存在shuffle情况下，需要等shuffle操作造成，才能发送到下游.那如果批应用有shuffle操作和没有shuffle的，是都要等这个shuffle操作完成了才能一起发给下游，还是说其他非shuffle操作完成了可以先发给下游，不用等shuffle操作完成一起再发送？&#010;&#010;&#010;&#010;发自我的iPhone&#010;&#010;&#010;------------------ 原始邮件 ------------------&#010;发件人: Jingsong Li &lt;jingsonglee0@gmail.com&amp;gt;&#010;发送时间: 2020年7月6日 11:03&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#010;主题: 回复：【Flink的shuffle mode】&#010;&#010;&#010;&#010;Hi,&#010;&#010;现在就两种：pipeline和batch&#010;&#010;batch的话是block住，直到执行完毕才发给下游的，所以这个shuffle mode一般只对批作业有用。&#010;&#010;理论上可以per transformation的来设置，see PartitionTransformation.&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Sun, Jul 5, 2020 at 10:48 PM 忝忝向仧 &lt;153488125@qq.com&amp;gt; wrote:&#010;&#010;&amp;gt; Hi,all:&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; 看Flink源码时候，在应用中使用keyBy后，源码的transformations会有shuffle&#010;mode方法，这个shuffle&#010;&amp;gt; mode看是UNDEFINED的。&#010;&amp;gt; 那么，shuffle mode有哪些方式?在应用里面可以设置么?&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; 谢谢.&#010;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee",
        "depth": "2",
        "reply": "<tencent_AA04F56AAB2D7F29D0B16DEA849F3723DE0A@qq.com>"
    },
    {
        "id": "<CABMA2UvQh=x+2EWbHkZcHyNngvTYJ=m15z9wxjou62qYVO1AUw@mail.gmail.com>",
        "from": "Jim Chen &lt;chenshuai19950...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 02:19:11 GMT",
        "subject": "flink1.10在通过TableFunction实现行转列时，Row一直是空",
        "content": "大家好：&#010;    我的业务场景，是想实现一个行转列的效果。然后通过自定义tableFunction来实现。&#010;在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010;那么在eval方法接收到的就是Row[]，&#010;问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010;&#010;通过下面的步骤和代码可还原车祸场景：&#010;kafka topic: test_action&#010;kafka message:&#010;    {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;\"id002\", \"actionName\": \"bbb\"} ] }&#010;&#010;代码1：Problem.java&#010;package com.flink;&#010;&#010;import&#010;org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#010;import org.apache.flink.table.api.Table;&#010;import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;import org.apache.flink.types.Row;&#010;&#010;/**&#010; *&#010; * 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010; * 那么在eval方法接收到的就是Row[]，&#010; * 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010; *&#010; * kafka topic: test_action&#010; *&#010; * kafka message:&#010; *   {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;\"id002\", \"actionName\": \"bbb\"} ] }&#010; */&#010;public class Problem {&#010;&#010;    public static void main(String[] args) throws Exception {&#010;        StreamExecutionEnvironment env =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;        EnvironmentSettings envSettings = EnvironmentSettings.newInstance()&#010;                .useBlinkPlanner()&#010;                .inStreamingMode()&#010;                .build();&#010;        StreamTableEnvironment bsEnv = StreamTableEnvironment.create(env,&#010;envSettings);&#010;        bsEnv.registerFunction(\"explode2\", new ExplodeFunction());&#010;&#010;        String ddlSource = \"CREATE TABLE actionTable (\\n\" +&#010;                \"    action ARRAY&lt;\\n\" +&#010;                \"               ROW&lt;\" +&#010;                \"                   actionID STRING,\\n\" +&#010;                \"                   actionName STRING\\n\" +&#010;                \"                       &gt;\\n\" +&#010;                \"                           &gt;\\n\" +&#010;                \") WITH (\\n\" +&#010;                \"    'connector.type' = 'kafka',\\n\" +&#010;                \"    'connector.version' = '0.11',\\n\" +&#010;                \"    'connector.topic' = 'test_action',\\n\" +&#010;                \"    'connector.startup-mode' = 'earliest-offset',\\n\" +&#010;                \"    'connector.properties.zookeeper.connect' =&#010;'localhost:2181',\\n\" +&#010;                \"    'connector.properties.bootstrap.servers' =&#010;'localhost:9092',\\n\" +&#010;                \"    'update-mode' = 'append',\\n\" +&#010;                \"    'format.type' = 'json'\\n\" +&#010;                \")\";&#010;        bsEnv.sqlUpdate(ddlSource);&#010;&#010;//        Table table = bsEnv.sqlQuery(\"select `action` from actionTable\");&#010;        Table table = bsEnv.sqlQuery(\"select * from actionTable, LATERAL&#010;TABLE(explode2(`action`)) as T(`word`)\");&#010;        table.printSchema();&#010;        bsEnv.toAppendStream(table, Row.class)&#010;                .print(\"==tb==\");&#010;&#010;&#010;        bsEnv.execute(\"ARRAY tableFunction Problem\");&#010;    }&#010;}&#010;&#010;代码2：ExplodeFunction.java&#010;package com.flink;&#010;&#010;import org.apache.flink.table.functions.TableFunction;&#010;import org.apache.flink.types.Row;&#010;&#010;import java.util.ArrayList;&#010;import java.util.Arrays;&#010;&#010;public class ExplodeFunction extends TableFunction&lt;Row&gt; {&#010;&#010;    public void eval(Row[] values) {&#010;        System.out.println(values.length);&#010;        if (values.length &gt; 0) {&#010;            for (Row row : values) {&#010;                if (row != null) {// 这里debug出来的row总是空&#010;                    ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;();&#010;                    for (int i = 0; i &lt; row.getArity(); i++) {&#010;                        Object field = row.getField(i);&#010;                        list.add(field);&#010;                    }&#010;&#010;collector.collect(Row.of(Arrays.toString(list.toArray())));&#010;                }&#010;            }&#010;        }&#010;    }&#010;}&#010;&#010;最后贴个debug的图&#010;[image: image.png]&#010;&#010;",
        "depth": "0",
        "reply": "<CABMA2UvQh=x+2EWbHkZcHyNngvTYJ=m15z9wxjou62qYVO1AUw@mail.gmail.com>"
    },
    {
        "id": "<CAELO933kXV_UZFNmQ-JBJq4b_H_zDzhG70KFBCmuMuoBA+9K-A@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 02:35:41 GMT",
        "subject": "Re: flink1.10在通过TableFunction实现行转列时，Row一直是空",
        "content": "Hi,&#010;&#010;当前还不支持 Row[] 作为参数。目前有一个 issue 在解决这个问题，可以关注下。&#010;https://issues.apache.org/jira/browse/FLINK-17855&#010;&#010;&#010;Best,&#010;Jark&#010;&#010;On Mon, 6 Jul 2020 at 10:19, Jim Chen &lt;chenshuai19950725@gmail.com&gt; wrote:&#010;&#010;&gt; 大家好：&#010;&gt;     我的业务场景，是想实现一个行转列的效果。然后通过自定义tableFunction来实现。&#010;&gt; 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010;&gt; 那么在eval方法接收到的就是Row[]，&#010;&gt; 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010;&gt;&#010;&gt; 通过下面的步骤和代码可还原车祸场景：&#010;&gt; kafka topic: test_action&#010;&gt; kafka message:&#010;&gt;     {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;&gt; \"id002\", \"actionName\": \"bbb\"} ] }&#010;&gt;&#010;&gt; 代码1：Problem.java&#010;&gt; package com.flink;&#010;&gt;&#010;&gt; import&#010;&gt; org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt; import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt; import org.apache.flink.table.api.Table;&#010;&gt; import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;&gt; import org.apache.flink.types.Row;&#010;&gt;&#010;&gt; /**&#010;&gt;  *&#010;&gt;  * 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010;&gt;  * 那么在eval方法接收到的就是Row[]，&#010;&gt;  * 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010;&gt;  *&#010;&gt;  * kafka topic: test_action&#010;&gt;  *&#010;&gt;  * kafka message:&#010;&gt;  *   {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;&gt; \"id002\", \"actionName\": \"bbb\"} ] }&#010;&gt;  */&#010;&gt; public class Problem {&#010;&gt;&#010;&gt;     public static void main(String[] args) throws Exception {&#010;&gt;         StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;         EnvironmentSettings envSettings = EnvironmentSettings.newInstance()&#010;&gt;                 .useBlinkPlanner()&#010;&gt;                 .inStreamingMode()&#010;&gt;                 .build();&#010;&gt;         StreamTableEnvironment bsEnv = StreamTableEnvironment.create(env,&#010;&gt; envSettings);&#010;&gt;         bsEnv.registerFunction(\"explode2\", new ExplodeFunction());&#010;&gt;&#010;&gt;         String ddlSource = \"CREATE TABLE actionTable (\\n\" +&#010;&gt;                 \"    action ARRAY&lt;\\n\" +&#010;&gt;                 \"               ROW&lt;\" +&#010;&gt;                 \"                   actionID STRING,\\n\" +&#010;&gt;                 \"                   actionName STRING\\n\" +&#010;&gt;                 \"                       &gt;\\n\" +&#010;&gt;                 \"                           &gt;\\n\" +&#010;&gt;                 \") WITH (\\n\" +&#010;&gt;                 \"    'connector.type' = 'kafka',\\n\" +&#010;&gt;                 \"    'connector.version' = '0.11',\\n\" +&#010;&gt;                 \"    'connector.topic' = 'test_action',\\n\" +&#010;&gt;                 \"    'connector.startup-mode' = 'earliest-offset',\\n\" +&#010;&gt;                 \"    'connector.properties.zookeeper.connect' =&#010;&gt; 'localhost:2181',\\n\" +&#010;&gt;                 \"    'connector.properties.bootstrap.servers' =&#010;&gt; 'localhost:9092',\\n\" +&#010;&gt;                 \"    'update-mode' = 'append',\\n\" +&#010;&gt;                 \"    'format.type' = 'json'\\n\" +&#010;&gt;                 \")\";&#010;&gt;         bsEnv.sqlUpdate(ddlSource);&#010;&gt;&#010;&gt; //        Table table = bsEnv.sqlQuery(\"select `action` from actionTable\");&#010;&gt;         Table table = bsEnv.sqlQuery(\"select * from actionTable, LATERAL&#010;&gt; TABLE(explode2(`action`)) as T(`word`)\");&#010;&gt;         table.printSchema();&#010;&gt;         bsEnv.toAppendStream(table, Row.class)&#010;&gt;                 .print(\"==tb==\");&#010;&gt;&#010;&gt;&#010;&gt;         bsEnv.execute(\"ARRAY tableFunction Problem\");&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&gt; 代码2：ExplodeFunction.java&#010;&gt; package com.flink;&#010;&gt;&#010;&gt; import org.apache.flink.table.functions.TableFunction;&#010;&gt; import org.apache.flink.types.Row;&#010;&gt;&#010;&gt; import java.util.ArrayList;&#010;&gt; import java.util.Arrays;&#010;&gt;&#010;&gt; public class ExplodeFunction extends TableFunction&lt;Row&gt; {&#010;&gt;&#010;&gt;     public void eval(Row[] values) {&#010;&gt;         System.out.println(values.length);&#010;&gt;         if (values.length &gt; 0) {&#010;&gt;             for (Row row : values) {&#010;&gt;                 if (row != null) {// 这里debug出来的row总是空&#010;&gt;                     ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;();&#010;&gt;                     for (int i = 0; i &lt; row.getArity(); i++) {&#010;&gt;                         Object field = row.getField(i);&#010;&gt;                         list.add(field);&#010;&gt;                     }&#010;&gt;&#010;&gt; collector.collect(Row.of(Arrays.toString(list.toArray())));&#010;&gt;                 }&#010;&gt;             }&#010;&gt;         }&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&gt; 最后贴个debug的图&#010;&gt; [image: image.png]&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CABMA2UvQh=x+2EWbHkZcHyNngvTYJ=m15z9wxjou62qYVO1AUw@mail.gmail.com>"
    },
    {
        "id": "<CABMA2UtU8-Kp8E9dGdrq4YsVc6s55Kx=NYHX+CRZkQqwKK9CKA@mail.gmail.com>",
        "from": "Jim Chen &lt;chenshuai19950...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 02:59:15 GMT",
        "subject": "Re: flink1.10在通过TableFunction实现行转列时，Row一直是空",
        "content": "Hi,&#010;我现在转换思路，就是在定义表的时候，把ARRYA看成STRING，&#010;那么，现在的问题，就是查询出来，都是空。&#010;&#010;基于上面的代码环境，新写了一个类&#010;Problem2.java&#010;&#010;package com.flink;&#010;&#010;import&#010;org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#010;import org.apache.flink.table.api.Table;&#010;import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;import org.apache.flink.types.Row;&#010;&#010;/**&#010; *&#010; * 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010; * 那么在eval方法接收到的就是Row[]，&#010; * 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010; *&#010; * 现在思路：就是在定义表的时候，把ARRYA看成STRING，&#010; * 现在的问题，就是查询出来，都是空&#010; *&#010; * kafka topic: test_action&#010; *&#010; * kafka message:&#010; *   {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;\"id002\", \"actionName\": \"bbb\"} ] }&#010; */&#010;public class Problem2 {&#010;&#010;    public static void main(String[] args) throws Exception {&#010;        StreamExecutionEnvironment env =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;        EnvironmentSettings envSettings = EnvironmentSettings.newInstance()&#010;                .useBlinkPlanner()&#010;                .inStreamingMode()&#010;                .build();&#010;        StreamTableEnvironment bsEnv = StreamTableEnvironment.create(env,&#010;envSettings);&#010;        bsEnv.registerFunction(\"explode3\", new ExplodeFunction());&#010;&#010;        String ddlSource = \"CREATE TABLE actionTable3 (\\n\" +&#010;                \"    action STRING\\n\" +&#010;                \") WITH (\\n\" +&#010;                \"    'connector.type' = 'kafka',\\n\" +&#010;                \"    'connector.version' = '0.11',\\n\" +&#010;                \"    'connector.topic' = 'test_action',\\n\" +&#010;                \"    'connector.startup-mode' = 'earliest-offset',\\n\" +&#010;                \"    'connector.properties.zookeeper.connect' =&#010;'localhost:2181',\\n\" +&#010;                \"    'connector.properties.bootstrap.servers' =&#010;'localhost:9092',\\n\" +&#010;                \"    'update-mode' = 'append',\\n\" +&#010;                \"    'format.type' = 'json',\\n\" +&#010;                \"    'format.derive-schema' = 'false',\\n\" +&#010;                \"    'format.json-schema' = '{\\\"type\\\": \\\"object\\\",&#010;\\\"properties\\\": {\\\"action\\\": {\\\"type\\\": \\\"string\\\"} } }'\" +&#010;                \")\";&#010;        System.out.println(ddlSource);&#010;        bsEnv.sqlUpdate(ddlSource);&#010;&#010;        Table table = bsEnv.sqlQuery(\"select * from actionTable3\");&#010;//        Table table = bsEnv.sqlQuery(\"select * from actionTable2, LATERAL&#010;TABLE(explode3(`action`)) as T(`word`)\");&#010;        table.printSchema();&#010;        bsEnv.toAppendStream(table, Row.class)&#010;                .print();// 输出都是空&#010;&#010;        bsEnv.execute(\"ARRAY tableFunction Problem\");&#010;    }&#010;}&#010;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月6日周一 上午10:36写道：&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; 当前还不支持 Row[] 作为参数。目前有一个 issue 在解决这个问题，可以关注下。&#010;&gt; https://issues.apache.org/jira/browse/FLINK-17855&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Jark&#010;&gt;&#010;&gt; On Mon, 6 Jul 2020 at 10:19, Jim Chen &lt;chenshuai19950725@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; 大家好：&#010;&gt; &gt;     我的业务场景，是想实现一个行转列的效果。然后通过自定义tableFunction来实现。&#010;&gt; &gt; 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010;&gt; &gt; 那么在eval方法接收到的就是Row[]，&#010;&gt; &gt; 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010;&gt; &gt;&#010;&gt; &gt; 通过下面的步骤和代码可还原车祸场景：&#010;&gt; &gt; kafka topic: test_action&#010;&gt; &gt; kafka message:&#010;&gt; &gt;     {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;&gt; &gt; \"id002\", \"actionName\": \"bbb\"} ] }&#010;&gt; &gt;&#010;&gt; &gt; 代码1：Problem.java&#010;&gt; &gt; package com.flink;&#010;&gt; &gt;&#010;&gt; &gt; import&#010;&gt; &gt; org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt; &gt; import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt; &gt; import org.apache.flink.table.api.Table;&#010;&gt; &gt; import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;&gt; &gt; import org.apache.flink.types.Row;&#010;&gt; &gt;&#010;&gt; &gt; /**&#010;&gt; &gt;  *&#010;&gt; &gt;  * 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010;&gt; &gt;  * 那么在eval方法接收到的就是Row[]，&#010;&gt; &gt;  * 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010;&gt; &gt;  *&#010;&gt; &gt;  * kafka topic: test_action&#010;&gt; &gt;  *&#010;&gt; &gt;  * kafka message:&#010;&gt; &gt;  *   {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;&gt; &gt; \"id002\", \"actionName\": \"bbb\"} ] }&#010;&gt; &gt;  */&#010;&gt; &gt; public class Problem {&#010;&gt; &gt;&#010;&gt; &gt;     public static void main(String[] args) throws Exception {&#010;&gt; &gt;         StreamExecutionEnvironment env =&#010;&gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt;         EnvironmentSettings envSettings =&#010;&gt; EnvironmentSettings.newInstance()&#010;&gt; &gt;                 .useBlinkPlanner()&#010;&gt; &gt;                 .inStreamingMode()&#010;&gt; &gt;                 .build();&#010;&gt; &gt;         StreamTableEnvironment bsEnv = StreamTableEnvironment.create(env,&#010;&gt; &gt; envSettings);&#010;&gt; &gt;         bsEnv.registerFunction(\"explode2\", new ExplodeFunction());&#010;&gt; &gt;&#010;&gt; &gt;         String ddlSource = \"CREATE TABLE actionTable (\\n\" +&#010;&gt; &gt;                 \"    action ARRAY&lt;\\n\" +&#010;&gt; &gt;                 \"               ROW&lt;\" +&#010;&gt; &gt;                 \"                   actionID STRING,\\n\" +&#010;&gt; &gt;                 \"                   actionName STRING\\n\" +&#010;&gt; &gt;                 \"                       &gt;\\n\" +&#010;&gt; &gt;                 \"                           &gt;\\n\" +&#010;&gt; &gt;                 \") WITH (\\n\" +&#010;&gt; &gt;                 \"    'connector.type' = 'kafka',\\n\" +&#010;&gt; &gt;                 \"    'connector.version' = '0.11',\\n\" +&#010;&gt; &gt;                 \"    'connector.topic' = 'test_action',\\n\" +&#010;&gt; &gt;                 \"    'connector.startup-mode' = 'earliest-offset',\\n\" +&#010;&gt; &gt;                 \"    'connector.properties.zookeeper.connect' =&#010;&gt; &gt; 'localhost:2181',\\n\" +&#010;&gt; &gt;                 \"    'connector.properties.bootstrap.servers' =&#010;&gt; &gt; 'localhost:9092',\\n\" +&#010;&gt; &gt;                 \"    'update-mode' = 'append',\\n\" +&#010;&gt; &gt;                 \"    'format.type' = 'json'\\n\" +&#010;&gt; &gt;                 \")\";&#010;&gt; &gt;         bsEnv.sqlUpdate(ddlSource);&#010;&gt; &gt;&#010;&gt; &gt; //        Table table = bsEnv.sqlQuery(\"select `action` from&#010;&gt; actionTable\");&#010;&gt; &gt;         Table table = bsEnv.sqlQuery(\"select * from actionTable, LATERAL&#010;&gt; &gt; TABLE(explode2(`action`)) as T(`word`)\");&#010;&gt; &gt;         table.printSchema();&#010;&gt; &gt;         bsEnv.toAppendStream(table, Row.class)&#010;&gt; &gt;                 .print(\"==tb==\");&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;         bsEnv.execute(\"ARRAY tableFunction Problem\");&#010;&gt; &gt;     }&#010;&gt; &gt; }&#010;&gt; &gt;&#010;&gt; &gt; 代码2：ExplodeFunction.java&#010;&gt; &gt; package com.flink;&#010;&gt; &gt;&#010;&gt; &gt; import org.apache.flink.table.functions.TableFunction;&#010;&gt; &gt; import org.apache.flink.types.Row;&#010;&gt; &gt;&#010;&gt; &gt; import java.util.ArrayList;&#010;&gt; &gt; import java.util.Arrays;&#010;&gt; &gt;&#010;&gt; &gt; public class ExplodeFunction extends TableFunction&lt;Row&gt; {&#010;&gt; &gt;&#010;&gt; &gt;     public void eval(Row[] values) {&#010;&gt; &gt;         System.out.println(values.length);&#010;&gt; &gt;         if (values.length &gt; 0) {&#010;&gt; &gt;             for (Row row : values) {&#010;&gt; &gt;                 if (row != null) {// 这里debug出来的row总是空&#010;&gt; &gt;                     ArrayList&lt;Object&gt; list = new ArrayList&lt;&gt;();&#010;&gt; &gt;                     for (int i = 0; i &lt; row.getArity(); i++) {&#010;&gt; &gt;                         Object field = row.getField(i);&#010;&gt; &gt;                         list.add(field);&#010;&gt; &gt;                     }&#010;&gt; &gt;&#010;&gt; &gt; collector.collect(Row.of(Arrays.toString(list.toArray())));&#010;&gt; &gt;                 }&#010;&gt; &gt;             }&#010;&gt; &gt;         }&#010;&gt; &gt;     }&#010;&gt; &gt; }&#010;&gt; &gt;&#010;&gt; &gt; 最后贴个debug的图&#010;&gt; &gt; [image: image.png]&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CABMA2UvQh=x+2EWbHkZcHyNngvTYJ=m15z9wxjou62qYVO1AUw@mail.gmail.com>"
    },
    {
        "id": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 03:00:07 GMT",
        "subject": "flink 1.11 作业执行异常",
        "content": "Hi，&#010;我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#010;org.apache.flink.table.api.TableExecution: Failed to execute sql&#010;&#010;&#010;caused by : java.lang.IlleagalStateException: No ExecutorFactory found to execute the application.&#010;  at org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;&#010;&#010;想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。",
        "depth": "0",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAELO930b1e0W=cx-tywKNs5SqHE-+wP98k+w3y3+FkN6Z-nkMA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 06:59:17 GMT",
        "subject": "Re: flink 1.11 作业执行异常",
        "content": "能分享下复现的作业代码不？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi，&#013;&#010;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#013;&#010;&gt; org.apache.flink.table.api.TableExecution: Failed to execute sql&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory found to&#013;&#010;&gt; execute the application.&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#013;&#010;",
        "depth": "1",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<49afc9c8.52d2.1732826728a.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 07:21:15 GMT",
        "subject": "Re:Re: flink 1.11 作业执行异常",
        "content": "&#010;&#010;&#010;hi, jark&#010;我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#010;configuration里的DeployOptions.TARGET （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#010;&#010;&#010;//构建StreamExecutionEnvironment&#010;public static final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&#010;//构建EnvironmentSettings 并指定Blink Planner&#010;private static final EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&#010;//构建StreamTableEnvironment&#010;public static final StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, bsSettings);&#010;&#010;&#010;&#010;&#010;&#010;       tEnv.executeSql(“ddl sql”);&#010;&#010;&#010;&#010;&#010;        //source注册成表&#010;&#010;        tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"), $(\"f1\").as(\"first\"), $(\"p\").proctime());&#010;&#010;&#010;&#010;&#010;        //join语句&#010;&#010;        Table table = tEnv.sqlQuery(\"select b.* from test a left join my_dim FOR SYSTEM_TIME&#010;AS OF a.p AS b on a.first = b.userId\");&#010;&#010;&#010;&#010;&#010;        //输出&#010;&#010;        tEnv.toAppendStream(table, Row.class).print(\"LookUpJoinJob\");&#010;&#010;&#010;&#010;&#010;        env.execute(\"LookUpJoinJob\");&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-06 14:59:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;能分享下复现的作业代码不？&#010;&gt;&#010;&gt;Best,&#010;&gt;Jark&#010;&gt;&#010;&gt;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi，&#010;&gt;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#010;&gt;&gt; org.apache.flink.table.api.TableExecution: Failed to execute sql&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory found to&#010;&gt;&gt; execute the application.&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#010;",
        "depth": "2",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAM2Y1LAUKah5PcWC1p3GqvAmN2wvPgPjG86utDgio=pbRrBZZw@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 07:31:24 GMT",
        "subject": "Re: Re: flink 1.11 作业执行异常",
        "content": "hi.sunfulin&#013;&#010;你有没有导入blink的planner呢，加入这个试试&#013;&#010;&#013;&#010;&lt;dependency&gt;&#013;&#010;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;    &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;    &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&lt;/dependency&gt;&#013;&#010;&#013;&#010;&#013;&#010;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月7日周二 下午3:21写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi, jark&#013;&#010;&gt; 我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#013;&#010;&gt; configuration里的DeployOptions.TARGET&#013;&#010;&gt; （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; //构建StreamExecutionEnvironment&#013;&#010;&gt; public static final StreamExecutionEnvironment env =&#013;&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt;&#013;&#010;&gt; //构建EnvironmentSettings 并指定Blink Planner&#013;&#010;&gt; private static final EnvironmentSettings bsSettings =&#013;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt;&#013;&#010;&gt; //构建StreamTableEnvironment&#013;&#010;&gt; public static final StreamTableEnvironment tEnv =&#013;&#010;&gt; StreamTableEnvironment.create(env, bsSettings);&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;        tEnv.executeSql(“ddl sql”);&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;         //source注册成表&#013;&#010;&gt;&#013;&#010;&gt;         tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#013;&#010;&gt; $(\"f1\").as(\"first\"), $(\"p\").proctime());&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;         //join语句&#013;&#010;&gt;&#013;&#010;&gt;         Table table = tEnv.sqlQuery(\"select b.* from test a left join&#013;&#010;&gt; my_dim FOR SYSTEM_TIME AS OF a.p AS b on a.first = b.userId\");&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;         //输出&#013;&#010;&gt;&#013;&#010;&gt;         tEnv.toAppendStream(table, Row.class).print(\"LookUpJoinJob\");&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;         env.execute(\"LookUpJoinJob\");&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-06 14:59:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;能分享下复现的作业代码不？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#013;&#010;&gt; &gt;&gt; org.apache.flink.table.api.TableExecution: Failed to execute sql&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory found&#013;&#010;&gt; to&#013;&#010;&gt; &gt;&gt; execute the application.&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAELO931_a_q=buweE2JDmyt+WxTy9ow9D-G4N3x=FGayM+tTHA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 07:40:17 GMT",
        "subject": "Re: Re: flink 1.11 作业执行异常",
        "content": "Hi,&#013;&#010;&#013;&#010;你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Tue, 7 Jul 2020 at 15:31, Jun Zhang &lt;zhangjunemail100@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi.sunfulin&#013;&#010;&gt; 你有没有导入blink的planner呢，加入这个试试&#013;&#010;&gt;&#013;&#010;&gt; &lt;dependency&gt;&#013;&#010;&gt;     &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;     &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt;     &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &lt;/dependency&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月7日周二 下午3:21写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; hi, jark&#013;&#010;&gt;&gt; 我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#013;&#010;&gt;&gt; configuration里的DeployOptions.TARGET&#013;&#010;&gt;&gt; （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; //构建StreamExecutionEnvironment&#013;&#010;&gt;&gt; public static final StreamExecutionEnvironment env =&#013;&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; //构建EnvironmentSettings 并指定Blink Planner&#013;&#010;&gt;&gt; private static final EnvironmentSettings bsSettings =&#013;&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; //构建StreamTableEnvironment&#013;&#010;&gt;&gt; public static final StreamTableEnvironment tEnv =&#013;&#010;&gt;&gt; StreamTableEnvironment.create(env, bsSettings);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;        tEnv.executeSql(“ddl sql”);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         //source注册成表&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#013;&#010;&gt;&gt; $(\"f1\").as(\"first\"), $(\"p\").proctime());&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         //join语句&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         Table table = tEnv.sqlQuery(\"select b.* from test a left join&#013;&#010;&gt;&gt; my_dim FOR SYSTEM_TIME AS OF a.p AS b on a.first = b.userId\");&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         //输出&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         tEnv.toAppendStream(table, Row.class).print(\"LookUpJoinJob\");&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         env.execute(\"LookUpJoinJob\");&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 在 2020-07-06 14:59:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt;&gt; &gt;能分享下复现的作业代码不？&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;Best,&#013;&#010;&gt;&gt; &gt;Jark&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; Hi，&#013;&#010;&gt;&gt; &gt;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#013;&#010;&gt;&gt; &gt;&gt; org.apache.flink.table.api.TableExecution: Failed to execute sql&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory found&#013;&#010;&gt;&gt; to&#013;&#010;&gt;&gt; &gt;&gt; execute the application.&#013;&#010;&gt;&gt; &gt;&gt;   at&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<4cb263cb.7acd.17328b93de4.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:01:35 GMT",
        "subject": "Re:Re: Re: flink 1.11 作业执行异常",
        "content": "hi, &#010; @Jun Zhang 我一直使用的就是blink planner，这个jar包一直都有的。&#010;&#010; @Jark Wu 我是在本地idea中直接运行的，还没有打包到集群跑。跟这个有关系么？&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-07 15:40:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？&#010;&gt;&#010;&gt;Best,&#010;&gt;Jark&#010;&gt;&#010;&gt;On Tue, 7 Jul 2020 at 15:31, Jun Zhang &lt;zhangjunemail100@gmail.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; hi.sunfulin&#010;&gt;&gt; 你有没有导入blink的planner呢，加入这个试试&#010;&gt;&gt;&#010;&gt;&gt; &lt;dependency&gt;&#010;&gt;&gt;     &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt;     &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt;     &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &lt;/dependency&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月7日周二 下午3:21写道：&#010;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; hi, jark&#010;&gt;&gt;&gt; 我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#010;&gt;&gt;&gt; configuration里的DeployOptions.TARGET&#010;&gt;&gt;&gt; （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; //构建StreamExecutionEnvironment&#010;&gt;&gt;&gt; public static final StreamExecutionEnvironment env =&#010;&gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; //构建EnvironmentSettings 并指定Blink Planner&#010;&gt;&gt;&gt; private static final EnvironmentSettings bsSettings =&#010;&gt;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; //构建StreamTableEnvironment&#010;&gt;&gt;&gt; public static final StreamTableEnvironment tEnv =&#010;&gt;&gt;&gt; StreamTableEnvironment.create(env, bsSettings);&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;        tEnv.executeSql(“ddl sql”);&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;         //source注册成表&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;         tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#010;&gt;&gt;&gt; $(\"f1\").as(\"first\"), $(\"p\").proctime());&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;         //join语句&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;         Table table = tEnv.sqlQuery(\"select b.* from test a left join&#010;&gt;&gt;&gt; my_dim FOR SYSTEM_TIME AS OF a.p AS b on a.first = b.userId\");&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;         //输出&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;         tEnv.toAppendStream(table, Row.class).print(\"LookUpJoinJob\");&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;         env.execute(\"LookUpJoinJob\");&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; 在 2020-07-06 14:59:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; &gt;能分享下复现的作业代码不？&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;Best,&#010;&gt;&gt;&gt; &gt;Jark&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; Hi，&#010;&gt;&gt;&gt; &gt;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#010;&gt;&gt;&gt; &gt;&gt; org.apache.flink.table.api.TableExecution: Failed to execute sql&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory found&#010;&gt;&gt;&gt; to&#010;&gt;&gt;&gt; &gt;&gt; execute the application.&#010;&gt;&gt;&gt; &gt;&gt;   at&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#010;&gt;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "5",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAELO932zM9KxTLfoyy_DH6HMZVGp2bR1=MWbZuik3SVFUq_6BA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:10:58 GMT",
        "subject": "Re: Re: Re: flink 1.11 作业执行异常",
        "content": "如果是在 IDEA 中运行的话，你看看 blink planner 这个依赖的 scope 是不是被&#010;provided 掉了？ 去掉 provided&#013;&#010;再试试看？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Tue, 7 Jul 2020 at 18:01, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi,&#013;&#010;&gt;  @Jun Zhang 我一直使用的就是blink planner，这个jar包一直都有的。&#013;&#010;&gt;&#013;&#010;&gt;  @Jark Wu 我是在本地idea中直接运行的，还没有打包到集群跑。跟这个有关系么？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-07 15:40:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;Hi,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;On Tue, 7 Jul 2020 at 15:31, Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; hi.sunfulin&#013;&#010;&gt; &gt;&gt; 你有没有导入blink的planner呢，加入这个试试&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt;     &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;  &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt;     &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt; &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月7日周二 下午3:21写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; hi, jark&#013;&#010;&gt; &gt;&gt;&gt; 我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#013;&#010;&gt; &gt;&gt;&gt; configuration里的DeployOptions.TARGET&#013;&#010;&gt; &gt;&gt;&gt; （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; //构建StreamExecutionEnvironment&#013;&#010;&gt; &gt;&gt;&gt; public static final StreamExecutionEnvironment env =&#013;&#010;&gt; &gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; //构建EnvironmentSettings 并指定Blink Planner&#013;&#010;&gt; &gt;&gt;&gt; private static final EnvironmentSettings bsSettings =&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; //构建StreamTableEnvironment&#013;&#010;&gt; &gt;&gt;&gt; public static final StreamTableEnvironment tEnv =&#013;&#010;&gt; &gt;&gt;&gt; StreamTableEnvironment.create(env, bsSettings);&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;        tEnv.executeSql(“ddl sql”);&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;         //source注册成表&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;         tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#013;&#010;&gt; &gt;&gt;&gt; $(\"f1\").as(\"first\"), $(\"p\").proctime());&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;         //join语句&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;         Table table = tEnv.sqlQuery(\"select b.* from test a left join&#013;&#010;&gt; &gt;&gt;&gt; my_dim FOR SYSTEM_TIME AS OF a.p AS b on a.first = b.userId\");&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;         //输出&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;         tEnv.toAppendStream(table, Row.class).print(\"LookUpJoinJob\");&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;         env.execute(\"LookUpJoinJob\");&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 在 2020-07-06 14:59:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&gt; &gt;能分享下复现的作业代码不？&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;Best,&#013;&#010;&gt; &gt;&gt;&gt; &gt;Jark&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt; org.apache.flink.table.api.TableExecution: Failed to execute sql&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory&#013;&#010;&gt; found&#013;&#010;&gt; &gt;&gt;&gt; to&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt; execute the application.&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<105013ee.7e55.17328d22f32.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 10:28:50 GMT",
        "subject": "Re:Re: Re: Re: flink 1.11 作业执行异常",
        "content": "&#010;&#010;&#010;hi,&#010;我的pom文件本地执行时，scope的provided都是去掉的。&#010;&lt;dependency&gt;&#010;    &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;   &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#010;   &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&lt;/dependency&gt;&#010;&#010;&#010;确实比较诡异。org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;这个异常在啥情况下会触发到。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-07 18:10:58，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;如果是在 IDEA 中运行的话，你看看 blink planner 这个依赖的 scope 是不是被&#010;provided 掉了？ 去掉 provided&#010;&gt;再试试看？&#010;&gt;&#010;&gt;Best,&#010;&gt;Jark&#010;&gt;&#010;&gt;On Tue, 7 Jul 2020 at 18:01, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; hi,&#010;&gt;&gt;  @Jun Zhang 我一直使用的就是blink planner，这个jar包一直都有的。&#010;&gt;&gt;&#010;&gt;&gt;  @Jark Wu 我是在本地idea中直接运行的，还没有打包到集群跑。跟这个有关系么？&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-07 15:40:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;Hi,&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;Jark&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Tue, 7 Jul 2020 at 15:31, Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#010;&gt;&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; hi.sunfulin&#010;&gt;&gt; &gt;&gt; 你有没有导入blink的planner呢，加入这个试试&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &lt;dependency&gt;&#010;&gt;&gt; &gt;&gt;     &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;  &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#010;&gt;&gt; &gt;&gt;     &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &gt;&gt; &lt;/dependency&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月7日周二 下午3:21写道：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; hi, jark&#010;&gt;&gt; &gt;&gt;&gt; 我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#010;&gt;&gt; &gt;&gt;&gt; configuration里的DeployOptions.TARGET&#010;&gt;&gt; &gt;&gt;&gt; （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; //构建StreamExecutionEnvironment&#010;&gt;&gt; &gt;&gt;&gt; public static final StreamExecutionEnvironment env =&#010;&gt;&gt; &gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; //构建EnvironmentSettings 并指定Blink Planner&#010;&gt;&gt; &gt;&gt;&gt; private static final EnvironmentSettings bsSettings =&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; //构建StreamTableEnvironment&#010;&gt;&gt; &gt;&gt;&gt; public static final StreamTableEnvironment tEnv =&#010;&gt;&gt; &gt;&gt;&gt; StreamTableEnvironment.create(env, bsSettings);&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;        tEnv.executeSql(“ddl sql”);&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;         //source注册成表&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;         tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#010;&gt;&gt; &gt;&gt;&gt; $(\"f1\").as(\"first\"), $(\"p\").proctime());&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;         //join语句&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;         Table table = tEnv.sqlQuery(\"select b.* from test a left join&#010;&gt;&gt; &gt;&gt;&gt; my_dim FOR SYSTEM_TIME AS OF a.p AS b on a.first = b.userId\");&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;         //输出&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;         tEnv.toAppendStream(table, Row.class).print(\"LookUpJoinJob\");&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;         env.execute(\"LookUpJoinJob\");&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; 在 2020-07-06 14:59:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&gt;&gt; &gt;能分享下复现的作业代码不？&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt;&gt; &gt;Best,&#010;&gt;&gt; &gt;&gt;&gt; &gt;Jark&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt;&gt; &gt;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt;&#010;wrote:&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi，&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt; org.apache.flink.table.api.TableExecution: Failed to execute&#010;sql&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory&#010;&gt;&gt; found&#010;&gt;&gt; &gt;&gt;&gt; to&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt; execute the application.&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt;   at&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#010;&gt;&gt; &gt;&gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "7",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAA8tFvviQsPqO_Piu_gkLz0uNCsxnji55UxnOtimZ9O+jMb3Qw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 07 Jul 2020 11:35:07 GMT",
        "subject": "Re: Re: Re: Re: flink 1.11 作业执行异常",
        "content": "Hi&#013;&#010;&#013;&#010;从这个报错看上去是尝试通过 serviceLoader 加载一些 factory 的时候出错了（找不到），可以看看对应的&#010;module 的&#013;&#010;resources 文件下是否有对应的 resource 文件&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月7日周二 下午6:29写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi,&#013;&#010;&gt; 我的pom文件本地执行时，scope的provided都是去掉的。&#013;&#010;&gt; &lt;dependency&gt;&#013;&#010;&gt;     &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;&#013;&#010;&gt;  &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt;    &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &lt;/dependency&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 确实比较诡异。org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt; 这个异常在啥情况下会触发到。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-07 18:10:58，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;如果是在 IDEA 中运行的话，你看看 blink planner 这个依赖的 scope&#010;是不是被 provided 掉了？ 去掉&#013;&#010;&gt; provided&#013;&#010;&gt; &gt;再试试看？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;On Tue, 7 Jul 2020 at 18:01, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; hi,&#013;&#010;&gt; &gt;&gt;  @Jun Zhang 我一直使用的就是blink planner，这个jar包一直都有的。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;  @Jark Wu 我是在本地idea中直接运行的，还没有打包到集群跑。跟这个有关系么？&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在 2020-07-07 15:40:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt; &gt;Hi,&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;你是作业打包后在集群执行的，还是在 IDEA 中运行的呢？&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;Best,&#013;&#010;&gt; &gt;&gt; &gt;Jark&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;On Tue, 7 Jul 2020 at 15:31, Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;&gt; &gt;&gt; wrote:&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; hi.sunfulin&#013;&#010;&gt; &gt;&gt; &gt;&gt; 你有没有导入blink的planner呢，加入这个试试&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &lt;dependency&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;     &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &lt;artifactId&gt;flink-table-planner-blink_${scala.binary.version}&lt;/artifactId&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;     &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &lt;/dependency&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月7日周二 下午3:21写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; hi, jark&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 我的执行代码其实很简单，就是下面的执行逻辑。不知道是不是我缺了什么依赖配置。我debug看了下异常执行，是说Flink&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; configuration里的DeployOptions.TARGET&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; （execution.target）没有匹配到配置？之前貌似从没有关注过这个配置。&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; //构建StreamExecutionEnvironment&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; public static final StreamExecutionEnvironment env =&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; //构建EnvironmentSettings 并指定Blink Planner&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; private static final EnvironmentSettings bsSettings =&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; //构建StreamTableEnvironment&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; public static final StreamTableEnvironment tEnv =&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; StreamTableEnvironment.create(env, bsSettings);&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;        tEnv.executeSql(“ddl sql”);&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;         //source注册成表&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;         tEnv.createTemporaryView(\"test\", ds, $(\"f0\").as(\"id\"),&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; $(\"f1\").as(\"first\"), $(\"p\").proctime());&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;         //join语句&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;         Table table = tEnv.sqlQuery(\"select b.* from test a left&#013;&#010;&gt; join&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; my_dim FOR SYSTEM_TIME AS OF a.p AS b on a.first = b.userId\");&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;         //输出&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;         tEnv.toAppendStream(table,&#013;&#010;&gt; Row.class).print(\"LookUpJoinJob\");&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;         env.execute(\"LookUpJoinJob\");&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 在 2020-07-06 14:59:17，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;能分享下复现的作业代码不？&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;Best,&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;Jark&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;On Mon, 6 Jul 2020 at 11:00, sunfulin &lt;sunfulin0321@163.com&gt;&#013;&#010;&gt; wrote:&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 我使用目前最新的Flink 1.11 rc4来测试我的作业。报了如下异常：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; org.apache.flink.table.api.TableExecution: Failed to execute&#010;sql&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; caused by : java.lang.IlleagalStateException: No ExecutorFactory&#013;&#010;&gt; &gt;&gt; found&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; to&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; execute the application.&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.core.execution.DefaultExecutorServiceLoader.getExecutorFactory(DefaultExecutorServiceLoader.java:84)&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&gt; 想请教下这个异常是啥原因？我使用1.10.1跑同样的逻辑，是没有异常的。&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<64202b1e.2a0e.173221106a6.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CABMA2Ut_NiZSVO6hv7-SPd19w51As4okNfKe049e0tJ4Rw_fyg@mail.gmail.com>",
        "from": "Jim Chen &lt;chenshuai19950...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 03:28:21 GMT",
        "subject": "flink1.10 定义表时，把json数组声明成STRING类型的，查询出来是空",
        "content": "Hi,&#010;可以通过以下步骤还原车祸现场：&#010;kafka topic: test_action&#010;kafka message:&#010;  {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;\"id002\", \"actionName\": \"bbb\"} ] }&#010;&#010;代码Problem2.java：&#010;package com.flink;&#010;&#010;import&#010;org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#010;import org.apache.flink.table.api.Table;&#010;import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;import org.apache.flink.types.Row;&#010;&#010;/**&#010; *&#010; * 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010; * 那么在eval方法接收到的就是Row[]，&#010; * 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010; *&#010; * 现在思路：就是在定义表的时候，把ARRYA看成STRING，&#010; * 现在的问题，就是查询出来，都是空&#010; *&#010; * kafka topic: test_action&#010; *&#010; * kafka message:&#010; *   {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;\"id002\", \"actionName\": \"bbb\"} ] }&#010; */&#010;public class Problem2 {&#010;&#010;    public static void main(String[] args) throws Exception {&#010;        StreamExecutionEnvironment env =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;        EnvironmentSettings envSettings = EnvironmentSettings.newInstance()&#010;                .useBlinkPlanner()&#010;                .inStreamingMode()&#010;                .build();&#010;        StreamTableEnvironment bsEnv = StreamTableEnvironment.create(env,&#010;envSettings);&#010;        bsEnv.registerFunction(\"explode3\", new ExplodeFunction());&#010;&#010;        String ddlSource = \"CREATE TABLE actionTable3 (\\n\" +&#010;                \"    action STRING\\n\" +&#010;                \") WITH (\\n\" +&#010;                \"    'connector.type' = 'kafka',\\n\" +&#010;                \"    'connector.version' = '0.11',\\n\" +&#010;                \"    'connector.topic' = 'test_action',\\n\" +&#010;                \"    'connector.startup-mode' = 'earliest-offset',\\n\" +&#010;                \"    'connector.properties.zookeeper.connect' =&#010;'localhost:2181',\\n\" +&#010;                \"    'connector.properties.bootstrap.servers' =&#010;'localhost:9092',\\n\" +&#010;                \"    'update-mode' = 'append',\\n\" +&#010;                \"    'format.type' = 'json',\\n\" +&#010;                \"    'format.derive-schema' = 'false',\\n\" +&#010;                \"    'format.json-schema' = '{\\\"type\\\": \\\"object\\\",&#010;\\\"properties\\\": {\\\"action\\\": {\\\"type\\\": \\\"string\\\"} } }'\" +&#010;                \")\";&#010;        System.out.println(ddlSource);&#010;        bsEnv.sqlUpdate(ddlSource);&#010;&#010;        Table table = bsEnv.sqlQuery(\"select * from actionTable3\");&#010;//        Table table = bsEnv.sqlQuery(\"select * from actionTable2, LATERAL&#010;TABLE(explode3(`action`)) as T(`word`)\");&#010;        table.printSchema();&#010;        bsEnv.toAppendStream(table, Row.class)&#010;                .print();// 输出都是空&#010;&#010;        bsEnv.execute(\"ARRAY tableFunction Problem\");&#010;    }&#010;}&#010;&#010;",
        "depth": "0",
        "reply": "<CABMA2Ut_NiZSVO6hv7-SPd19w51As4okNfKe049e0tJ4Rw_fyg@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_R2WZWe8JXvvcz43dwFc8Ay5z2YcKH16sSKb4oKNtZL+w@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 07:34:17 GMT",
        "subject": "Re: flink1.10 定义表时，把json数组声明成STRING类型的，查询出来是空",
        "content": "Hi Jim,&#010;&#010;这是一个已知问题[1]，你可以看下这个issue，是否可以解决你的问题？&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18002&#010;&#010;Jim Chen &lt;chenshuai19950725@gmail.com&gt; 于2020年7月6日周一 上午11:28写道：&#010;&#010;&gt; Hi,&#010;&gt; 可以通过以下步骤还原车祸现场：&#010;&gt; kafka topic: test_action&#010;&gt; kafka message:&#010;&gt;   {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;&gt; \"id002\", \"actionName\": \"bbb\"} ] }&#010;&gt;&#010;&gt; 代码Problem2.java：&#010;&gt; package com.flink;&#010;&gt;&#010;&gt; import&#010;&gt; org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt; import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt; import org.apache.flink.table.api.Table;&#010;&gt; import org.apache.flink.table.api.java.StreamTableEnvironment;&#010;&gt; import org.apache.flink.types.Row;&#010;&gt;&#010;&gt; /**&#010;&gt;  *&#010;&gt;  * 在自定义tableFunction的时候，如果SQL中传入的是ARRAY类型，&#010;&gt;  * 那么在eval方法接收到的就是Row[]，&#010;&gt;  * 问题出在，Row[]中的数据获取不到，里面的元素都是NULL&#010;&gt;  *&#010;&gt;  * 现在思路：就是在定义表的时候，把ARRYA看成STRING，&#010;&gt;  * 现在的问题，就是查询出来，都是空&#010;&gt;  *&#010;&gt;  * kafka topic: test_action&#010;&gt;  *&#010;&gt;  * kafka message:&#010;&gt;  *   {\"action\": [{\"actionID\": \"id001\", \"actionName\": \"aaa\"}, {\"actionID\":&#010;&gt; \"id002\", \"actionName\": \"bbb\"} ] }&#010;&gt;  */&#010;&gt; public class Problem2 {&#010;&gt;&#010;&gt;     public static void main(String[] args) throws Exception {&#010;&gt;         StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;         EnvironmentSettings envSettings = EnvironmentSettings.newInstance()&#010;&gt;                 .useBlinkPlanner()&#010;&gt;                 .inStreamingMode()&#010;&gt;                 .build();&#010;&gt;         StreamTableEnvironment bsEnv = StreamTableEnvironment.create(env,&#010;&gt; envSettings);&#010;&gt;         bsEnv.registerFunction(\"explode3\", new ExplodeFunction());&#010;&gt;&#010;&gt;         String ddlSource = \"CREATE TABLE actionTable3 (\\n\" +&#010;&gt;                 \"    action STRING\\n\" +&#010;&gt;                 \") WITH (\\n\" +&#010;&gt;                 \"    'connector.type' = 'kafka',\\n\" +&#010;&gt;                 \"    'connector.version' = '0.11',\\n\" +&#010;&gt;                 \"    'connector.topic' = 'test_action',\\n\" +&#010;&gt;                 \"    'connector.startup-mode' = 'earliest-offset',\\n\" +&#010;&gt;                 \"    'connector.properties.zookeeper.connect' =&#010;&gt; 'localhost:2181',\\n\" +&#010;&gt;                 \"    'connector.properties.bootstrap.servers' =&#010;&gt; 'localhost:9092',\\n\" +&#010;&gt;                 \"    'update-mode' = 'append',\\n\" +&#010;&gt;                 \"    'format.type' = 'json',\\n\" +&#010;&gt;                 \"    'format.derive-schema' = 'false',\\n\" +&#010;&gt;                 \"    'format.json-schema' = '{\\\"type\\\": \\\"object\\\",&#010;&gt; \\\"properties\\\": {\\\"action\\\": {\\\"type\\\": \\\"string\\\"} } }'\" +&#010;&gt;                 \")\";&#010;&gt;         System.out.println(ddlSource);&#010;&gt;         bsEnv.sqlUpdate(ddlSource);&#010;&gt;&#010;&gt;         Table table = bsEnv.sqlQuery(\"select * from actionTable3\");&#010;&gt; //        Table table = bsEnv.sqlQuery(\"select * from actionTable2, LATERAL&#010;&gt; TABLE(explode3(`action`)) as T(`word`)\");&#010;&gt;         table.printSchema();&#010;&gt;         bsEnv.toAppendStream(table, Row.class)&#010;&gt;                 .print();// 输出都是空&#010;&gt;&#010;&gt;         bsEnv.execute(\"ARRAY tableFunction Problem\");&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "1",
        "reply": "<CABMA2Ut_NiZSVO6hv7-SPd19w51As4okNfKe049e0tJ4Rw_fyg@mail.gmail.com>"
    },
    {
        "id": "<tencent_2720929641F9696206CCAB4443962431E906@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 03:31:15 GMT",
        "subject": "flink sql子查询状态清理不掉",
        "content": "大家好，我现在程序里面有像这样一段sql：&amp;nbsp; select day,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;count(id),&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;sum(v1) from&#013;&#010;(&#013;&#010;select&amp;nbsp;&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; day ,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; id ,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; sum(v1) v1 from source&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; group by day,&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;id&#013;&#010;)t&#013;&#010;&#013;&#010;&#013;&#010;group by day&#013;&#010;&#013;&#010;&#013;&#010;我设置了&#013;&#010;tConfig.setIdleStateRetentionTime(Time.minutes(1440),Time.minutes(1450))&#013;&#010;&#013;&#010;&#013;&#010;子查询里面的聚合是按照天和id聚合的，按道理1天之后就会自动清理，但是运行4天过程中，我在checkpoint的页面里面看到这个子查询的状态一直在增大，这是什么原因呢&#013;&#010;我的版本是1.10.0",
        "depth": "0",
        "reply": "<tencent_2720929641F9696206CCAB4443962431E906@qq.com>"
    },
    {
        "id": "<CABKuJ_RS8Y6WovaX_0ojc+5betV9CHvjG8TuxeG=umF-N1vo6Q@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 06 Jul 2020 15:09:23 GMT",
        "subject": "Re: flink sql子查询状态清理不掉",
        "content": "感觉不太应该。你用的是哪个Flink版本，以及哪个planner呢？&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月6日周一 上午11:31写道：&#013;&#010;&#013;&#010;&gt; 大家好，我现在程序里面有像这样一段sql：&amp;nbsp; select day,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;count(id),&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;sum(v1) from&#013;&#010;&gt; (&#013;&#010;&gt; select&amp;nbsp;&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; day ,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; id ,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; sum(v1) v1 from source&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; group by day,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;id&#013;&#010;&gt; )t&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; group by day&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我设置了&#013;&#010;&gt; tConfig.setIdleStateRetentionTime(Time.minutes(1440),Time.minutes(1450))&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 子查询里面的聚合是按照天和id聚合的，按道理1天之后就会自动清理，但是运行4天过程中，我在checkpoint的页面里面看到这个子查询的状态一直在增大，这是什么原因呢&#013;&#010;&gt; 我的版本是1.10.0&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_2720929641F9696206CCAB4443962431E906@qq.com>"
    }
]