[
    {
        "id": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>",
        "from": "RS &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 09:02:04 GMT",
        "subject": "Could not find any factory for identifier 'kafka'",
        "content": "hi，&#010;Flink-1.11.1 尝试运行SQL DDL 读取kafka的数据，执行create 语句的时候报错了&#010;编译的jar包是jar-with-dependencies的&#010;&#010;&#010;代码片段：&#010;    public String ddlSql = String.format(\"CREATE TABLE %s (\\n\" +&#010;            \"  number BIGINT,\\n\" +&#010;            \"  msg STRING,\\n\" +&#010;            \"  username STRING,\\n\" +&#010;            \"  update_time TIMESTAMP(3)\\n\" +&#010;            \") WITH (\\n\" +&#010;            \" 'connector' = 'kafka',\\n\" +&#010;            \" 'topic' = '%s',\\n\" +&#010;            \" 'properties.bootstrap.servers' = '%s',\\n\" +&#010;            \" 'properties.group.id' = '%s',\\n\" +&#010;            \" 'format' = 'json',\\n\" +&#010;            \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;            \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;            \")\\n\", tableName, topic, servers, group);&#010;&#010;&#010;        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#010;        tableEnv.executeSql(ddlSql);&#010;&#010;&#010;报错信息：&#010;Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for&#010;identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableSourceFactory'&#010;in the classpath.&#010;Available factory identifiers are:&#010;datagen&#010;at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;... 33 more&#010;&#010;&#010;参考了这个 http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-DDL-td4890.html#a4893&#010;补充了flink-connector-kafka_2.12，flink-sql-connector-kafka_2.12， 还是会报一样的错&#010;&#010;&#010;附上pom依赖：&#010;&lt;dependencies&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-java&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;    &lt;/dependencies&gt;&#010;&#010;&#010;感谢各位~",
        "depth": "0",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<CABKuJ_RdkwA6rGkPP=gTEkQU0W_96L+3xxzXVC3YxLRLGUTRmA@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 09:36:46 GMT",
        "subject": "Re: Could not find any factory for identifier 'kafka'",
        "content": "可能跟你的打包方式有关系。你这个程序如果直接在idea里面运行是可以运行的么？&#010;&#010;如果可以在idea运行，但是打出来的jar包不能提交运行的话，很有可能跟SPI文件有关系。&#010;如果你用的是shade plugin，需要看下这个transformer[1]&#010;&#010;[1]&#010;https://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#AppendingTransformer&#010;&#010;RS &lt;tinyshrimp@163.com&gt; 于2020年7月24日周五 下午5:02写道：&#010;&#010;&gt; hi，&#010;&gt; Flink-1.11.1 尝试运行SQL DDL 读取kafka的数据，执行create 语句的时候报错了&#010;&gt; 编译的jar包是jar-with-dependencies的&#010;&gt;&#010;&gt;&#010;&gt; 代码片段：&#010;&gt;     public String ddlSql = String.format(\"CREATE TABLE %s (\\n\" +&#010;&gt;             \"  number BIGINT,\\n\" +&#010;&gt;             \"  msg STRING,\\n\" +&#010;&gt;             \"  username STRING,\\n\" +&#010;&gt;             \"  update_time TIMESTAMP(3)\\n\" +&#010;&gt;             \") WITH (\\n\" +&#010;&gt;             \" 'connector' = 'kafka',\\n\" +&#010;&gt;             \" 'topic' = '%s',\\n\" +&#010;&gt;             \" 'properties.bootstrap.servers' = '%s',\\n\" +&#010;&gt;             \" 'properties.group.id' = '%s',\\n\" +&#010;&gt;             \" 'format' = 'json',\\n\" +&#010;&gt;             \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt;             \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt;             \")\\n\", tableName, topic, servers, group);&#010;&gt;&#010;&gt;&#010;&gt;         StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;         StreamTableEnvironment tableEnv =&#010;&gt; StreamTableEnvironment.create(env);&#010;&gt;         tableEnv.executeSql(ddlSql);&#010;&gt;&#010;&gt;&#010;&gt; 报错信息：&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;&gt; any factory for identifier 'kafka' that implements&#010;&gt; 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the&#010;&gt; classpath.&#010;&gt; Available factory identifiers are:&#010;&gt; datagen&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt; ... 33 more&#010;&gt;&#010;&gt;&#010;&gt; 参考了这个&#010;&gt; http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-DDL-td4890.html#a4893&#010;&gt; 补充了flink-connector-kafka_2.12，flink-sql-connector-kafka_2.12， 还是会报一样的错&#010;&gt;&#010;&gt;&#010;&gt; 附上pom依赖：&#010;&gt; &lt;dependencies&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-java&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;         &lt;dependency&gt;&#010;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;             &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;         &lt;/dependency&gt;&#010;&gt;     &lt;/dependencies&gt;&#010;&gt;&#010;&gt;&#010;&gt; 感谢各位~&#010;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "1",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<199f60d7.5449.173803c76d9.Coremail.tinyshrimp@163.com>",
        "from": "RS &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 09:51:53 GMT",
        "subject": "Re:Re: Could not find any factory for identifier 'kafka'",
        "content": "我这边是直接打成jar包扔到服务器上运行的(bin/flink run xxx)，没有在IDEA运行过。&lt;br/&gt;maven编译没配置shade-plugin，maven&#010;build参数如下：&lt;br/&gt;    &amp;lt;properties&amp;gt;&lt;br/&gt;        &amp;lt;jdk.version&amp;gt;1.8&amp;lt;/jdk.version&amp;gt;&lt;br/&gt;&#010;       &amp;lt;flink.version&amp;gt;1.11.1&amp;lt;/flink.version&amp;gt;&lt;br/&gt;    &amp;lt;/properties&amp;gt;&lt;br/&gt;&#010;   &amp;lt;build&amp;gt;&lt;br/&gt;        &amp;lt;plugins&amp;gt;&lt;br/&gt;            &amp;lt;plugin&amp;gt;&lt;br/&gt;&#010;               &amp;lt;artifactId&amp;gt;maven-compiler-plugin&amp;lt;/artifactId&amp;gt;&lt;br/&gt;&#010;               &amp;lt;configuration&amp;gt;&lt;br/&gt;                    &amp;lt;source&amp;gt;${jdk.version}&amp;lt;/source&amp;gt;&lt;br/&gt;&#010;                   &amp;lt;target&amp;gt;${jdk.version}&amp;lt;/target&amp;gt;&lt;br/&gt;&#010;               &amp;lt;/configuration&amp;gt;&lt;br/&gt;            &amp;lt;/plugin&amp;gt;&lt;br/&gt;&#010;           &amp;lt;plugin&amp;gt;&lt;br/&gt;                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;&lt;br/&gt;&#010;               &amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;&lt;br/&gt;&#010;               &amp;lt;executions&amp;gt;&lt;br/&gt;                    &amp;lt;execution&amp;gt;&lt;br/&gt;&#010;                       &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;&lt;br/&gt;     &#010;                  &amp;lt;goals&amp;gt;&lt;br/&gt;                            &amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;&lt;br/&gt;&#010;                       &amp;lt;/goals&amp;gt;&lt;br/&gt;                    &amp;lt;/execution&amp;gt;&lt;br/&gt;&#010;               &amp;lt;/executions&amp;gt;&lt;br/&gt;                &amp;lt;configuration&amp;gt;&lt;br/&gt;&#010;                   &amp;lt;descriptorRefs&amp;gt;&lt;br/&gt;                        &amp;lt;descriptorRef&amp;gt;jar-with-dependencies&amp;lt;/descriptorRef&amp;gt;&lt;br/&gt;&#010;                   &amp;lt;/descriptorRefs&amp;gt;&lt;br/&gt;                &amp;lt;/configuration&amp;gt;&lt;br/&gt;&#010;           &amp;lt;/plugin&amp;gt;&lt;br/&gt;        &amp;lt;/plugins&amp;gt;&lt;br/&gt; &#010;  &amp;lt;/build&amp;gt;&lt;br/&gt;&lt;br/&gt;thx&#010;在 2020-07-24 17:36:46，\"Benchao Li\" &lt;libenchao@apache.org&gt; 写道：&#010;&gt;可能跟你的打包方式有关系。你这个程序如果直接在idea里面运行是可以运行的么？&#013;&#010;&gt;&#013;&#010;&gt;如果可以在idea运行，但是打出来的jar包不能提交运行的话，很有可能跟SPI文件有关系。&#013;&#010;&gt;如果你用的是shade plugin，需要看下这个transformer[1]&#013;&#010;&gt;&#013;&#010;&gt;[1]&#013;&#010;&gt;https://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#AppendingTransformer&#013;&#010;&gt;&#013;&#010;&gt;RS &lt;tinyshrimp@163.com&gt; 于2020年7月24日周五 下午5:02写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; hi，&#013;&#010;&gt;&gt; Flink-1.11.1 尝试运行SQL DDL 读取kafka的数据，执行create 语句的时候报错了&#013;&#010;&gt;&gt; 编译的jar包是jar-with-dependencies的&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 代码片段：&#013;&#010;&gt;&gt;     public String ddlSql = String.format(\"CREATE TABLE %s (\\n\" +&#013;&#010;&gt;&gt;             \"  number BIGINT,\\n\" +&#013;&#010;&gt;&gt;             \"  msg STRING,\\n\" +&#013;&#010;&gt;&gt;             \"  username STRING,\\n\" +&#013;&#010;&gt;&gt;             \"  update_time TIMESTAMP(3)\\n\" +&#013;&#010;&gt;&gt;             \") WITH (\\n\" +&#013;&#010;&gt;&gt;             \" 'connector' = 'kafka',\\n\" +&#013;&#010;&gt;&gt;             \" 'topic' = '%s',\\n\" +&#013;&#010;&gt;&gt;             \" 'properties.bootstrap.servers' = '%s',\\n\" +&#013;&#010;&gt;&gt;             \" 'properties.group.id' = '%s',\\n\" +&#013;&#010;&gt;&gt;             \" 'format' = 'json',\\n\" +&#013;&#010;&gt;&gt;             \" 'json.fail-on-missing-field' = 'false',\\n\" +&#013;&#010;&gt;&gt;             \" 'json.ignore-parse-errors' = 'true'\\n\" +&#013;&#010;&gt;&gt;             \")\\n\", tableName, topic, servers, group);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;         StreamExecutionEnvironment env =&#013;&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt;&gt;         StreamTableEnvironment tableEnv =&#013;&#010;&gt;&gt; StreamTableEnvironment.create(env);&#013;&#010;&gt;&gt;         tableEnv.executeSql(ddlSql);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 报错信息：&#013;&#010;&gt;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find&#013;&#010;&gt;&gt; any factory for identifier 'kafka' that implements&#013;&#010;&gt;&gt; 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the&#013;&#010;&gt;&gt; classpath.&#013;&#010;&gt;&gt; Available factory identifiers are:&#013;&#010;&gt;&gt; datagen&#013;&#010;&gt;&gt; at&#013;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#013;&#010;&gt;&gt; at&#013;&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#013;&#010;&gt;&gt; ... 33 more&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 参考了这个&#013;&#010;&gt;&gt; http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-DDL-td4890.html#a4893&#013;&#010;&gt;&gt; 补充了flink-connector-kafka_2.12，flink-sql-connector-kafka_2.12， 还是会报一样的错&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 附上pom依赖：&#013;&#010;&gt;&gt; &lt;dependencies&gt;&#013;&#010;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;&gt;             &lt;artifactId&gt;flink-java&lt;/artifactId&gt;&#013;&#010;&gt;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt;&gt;         &lt;/dependency&gt;&#013;&#010;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;&gt;             &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;&#013;&#010;&gt;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt;&gt;         &lt;/dependency&gt;&#013;&#010;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;&gt;             &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;&#013;&#010;&gt;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt;&gt;         &lt;/dependency&gt;&#013;&#010;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;&gt;             &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#013;&#010;&gt;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt;&gt;         &lt;/dependency&gt;&#013;&#010;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;&gt;             &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#013;&#010;&gt;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt;&gt;         &lt;/dependency&gt;&#013;&#010;&gt;&gt;         &lt;dependency&gt;&#013;&#010;&gt;&gt;             &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt;&gt;             &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#013;&#010;&gt;&gt;             &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt;&gt;         &lt;/dependency&gt;&#013;&#010;&gt;&gt;     &lt;/dependencies&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 感谢各位~&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;-- &#013;&#010;&gt;&#013;&#010;&gt;Best,&#013;&#010;&gt;Benchao Li&#013;&#010;",
        "depth": "2",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<704f188b.556a.1738045dd84.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 10:02:09 GMT",
        "subject": "Re:Re:Re: Could not find any factory for identifier 'kafka'",
        "content": "邮件格式不对，我重新回复下&#010;&#010;&#010;我这边是直接打成jar包扔到服务器上运行的，没有在IDEA运行过。&#010;&#010;&gt; flink run xxx&#010;&#010;没有使用shade-plugin&#010;&#010;maven build参数：&#010;    &lt;properties&gt;&#010;        &lt;jdk.version&gt;1.8&lt;/jdk.version&gt;&#010;        &lt;flink.version&gt;1.11.1&lt;/flink.version&gt;&#010;    &lt;/properties&gt;&#010;&#010;&#010;&#010;    &lt;build&gt;&#010;&#010;        &lt;plugins&gt;&#010;&#010;            &lt;plugin&gt;&#010;&#010;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;&#010;&#010;                &lt;configuration&gt;&#010;&#010;                    &lt;source&gt;${jdk.version}&lt;/source&gt;&#010;&#010;                    &lt;target&gt;${jdk.version}&lt;/target&gt;&#010;&#010;                &lt;/configuration&gt;&#010;&#010;            &lt;/plugin&gt;&#010;&#010;            &lt;plugin&gt;&#010;&#010;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;&#010;&#010;                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;&#010;&#010;                &lt;executions&gt;&#010;&#010;                    &lt;execution&gt;&#010;&#010;                        &lt;phase&gt;package&lt;/phase&gt;&#010;&#010;                        &lt;goals&gt;&#010;&#010;                            &lt;goal&gt;single&lt;/goal&gt;&#010;&#010;                        &lt;/goals&gt;&#010;&#010;                    &lt;/execution&gt;&#010;&#010;                &lt;/executions&gt;&#010;&#010;                &lt;configuration&gt;&#010;&#010;                    &lt;descriptorRefs&gt;&#010;&#010;                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;&#010;&#010;                    &lt;/descriptorRefs&gt;&#010;&#010;                &lt;/configuration&gt;&#010;&#010;            &lt;/plugin&gt;&#010;&#010;        &lt;/plugins&gt;&#010;&#010;    &lt;/build&gt;&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<46C1A69C-63EE-47D2-B30F-A11638EC63D5@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 11:34:10 GMT",
        "subject": "Re: Could not find any factory for identifier 'kafka'",
        "content": "  &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;&#010;这两个会有冲突，去掉上面那个&#010;&#010;&gt; 2020年7月24日 下午5:02，RS &lt;tinyshrimp@163.com&gt; 写道：&#010;&gt; &#010;&gt;   &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;            &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;        &lt;/dependency&gt;&#010;&gt;        &lt;dependency&gt;&#010;&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;            &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;            &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;        &lt;/dependency&gt;&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<22a3092f.e0b4.17380c0af70.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 12:16:18 GMT",
        "subject": "Re: Could not find any factory for identifier 'kafka'",
        "content": "hi&#010;只需要-sql和-json两个包就可以了&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;On 07/24/2020 17:02, RS wrote:&#010;hi，&#010;Flink-1.11.1 尝试运行SQL DDL 读取kafka的数据，执行create 语句的时候报错了&#010;编译的jar包是jar-with-dependencies的&#010;&#010;&#010;代码片段：&#010;   public String ddlSql = String.format(\"CREATE TABLE %s (\\n\" +&#010;           \"  number BIGINT,\\n\" +&#010;           \"  msg STRING,\\n\" +&#010;           \"  username STRING,\\n\" +&#010;           \"  update_time TIMESTAMP(3)\\n\" +&#010;           \") WITH (\\n\" +&#010;           \" 'connector' = 'kafka',\\n\" +&#010;           \" 'topic' = '%s',\\n\" +&#010;           \" 'properties.bootstrap.servers' = '%s',\\n\" +&#010;           \" 'properties.group.id' = '%s',\\n\" +&#010;           \" 'format' = 'json',\\n\" +&#010;           \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;           \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;           \")\\n\", tableName, topic, servers, group);&#010;&#010;&#010;       StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;       StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#010;       tableEnv.executeSql(ddlSql);&#010;&#010;&#010;报错信息：&#010;Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory for&#010;identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableSourceFactory'&#010;in the classpath.&#010;Available factory identifiers are:&#010;datagen&#010;at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;... 33 more&#010;&#010;&#010;参考了这个 http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-DDL-td4890.html#a4893&#010;补充了flink-connector-kafka_2.12，flink-sql-connector-kafka_2.12， 还是会报一样的错&#010;&#010;&#010;附上pom依赖：&#010;&lt;dependencies&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-java&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;       &lt;dependency&gt;&#010;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;           &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;       &lt;/dependency&gt;&#010;   &lt;/dependencies&gt;&#010;&#010;&#010;感谢各位~",
        "depth": "1",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<41a2c197.6055.17380cd679c.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 12:30:11 GMT",
        "subject": "Re:Re: Could not find any factory for identifier 'kafka'",
        "content": "hi，&#010;感谢回复，尝试了多次之后，发现应该不是依赖包的问题&#010;&#010;&#010;我项目中新增目录：resources/META-INF/services&#010;然后从Flink源码中复制了2个文件 org.apache.flink.table.factories.TableFactory和org.apache.flink.table.factories.Factory&#010;这样编译就不会报错了，原理不太清楚，但是确实解决了报错的问题。&#010;&#010;&#010;在 2020-07-24 20:16:18，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#010;&gt;hi&#010;&gt;只需要-sql和-json两个包就可以了&#010;&gt;&#010;&gt;&#010;&gt;| |&#010;&gt;JasonLee&#010;&gt;|&#010;&gt;|&#010;&gt;邮箱：17610775726@163.com&#010;&gt;|&#010;&gt;&#010;&gt;Signature is customized by Netease Mail Master&#010;&gt;&#010;&gt;On 07/24/2020 17:02, RS wrote:&#010;&gt;hi，&#010;&gt;Flink-1.11.1 尝试运行SQL DDL 读取kafka的数据，执行create 语句的时候报错了&#010;&gt;编译的jar包是jar-with-dependencies的&#010;&gt;&#010;&gt;&#010;&gt;代码片段：&#010;&gt;   public String ddlSql = String.format(\"CREATE TABLE %s (\\n\" +&#010;&gt;           \"  number BIGINT,\\n\" +&#010;&gt;           \"  msg STRING,\\n\" +&#010;&gt;           \"  username STRING,\\n\" +&#010;&gt;           \"  update_time TIMESTAMP(3)\\n\" +&#010;&gt;           \") WITH (\\n\" +&#010;&gt;           \" 'connector' = 'kafka',\\n\" +&#010;&gt;           \" 'topic' = '%s',\\n\" +&#010;&gt;           \" 'properties.bootstrap.servers' = '%s',\\n\" +&#010;&gt;           \" 'properties.group.id' = '%s',\\n\" +&#010;&gt;           \" 'format' = 'json',\\n\" +&#010;&gt;           \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt;           \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt;           \")\\n\", tableName, topic, servers, group);&#010;&gt;&#010;&gt;&#010;&gt;       StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;       StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#010;&gt;       tableEnv.executeSql(ddlSql);&#010;&gt;&#010;&gt;&#010;&gt;报错信息：&#010;&gt;Caused by: org.apache.flink.table.api.ValidationException: Could not find any factory&#010;for identifier 'kafka' that implements 'org.apache.flink.table.factories.DynamicTableSourceFactory'&#010;in the classpath.&#010;&gt;Available factory identifiers are:&#010;&gt;datagen&#010;&gt;at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt;at org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt;... 33 more&#010;&gt;&#010;&gt;&#010;&gt;参考了这个 http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-DDL-td4890.html#a4893&#010;&gt;补充了flink-connector-kafka_2.12，flink-sql-connector-kafka_2.12， 还是会报一样的错&#010;&gt;&#010;&gt;&#010;&gt;附上pom依赖：&#010;&gt;&lt;dependencies&gt;&#010;&gt;       &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-java&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;       &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;       &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;       &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;       &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;       &lt;dependency&gt;&#010;&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;           &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;       &lt;/dependency&gt;&#010;&gt;   &lt;/dependencies&gt;&#010;&gt;&#010;&gt;&#010;&gt;感谢各位~&#010;",
        "depth": "2",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<CAHjjfOSG=PBsZwL-=LpLjG2mXvSrpNiECjifGy=1eQ0D2EY_jQ@mail.gmail.com>",
        "from": "Caizhi Weng &lt;tsreape...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 03:42:50 GMT",
        "subject": "Re: Re: Could not find any factory for identifier 'kafka'",
        "content": "Hi,&#013;&#010;&#013;&#010;Flink 的 TableFactory 利用了 Java 的服务发现功能，所以需要这两个文件。需要确认&#010;jar-with-dependencies&#013;&#010;是否能把这些资源文件打进去。&#013;&#010;&#013;&#010;另外为什么需要把 Flink 的依赖也打在大包里呢？因为 Flink 本身的 classpath&#010;里就已经有这些依赖了，这个大包作为 Flink&#013;&#010;的用户 jar 的话，并不需要把 Flink 的依赖也放进去。&#013;&#010;&#013;&#010;RS &lt;tinyshrimp@163.com&gt; 于2020年7月24日周五 下午8:30写道：&#013;&#010;&#013;&#010;&gt; hi，&#013;&#010;&gt; 感谢回复，尝试了多次之后，发现应该不是依赖包的问题&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我项目中新增目录：resources/META-INF/services&#013;&#010;&gt; 然后从Flink源码中复制了2个文件&#013;&#010;&gt; org.apache.flink.table.factories.TableFactory和org.apache.flink.table.factories.Factory&#013;&#010;&gt; 这样编译就不会报错了，原理不太清楚，但是确实解决了报错的问题。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-24 20:16:18，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#013;&#010;&gt; &gt;hi&#013;&#010;&gt; &gt;只需要-sql和-json两个包就可以了&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;| |&#013;&#010;&gt; &gt;JasonLee&#013;&#010;&gt; &gt;|&#013;&#010;&gt; &gt;|&#013;&#010;&gt; &gt;邮箱：17610775726@163.com&#013;&#010;&gt; &gt;|&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Signature is customized by Netease Mail Master&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;On 07/24/2020 17:02, RS wrote:&#013;&#010;&gt; &gt;hi，&#013;&#010;&gt; &gt;Flink-1.11.1 尝试运行SQL DDL 读取kafka的数据，执行create 语句的时候报错了&#013;&#010;&gt; &gt;编译的jar包是jar-with-dependencies的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;代码片段：&#013;&#010;&gt; &gt;   public String ddlSql = String.format(\"CREATE TABLE %s (\\n\" +&#013;&#010;&gt; &gt;           \"  number BIGINT,\\n\" +&#013;&#010;&gt; &gt;           \"  msg STRING,\\n\" +&#013;&#010;&gt; &gt;           \"  username STRING,\\n\" +&#013;&#010;&gt; &gt;           \"  update_time TIMESTAMP(3)\\n\" +&#013;&#010;&gt; &gt;           \") WITH (\\n\" +&#013;&#010;&gt; &gt;           \" 'connector' = 'kafka',\\n\" +&#013;&#010;&gt; &gt;           \" 'topic' = '%s',\\n\" +&#013;&#010;&gt; &gt;           \" 'properties.bootstrap.servers' = '%s',\\n\" +&#013;&#010;&gt; &gt;           \" 'properties.group.id' = '%s',\\n\" +&#013;&#010;&gt; &gt;           \" 'format' = 'json',\\n\" +&#013;&#010;&gt; &gt;           \" 'json.fail-on-missing-field' = 'false',\\n\" +&#013;&#010;&gt; &gt;           \" 'json.ignore-parse-errors' = 'true'\\n\" +&#013;&#010;&gt; &gt;           \")\\n\", tableName, topic, servers, group);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       StreamExecutionEnvironment env =&#013;&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; &gt;       StreamTableEnvironment tableEnv =&#013;&#010;&gt; StreamTableEnvironment.create(env);&#013;&#010;&gt; &gt;       tableEnv.executeSql(ddlSql);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;报错信息：&#013;&#010;&gt; &gt;Caused by: org.apache.flink.table.api.ValidationException: Could not find&#013;&#010;&gt; any factory for identifier 'kafka' that implements&#013;&#010;&gt; 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the&#013;&#010;&gt; classpath.&#013;&#010;&gt; &gt;Available factory identifiers are:&#013;&#010;&gt; &gt;datagen&#013;&#010;&gt; &gt;at&#013;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#013;&#010;&gt; &gt;at&#013;&#010;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#013;&#010;&gt; &gt;... 33 more&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;参考了这个&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-DDL-td4890.html#a4893&#013;&#010;&gt; &gt;补充了flink-connector-kafka_2.12，flink-sql-connector-kafka_2.12， 还是会报一样的错&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;附上pom依赖：&#013;&#010;&gt; &gt;&lt;dependencies&gt;&#013;&#010;&gt; &gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;           &lt;artifactId&gt;flink-java&lt;/artifactId&gt;&#013;&#010;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;           &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;&#013;&#010;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;           &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;&#013;&#010;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;           &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#013;&#010;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;           &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#013;&#010;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;       &lt;dependency&gt;&#013;&#010;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#013;&#010;&gt; &gt;           &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#013;&#010;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#013;&#010;&gt; &gt;       &lt;/dependency&gt;&#013;&#010;&gt; &gt;   &lt;/dependencies&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;感谢各位~&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<33bd01bf.506a.1738f5ebbc0.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 08:23:37 GMT",
        "subject": "Re:Re: Re: Could not find any factory for identifier 'kafka'",
        "content": "Hi,&#010;1. 好的，学习了&#010;2. 确实，部分Flink依赖调整为provided，打包测试也可以正常执行，但是flink-walkthrough-common_2.11这种包在Flink的lib中没有看到，还是打包进去了&#010;&#010;&#010;&#010;&#010;在 2020-07-27 11:42:50，\"Caizhi Weng\" &lt;tsreaper96@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;Flink 的 TableFactory 利用了 Java 的服务发现功能，所以需要这两个文件。需要确认&#010;jar-with-dependencies&#010;&gt;是否能把这些资源文件打进去。&#010;&gt;&#010;&gt;另外为什么需要把 Flink 的依赖也打在大包里呢？因为 Flink 本身的&#010;classpath 里就已经有这些依赖了，这个大包作为 Flink&#010;&gt;的用户 jar 的话，并不需要把 Flink 的依赖也放进去。&#010;&gt;&#010;&gt;RS &lt;tinyshrimp@163.com&gt; 于2020年7月24日周五 下午8:30写道：&#010;&gt;&#010;&gt;&gt; hi，&#010;&gt;&gt; 感谢回复，尝试了多次之后，发现应该不是依赖包的问题&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 我项目中新增目录：resources/META-INF/services&#010;&gt;&gt; 然后从Flink源码中复制了2个文件&#010;&gt;&gt; org.apache.flink.table.factories.TableFactory和org.apache.flink.table.factories.Factory&#010;&gt;&gt; 这样编译就不会报错了，原理不太清楚，但是确实解决了报错的问题。&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-24 20:16:18，\"JasonLee\" &lt;17610775726@163.com&gt; 写道：&#010;&gt;&gt; &gt;hi&#010;&gt;&gt; &gt;只需要-sql和-json两个包就可以了&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;| |&#010;&gt;&gt; &gt;JasonLee&#010;&gt;&gt; &gt;|&#010;&gt;&gt; &gt;|&#010;&gt;&gt; &gt;邮箱：17610775726@163.com&#010;&gt;&gt; &gt;|&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Signature is customized by Netease Mail Master&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On 07/24/2020 17:02, RS wrote:&#010;&gt;&gt; &gt;hi，&#010;&gt;&gt; &gt;Flink-1.11.1 尝试运行SQL DDL 读取kafka的数据，执行create 语句的时候报错了&#010;&gt;&gt; &gt;编译的jar包是jar-with-dependencies的&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;代码片段：&#010;&gt;&gt; &gt;   public String ddlSql = String.format(\"CREATE TABLE %s (\\n\" +&#010;&gt;&gt; &gt;           \"  number BIGINT,\\n\" +&#010;&gt;&gt; &gt;           \"  msg STRING,\\n\" +&#010;&gt;&gt; &gt;           \"  username STRING,\\n\" +&#010;&gt;&gt; &gt;           \"  update_time TIMESTAMP(3)\\n\" +&#010;&gt;&gt; &gt;           \") WITH (\\n\" +&#010;&gt;&gt; &gt;           \" 'connector' = 'kafka',\\n\" +&#010;&gt;&gt; &gt;           \" 'topic' = '%s',\\n\" +&#010;&gt;&gt; &gt;           \" 'properties.bootstrap.servers' = '%s',\\n\" +&#010;&gt;&gt; &gt;           \" 'properties.group.id' = '%s',\\n\" +&#010;&gt;&gt; &gt;           \" 'format' = 'json',\\n\" +&#010;&gt;&gt; &gt;           \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt;&gt; &gt;           \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt;&gt; &gt;           \")\\n\", tableName, topic, servers, group);&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;       StreamExecutionEnvironment env =&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt; &gt;       StreamTableEnvironment tableEnv =&#010;&gt;&gt; StreamTableEnvironment.create(env);&#010;&gt;&gt; &gt;       tableEnv.executeSql(ddlSql);&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;报错信息：&#010;&gt;&gt; &gt;Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;&gt;&gt; any factory for identifier 'kafka' that implements&#010;&gt;&gt; 'org.apache.flink.table.factories.DynamicTableSourceFactory' in the&#010;&gt;&gt; classpath.&#010;&gt;&gt; &gt;Available factory identifiers are:&#010;&gt;&gt; &gt;datagen&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:240)&#010;&gt;&gt; &gt;at&#010;&gt;&gt; org.apache.flink.table.factories.FactoryUtil.getDynamicTableFactory(FactoryUtil.java:326)&#010;&gt;&gt; &gt;... 33 more&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;参考了这个&#010;&gt;&gt; http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-DDL-td4890.html#a4893&#010;&gt;&gt; &gt;补充了flink-connector-kafka_2.12，flink-sql-connector-kafka_2.12， 还是会报一样的错&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;附上pom依赖：&#010;&gt;&gt; &gt;&lt;dependencies&gt;&#010;&gt;&gt; &gt;       &lt;dependency&gt;&#010;&gt;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt; &gt;           &lt;artifactId&gt;flink-java&lt;/artifactId&gt;&#010;&gt;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &gt;       &lt;/dependency&gt;&#010;&gt;&gt; &gt;       &lt;dependency&gt;&#010;&gt;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt; &gt;           &lt;artifactId&gt;flink-table-api-java-bridge_2.12&lt;/artifactId&gt;&#010;&gt;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &gt;       &lt;/dependency&gt;&#010;&gt;&gt; &gt;       &lt;dependency&gt;&#010;&gt;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt; &gt;           &lt;artifactId&gt;flink-table-api-java&lt;/artifactId&gt;&#010;&gt;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &gt;       &lt;/dependency&gt;&#010;&gt;&gt; &gt;       &lt;dependency&gt;&#010;&gt;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt; &gt;           &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &gt;       &lt;/dependency&gt;&#010;&gt;&gt; &gt;       &lt;dependency&gt;&#010;&gt;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt; &gt;           &lt;artifactId&gt;flink-sql-connector-kafka_2.12&lt;/artifactId&gt;&#010;&gt;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &gt;       &lt;/dependency&gt;&#010;&gt;&gt; &gt;       &lt;dependency&gt;&#010;&gt;&gt; &gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;&gt;&gt; &gt;           &lt;artifactId&gt;flink-json&lt;/artifactId&gt;&#010;&gt;&gt; &gt;           &lt;version&gt;${flink.version}&lt;/version&gt;&#010;&gt;&gt; &gt;       &lt;/dependency&gt;&#010;&gt;&gt; &gt;   &lt;/dependencies&gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;感谢各位~&#010;&gt;&gt;&#010;",
        "depth": "4",
        "reply": "<22d88185.4cec.173800edcd4.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 10:18:57 GMT",
        "subject": "flink1.11查询结果每秒入库到mysql数量很少",
        "content": "各位大佬好，请教一个问题，在使用flink1.11消费kafka数据，查询结果写入到mysql库表时，发现读取kafka的速度很快（300条/秒），但是查询结果每秒写入mysql的条数只有6条左右，请问这是怎么回事，以及优化的点在哪里？下面是我的代码。&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;source_Kafka = \"\"\"&#013;&#010;CREATE TABLE kafka_source (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'test',&amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '*',&#013;&#010;&amp;nbsp;'properties.group.id' = 'flink_grouper',&#013;&#010;&amp;nbsp;'scan.startup.mode' = 'earliest-offset',&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'format' = 'json',&#013;&#010;&amp;nbsp;'json.fail-on-missing-field' = 'false',&#013;&#010;&amp;nbsp;'json.ignore-parse-errors' = 'true'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;source_W_detail_ddl = \"\"\"&#013;&#010;CREATE TABLE source_W_detail (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://198.2.2.71:3306/bda?useSSL=false',&#013;&#010;&amp;nbsp;'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&amp;nbsp;'table-name' = 'detail',&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.max-rows' = '1000',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '2s'&#013;&#010;\"\"\"&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;t_env.execute_sql(source_Kafka)&#013;&#010;t_env.execute_sql(source_W_detail_ddl)&#013;&#010;table_result1=t_env.execute_sql('''insert into source_W_detail select id,alarm_id,trck_id&#010;from kafka_source''')&#013;&#010;table_result1.get_job_client().get_job_execution_result().result()",
        "depth": "0",
        "reply": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>"
    },
    {
        "id": "<1595641882929-0.post@n8.nabble.com>",
        "from": "WeiXubin &lt;18925434...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 01:51:22 GMT",
        "subject": "Re: flink1.11查询结果每秒入库到mysql数量很少",
        "content": "Hi,&#010;你可以尝试改写url，加上rewritebatchedstatements=true，如下：&#010;jdbc:mysql://198.2.2.71:3306/bda?useSSL=false&amp;rewritebatchedstatements=true&#010;&#010;MySQL&#010;Jdbc驱动在默认情况下会无视executeBatch()语句，把期望批量执行的一组sql语句拆散，一条一条地发给MySQL数据库，直接造成较低的性能。把rewriteBatchedStatements参数置为true,&#010;驱动才会帮你批量执行SQL。&#010;&#010;祝好&#010;weixubin&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>"
    },
    {
        "id": "<tencent_D41EE88A329ECC35407645B655DD170AC105@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 02:20:35 GMT",
        "subject": "Re: flink1.11查询结果每秒入库到mysql数量很少",
        "content": "您好，谢谢您的解答，但是我测试了按您这个方式添加以后，每秒入mysql数据量变成了8条左右，提升还达不到需要。",
        "depth": "2",
        "reply": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>"
    },
    {
        "id": "<6c28f43e.1067.17383d9667a.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 02:42:09 GMT",
        "subject": "Re:Re: flink1.11查询结果每秒入库到mysql数量很少",
        "content": "你看下INERT SQL的执行时长，看下是不是MySQL那边的瓶颈？比如写入的数据较大，索引创建比较慢等其他问题？&#010;&#010;或者你手动模拟执行下SQL写数据对比下速度？&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-25 10:20:35，\"小学生\" &lt;201782053@qq.com&gt; 写道：&#010;&gt;您好，谢谢您的解答，但是我测试了按您这个方式添加以后，每秒入mysql数据量变成了8条左右，提升还达不到需要。&#010;",
        "depth": "3",
        "reply": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>"
    },
    {
        "id": "<tencent_B351F653574EF15CAE42B23BB226EAD25008@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 02:53:23 GMT",
        "subject": "Re:Re: flink1.11查询结果每秒入库到mysql数量很少",
        "content": "额，这个我对比过了，相同的数据自己写的Python程序去插入的话，每秒写入mysql的记录有150左右；mysql应该没有瓶颈的，再者这个里面没加索引",
        "depth": "4",
        "reply": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>"
    },
    {
        "id": "<tencent_232982A7919584D329F5F3D6729663995D08@qq.com>",
        "from": "&quot;chengyanan1008@foxmail.com&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 06:10:22 GMT",
        "subject": "回复: flink1.11查询结果每秒入库到mysql数量很少",
        "content": "Hello：&#013;&#010;    我拿你的代码做测试，也是flink1.11.0版本，除了字段修改了一下（要和我的kafka数据对应），其他没有任何变动，速度还是比较快的&#013;&#010;    代码全部都是你的代码，一开始mysql没有数据是因为你的“sink.buffer-flush.max-rows”设置为1000了，&#013;&#010;    大约过了几秒之后，mysql里直接就写进去1000条记录，所以你的代码是没有问题的&#013;&#010;    建议你看一下应用日志或者mysql&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;chengyanan1008@foxmail.com&#013;&#010; &#013;&#010;发件人： 小学生&#013;&#010;发送时间： 2020-07-24 18:18&#013;&#010;收件人： user-zh&#013;&#010;主题： flink1.11查询结果每秒入库到mysql数量很少&#013;&#010;各位大佬好，请教一个问题，在使用flink1.11消费kafka数据，查询结果写入到mysql库表时，发现读取kafka的速度很快（300条/秒），但是查询结果每秒写入mysql的条数只有6条左右，请问这是怎么回事，以及优化的点在哪里？下面是我的代码。&#013;&#010; &#013;&#010; &#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;source_Kafka = \"\"\"&#013;&#010;CREATE TABLE kafka_source (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&#013;&#010;&amp;nbsp;trck_id VARCHAR&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'test',&amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '*',&#013;&#010;&amp;nbsp;'properties.group.id' = 'flink_grouper',&#013;&#010;&amp;nbsp;'scan.startup.mode' = 'earliest-offset',&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'format' = 'json',&#013;&#010;&amp;nbsp;'json.fail-on-missing-field' = 'false',&#013;&#010;&amp;nbsp;'json.ignore-parse-errors' = 'true'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;source_W_detail_ddl = \"\"\"&#013;&#010;CREATE TABLE source_W_detail (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://198.2.2.71:3306/bda?useSSL=false',&#013;&#010;&amp;nbsp;'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&amp;nbsp;'table-name' = 'detail',&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.max-rows' = '1000',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '2s'&#013;&#010;\"\"\"&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.ProcessingTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;t_env.execute_sql(source_Kafka)&#013;&#010;t_env.execute_sql(source_W_detail_ddl)&#013;&#010;table_result1=t_env.execute_sql('''insert into source_W_detail select id,alarm_id,trck_id&#010;from kafka_source''')&#013;&#010;table_result1.get_job_client().get_job_execution_result().result()&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>"
    },
    {
        "id": "<1595677902603-0.post@n8.nabble.com>",
        "from": "咿咿呀呀 &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 11:51:42 GMT",
        "subject": "Re: 回复: flink1.11查询结果每秒入库到mysql数量很少",
        "content": "您好，您讲的应用日志或者mysql 这个具体怎么排查呢，谢谢&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "1",
        "reply": "<tencent_36571AF1B3AE2CB2E6B298EAA05072F69308@qq.com>"
    },
    {
        "id": "<tencent_3D9EAF763AB7CD8DF0D9E69F1B92DE08B507@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 26 Jul 2020 13:58:10 GMT",
        "subject": "【Flink sql 1.10.0问题】",
        "content": "Hi,all:&#013;&#010;&#013;&#010;Flink 1.10.0&amp;nbsp; sql提交报错如下，请问是什么原因呢?&#013;&#010;谢谢.",
        "depth": "0",
        "reply": "<tencent_3D9EAF763AB7CD8DF0D9E69F1B92DE08B507@qq.com>"
    },
    {
        "id": "<0da09178-5155-482d-9db3-faf74b4b27e4.zhengbinbin@heint.cn>",
        "from": "&quot;郑斌斌&quot; &lt;zhengbin...@heint.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 03:33:31 GMT",
        "subject": "flink 聚合 job 重启问题",
        "content": "hi all :&#010;&#010;     请教个问题，我通过程序拉取kafka消息后，注册为flink流表。然后执行sql:&#010;\"select user_id, count(*)cnt from 流表\"， 将结果写入到mysql 聚合表中（SINK组件为：flink1.11版本JdbcUpsertTableSink)。&#010;但问题是，每次JOB重启后，之前mysql 聚合表结果会被清空。我设置了checkpoint和racksdbbackendstate.&#010;&#010;Thanks&amp;Regards&#010;&#010;&#010;",
        "depth": "0",
        "reply": "<0da09178-5155-482d-9db3-faf74b4b27e4.zhengbinbin@heint.cn>"
    },
    {
        "id": "<4c815ff6.4a3f.1738f40581a.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 07:50:25 GMT",
        "subject": "Re:flink 聚合 job 重启问题",
        "content": "伪代码发下看看？看下jdbc sink的配置，是不是支持删除记录，更新的时候旧记录被删除了&#010;&#010;&#010;在 2020-07-27 11:33:31，\"郑斌斌\" &lt;zhengbinbin@heint.cn&gt; 写道：&#010;&gt;hi all :&#010;&gt;&#010;&gt;     请教个问题，我通过程序拉取kafka消息后，注册为flink流表。然后执行sql:&#010;\"select user_id, count(*)cnt from 流表\"， 将结果写入到mysql 聚合表中（SINK组件为：flink1.11版本JdbcUpsertTableSink)。&#010;&gt;但问题是，每次JOB重启后，之前mysql 聚合表结果会被清空。我设置了checkpoint和racksdbbackendstate.&#010;&gt;&#010;&gt;Thanks&amp;Regards&#010;&gt;&#010;&gt;&#010;",
        "depth": "1",
        "reply": "<0da09178-5155-482d-9db3-faf74b4b27e4.zhengbinbin@heint.cn>"
    },
    {
        "id": "<56e2b543-bc09-493f-9315-43e2e481e01c.zhengbinbin@heint.cn>",
        "from": "&quot;郑斌斌&quot; &lt;zhengbin...@heint.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 11:30:05 GMT",
        "subject": "回复：flink 聚合 job 重启问题",
        "content": " 需要通过checkpoint恢复启动才没有问题，不知道为什么是这样&#010;------------------------------------------------------------------&#010;发件人：RS &lt;tinyshrimp@163.com&gt;&#010;发送时间：2020年7月27日(星期一) 15:50&#010;收件人：user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;; 郑斌斌 &lt;zhengbinbin@heint.cn&gt;&#010;主　题：Re:flink 聚合 job 重启问题&#010;&#010;伪代码发下看看？看下jdbc sink的配置，是不是支持删除记录，更新的时候旧记录被删除了&#010;&#010;在 2020-07-27 11:33:31，\"郑斌斌\" &lt;zhengbinbin@heint.cn&gt; 写道：&#010;&gt;hi all :&#010;&gt;&#010;&gt;     请教个问题，我通过程序拉取kafka消息后，注册为flink流表。然后执行sql:&#010;\"select user_id, count(*)cnt from 流表\"， 将结果写入到mysql 聚合表中（SINK组件为：flink1.11版本JdbcUpsertTableSink)。&#010;&gt;但问题是，每次JOB重启后，之前mysql 聚合表结果会被清空。我设置了checkpoint和racksdbbackendstate.&#010;&gt;&#010;&gt;Thanks&amp;Regards&#010;&gt;&#010;&gt;&#010;&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<0da09178-5155-482d-9db3-faf74b4b27e4.zhengbinbin@heint.cn>"
    },
    {
        "id": "<061101d663c7$0dd653c0$2982fb40$@aliyun.com>",
        "from": "&lt;slle...@aliyun.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 03:36:02 GMT",
        "subject": "【flink sql】flink sql insert into插入语句的问题",
        "content": "测试Flink版本：1.11.0&#010;&#010; &#010;&#010;Flink sql支持这种语法插入吗，在插入时指定具体的字段顺序和要插入的列&#010;&#010;Insert into tableName(col1[,col2]) select col1[,col2]&#010;&#010; &#010;&#010;目前通过测试发现了以下问题&#010;&#010;建表语句：&#010;&#010;create table t1(a int,b string,c int) with ();&#010;&#010;create table t2(a int,b string,c int) with ();&#010;&#010; &#010;&#010;问题1：测试发现insert into时查询和sink schema的匹配规则是按照定义的顺序进行&#010;&#010;测试语句：&#010;&#010;insert into t2 select t1.a,t1.c, t1.b from  t1;&#010;&#010;报错信息：&#010;&#010;org.apache.flink.table.api.ValidationException: Field types of query result&#010;and registered TableSink default_catalog.default_database.t2 do not match.&#010;&#010;Query schema: [a: INT, c: INT, b: VARCHAR(2147483647)]&#010;&#010;Sink schema: [a: INT, b: VARCHAR(2147483647), c: INT]&#010;&#010; &#010;&#010;       at&#010;org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyI&#010;mplicitCast(TableSinkUtils.scala:100)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;erBase.scala:213)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;erBase.scala:204)&#010;&#010;       at scala.Option.map(Option.scala:146)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(Planner&#010;Base.scala:204)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;eamPlanner.scala:98)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;eamPlanner.scala:80)&#010;&#010;       at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;234)&#010;&#010;       at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;234)&#010;&#010;       at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&#010;       at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&#010;       at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&#010;       at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&#010;       at&#010;scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&#010;       at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;r.scala:80)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;r.scala:43)&#010;&#010;       at&#010;org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnviro&#010;nmentImpl.java:632)&#010;&#010; &#010;&#010;问题2：支持Insert into tableName(col1[,col2]) select col1[,col2]的语法，但并&#010;没有真正起作用，还是按照定义的顺序进行匹配&#010;&#010;测试语句：&#010;&#010;insert into t2(a,c,b) select t1.a,t1.c, t1.b from  t1;&#010;&#010;报错信息：&#010;&#010; &#010;&#010;org.apache.flink.table.api.ValidationException: Field types of query result&#010;and registered TableSink default_catalog.default_database.t2 do not match.&#010;&#010;Query schema: [a: INT, c: INT, b: VARCHAR(2147483647)]&#010;&#010;Sink schema: [a: INT, b: VARCHAR(2147483647), c: INT]&#010;&#010; &#010;&#010;       at&#010;org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyI&#010;mplicitCast(TableSinkUtils.scala:100)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;erBase.scala:213)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;erBase.scala:204)&#010;&#010;       at scala.Option.map(Option.scala:146)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(Planner&#010;Base.scala:204)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;eamPlanner.scala:98)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;eamPlanner.scala:80)&#010;&#010;       at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;234)&#010;&#010;       at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;234)&#010;&#010;       at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&#010;       at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&#010;       at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&#010;       at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&#010;       at&#010;scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&#010;       at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;r.scala:80)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;r.scala:43)&#010;&#010;       at&#010;org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnviro&#010;nmentImpl.java:632)&#010;&#010; &#010;&#010;问题3：当insert into的字段比sink的schema的字段少也会如此&#010;&#010;测试语句：&#010;&#010;insert into t2(a,b) &#010;&#010;select t1.a, t1.b from t1;&#010;&#010;报错信息：&#010;&#010;org.apache.flink.table.api.ValidationException: Field types of query result&#010;and registered TableSink default_catalog.default_database.t2 do not match.&#010;&#010;Query schema: [a: INT, c: VARCHAR(2147483647)]&#010;&#010;Sink schema: [a: INT, b: VARCHAR(2147483647), c: INT]&#010;&#010; &#010;&#010;       at&#010;org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyI&#010;mplicitCast(TableSinkUtils.scala:100)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;erBase.scala:213)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;erBase.scala:204)&#010;&#010;       at scala.Option.map(Option.scala:146)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(Planner&#010;Base.scala:204)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;eamPlanner.scala:98)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;eamPlanner.scala:80)&#010;&#010;       at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;234)&#010;&#010;       at&#010;scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;234)&#010;&#010;       at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&#010;       at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&#010;       at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&#010;       at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&#010;       at&#010;scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&#010;       at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;r.scala:80)&#010;&#010;       at&#010;org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;r.scala:43)&#010;&#010;       at&#010;org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnviro&#010;nmentImpl.java:632)&#010;&#010; &#010;&#010;总结：&#010;&#010;目前的实现限制了查询的和写人的灵活性，&#010;&#010;只有找到schema定义的字段顺序才能进行正确的插入，&#010;&#010;当字段很多时会比较麻烦，&#010;&#010;还有，只插入某些列的需求也是存在的，目前不能支持&#010;&#010;&#010;",
        "depth": "0",
        "reply": "<061101d663c7$0dd653c0$2982fb40$@aliyun.com>"
    },
    {
        "id": "<CAHjjfOT=rnq8PShEFELc-wWuyPj_R_ZDxeT-gYpi1f8iQteoMA@mail.gmail.com>",
        "from": "Caizhi Weng &lt;tsreape...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 06:01:03 GMT",
        "subject": "Re: 【flink sql】flink sql insert into插入语句的问题",
        "content": "Hi，&#010;&#010;Flink 目前的确不支持这个语法... 我已经创建了一个 issue[1]，可以在那里跟踪这个&#010;feature 的进展。&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18726&#010;&#010;&lt;sllence@aliyun.com.invalid&gt; 于2020年7月27日周一 上午11:36写道：&#010;&#010;&gt; 测试Flink版本：1.11.0&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Flink sql支持这种语法插入吗，在插入时指定具体的字段顺序和要插入的列&#010;&gt;&#010;&gt; Insert into tableName(col1[,col2]) select col1[,col2]&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 目前通过测试发现了以下问题&#010;&gt;&#010;&gt; 建表语句：&#010;&gt;&#010;&gt; create table t1(a int,b string,c int) with ();&#010;&gt;&#010;&gt; create table t2(a int,b string,c int) with ();&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 问题1：测试发现insert into时查询和sink schema的匹配规则是按照定义的顺序进行&#010;&gt;&#010;&gt; 测试语句：&#010;&gt;&#010;&gt; insert into t2 select t1.a,t1.c, t1.b from  t1;&#010;&gt;&#010;&gt; 报错信息：&#010;&gt;&#010;&gt; org.apache.flink.table.api.ValidationException: Field types of query result&#010;&gt; and registered TableSink default_catalog.default_database.t2 do not match.&#010;&gt;&#010;&gt; Query schema: [a: INT, c: INT, b: VARCHAR(2147483647)]&#010;&gt;&#010;&gt; Sink schema: [a: INT, b: VARCHAR(2147483647), c: INT]&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyI&#010;&gt; mplicitCast(TableSinkUtils.scala:100)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;&gt; erBase.scala:213)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;&gt; erBase.scala:204)&#010;&gt;&#010;&gt;        at scala.Option.map(Option.scala:146)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(Planner&#010;&gt; Base.scala:204)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;&gt; eamPlanner.scala:98)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;&gt; eamPlanner.scala:80)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;&gt; 234)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;&gt; 234)&#010;&gt;&#010;&gt;        at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&#010;&gt;        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&#010;&gt;        at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&#010;&gt;        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&#010;&gt;        at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&#010;&gt;        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;&gt; r.scala:80)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;&gt; r.scala:43)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnviro&#010;&gt; nmentImpl.java:632)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 问题2：支持Insert into tableName(col1[,col2]) select col1[,col2]的语法，但并&#010;&gt; 没有真正起作用，还是按照定义的顺序进行匹配&#010;&gt;&#010;&gt; 测试语句：&#010;&gt;&#010;&gt; insert into t2(a,c,b) select t1.a,t1.c, t1.b from  t1;&#010;&gt;&#010;&gt; 报错信息：&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; org.apache.flink.table.api.ValidationException: Field types of query result&#010;&gt; and registered TableSink default_catalog.default_database.t2 do not match.&#010;&gt;&#010;&gt; Query schema: [a: INT, c: INT, b: VARCHAR(2147483647)]&#010;&gt;&#010;&gt; Sink schema: [a: INT, b: VARCHAR(2147483647), c: INT]&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyI&#010;&gt; mplicitCast(TableSinkUtils.scala:100)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;&gt; erBase.scala:213)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;&gt; erBase.scala:204)&#010;&gt;&#010;&gt;        at scala.Option.map(Option.scala:146)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(Planner&#010;&gt; Base.scala:204)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;&gt; eamPlanner.scala:98)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;&gt; eamPlanner.scala:80)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;&gt; 234)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;&gt; 234)&#010;&gt;&#010;&gt;        at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&#010;&gt;        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&#010;&gt;        at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&#010;&gt;        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&#010;&gt;        at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&#010;&gt;        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;&gt; r.scala:80)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;&gt; r.scala:43)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnviro&#010;&gt; nmentImpl.java:632)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 问题3：当insert into的字段比sink的schema的字段少也会如此&#010;&gt;&#010;&gt; 测试语句：&#010;&gt;&#010;&gt; insert into t2(a,b)&#010;&gt;&#010;&gt; select t1.a, t1.b from t1;&#010;&gt;&#010;&gt; 报错信息：&#010;&gt;&#010;&gt; org.apache.flink.table.api.ValidationException: Field types of query result&#010;&gt; and registered TableSink default_catalog.default_database.t2 do not match.&#010;&gt;&#010;&gt; Query schema: [a: INT, c: VARCHAR(2147483647)]&#010;&gt;&#010;&gt; Sink schema: [a: INT, b: VARCHAR(2147483647), c: INT]&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.sinks.TableSinkUtils$.validateSchemaAndApplyI&#010;&gt; mplicitCast(TableSinkUtils.scala:100)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;&gt; erBase.scala:213)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(Plann&#010;&gt; erBase.scala:204)&#010;&gt;&#010;&gt;        at scala.Option.map(Option.scala:146)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(Planner&#010;&gt; Base.scala:204)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;&gt; eamPlanner.scala:98)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$1.apply(Str&#010;&gt; eamPlanner.scala:80)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;&gt; 234)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:&#010;&gt; 234)&#010;&gt;&#010;&gt;        at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&#010;&gt;        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&#010;&gt;        at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&#010;&gt;        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&#010;&gt;        at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&#010;&gt;        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;&gt; r.scala:80)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.explain(StreamPlanne&#010;&gt; r.scala:43)&#010;&gt;&#010;&gt;        at&#010;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.explain(TableEnviro&#010;&gt; nmentImpl.java:632)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 总结：&#010;&gt;&#010;&gt; 目前的实现限制了查询的和写人的灵活性，&#010;&gt;&#010;&gt; 只有找到schema定义的字段顺序才能进行正确的插入，&#010;&gt;&#010;&gt; 当字段很多时会比较麻烦，&#010;&gt;&#010;&gt; 还有，只插入某些列的需求也是存在的，目前不能支持&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<061101d663c7$0dd653c0$2982fb40$@aliyun.com>"
    },
    {
        "id": "<CAP+gf36Jxc=E-Cdx4Yb6vuFrZy5K6FhEjk=LR_5ik6RgO4=dHA@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 03:57:21 GMT",
        "subject": "Re: Flink JOB_MANAGER_LEADER_PATH Znode的清理时机",
        "content": "如果是Yarn perjob任务的话，当任务正常结束或者被cancel以后，zk整个clusterID的节点会被直接清理掉&#013;&#010;如果是Yarn session或Standalone Session的话，和job相关的持久节点是不会被清理的，只有在session结束以后会清理&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;林恬 &lt;lin.tian@yottabyte.cn&gt; 于2020年6月23日周二 上午11:34写道：&#013;&#010;&#013;&#010;&gt; 各位好：&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; 目前我使用的是Flink 1.9.2, 使用过程中发现ZK上的/leader/${job_id}&#013;&#010;&gt; 节点即使作业被Cancel了也不会被清理，导致运行久了之后，/leader/下有大量job_id的空ZNode，请问这块清理时机是什么时候呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &amp;nbsp;&#013;&#010;",
        "depth": "1",
        "reply": "<CAP+gf36Jxc=E-Cdx4Yb6vuFrZy5K6FhEjk=LR_5ik6RgO4=dHA@mail.gmail.com>"
    },
    {
        "id": "<CAE4Md6nseeQRh_+rprOHqV=a0oz7_4pMij6t7ueBwh3gcnwx8Q@mail.gmail.com>",
        "from": "jun su &lt;sujun891...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 06:29:39 GMT",
        "subject": "Re: Blink Planner构造Remote Env",
        "content": "hi Jark,&#010;&#010;抱歉这么晚回复邮件， 在flink 1.11.0版本上按照你的方式试验了下,&#010;创建StreamTableEnvironmentImpl的方式参照了StreamTableEnvironmentImpl#create,&#010;只是删除了方法中检查模式的代码 settings.isStreamingMode() ， 结果报以下错误:&#010;&#010;Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException:&#010;Cannot instantiate user function.&#010;    at org.apache.flink.streaming.api.graph.StreamConfig&#010;.getStreamOperatorFactory(StreamConfig.java:291)&#010;    at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(&#010;OperatorChain.java:126)&#010;    at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(&#010;StreamTask.java:453)&#010;    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask&#010;.java:522)&#010;    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#010;    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#010;    at java.lang.Thread.run(Thread.java:748)&#010;Caused by: java.io.StreamCorruptedException: invalid type code: 00&#010;    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1563)&#010;    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;2245)&#010;    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2125)&#010;    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:&#010;2027)&#010;    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;2245)&#010;    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)&#010;    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:&#010;2027)&#010;    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;2245)&#010;    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)&#010;    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:&#010;2027)&#010;    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;2245)&#010;    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)&#010;    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:&#010;2027)&#010;    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)&#010;    at org.apache.flink.util.InstantiationUtil.deserializeObject(&#010;InstantiationUtil.java:576)&#010;    at org.apache.flink.util.InstantiationUtil.deserializeObject(&#010;InstantiationUtil.java:562)&#010;    at org.apache.flink.util.InstantiationUtil.deserializeObject(&#010;InstantiationUtil.java:550)&#010;    at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(&#010;InstantiationUtil.java:511)&#010;    at org.apache.flink.streaming.api.graph.StreamConfig&#010;.getStreamOperatorFactory(StreamConfig.java:276)&#010;    ... 6 more&#010;&#010;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年5月20日周三 下午2:30写道：&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; 因为 Blink planner&#010;&gt; 不支持 org.apache.flink.table.api.java.BatchTableEnvironment，所以无法对接&#010;&gt; ExecutionEnvironment。&#010;&gt; Blink planner 的 batch 模式，目前只支持 TableEnvironemnt，不过也可以通过&#010;hack 的方式去使用&#010;&gt; StreamTableEnvironment，&#010;&gt; 需要直接去构造 StreamTableEnvironmentImpl:&#010;&gt;&#010;&gt; StreamExecutionEnvironment execEnv =&#010;&gt; StreamExecutionEnvironment.createRemoteEnvironment(...);&#010;&gt; StreamTableEnvironmentImpl tEnv = new StreamTableEnvironmentImpl(..&#010;&gt; execEnv, .., false); // 构造的参数可以参考 StreamTableEnvironmentImpl#create&#010;的实现&#010;&gt;&#010;&gt; Best,&#010;&gt; Jark&#010;&gt;&#010;&gt; On Tue, 19 May 2020 at 15:27, jun su &lt;sujun891020@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; hi all,&#010;&gt; &gt;&#010;&gt; &gt; 过去在ide中想连接远程flink集群可以 ExecutionEnvironment.createRemoteEnvironment()&#010;&gt; &gt;&#010;&gt; &gt; 官网Blink构建方式是:&#010;&gt; &gt;&#010;&gt; &gt; val bbSettings =&#010;&gt; &gt; EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build()&#010;&gt; &gt; val bbTableEnv = TableEnvironment.create(bbSettings)&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 请问如何连接远程集群呢？&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt; Best,&#010;&gt; &gt; Jun Su&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;Best,&#010;Jun Su&#010;&#010;",
        "depth": "1",
        "reply": "<CAE4Md6nseeQRh_+rprOHqV=a0oz7_4pMij6t7ueBwh3gcnwx8Q@mail.gmail.com>"
    },
    {
        "id": "<CAE4Md6knNMpT8m0CXvLw6LKksuLo0UonUjKctapuspxzREPoSQ@mail.gmail.com>",
        "from": "jun su &lt;sujun891...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 07:37:36 GMT",
        "subject": "Re: Blink Planner构造Remote Env",
        "content": "是依赖问题，解决了&#010;&#010;jun su &lt;sujun891020@gmail.com&gt; 于2020年7月27日周一 下午2:29写道：&#010;&#010;&gt; hi Jark,&#010;&gt;&#010;&gt; 抱歉这么晚回复邮件， 在flink 1.11.0版本上按照你的方式试验了下,&#010;&gt; 创建StreamTableEnvironmentImpl的方式参照了StreamTableEnvironmentImpl#create,&#010;&gt; 只是删除了方法中检查模式的代码 settings.isStreamingMode() ， 结果报以下错误:&#010;&gt;&#010;&gt; Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException:&#010;&gt; Cannot instantiate user function.&#010;&gt;     at org.apache.flink.streaming.api.graph.StreamConfig&#010;&gt; .getStreamOperatorFactory(StreamConfig.java:291)&#010;&gt;     at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(&#010;&gt; OperatorChain.java:126)&#010;&gt;     at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(&#010;&gt; StreamTask.java:453)&#010;&gt;     at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(&#010;&gt; StreamTask.java:522)&#010;&gt;     at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#010;&gt;     at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#010;&gt;     at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: java.io.StreamCorruptedException: invalid type code: 00&#010;&gt;     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1563)&#010;&gt;     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;&gt; 2245)&#010;&gt;     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:&#010;&gt; 2125)&#010;&gt;     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream&#010;&gt; .java:2027)&#010;&gt;     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;&gt;     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;&gt; 2245)&#010;&gt;     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:&#010;&gt; 2169)&#010;&gt;     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream&#010;&gt; .java:2027)&#010;&gt;     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;&gt;     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;&gt; 2245)&#010;&gt;     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:&#010;&gt; 2169)&#010;&gt;     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream&#010;&gt; .java:2027)&#010;&gt;     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;&gt;     at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:&#010;&gt; 2245)&#010;&gt;     at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:&#010;&gt; 2169)&#010;&gt;     at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream&#010;&gt; .java:2027)&#010;&gt;     at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)&#010;&gt;     at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)&#010;&gt;     at org.apache.flink.util.InstantiationUtil.deserializeObject(&#010;&gt; InstantiationUtil.java:576)&#010;&gt;     at org.apache.flink.util.InstantiationUtil.deserializeObject(&#010;&gt; InstantiationUtil.java:562)&#010;&gt;     at org.apache.flink.util.InstantiationUtil.deserializeObject(&#010;&gt; InstantiationUtil.java:550)&#010;&gt;     at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(&#010;&gt; InstantiationUtil.java:511)&#010;&gt;     at org.apache.flink.streaming.api.graph.StreamConfig&#010;&gt; .getStreamOperatorFactory(StreamConfig.java:276)&#010;&gt;     ... 6 more&#010;&gt;&#010;&gt;&#010;&gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年5月20日周三 下午2:30写道：&#010;&gt;&#010;&gt;&gt; Hi,&#010;&gt;&gt;&#010;&gt;&gt; 因为 Blink planner&#010;&gt;&gt; 不支持 org.apache.flink.table.api.java.BatchTableEnvironment，所以无法对接&#010;&gt;&gt; ExecutionEnvironment。&#010;&gt;&gt; Blink planner 的 batch 模式，目前只支持 TableEnvironemnt，不过也可以通过&#010;hack 的方式去使用&#010;&gt;&gt; StreamTableEnvironment，&#010;&gt;&gt; 需要直接去构造 StreamTableEnvironmentImpl:&#010;&gt;&gt;&#010;&gt;&gt; StreamExecutionEnvironment execEnv =&#010;&gt;&gt; StreamExecutionEnvironment.createRemoteEnvironment(...);&#010;&gt;&gt; StreamTableEnvironmentImpl tEnv = new StreamTableEnvironmentImpl(..&#010;&gt;&gt; execEnv, .., false); // 构造的参数可以参考 StreamTableEnvironmentImpl#create&#010;的实现&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Jark&#010;&gt;&gt;&#010;&gt;&gt; On Tue, 19 May 2020 at 15:27, jun su &lt;sujun891020@gmail.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt; &gt; hi all,&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 过去在ide中想连接远程flink集群可以 ExecutionEnvironment.createRemoteEnvironment()&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 官网Blink构建方式是:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; val bbSettings =&#010;&gt;&gt; &gt;&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build()&#010;&gt;&gt; &gt; val bbTableEnv = TableEnvironment.create(bbSettings)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 请问如何连接远程集群呢？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; --&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Jun Su&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Best,&#010;&gt; Jun Su&#010;&gt;&#010;&#010;&#010;-- &#010;Best,&#010;Jun Su&#010;&#010;",
        "depth": "2",
        "reply": "<CAE4Md6nseeQRh_+rprOHqV=a0oz7_4pMij6t7ueBwh3gcnwx8Q@mail.gmail.com>"
    },
    {
        "id": "<tencent_EF8CABC475D60831C8E08B1675B940A20A06@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 06:45:17 GMT",
        "subject": "sql-client 的jdbc表出错",
        "content": "您好，我创建了一个jdbc的表&#013;&#010;&#013;&#010;&#013;&#010;CREATE TABLE mvp_dim_anticheat_args_all (&#013;&#010;&amp;nbsp; &amp;nbsp; id BIGINT,&#013;&#010;&amp;nbsp; &amp;nbsp; dt STRING,&#013;&#010;&amp;nbsp; &amp;nbsp; cnt_7d INT,&#013;&#010;&amp;nbsp; &amp;nbsp;cnt_30d INT,&#013;&#010;&amp;nbsp; PRIMARY KEY (id) NOT ENFORCED&#013;&#010;) WITH (&#013;&#010;&amp;nbsp; &amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp; &amp;nbsp;'driver'='com.mysql.jdbc.Driver',&#013;&#010;&amp;nbsp; &amp;nbsp;'url' = 'jdbc:mysql://localhost:3306/huyou_oi',&#013;&#010;&amp;nbsp; &amp;nbsp;'table-name' = 'mvp_dim_ll',&#013;&#010;&amp;nbsp; &amp;nbsp;'username' = 'huy_oi',&#013;&#010;&amp;nbsp; &amp;nbsp;'password' = '420123'&#013;&#010;);&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;查询的时候报&amp;nbsp;&#013;&#010;[ERROR] Could not execute SQL statement. Reason:&#013;&#010;java.lang.ClassNotFoundException: com.mysql.jdbc.Driver&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;我的安装目录lib下面有&amp;nbsp;flink-connector-jdbc_2.11-1.11.0.jar 和&amp;nbsp;mysql-connector-java-5.1.38.jar&#010;这俩，请问是什么原因？？&#013;&#010;谢谢",
        "depth": "0",
        "reply": "<tencent_EF8CABC475D60831C8E08B1675B940A20A06@qq.com>"
    },
    {
        "id": "<CAHjjfOTdsUYOxKbmNC7Oh4y2HU-6hvoV6OoLstAFDD11nr1-yQ@mail.gmail.com>",
        "from": "Caizhi Weng &lt;tsreape...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:23:33 GMT",
        "subject": "Re: sql-client 的jdbc表出错",
        "content": "Hi，&#013;&#010;&#013;&#010;mysql-connector-java-5.1.38.jar 应该已经包含了 com.mysql.jdbc.Driver 才对；Flink&#013;&#010;是以什么模式运行的呢？如果是 standalone session，在 Flink 的 lib 下添加&#010;jar 包之后是否重启过 session&#013;&#010;集群呢？另外是否所有的 worker 都添加了 jar 包呢？如果能打出完整的错误栈会更好。&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月27日周一 下午2:45写道：&#013;&#010;&#013;&#010;&gt; 您好，我创建了一个jdbc的表&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE mvp_dim_anticheat_args_all (&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; id BIGINT,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; dt STRING,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; cnt_7d INT,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;cnt_30d INT,&#013;&#010;&gt; &amp;nbsp; PRIMARY KEY (id) NOT ENFORCED&#013;&#010;&gt; ) WITH (&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'connector' = 'jdbc',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'driver'='com.mysql.jdbc.Driver',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'url' = 'jdbc:mysql://localhost:3306/huyou_oi',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'table-name' = 'mvp_dim_ll',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'username' = 'huy_oi',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'password' = '420123'&#013;&#010;&gt; );&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 查询的时候报&amp;nbsp;&#013;&#010;&gt; [ERROR] Could not execute SQL statement. Reason:&#013;&#010;&gt; java.lang.ClassNotFoundException: com.mysql.jdbc.Driver&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我的安装目录lib下面有&amp;nbsp;flink-connector-jdbc_2.11-1.11.0.jar&#013;&#010;&gt; 和&amp;nbsp;mysql-connector-java-5.1.38.jar 这俩，请问是什么原因？？&#013;&#010;&gt; 谢谢&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_EF8CABC475D60831C8E08B1675B940A20A06@qq.com>"
    },
    {
        "id": "<CABz5Tv=NxuFuzV4makbFNTyv8OxFjyPovc5uJKEQK1HwMAERVg@mail.gmail.com>",
        "from": "杨荣 &lt;samyang31...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:25:05 GMT",
        "subject": "Re: sql-client 的jdbc表出错",
        "content": "hi,&#013;&#010;&#013;&#010;你能确定你的 class path 下有 mysql-connector-java-5.1.38.jar 依赖吗？请在运行时确认下这一点。&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月27日周一 下午2:45写道：&#013;&#010;&#013;&#010;&gt; 您好，我创建了一个jdbc的表&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE mvp_dim_anticheat_args_all (&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; id BIGINT,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; dt STRING,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp; cnt_7d INT,&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;cnt_30d INT,&#013;&#010;&gt; &amp;nbsp; PRIMARY KEY (id) NOT ENFORCED&#013;&#010;&gt; ) WITH (&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'connector' = 'jdbc',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'driver'='com.mysql.jdbc.Driver',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'url' = 'jdbc:mysql://localhost:3306/huyou_oi',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'table-name' = 'mvp_dim_ll',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'username' = 'huy_oi',&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;'password' = '420123'&#013;&#010;&gt; );&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 查询的时候报&amp;nbsp;&#013;&#010;&gt; [ERROR] Could not execute SQL statement. Reason:&#013;&#010;&gt; java.lang.ClassNotFoundException: com.mysql.jdbc.Driver&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我的安装目录lib下面有&amp;nbsp;flink-connector-jdbc_2.11-1.11.0.jar&#013;&#010;&gt; 和&amp;nbsp;mysql-connector-java-5.1.38.jar 这俩，请问是什么原因？？&#013;&#010;&gt; 谢谢&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_EF8CABC475D60831C8E08B1675B940A20A06@qq.com>"
    },
    {
        "id": "<tencent_6ABECA2913DB60D42514B31E89C706ADB509@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:28:40 GMT",
        "subject": "回复： sql-client 的jdbc表出错",
        "content": "你好，&#013;&#010;&amp;nbsp; 很确定，检查了好几遍，提交到yarn上执行都没问题,sql-client里面报这个错，1.11.0版本&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;samyang3125c@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月27日(星期一) 下午5:25&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: sql-client 的jdbc表出错&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hi,&#013;&#010;&#013;&#010;你能确定你的 class path 下有 mysql-connector-java-5.1.38.jar 依赖吗？请在运行时确认下这一点。&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&amp;gt; 于2020年7月27日周一 下午2:45写道：&#013;&#010;&#013;&#010;&amp;gt; 您好，我创建了一个jdbc的表&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; CREATE TABLE mvp_dim_anticheat_args_all (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; id BIGINT,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; dt STRING,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; cnt_7d INT,&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;cnt_30d INT,&#013;&#010;&amp;gt; &amp;amp;nbsp; PRIMARY KEY (id) NOT ENFORCED&#013;&#010;&amp;gt; ) WITH (&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'driver'='com.mysql.jdbc.Driver',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'url' = 'jdbc:mysql://localhost:3306/huyou_oi',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'table-name' = 'mvp_dim_ll',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'username' = 'huy_oi',&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'password' = '420123'&#013;&#010;&amp;gt; );&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 查询的时候报&amp;amp;nbsp;&#013;&#010;&amp;gt; [ERROR] Could not execute SQL statement. Reason:&#013;&#010;&amp;gt; java.lang.ClassNotFoundException: com.mysql.jdbc.Driver&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我的安装目录lib下面有&amp;amp;nbsp;flink-connector-jdbc_2.11-1.11.0.jar&#013;&#010;&amp;gt; 和&amp;amp;nbsp;mysql-connector-java-5.1.38.jar 这俩，请问是什么原因？？&#013;&#010;&amp;gt; 谢谢",
        "depth": "2",
        "reply": "<tencent_EF8CABC475D60831C8E08B1675B940A20A06@qq.com>"
    },
    {
        "id": "<CABz5Tvk3XYqePfopP2Lu0R1yH9d9as9qi7yxiByd1heApbTVUg@mail.gmail.com>",
        "from": "杨荣 &lt;samyang31...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:47:18 GMT",
        "subject": "Re: sql-client 的jdbc表出错",
        "content": "在 yarn 上提交 job 可以，不代表通过 sql-client 可以，他们使用的是不同的脚本和配置。前者跟&#010;bin/flink,&#013;&#010;bin/yarn-session.sh, conf/flink-conf.yaml 有关，后跟 bin/sql-client.sh,&#013;&#010;conf/sql-client-defaults.yaml 有关。&#013;&#010;&#013;&#010;你可以理一下这个逻辑，或者给出你的相关配置文件，以及 sql-client.sh&#010;启动完整命令。&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月27日周一 下午5:29写道：&#013;&#010;&#013;&#010;&gt; 你好，&#013;&#010;&gt; &amp;nbsp; 很确定，检查了好几遍，提交到yarn上执行都没问题,sql-client里面报这个错，1.11.0版本&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&#013;&#010;&gt;                                                   \"user-zh\"&#013;&#010;&gt;                                                                     &lt;&#013;&#010;&gt; samyang3125c@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月27日(星期一) 下午5:25&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;Re: sql-client 的jdbc表出错&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi,&#013;&#010;&gt;&#013;&#010;&gt; 你能确定你的 class path 下有 mysql-connector-java-5.1.38.jar 依赖吗？请在运行时确认下这一点。&#013;&#010;&gt;&#013;&#010;&gt; op &lt;520075694@qq.com&amp;gt; 于2020年7月27日周一 下午2:45写道：&#013;&#010;&gt;&#013;&#010;&gt; &amp;gt; 您好，我创建了一个jdbc的表&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; CREATE TABLE mvp_dim_anticheat_args_all (&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; id BIGINT,&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; dt STRING,&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; cnt_7d INT,&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;cnt_30d INT,&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; PRIMARY KEY (id) NOT ENFORCED&#013;&#010;&gt; &amp;gt; ) WITH (&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'connector' = 'jdbc',&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'driver'='com.mysql.jdbc.Driver',&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'url' = 'jdbc:mysql://localhost:3306/huyou_oi',&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'table-name' = 'mvp_dim_ll',&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'username' = 'huy_oi',&#013;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp;'password' = '420123'&#013;&#010;&gt; &amp;gt; );&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 查询的时候报&amp;amp;nbsp;&#013;&#010;&gt; &amp;gt; [ERROR] Could not execute SQL statement. Reason:&#013;&#010;&gt; &amp;gt; java.lang.ClassNotFoundException: com.mysql.jdbc.Driver&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt;&#013;&#010;&gt; &amp;gt; 我的安装目录lib下面有&amp;amp;nbsp;flink-connector-jdbc_2.11-1.11.0.jar&#013;&#010;&gt; &amp;gt; 和&amp;amp;nbsp;mysql-connector-java-5.1.38.jar 这俩，请问是什么原因？？&#013;&#010;&gt; &amp;gt; 谢谢&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_EF8CABC475D60831C8E08B1675B940A20A06@qq.com>"
    },
    {
        "id": "<tencent_F6E0989669795CE2228C9BC4475623F1FF06@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 10:31:58 GMT",
        "subject": "回复： sql-client 的jdbc表出错",
        "content": "谢谢 我检查下&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;samyang3125c@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月27日(星期一) 下午5:47&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: sql-client 的jdbc表出错&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;在 yarn 上提交 job 可以，不代表通过 sql-client 可以，他们使用的是不同的脚本和配置。前者跟&#010;bin/flink,&#013;&#010;bin/yarn-session.sh, conf/flink-conf.yaml 有关，后跟 bin/sql-client.sh,&#013;&#010;conf/sql-client-defaults.yaml 有关。&#013;&#010;&#013;&#010;你可以理一下这个逻辑，或者给出你的相关配置文件，以及 sql-client.sh&#010;启动完整命令。&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&amp;gt; 于2020年7月27日周一 下午5:29写道：&#013;&#010;&#013;&#010;&amp;gt; 你好，&#013;&#010;&amp;gt; &amp;amp;nbsp; 很确定，检查了好几遍，提交到yarn上执行都没问题,sql-client里面报这个错，1.11.0版本&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; samyang3125c@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月27日(星期一) 下午5:25&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: sql-client 的jdbc表出错&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; hi,&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 你能确定你的 class path 下有 mysql-connector-java-5.1.38.jar 依赖吗？请在运行时确认下这一点。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; op &lt;520075694@qq.com&amp;amp;gt; 于2020年7月27日周一 下午2:45写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 您好，我创建了一个jdbc的表&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; CREATE TABLE mvp_dim_anticheat_args_all (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; id BIGINT,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; dt STRING,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; cnt_7d INT,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;cnt_30d INT,&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; PRIMARY KEY (id) NOT ENFORCED&#013;&#010;&amp;gt; &amp;amp;gt; ) WITH (&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;'driver'='com.mysql.jdbc.Driver',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;'url' = 'jdbc:mysql://localhost:3306/huyou_oi',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;'table-name' = 'mvp_dim_ll',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;'username' = 'huy_oi',&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;'password' = '420123'&#013;&#010;&amp;gt; &amp;amp;gt; );&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 查询的时候报&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; [ERROR] Could not execute SQL statement. Reason:&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.ClassNotFoundException: com.mysql.jdbc.Driver&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 我的安装目录lib下面有&amp;amp;amp;nbsp;flink-connector-jdbc_2.11-1.11.0.jar&#013;&#010;&amp;gt; &amp;amp;gt; 和&amp;amp;amp;nbsp;mysql-connector-java-5.1.38.jar 这俩，请问是什么原因？？&#013;&#010;&amp;gt; &amp;amp;gt; 谢谢",
        "depth": "4",
        "reply": "<tencent_EF8CABC475D60831C8E08B1675B940A20A06@qq.com>"
    },
    {
        "id": "<CAE4Md6=MbdT48Gx4onVQSw6ruRiPwd0T_RYG6Rp_EvW47owuOA@mail.gmail.com>",
        "from": "jun su &lt;sujun891...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 07:49:46 GMT",
        "subject": "Blink的Batch模式的并行度问题",
        "content": "hi all,&#013;&#010;&#013;&#010;Flink 目前的blink table planner batch mode&#013;&#010;(读hdfs上的orc文件)只支持StreamTableSource和LookupableTableSource，&#013;&#010;但是StreamTableSource的并行度默认应该是1 ， 底层是ContinuousFileMonitoringFunction&#010;,&#013;&#010;那么如何能扩大并行度来优化性能呢？&#013;&#010;&#013;&#010;-- &#013;&#010;Best,&#013;&#010;Jun Su&#013;&#010;",
        "depth": "0",
        "reply": "<CAE4Md6=MbdT48Gx4onVQSw6ruRiPwd0T_RYG6Rp_EvW47owuOA@mail.gmail.com>"
    },
    {
        "id": "<CAHjjfOSiYRgybLdgSnioLU3nCS7nffPvCMg6zh72JqUeFQLB5A@mail.gmail.com>",
        "from": "Caizhi Weng &lt;tsreape...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:30:06 GMT",
        "subject": "Re: Blink的Batch模式的并行度问题",
        "content": "Hi，&#013;&#010;&#013;&#010;可以配置 table.exec.resource.default-parallelism 为需要的并发。详见文档[1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html#table-exec-resource-default-parallelism&#013;&#010;&#013;&#010;jun su &lt;sujun891020@gmail.com&gt; 于2020年7月27日周一 下午3:50写道：&#013;&#010;&#013;&#010;&gt; hi all,&#013;&#010;&gt;&#013;&#010;&gt; Flink 目前的blink table planner batch mode&#013;&#010;&gt; (读hdfs上的orc文件)只支持StreamTableSource和LookupableTableSource，&#013;&#010;&gt; 但是StreamTableSource的并行度默认应该是1 ， 底层是ContinuousFileMonitoringFunction&#010;,&#013;&#010;&gt; 那么如何能扩大并行度来优化性能呢？&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best,&#013;&#010;&gt; Jun Su&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAE4Md6=MbdT48Gx4onVQSw6ruRiPwd0T_RYG6Rp_EvW47owuOA@mail.gmail.com>"
    },
    {
        "id": "<CAE4Md6mMetDWVmZJ+PcV4D5MOKChEKtr6gtNNC=vPTSa2ACjHA@mail.gmail.com>",
        "from": "jun su &lt;sujun891...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:46:06 GMT",
        "subject": "Re: Blink的Batch模式的并行度问题",
        "content": "hi,&#013;&#010;&#013;&#010;如果底层是FileInputFormat ，默认就是1个并行度, 这个参数我尝试了并不起作用，&#013;&#010;看代码是创建了一个SingleOutputStreamOperator ， 感觉得重写下我使用的OrcInputFormat&#010;,&#013;&#010;让他不继承FileInputFormat ， 像源码里的HiveInputFormat一样&#013;&#010;&#013;&#010;Caizhi Weng &lt;tsreaper96@gmail.com&gt; 于2020年7月27日周一 下午5:31写道：&#013;&#010;&#013;&#010;&gt; Hi，&#013;&#010;&gt;&#013;&#010;&gt; 可以配置 table.exec.resource.default-parallelism 为需要的并发。详见文档[1]&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/config.html#table-exec-resource-default-parallelism&#013;&#010;&gt;&#013;&#010;&gt; jun su &lt;sujun891020@gmail.com&gt; 于2020年7月27日周一 下午3:50写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi all,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Flink 目前的blink table planner batch mode&#013;&#010;&gt; &gt; (读hdfs上的orc文件)只支持StreamTableSource和LookupableTableSource，&#013;&#010;&gt; &gt; 但是StreamTableSource的并行度默认应该是1 ， 底层是ContinuousFileMonitoringFunction&#010;,&#013;&#010;&gt; &gt; 那么如何能扩大并行度来优化性能呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Jun Su&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best,&#013;&#010;Jun Su&#013;&#010;",
        "depth": "2",
        "reply": "<CAE4Md6=MbdT48Gx4onVQSw6ruRiPwd0T_RYG6Rp_EvW47owuOA@mail.gmail.com>"
    },
    {
        "id": "<tencent_F86921972725C6C1E8483C6689118F0C9E0A@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:09:17 GMT",
        "subject": "Hbase connector写入问题",
        "content": "&amp;nbsp; 您好，往habse表写数据时可不可以跳过某些字段，比如，family1&#010;有两个列，我只想插入其中一个列，置为空会报错&#013;&#010;INSERT INTO hTable SELECT rowkey, ROW(null,f1q1) FROM T;&#013;&#010;谢谢",
        "depth": "0",
        "reply": "<tencent_F86921972725C6C1E8483C6689118F0C9E0A@qq.com>"
    },
    {
        "id": "<7999a396.5bdc.1738f9ce185.Coremail.tinyshrimp@163.com>",
        "from": "RS &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:31:30 GMT",
        "subject": "kafka-connect json格式适配问题？",
        "content": "hi，&#010;kafka-&gt;Flink-&gt;kafka-&gt;mysql&#010;Flink用sql处理之后数据写入kafka里面，格式为json，再用kafka-connect-jdbc将数据导出到mysql中。&#010;使用kafka-connect是方便数据同时导出到其他存储&#010;&#010;&#010;&#010;Flink定义输出表结构：&#010;&#010;CREATE TABLE print_table \\&#010;&#010;(total_count BIGINT, username STRING, update_time TIMESTAMP(6)) \\&#010;&#010;WITH (\\&#010;&#010;'connector' = 'kafka', \\&#010;&#010;'topic' = 'test_out', \\&#010;&#010;'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#010;&#010;'sink.partitioner' = 'round-robin', \\&#010;&#010;'format' = 'json')&#010;&#010;&#010;&#010;&#010;输出的数据格式示例：&#010;&#010;{\"total_count\":12,\"username\":\"admin\",\"update_time\":\"2020-07-27 17:23:00\"}&#010;&#010;&#010;&#010;&#010;但是kafka-connect-jdbc的json格式需要schema和payload，示例：&#010;&#010;{&#010;&#010;  \"schema\": {&#010;&#010;    \"type\": \"struct\",&#010;&#010;    \"fields\": [&#010;&#010;      {&#010;&#010;        \"type\": \"int64\",&#010;&#010;        \"optional\": false,&#010;&#010;        \"field\": \"id\"&#010;&#010;      },&#010;&#010;      {&#010;&#010;        \"type\": \"string\",&#010;&#010;        \"optional\": true,&#010;&#010;        \"field\": \"name\"&#010;&#010;      }&#010;&#010;    ],&#010;&#010;    \"optional\": true,&#010;&#010;    \"name\": \"user\"&#010;&#010;  },&#010;&#010;  \"payload\": {&#010;&#010;    \"id\": 1,&#010;&#010;    \"name\": \"admin\"&#010;&#010;  }&#010;&#010;}&#010;&#010;&#010;&#010;&#010;请教下在Flink里面如何处理（补上schema和payload？），才能形成kafka connect匹配的json格式？&#010;&#010;当前Flink处理sql：&#010;&#010;INSERT INTO test_out(total_count,username,update_time) SELECT count(1) AS total_count,username,TUMBLE_START(update_time,INTERVAL&#010;'1' MINUTE) as update_time FROM table1 GROUP BY username,TUMBLE(update_time,INTERVAL '1' MINUTE)&#010;&#010;&#010;",
        "depth": "0",
        "reply": "<7999a396.5bdc.1738f9ce185.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<CAELO933yFQLqTY8HcEdZsMAT2VUCwcuBPwdR-ZwKr8SGA+S3WQ@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 09:49:18 GMT",
        "subject": "Re: kafka-connect json格式适配问题？",
        "content": "Hi,&#013;&#010;&#013;&#010;你需要在 DDL 和 query 上都补上 schema 和 payload:&#013;&#010;&#013;&#010;CREATE TABLE print_table \\&#013;&#010;(`schema` STRING, `payload` ROW&lt;total_count BIGINT, username STRING,&#013;&#010;update_time TIMESTAMP(6)&gt;) \\&#013;&#010;WITH (\\&#013;&#010;'connector' = 'kafka', \\&#013;&#010;'topic' = 'test_out', \\&#013;&#010;'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#013;&#010;'sink.partitioner' = 'round-robin', \\&#013;&#010;'format' = 'json')&#013;&#010;&#013;&#010;-- DML 上可以用常量写死 schema, 用 ROW 函数封装 payload&#013;&#010;INSERT INTO output&#013;&#010;SELECT '{ \"type\": \"struct\", ...}' as schema, ROW(totall_count, username,&#013;&#010;update_time) as payload&#013;&#010;FROM ...&#013;&#010;&#013;&#010;&#013;&#010;Btw, 我想问一下，为什么一定要用 kafka-jdbc-connect 去同步到 mysql 呢？个人觉得直接用&#010;Flink SQL 同步到&#013;&#010;mysql 不是很方便么？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;&#013;&#010;On Mon, 27 Jul 2020 at 17:33, RS &lt;tinyshrimp@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi，&#013;&#010;&gt; kafka-&gt;Flink-&gt;kafka-&gt;mysql&#013;&#010;&gt; Flink用sql处理之后数据写入kafka里面，格式为json，再用kafka-connect-jdbc将数据导出到mysql中。&#013;&#010;&gt; 使用kafka-connect是方便数据同时导出到其他存储&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Flink定义输出表结构：&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE print_table \\&#013;&#010;&gt;&#013;&#010;&gt; (total_count BIGINT, username STRING, update_time TIMESTAMP(6)) \\&#013;&#010;&gt;&#013;&#010;&gt; WITH (\\&#013;&#010;&gt;&#013;&#010;&gt; 'connector' = 'kafka', \\&#013;&#010;&gt;&#013;&#010;&gt; 'topic' = 'test_out', \\&#013;&#010;&gt;&#013;&#010;&gt; 'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#013;&#010;&gt;&#013;&#010;&gt; 'sink.partitioner' = 'round-robin', \\&#013;&#010;&gt;&#013;&#010;&gt; 'format' = 'json')&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 输出的数据格式示例：&#013;&#010;&gt;&#013;&#010;&gt; {\"total_count\":12,\"username\":\"admin\",\"update_time\":\"2020-07-27 17:23:00\"}&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 但是kafka-connect-jdbc的json格式需要schema和payload，示例：&#013;&#010;&gt;&#013;&#010;&gt; {&#013;&#010;&gt;&#013;&#010;&gt;   \"schema\": {&#013;&#010;&gt;&#013;&#010;&gt;     \"type\": \"struct\",&#013;&#010;&gt;&#013;&#010;&gt;     \"fields\": [&#013;&#010;&gt;&#013;&#010;&gt;       {&#013;&#010;&gt;&#013;&#010;&gt;         \"type\": \"int64\",&#013;&#010;&gt;&#013;&#010;&gt;         \"optional\": false,&#013;&#010;&gt;&#013;&#010;&gt;         \"field\": \"id\"&#013;&#010;&gt;&#013;&#010;&gt;       },&#013;&#010;&gt;&#013;&#010;&gt;       {&#013;&#010;&gt;&#013;&#010;&gt;         \"type\": \"string\",&#013;&#010;&gt;&#013;&#010;&gt;         \"optional\": true,&#013;&#010;&gt;&#013;&#010;&gt;         \"field\": \"name\"&#013;&#010;&gt;&#013;&#010;&gt;       }&#013;&#010;&gt;&#013;&#010;&gt;     ],&#013;&#010;&gt;&#013;&#010;&gt;     \"optional\": true,&#013;&#010;&gt;&#013;&#010;&gt;     \"name\": \"user\"&#013;&#010;&gt;&#013;&#010;&gt;   },&#013;&#010;&gt;&#013;&#010;&gt;   \"payload\": {&#013;&#010;&gt;&#013;&#010;&gt;     \"id\": 1,&#013;&#010;&gt;&#013;&#010;&gt;     \"name\": \"admin\"&#013;&#010;&gt;&#013;&#010;&gt;   }&#013;&#010;&gt;&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 请教下在Flink里面如何处理（补上schema和payload？），才能形成kafka&#010;connect匹配的json格式？&#013;&#010;&gt;&#013;&#010;&gt; 当前Flink处理sql：&#013;&#010;&gt;&#013;&#010;&gt; INSERT INTO test_out(total_count,username,update_time) SELECT count(1) AS&#013;&#010;&gt; total_count,username,TUMBLE_START(update_time,INTERVAL '1' MINUTE) as&#013;&#010;&gt; update_time FROM table1 GROUP BY username,TUMBLE(update_time,INTERVAL '1'&#013;&#010;&gt; MINUTE)&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<7999a396.5bdc.1738f9ce185.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<3d3b392a.669f.1739029d537.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 12:05:27 GMT",
        "subject": "Re:Re: kafka-connect json格式适配问题？",
        "content": "Hi,&#010;改了下sql，遇到一个新的问题：&#010;Caused by: org.apache.flink.table.planner.codegen.CodeGenException: Unsupported cast from&#010;'ROW&lt;`EXPR$0` BIGINT NOT NULL, `EXPR$1` STRING, `EXPR$2` TIMESTAMP(3) *ROWTIME*&gt; NOT&#010;NULL' to 'ROW&lt;`total_count` BIGINT, `username` STRING, `update_time` TIMESTAMP(6)&gt;'.&#010;&#010;&#010;SELECT里面的时间是这样定义的：TUMBLE_START(update_time,INTERVAL '1' MINUTE) as&#010;update_time) as payload&#010;&#010;&#010;我把TIMESTAMP(6)修改为TIMESTAMP(3)之后，就没有报错了，所以Flink里面窗口的时间精度只是3位吗？&#010;&#010;&#010;Thanks&#010;在 2020-07-27 17:49:18，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;你需要在 DDL 和 query 上都补上 schema 和 payload:&#010;&gt;&#010;&gt;CREATE TABLE print_table \\&#010;&gt;(`schema` STRING, `payload` ROW&lt;total_count BIGINT, username STRING,&#010;&gt;update_time TIMESTAMP(6)&gt;) \\&#010;&gt;WITH (\\&#010;&gt;'connector' = 'kafka', \\&#010;&gt;'topic' = 'test_out', \\&#010;&gt;'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#010;&gt;'sink.partitioner' = 'round-robin', \\&#010;&gt;'format' = 'json')&#010;&gt;&#010;&gt;-- DML 上可以用常量写死 schema, 用 ROW 函数封装 payload&#010;&gt;INSERT INTO output&#010;&gt;SELECT '{ \"type\": \"struct\", ...}' as schema, ROW(totall_count, username,&#010;&gt;update_time) as payload&#010;&gt;FROM ...&#010;&gt;&#010;&gt;&#010;&gt;Btw, 我想问一下，为什么一定要用 kafka-jdbc-connect 去同步到 mysql 呢？个人觉得直接用&#010;Flink SQL 同步到&#010;&gt;mysql 不是很方便么？&#010;&gt;&#010;&gt;Best,&#010;&gt;Jark&#010;&gt;&#010;&gt;&#010;&gt;On Mon, 27 Jul 2020 at 17:33, RS &lt;tinyshrimp@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; hi，&#010;&gt;&gt; kafka-&gt;Flink-&gt;kafka-&gt;mysql&#010;&gt;&gt; Flink用sql处理之后数据写入kafka里面，格式为json，再用kafka-connect-jdbc将数据导出到mysql中。&#010;&gt;&gt; 使用kafka-connect是方便数据同时导出到其他存储&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Flink定义输出表结构：&#010;&gt;&gt;&#010;&gt;&gt; CREATE TABLE print_table \\&#010;&gt;&gt;&#010;&gt;&gt; (total_count BIGINT, username STRING, update_time TIMESTAMP(6)) \\&#010;&gt;&gt;&#010;&gt;&gt; WITH (\\&#010;&gt;&gt;&#010;&gt;&gt; 'connector' = 'kafka', \\&#010;&gt;&gt;&#010;&gt;&gt; 'topic' = 'test_out', \\&#010;&gt;&gt;&#010;&gt;&gt; 'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#010;&gt;&gt;&#010;&gt;&gt; 'sink.partitioner' = 'round-robin', \\&#010;&gt;&gt;&#010;&gt;&gt; 'format' = 'json')&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 输出的数据格式示例：&#010;&gt;&gt;&#010;&gt;&gt; {\"total_count\":12,\"username\":\"admin\",\"update_time\":\"2020-07-27 17:23:00\"}&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 但是kafka-connect-jdbc的json格式需要schema和payload，示例：&#010;&gt;&gt;&#010;&gt;&gt; {&#010;&gt;&gt;&#010;&gt;&gt;   \"schema\": {&#010;&gt;&gt;&#010;&gt;&gt;     \"type\": \"struct\",&#010;&gt;&gt;&#010;&gt;&gt;     \"fields\": [&#010;&gt;&gt;&#010;&gt;&gt;       {&#010;&gt;&gt;&#010;&gt;&gt;         \"type\": \"int64\",&#010;&gt;&gt;&#010;&gt;&gt;         \"optional\": false,&#010;&gt;&gt;&#010;&gt;&gt;         \"field\": \"id\"&#010;&gt;&gt;&#010;&gt;&gt;       },&#010;&gt;&gt;&#010;&gt;&gt;       {&#010;&gt;&gt;&#010;&gt;&gt;         \"type\": \"string\",&#010;&gt;&gt;&#010;&gt;&gt;         \"optional\": true,&#010;&gt;&gt;&#010;&gt;&gt;         \"field\": \"name\"&#010;&gt;&gt;&#010;&gt;&gt;       }&#010;&gt;&gt;&#010;&gt;&gt;     ],&#010;&gt;&gt;&#010;&gt;&gt;     \"optional\": true,&#010;&gt;&gt;&#010;&gt;&gt;     \"name\": \"user\"&#010;&gt;&gt;&#010;&gt;&gt;   },&#010;&gt;&gt;&#010;&gt;&gt;   \"payload\": {&#010;&gt;&gt;&#010;&gt;&gt;     \"id\": 1,&#010;&gt;&gt;&#010;&gt;&gt;     \"name\": \"admin\"&#010;&gt;&gt;&#010;&gt;&gt;   }&#010;&gt;&gt;&#010;&gt;&gt; }&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 请教下在Flink里面如何处理（补上schema和payload？），才能形成kafka&#010;connect匹配的json格式？&#010;&gt;&gt;&#010;&gt;&gt; 当前Flink处理sql：&#010;&gt;&gt;&#010;&gt;&gt; INSERT INTO test_out(total_count,username,update_time) SELECT count(1) AS&#010;&gt;&gt; total_count,username,TUMBLE_START(update_time,INTERVAL '1' MINUTE) as&#010;&gt;&gt; update_time FROM table1 GROUP BY username,TUMBLE(update_time,INTERVAL '1'&#010;&gt;&gt; MINUTE)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "2",
        "reply": "<7999a396.5bdc.1738f9ce185.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<74E79A84-E8B5-471C-BF29-597BBBA87226@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 12:10:44 GMT",
        "subject": "Re: kafka-connect json格式适配问题？",
        "content": "&gt; 我把TIMESTAMP(6)修改为TIMESTAMP(3)之后，就没有报错了，所以Flink里面窗口的时间精度只是3位吗？&#010;&#010;窗口里的时间用来做time attribute 列了吧，只能是TIMESTAMP(3), 其TIMESTAMP字段Flink是可以支持到TIMESTAMP(9)的&#010;&#010;祝好&#010;Leonard &#010;&#010;&gt; 在 2020年7月27日，20:05，RS &lt;tinyshrimp@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,&#010;&gt; 改了下sql，遇到一个新的问题：&#010;&gt; Caused by: org.apache.flink.table.planner.codegen.CodeGenException: Unsupported cast&#010;from 'ROW&lt;`EXPR$0` BIGINT NOT NULL, `EXPR$1` STRING, `EXPR$2` TIMESTAMP(3) *ROWTIME*&gt;&#010;NOT NULL' to 'ROW&lt;`total_count` BIGINT, `username` STRING, `update_time` TIMESTAMP(6)&gt;'.&#010;&gt; &#010;&gt; &#010;&gt; SELECT里面的时间是这样定义的：TUMBLE_START(update_time,INTERVAL '1' MINUTE)&#010;as update_time) as payload&#010;&gt; &#010;&gt; &#010;&gt; 我把TIMESTAMP(6)修改为TIMESTAMP(3)之后，就没有报错了，所以Flink里面窗口的时间精度只是3位吗？&#010;&gt; &#010;&gt; &#010;&gt; Thanks&#010;&gt; 在 2020-07-27 17:49:18，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; Hi,&#010;&gt;&gt; &#010;&gt;&gt; 你需要在 DDL 和 query 上都补上 schema 和 payload:&#010;&gt;&gt; &#010;&gt;&gt; CREATE TABLE print_table \\&#010;&gt;&gt; (`schema` STRING, `payload` ROW&lt;total_count BIGINT, username STRING,&#010;&gt;&gt; update_time TIMESTAMP(6)&gt;) \\&#010;&gt;&gt; WITH (\\&#010;&gt;&gt; 'connector' = 'kafka', \\&#010;&gt;&gt; 'topic' = 'test_out', \\&#010;&gt;&gt; 'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#010;&gt;&gt; 'sink.partitioner' = 'round-robin', \\&#010;&gt;&gt; 'format' = 'json')&#010;&gt;&gt; &#010;&gt;&gt; -- DML 上可以用常量写死 schema, 用 ROW 函数封装 payload&#010;&gt;&gt; INSERT INTO output&#010;&gt;&gt; SELECT '{ \"type\": \"struct\", ...}' as schema, ROW(totall_count, username,&#010;&gt;&gt; update_time) as payload&#010;&gt;&gt; FROM ...&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; Btw, 我想问一下，为什么一定要用 kafka-jdbc-connect 去同步到 mysql&#010;呢？个人觉得直接用 Flink SQL 同步到&#010;&gt;&gt; mysql 不是很方便么？&#010;&gt;&gt; &#010;&gt;&gt; Best,&#010;&gt;&gt; Jark&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; On Mon, 27 Jul 2020 at 17:33, RS &lt;tinyshrimp@163.com&gt; wrote:&#010;&gt;&gt; &#010;&gt;&gt;&gt; hi，&#010;&gt;&gt;&gt; kafka-&gt;Flink-&gt;kafka-&gt;mysql&#010;&gt;&gt;&gt; Flink用sql处理之后数据写入kafka里面，格式为json，再用kafka-connect-jdbc将数据导出到mysql中。&#010;&gt;&gt;&gt; 使用kafka-connect是方便数据同时导出到其他存储&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Flink定义输出表结构：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; CREATE TABLE print_table \\&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; (total_count BIGINT, username STRING, update_time TIMESTAMP(6)) \\&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; WITH (\\&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 'connector' = 'kafka', \\&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 'topic' = 'test_out', \\&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 'sink.partitioner' = 'round-robin', \\&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 'format' = 'json')&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 输出的数据格式示例：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; {\"total_count\":12,\"username\":\"admin\",\"update_time\":\"2020-07-27 17:23:00\"}&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 但是kafka-connect-jdbc的json格式需要schema和payload，示例：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; {&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;  \"schema\": {&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    \"type\": \"struct\",&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    \"fields\": [&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;      {&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        \"type\": \"int64\",&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        \"optional\": false,&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        \"field\": \"id\"&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;      },&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;      {&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        \"type\": \"string\",&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        \"optional\": true,&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        \"field\": \"name\"&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;      }&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    ],&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    \"optional\": true,&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    \"name\": \"user\"&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;  },&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;  \"payload\": {&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    \"id\": 1,&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    \"name\": \"admin\"&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;  }&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; }&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 请教下在Flink里面如何处理（补上schema和payload？），才能形成kafka&#010;connect匹配的json格式？&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 当前Flink处理sql：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; INSERT INTO test_out(total_count,username,update_time) SELECT count(1) AS&#010;&gt;&gt;&gt; total_count,username,TUMBLE_START(update_time,INTERVAL '1' MINUTE) as&#010;&gt;&gt;&gt; update_time FROM table1 GROUP BY username,TUMBLE(update_time,INTERVAL '1'&#010;&gt;&gt;&gt; MINUTE)&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&#010;&#010;",
        "depth": "3",
        "reply": "<7999a396.5bdc.1738f9ce185.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<7418f593.69e9.1739056a1da.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 12:54:23 GMT",
        "subject": "Re:Re: kafka-connect json格式适配问题？",
        "content": "Hi,&#010;啊，发现不太对，`schema`需要一个dict，不是STRING。请教下这个如何用SQL定义出来？&#010;&#010;&#010;Thanks&#010;在 2020-07-27 17:49:18，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;你需要在 DDL 和 query 上都补上 schema 和 payload:&#010;&gt;&#010;&gt;CREATE TABLE print_table \\&#010;&gt;(`schema` STRING, `payload` ROW&lt;total_count BIGINT, username STRING,&#010;&gt;update_time TIMESTAMP(6)&gt;) \\&#010;&gt;WITH (\\&#010;&gt;'connector' = 'kafka', \\&#010;&gt;'topic' = 'test_out', \\&#010;&gt;'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#010;&gt;'sink.partitioner' = 'round-robin', \\&#010;&gt;'format' = 'json')&#010;&gt;&#010;&gt;-- DML 上可以用常量写死 schema, 用 ROW 函数封装 payload&#010;&gt;INSERT INTO output&#010;&gt;SELECT '{ \"type\": \"struct\", ...}' as schema, ROW(totall_count, username,&#010;&gt;update_time) as payload&#010;&gt;FROM ...&#010;&gt;&#010;&gt;&#010;&gt;Btw, 我想问一下，为什么一定要用 kafka-jdbc-connect 去同步到 mysql 呢？个人觉得直接用&#010;Flink SQL 同步到&#010;&gt;mysql 不是很方便么？&#010;&gt;&#010;&gt;Best,&#010;&gt;Jark&#010;&gt;&#010;&gt;&#010;&gt;On Mon, 27 Jul 2020 at 17:33, RS &lt;tinyshrimp@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; hi，&#010;&gt;&gt; kafka-&gt;Flink-&gt;kafka-&gt;mysql&#010;&gt;&gt; Flink用sql处理之后数据写入kafka里面，格式为json，再用kafka-connect-jdbc将数据导出到mysql中。&#010;&gt;&gt; 使用kafka-connect是方便数据同时导出到其他存储&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Flink定义输出表结构：&#010;&gt;&gt;&#010;&gt;&gt; CREATE TABLE print_table \\&#010;&gt;&gt;&#010;&gt;&gt; (total_count BIGINT, username STRING, update_time TIMESTAMP(6)) \\&#010;&gt;&gt;&#010;&gt;&gt; WITH (\\&#010;&gt;&gt;&#010;&gt;&gt; 'connector' = 'kafka', \\&#010;&gt;&gt;&#010;&gt;&gt; 'topic' = 'test_out', \\&#010;&gt;&gt;&#010;&gt;&gt; 'properties.bootstrap.servers' = '127.0.0.1:9092', \\&#010;&gt;&gt;&#010;&gt;&gt; 'sink.partitioner' = 'round-robin', \\&#010;&gt;&gt;&#010;&gt;&gt; 'format' = 'json')&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 输出的数据格式示例：&#010;&gt;&gt;&#010;&gt;&gt; {\"total_count\":12,\"username\":\"admin\",\"update_time\":\"2020-07-27 17:23:00\"}&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 但是kafka-connect-jdbc的json格式需要schema和payload，示例：&#010;&gt;&gt;&#010;&gt;&gt; {&#010;&gt;&gt;&#010;&gt;&gt;   \"schema\": {&#010;&gt;&gt;&#010;&gt;&gt;     \"type\": \"struct\",&#010;&gt;&gt;&#010;&gt;&gt;     \"fields\": [&#010;&gt;&gt;&#010;&gt;&gt;       {&#010;&gt;&gt;&#010;&gt;&gt;         \"type\": \"int64\",&#010;&gt;&gt;&#010;&gt;&gt;         \"optional\": false,&#010;&gt;&gt;&#010;&gt;&gt;         \"field\": \"id\"&#010;&gt;&gt;&#010;&gt;&gt;       },&#010;&gt;&gt;&#010;&gt;&gt;       {&#010;&gt;&gt;&#010;&gt;&gt;         \"type\": \"string\",&#010;&gt;&gt;&#010;&gt;&gt;         \"optional\": true,&#010;&gt;&gt;&#010;&gt;&gt;         \"field\": \"name\"&#010;&gt;&gt;&#010;&gt;&gt;       }&#010;&gt;&gt;&#010;&gt;&gt;     ],&#010;&gt;&gt;&#010;&gt;&gt;     \"optional\": true,&#010;&gt;&gt;&#010;&gt;&gt;     \"name\": \"user\"&#010;&gt;&gt;&#010;&gt;&gt;   },&#010;&gt;&gt;&#010;&gt;&gt;   \"payload\": {&#010;&gt;&gt;&#010;&gt;&gt;     \"id\": 1,&#010;&gt;&gt;&#010;&gt;&gt;     \"name\": \"admin\"&#010;&gt;&gt;&#010;&gt;&gt;   }&#010;&gt;&gt;&#010;&gt;&gt; }&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 请教下在Flink里面如何处理（补上schema和payload？），才能形成kafka&#010;connect匹配的json格式？&#010;&gt;&gt;&#010;&gt;&gt; 当前Flink处理sql：&#010;&gt;&gt;&#010;&gt;&gt; INSERT INTO test_out(total_count,username,update_time) SELECT count(1) AS&#010;&gt;&gt; total_count,username,TUMBLE_START(update_time,INTERVAL '1' MINUTE) as&#010;&gt;&gt; update_time FROM table1 GROUP BY username,TUMBLE(update_time,INTERVAL '1'&#010;&gt;&gt; MINUTE)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "2",
        "reply": "<7999a396.5bdc.1738f9ce185.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<215c10bc.a3d9.1738fc30bf8.Coremail.wangfei23_job@163.com>",
        "from": "air23 &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 10:13:11 GMT",
        "subject": "Flink Sql 问题",
        "content": "你好 &#010;&#010;",
        "depth": "0",
        "reply": "<215c10bc.a3d9.1738fc30bf8.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>",
        "from": "air23 &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 10:16:43 GMT",
        "subject": "解析kafka的mysql binlog问题",
        "content": "你好。这个是我的解析sql。我想读取binlog的 data数据和table 数据。 为什么可以取到table&#010;不能取到data呢？&#010;&#010;private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;\" `data` VARCHAR , \" +&#010;\" `table` VARCHAR \" +&#010;\") WITH (\" +&#010;\" 'connector' = 'kafka',\" +&#010;\" 'topic' = 'order_source',\" +&#010;\" 'properties.bootstrap.servers' = '***',\" +&#010;\" 'properties.group.id' = 'real1',\" +&#010;\" 'format' = 'json',\" +&#010;\" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&#010;\")\";&#010;&#010;&#010;&#010;&#010;具体见附件 有打印",
        "depth": "0",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<CAELO9317BWi6Sjdr-tsGvk9dBqOK0=9gK_-ER+J7AEi=BymJ2w@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 10:55:44 GMT",
        "subject": "Re: 解析kafka的mysql binlog问题",
        "content": "Hi,&#013;&#010;你的附件好像没有上传。&#013;&#010;&#013;&#010;On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table 数据。&#010;为什么可以取到table 不能取到data呢？*&#013;&#010;&gt;&#013;&#010;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#013;&#010;&gt;         \" `data` VARCHAR , \" +&#013;&#010;&gt;         \" `table` VARCHAR \" +&#013;&#010;&gt;         \") WITH (\" +&#013;&#010;&gt;         \" 'connector' = 'kafka',\" +&#013;&#010;&gt;         \" 'topic' = 'order_source',\" +&#013;&#010;&gt;         \" 'properties.bootstrap.servers' = '***',\" +&#013;&#010;&gt;         \" 'properties.group.id' = 'real1',\" +&#013;&#010;&gt;         \" 'format' = 'json',\" +&#013;&#010;&gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#013;&#010;&gt;         \")\";&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 具体见附件 有打印&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<6c5a18fb.711d.1739001eb4d.Coremail.wangfei23_job@163.com>",
        "from": "air23 &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 11:21:51 GMT",
        "subject": "回复：解析kafka的mysql binlog问题",
        "content": "我再上传一次 &#010;&#010;&#010;在2020年07月27日 18:55，Jark Wu 写道：&#010;Hi,&#010;你的附件好像没有上传。&#010;&#010;On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&#010;&gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table 数据。&#010;为什么可以取到table 不能取到data呢？*&#010;&gt;&#010;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;         \" `data` VARCHAR , \" +&#010;&gt;         \" `table` VARCHAR \" +&#010;&gt;         \") WITH (\" +&#010;&gt;         \" 'connector' = 'kafka',\" +&#010;&gt;         \" 'topic' = 'order_source',\" +&#010;&gt;         \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;         \" 'properties.group.id' = 'real1',\" +&#010;&gt;         \" 'format' = 'json',\" +&#010;&gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;         \")\";&#010;&gt;&#010;&gt;&#010;&gt; 具体见附件 有打印&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;",
        "depth": "2",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<CAELO932Vi=6JMgNX2w7oV+C1W1YwmPiaj3bbNx=E_K2ofVnDNA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 11:44:10 GMT",
        "subject": "Re: 解析kafka的mysql binlog问题",
        "content": "抱歉，还是没有看到附件。&#013;&#010;如果是文本的话，你可以直接贴到邮件里。&#013;&#010;&#013;&#010;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 我再上传一次&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt; Hi,&#013;&#010;&gt; 你的附件好像没有上传。&#013;&#010;&gt;&#013;&#010;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table 数据。&#010;为什么可以取到table 不能取到data呢？*&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#013;&#010;&gt; &gt;         \" `data` VARCHAR , \" +&#013;&#010;&gt; &gt;         \" `table` VARCHAR \" +&#013;&#010;&gt; &gt;         \") WITH (\" +&#013;&#010;&gt; &gt;         \" 'connector' = 'kafka',\" +&#013;&#010;&gt; &gt;         \" 'topic' = 'order_source',\" +&#013;&#010;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#013;&#010;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#013;&#010;&gt; &gt;         \" 'format' = 'json',\" +&#013;&#010;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#013;&#010;&gt; &gt;         \")\";&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 具体见附件 有打印&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<229a0b32.1b16.17393139fb7.Coremail.wangfei23_job@163.com>",
        "from": "air23  &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 01:40:03 GMT",
        "subject": "Re:Re: 解析kafka的mysql binlog问题",
        "content": "你好 测试代码如下&#010;&#010;&#010;private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;\" `data` VARCHAR , \" +&#010;\" `table` VARCHAR \" +&#010;\") WITH (\" +&#010;\" 'connector' = 'kafka',\" +&#010;\" 'topic' = 'source_databases',\" +&#010;\" 'properties.bootstrap.servers' = '***',\" +&#010;\" 'properties.group.id' = 'real1',\" +&#010;\" 'format' = 'json',\" +&#010;\" 'scan.startup.mode' = 'earliest-offset'\" +&#010;\")\";&#010;public static void main(String[] args) throws Exception {&#010;&#010;&#010;//bink table&#010;StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#010;    EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;    StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&#010;    TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&#010;&#010;tableResult.print();&#010;&#010;    Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&#010;bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&#010;bsEnv.execute(\"aa\");&#010;&#010;}&#010;&#010;&#010;&#010;&#010;输出结果如下  data都是空的。数据格式为canal解析的mysql binlog&#010;,order_operation_time&#010;,inventory_batch_log&#010;,order_log&#010;,order_address_book&#010;,product_inventory&#010;,order_physical_relation&#010;,bil_business_attach&#010;,picking_detail&#010;,picking_detail&#010;,orders&#010;&#010;&#010;&#010;&#010;另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;看到例子都是useOldPlanner 来转table的。&#010;致谢&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;抱歉，还是没有看到附件。&#010;&gt;如果是文本的话，你可以直接贴到邮件里。&#010;&gt;&#010;&gt;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; 我再上传一次&#010;&gt;&gt;&#010;&gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&#010;&gt;&gt; Hi,&#010;&gt;&gt; 你的附件好像没有上传。&#010;&gt;&gt;&#010;&gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table 不能取到data呢？*&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;&gt; &gt;         \" `data` VARCHAR , \" +&#010;&gt;&gt; &gt;         \" `table` VARCHAR \" +&#010;&gt;&gt; &gt;         \") WITH (\" +&#010;&gt;&gt; &gt;         \" 'connector' = 'kafka',\" +&#010;&gt;&gt; &gt;         \" 'topic' = 'order_source',\" +&#010;&gt;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt; &gt;         \" 'format' = 'json',\" +&#010;&gt;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt; &gt;         \")\";&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 具体见附件 有打印&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "4",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<CAELO931VKN6R6m1peE+xX9tPi0DaVvF00qVEhVV0j9e4NUU0vA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 06:44:35 GMT",
        "subject": "Re: Re: 解析kafka的mysql binlog问题",
        "content": "有kafka 中json 数据的样例不？&#013;&#010;有没有看过 TaskManager 中有没有异常 log 信息？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 你好 测试代码如下&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#013;&#010;&gt; \" `data` VARCHAR , \" +&#013;&#010;&gt; \" `table` VARCHAR \" +&#013;&#010;&gt; \") WITH (\" +&#013;&#010;&gt; \" 'connector' = 'kafka',\" +&#013;&#010;&gt; \" 'topic' = 'source_databases',\" +&#013;&#010;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#013;&#010;&gt; \" 'properties.group.id' = 'real1',\" +&#013;&#010;&gt; \" 'format' = 'json',\" +&#013;&#010;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#013;&#010;&gt; \")\";&#013;&#010;&gt; public static void main(String[] args) throws Exception {&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; //bink table&#013;&#010;&gt; StreamExecutionEnvironment bsEnv =&#013;&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt;     EnvironmentSettings bsSettings =&#013;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt;     StreamTableEnvironment bsTableEnv =&#013;&#010;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#013;&#010;&gt;&#013;&#010;&gt;     TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; tableResult.print();&#013;&#010;&gt;&#013;&#010;&gt;     Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#013;&#010;&gt;&#013;&#010;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#013;&#010;&gt;&#013;&#010;&gt; bsEnv.execute(\"aa\");&#013;&#010;&gt;&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql binlog&#013;&#010;&gt; ,order_operation_time&#013;&#010;&gt; ,inventory_batch_log&#013;&#010;&gt; ,order_log&#013;&#010;&gt; ,order_address_book&#013;&#010;&gt; ,product_inventory&#013;&#010;&gt; ,order_physical_relation&#013;&#010;&gt; ,bil_business_attach&#013;&#010;&gt; ,picking_detail&#013;&#010;&gt; ,picking_detail&#013;&#010;&gt; ,orders&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#013;&#010;&gt; 看到例子都是useOldPlanner 来转table的。&#013;&#010;&gt; 致谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;抱歉，还是没有看到附件。&#013;&#010;&gt; &gt;如果是文本的话，你可以直接贴到邮件里。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; 我再上传一次&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Hi,&#013;&#010;&gt; &gt;&gt; 你的附件好像没有上传。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table 不能取到data呢？*&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\"&#013;&#010;&gt; +&#013;&#010;&gt; &gt;&gt; &gt;         \" `data` VARCHAR , \" +&#013;&#010;&gt; &gt;&gt; &gt;         \" `table` VARCHAR \" +&#013;&#010;&gt; &gt;&gt; &gt;         \") WITH (\" +&#013;&#010;&gt; &gt;&gt; &gt;         \" 'connector' = 'kafka',\" +&#013;&#010;&gt; &gt;&gt; &gt;         \" 'topic' = 'order_source',\" +&#013;&#010;&gt; &gt;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#013;&#010;&gt; &gt;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#013;&#010;&gt; &gt;&gt; &gt;         \" 'format' = 'json',\" +&#013;&#010;&gt; &gt;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#013;&#010;&gt; &gt;&gt; &gt;         \")\";&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 具体见附件 有打印&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<1a6257e9.5e1a.173945973e5.Coremail.wangfei23_job@163.com>",
        "from": "air23  &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 07:35:57 GMT",
        "subject": "Re:Re: Re: 解析kafka的mysql binlog问题",
        "content": "格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#010;&#010;&#010;{&#010;    \"data\":[&#010;        {&#010;            \"op_id\":\"97037138\",&#010;            \"order_id\":\"84172164\"&#010;        }&#010;    ],&#010;    \"database\":\"order_11\",&#010;    \"es\":1595720375000,&#010;    \"id\":17469027,&#010;    \"isDdl\":false,&#010;    \"mysqlType\":{&#010;        \"op_id\":\"int(11)\",&#010;        \"order_id\":\"int(11)\"&#010;    },&#010;    \"old\":null,&#010;    \"pkNames\":[&#010;        \"op_id\"&#010;    ],&#010;    \"sql\":\"\",&#010;    \"sqlType\":{&#010;        \"op_id\":4,&#010;        \"order_id\":4&#010;    },&#010;    \"table\":\"order_product\",&#010;    \"ts\":1595720375837,&#010;    \"type\":\"INSERT\"&#010;}&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;有kafka 中json 数据的样例不？&#010;&gt;有没有看过 TaskManager 中有没有异常 log 信息？&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; 你好 测试代码如下&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;&gt; \" `data` VARCHAR , \" +&#010;&gt;&gt; \" `table` VARCHAR \" +&#010;&gt;&gt; \") WITH (\" +&#010;&gt;&gt; \" 'connector' = 'kafka',\" +&#010;&gt;&gt; \" 'topic' = 'source_databases',\" +&#010;&gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt; \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt; \" 'format' = 'json',\" +&#010;&gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt; \")\";&#010;&gt;&gt; public static void main(String[] args) throws Exception {&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; //bink table&#010;&gt;&gt; StreamExecutionEnvironment bsEnv =&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;     EnvironmentSettings bsSettings =&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt;     StreamTableEnvironment bsTableEnv =&#010;&gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;&gt;&#010;&gt;&gt;     TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; tableResult.print();&#010;&gt;&gt;&#010;&gt;&gt;     Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&gt;&gt;&#010;&gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&gt;&gt;&#010;&gt;&gt; bsEnv.execute(\"aa\");&#010;&gt;&gt;&#010;&gt;&gt; }&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql binlog&#010;&gt;&gt; ,order_operation_time&#010;&gt;&gt; ,inventory_batch_log&#010;&gt;&gt; ,order_log&#010;&gt;&gt; ,order_address_book&#010;&gt;&gt; ,product_inventory&#010;&gt;&gt; ,order_physical_relation&#010;&gt;&gt; ,bil_business_attach&#010;&gt;&gt; ,picking_detail&#010;&gt;&gt; ,picking_detail&#010;&gt;&gt; ,orders&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;&gt;&gt; 看到例子都是useOldPlanner 来转table的。&#010;&gt;&gt; 致谢&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;抱歉，还是没有看到附件。&#010;&gt;&gt; &gt;如果是文本的话，你可以直接贴到邮件里。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; 我再上传一次&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; Hi,&#010;&gt;&gt; &gt;&gt; 你的附件好像没有上传。&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table 不能取到data呢？*&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\"&#010;&gt;&gt; +&#010;&gt;&gt; &gt;&gt; &gt;         \" `data` VARCHAR , \" +&#010;&gt;&gt; &gt;&gt; &gt;         \" `table` VARCHAR \" +&#010;&gt;&gt; &gt;&gt; &gt;         \") WITH (\" +&#010;&gt;&gt; &gt;&gt; &gt;         \" 'connector' = 'kafka',\" +&#010;&gt;&gt; &gt;&gt; &gt;         \" 'topic' = 'order_source',\" +&#010;&gt;&gt; &gt;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt; &gt;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt; &gt;&gt; &gt;         \" 'format' = 'json',\" +&#010;&gt;&gt; &gt;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt; &gt;&gt; &gt;         \")\";&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; 具体见附件 有打印&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "6",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<CAELO933qPhSeQw69GmYpmV=bxYokhezSP=CukCy_NO3N9CjXFg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 08:02:18 GMT",
        "subject": "Re: Re: Re: 解析kafka的mysql binlog问题",
        "content": "因为 \"data\" 是一个复杂结构，不是单纯的 string 结构。所以1.11为止，这个功能还不支持。&#010;1.12 中已经支持读取复杂结构为 string 类型了。&#010;&#010;Best,&#010;Jark&#010;&#010;On Tue, 28 Jul 2020 at 15:36, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&#010;&gt; 格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#010;&gt;&#010;&gt;&#010;&gt; {&#010;&gt;     \"data\":[&#010;&gt;         {&#010;&gt;             \"op_id\":\"97037138\",&#010;&gt;             \"order_id\":\"84172164\"&#010;&gt;         }&#010;&gt;     ],&#010;&gt;     \"database\":\"order_11\",&#010;&gt;     \"es\":1595720375000,&#010;&gt;     \"id\":17469027,&#010;&gt;     \"isDdl\":false,&#010;&gt;     \"mysqlType\":{&#010;&gt;         \"op_id\":\"int(11)\",&#010;&gt;         \"order_id\":\"int(11)\"&#010;&gt;     },&#010;&gt;     \"old\":null,&#010;&gt;     \"pkNames\":[&#010;&gt;         \"op_id\"&#010;&gt;     ],&#010;&gt;     \"sql\":\"\",&#010;&gt;     \"sqlType\":{&#010;&gt;         \"op_id\":4,&#010;&gt;         \"order_id\":4&#010;&gt;     },&#010;&gt;     \"table\":\"order_product\",&#010;&gt;     \"ts\":1595720375837,&#010;&gt;     \"type\":\"INSERT\"&#010;&gt; }&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt; &gt;有kafka 中json 数据的样例不？&#010;&gt; &gt;有没有看过 TaskManager 中有没有异常 log 信息？&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; 你好 测试代码如下&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt; &gt;&gt; \" `data` VARCHAR , \" +&#010;&gt; &gt;&gt; \" `table` VARCHAR \" +&#010;&gt; &gt;&gt; \") WITH (\" +&#010;&gt; &gt;&gt; \" 'connector' = 'kafka',\" +&#010;&gt; &gt;&gt; \" 'topic' = 'source_databases',\" +&#010;&gt; &gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt; &gt;&gt; \" 'properties.group.id' = 'real1',\" +&#010;&gt; &gt;&gt; \" 'format' = 'json',\" +&#010;&gt; &gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt; &gt;&gt; \")\";&#010;&gt; &gt;&gt; public static void main(String[] args) throws Exception {&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; //bink table&#010;&gt; &gt;&gt; StreamExecutionEnvironment bsEnv =&#010;&gt; &gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt;&gt;     EnvironmentSettings bsSettings =&#010;&gt; &gt;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; &gt;&gt;     StreamTableEnvironment bsTableEnv =&#010;&gt; &gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; tableResult.print();&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;     Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; bsEnv.execute(\"aa\");&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; }&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql binlog&#010;&gt; &gt;&gt; ,order_operation_time&#010;&gt; &gt;&gt; ,inventory_batch_log&#010;&gt; &gt;&gt; ,order_log&#010;&gt; &gt;&gt; ,order_address_book&#010;&gt; &gt;&gt; ,product_inventory&#010;&gt; &gt;&gt; ,order_physical_relation&#010;&gt; &gt;&gt; ,bil_business_attach&#010;&gt; &gt;&gt; ,picking_detail&#010;&gt; &gt;&gt; ,picking_detail&#010;&gt; &gt;&gt; ,orders&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;&gt; &gt;&gt; 看到例子都是useOldPlanner 来转table的。&#010;&gt; &gt;&gt; 致谢&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;抱歉，还是没有看到附件。&#010;&gt; &gt;&gt; &gt;如果是文本的话，你可以直接贴到邮件里。&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; 我再上传一次&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt; &gt;&gt; &gt;&gt; 你的附件好像没有上传。&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table&#010;&gt; 不能取到data呢？*&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable&#010;&gt; (\\n\"&#010;&gt; &gt;&gt; +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" `data` VARCHAR , \" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" `table` VARCHAR \" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \") WITH (\" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'connector' = 'kafka',\" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'topic' = 'order_source',\" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'format' = 'json',\" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt; &gt;&gt; &gt;&gt; &gt;         \")\";&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt; 具体见附件 有打印&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt;&#010;&#010;",
        "depth": "7",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<4999c83c.8247.17394b92d15.Coremail.wangfei23_job@163.com>",
        "from": "air23  &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 09:20:30 GMT",
        "subject": "Re:Re: Re: Re: 解析kafka的mysql binlog问题",
        "content": "你好 收到。谢谢。 因为这个topic 是有很多表的binlog的。所以解析成array&#010;也是不行的。长度和字段都不一致&#010;另外想请教下 1.11 版本  datastream 可以转换为 blink的table吗。 看到例子&#010;好像都是 useOldPlanner 来转的，&#010;但是我想用blink 写入到hive。 我这边需求 就是通过binlog - flink - hive。&#010;不同的mysql表写入到不同hive表。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-28 16:02:18，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;因为 \"data\" 是一个复杂结构，不是单纯的 string 结构。所以1.11为止，这个功能还不支持。&#010;&gt;1.12 中已经支持读取复杂结构为 string 类型了。&#010;&gt;&#010;&gt;Best,&#010;&gt;Jark&#010;&gt;&#010;&gt;On Tue, 28 Jul 2020 at 15:36, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; 格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; {&#010;&gt;&gt;     \"data\":[&#010;&gt;&gt;         {&#010;&gt;&gt;             \"op_id\":\"97037138\",&#010;&gt;&gt;             \"order_id\":\"84172164\"&#010;&gt;&gt;         }&#010;&gt;&gt;     ],&#010;&gt;&gt;     \"database\":\"order_11\",&#010;&gt;&gt;     \"es\":1595720375000,&#010;&gt;&gt;     \"id\":17469027,&#010;&gt;&gt;     \"isDdl\":false,&#010;&gt;&gt;     \"mysqlType\":{&#010;&gt;&gt;         \"op_id\":\"int(11)\",&#010;&gt;&gt;         \"order_id\":\"int(11)\"&#010;&gt;&gt;     },&#010;&gt;&gt;     \"old\":null,&#010;&gt;&gt;     \"pkNames\":[&#010;&gt;&gt;         \"op_id\"&#010;&gt;&gt;     ],&#010;&gt;&gt;     \"sql\":\"\",&#010;&gt;&gt;     \"sqlType\":{&#010;&gt;&gt;         \"op_id\":4,&#010;&gt;&gt;         \"order_id\":4&#010;&gt;&gt;     },&#010;&gt;&gt;     \"table\":\"order_product\",&#010;&gt;&gt;     \"ts\":1595720375837,&#010;&gt;&gt;     \"type\":\"INSERT\"&#010;&gt;&gt; }&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;有kafka 中json 数据的样例不？&#010;&gt;&gt; &gt;有没有看过 TaskManager 中有没有异常 log 信息？&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; 你好 测试代码如下&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;&gt; &gt;&gt; \" `data` VARCHAR , \" +&#010;&gt;&gt; &gt;&gt; \" `table` VARCHAR \" +&#010;&gt;&gt; &gt;&gt; \") WITH (\" +&#010;&gt;&gt; &gt;&gt; \" 'connector' = 'kafka',\" +&#010;&gt;&gt; &gt;&gt; \" 'topic' = 'source_databases',\" +&#010;&gt;&gt; &gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt; &gt;&gt; \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt; &gt;&gt; \" 'format' = 'json',\" +&#010;&gt;&gt; &gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt; &gt;&gt; \")\";&#010;&gt;&gt; &gt;&gt; public static void main(String[] args) throws Exception {&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; //bink table&#010;&gt;&gt; &gt;&gt; StreamExecutionEnvironment bsEnv =&#010;&gt;&gt; &gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt; &gt;&gt;     EnvironmentSettings bsSettings =&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt; &gt;&gt;     StreamTableEnvironment bsTableEnv =&#010;&gt;&gt; &gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; tableResult.print();&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; bsEnv.execute(\"aa\");&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; }&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql&#010;binlog&#010;&gt;&gt; &gt;&gt; ,order_operation_time&#010;&gt;&gt; &gt;&gt; ,inventory_batch_log&#010;&gt;&gt; &gt;&gt; ,order_log&#010;&gt;&gt; &gt;&gt; ,order_address_book&#010;&gt;&gt; &gt;&gt; ,product_inventory&#010;&gt;&gt; &gt;&gt; ,order_physical_relation&#010;&gt;&gt; &gt;&gt; ,bil_business_attach&#010;&gt;&gt; &gt;&gt; ,picking_detail&#010;&gt;&gt; &gt;&gt; ,picking_detail&#010;&gt;&gt; &gt;&gt; ,orders&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;&gt;&gt; &gt;&gt; 看到例子都是useOldPlanner 来转table的。&#010;&gt;&gt; &gt;&gt; 致谢&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&gt; &gt;抱歉，还是没有看到附件。&#010;&gt;&gt; &gt;&gt; &gt;如果是文本的话，你可以直接贴到邮件里。&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 我再上传一次&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Hi,&#010;&gt;&gt; &gt;&gt; &gt;&gt; 你的附件好像没有上传。&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt;&#010;wrote:&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的&#010;data数据和table 数据。 为什么可以取到table&#010;&gt;&gt; 不能取到data呢？*&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable&#010;&gt;&gt; (\\n\"&#010;&gt;&gt; &gt;&gt; +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" `data` VARCHAR , \" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" `table` VARCHAR \" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \") WITH (\" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'connector' = 'kafka',\" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'topic' = 'order_source',\" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'format' = 'json',\" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;         \")\";&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt; 具体见附件 有打印&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "8",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<FF8B2E11-B428-4715-8D3C-018348430F6D@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 02:49:44 GMT",
        "subject": "Re: 解析kafka的mysql binlog问题",
        "content": "直接转成string1.11版本还不支持，会在1.12修复，参考jira[1]&#010;&#010;[1]https://issues.apache.org/jira/browse/FLINK-18002 &lt;https://issues.apache.org/jira/browse/FLINK-18002&gt;&#010;&#010;&gt; 2020年7月28日 下午5:20，air23 &lt;wangfei23_job@163.com&gt; 写道：&#010;&gt; &#010;&gt; 你好 收到。谢谢。 因为这个topic 是有很多表的binlog的。所以解析成array&#010;也是不行的。长度和字段都不一致&#010;&gt; 另外想请教下 1.11 版本  datastream 可以转换为 blink的table吗。 看到例子&#010;好像都是 useOldPlanner 来转的，&#010;&gt; 但是我想用blink 写入到hive。 我这边需求 就是通过binlog - flink - hive。&#010;不同的mysql表写入到不同hive表。&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在 2020-07-28 16:02:18，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; 因为 \"data\" 是一个复杂结构，不是单纯的 string 结构。所以1.11为止，这个功能还不支持。&#010;&gt;&gt; 1.12 中已经支持读取复杂结构为 string 类型了。&#010;&gt;&gt; &#010;&gt;&gt; Best,&#010;&gt;&gt; Jark&#010;&gt;&gt; &#010;&gt;&gt; On Tue, 28 Jul 2020 at 15:36, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt; &#010;&gt;&gt;&gt; 格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; {&#010;&gt;&gt;&gt;    \"data\":[&#010;&gt;&gt;&gt;        {&#010;&gt;&gt;&gt;            \"op_id\":\"97037138\",&#010;&gt;&gt;&gt;            \"order_id\":\"84172164\"&#010;&gt;&gt;&gt;        }&#010;&gt;&gt;&gt;    ],&#010;&gt;&gt;&gt;    \"database\":\"order_11\",&#010;&gt;&gt;&gt;    \"es\":1595720375000,&#010;&gt;&gt;&gt;    \"id\":17469027,&#010;&gt;&gt;&gt;    \"isDdl\":false,&#010;&gt;&gt;&gt;    \"mysqlType\":{&#010;&gt;&gt;&gt;        \"op_id\":\"int(11)\",&#010;&gt;&gt;&gt;        \"order_id\":\"int(11)\"&#010;&gt;&gt;&gt;    },&#010;&gt;&gt;&gt;    \"old\":null,&#010;&gt;&gt;&gt;    \"pkNames\":[&#010;&gt;&gt;&gt;        \"op_id\"&#010;&gt;&gt;&gt;    ],&#010;&gt;&gt;&gt;    \"sql\":\"\",&#010;&gt;&gt;&gt;    \"sqlType\":{&#010;&gt;&gt;&gt;        \"op_id\":4,&#010;&gt;&gt;&gt;        \"order_id\":4&#010;&gt;&gt;&gt;    },&#010;&gt;&gt;&gt;    \"table\":\"order_product\",&#010;&gt;&gt;&gt;    \"ts\":1595720375837,&#010;&gt;&gt;&gt;    \"type\":\"INSERT\"&#010;&gt;&gt;&gt; }&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt; 有kafka 中json 数据的样例不？&#010;&gt;&gt;&gt;&gt; 有没有看过 TaskManager 中有没有异常 log 信息？&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 你好 测试代码如下&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\"&#010;+&#010;&gt;&gt;&gt;&gt;&gt; \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt;&gt;&gt; \" `table` VARCHAR \" +&#010;&gt;&gt;&gt;&gt;&gt; \") WITH (\" +&#010;&gt;&gt;&gt;&gt;&gt; \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt;&gt;&gt; \" 'topic' = 'source_databases',\" +&#010;&gt;&gt;&gt;&gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt;&gt;&gt; \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt;&gt;&gt; \" 'format' = 'json',\" +&#010;&gt;&gt;&gt;&gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt;&gt;&gt; \")\";&#010;&gt;&gt;&gt;&gt;&gt; public static void main(String[] args) throws Exception {&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; //bink table&#010;&gt;&gt;&gt;&gt;&gt; StreamExecutionEnvironment bsEnv =&#010;&gt;&gt;&gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;&gt;&gt;&gt;    EnvironmentSettings bsSettings =&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt;&gt;&gt;&gt;    StreamTableEnvironment bsTableEnv =&#010;&gt;&gt;&gt;&gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;    TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; tableResult.print();&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;    Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; bsEnv.execute(\"aa\");&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; }&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql&#010;binlog&#010;&gt;&gt;&gt;&gt;&gt; ,order_operation_time&#010;&gt;&gt;&gt;&gt;&gt; ,inventory_batch_log&#010;&gt;&gt;&gt;&gt;&gt; ,order_log&#010;&gt;&gt;&gt;&gt;&gt; ,order_address_book&#010;&gt;&gt;&gt;&gt;&gt; ,product_inventory&#010;&gt;&gt;&gt;&gt;&gt; ,order_physical_relation&#010;&gt;&gt;&gt;&gt;&gt; ,bil_business_attach&#010;&gt;&gt;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt;&gt;&gt; ,orders&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;&gt;&gt;&gt;&gt;&gt; 看到例子都是useOldPlanner 来转table的。&#010;&gt;&gt;&gt;&gt;&gt; 致谢&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt;&gt;&gt; 抱歉，还是没有看到附件。&#010;&gt;&gt;&gt;&gt;&gt;&gt; 如果是文本的话，你可以直接贴到邮件里。&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt;&#010;wrote:&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 我再上传一次&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt;&#010;写道：&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Hi,&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 你的附件好像没有上传。&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt;&#010;wrote:&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; *你好。这个是我的解析sql。我想读取binlog的&#010;data数据和table 数据。 为什么可以取到table&#010;&gt;&gt;&gt; 不能取到data呢？*&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable&#010;&gt;&gt;&gt; (\\n\"&#010;&gt;&gt;&gt;&gt;&gt; +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" `table` VARCHAR \" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \") WITH (\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'topic' = 'order_source',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'format' = 'json',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \")\";&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 具体见附件 有打印&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&#010;&#010;",
        "depth": "9",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<tencent_16CC38230E2C6DA10D03AA2F31E0426F1609@qq.com>",
        "from": "明启 孙 &lt;374060...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 08:02:30 GMT",
        "subject": "回复: Re: Re: 解析kafka的mysql binlog问题",
        "content": "你的flink什么版本&#013;&#010;&#013;&#010;发送自 Windows 10 版邮件应用&#013;&#010;&#013;&#010;发件人: air23&#013;&#010;发送时间: 2020年7月28日 15:36&#013;&#010;收件人: user-zh@flink.apache.org&#013;&#010;主题: Re:Re: Re: 解析kafka的mysql binlog问题&#013;&#010;&#013;&#010;格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#013;&#010;&#013;&#010;&#013;&#010;{&#013;&#010;    \"data\":[&#013;&#010;        {&#013;&#010;            \"op_id\":\"97037138\",&#013;&#010;            \"order_id\":\"84172164\"&#013;&#010;        }&#013;&#010;    ],&#013;&#010;    \"database\":\"order_11\",&#013;&#010;    \"es\":1595720375000,&#013;&#010;    \"id\":17469027,&#013;&#010;    \"isDdl\":false,&#013;&#010;    \"mysqlType\":{&#013;&#010;        \"op_id\":\"int(11)\",&#013;&#010;        \"order_id\":\"int(11)\"&#013;&#010;    },&#013;&#010;    \"old\":null,&#013;&#010;    \"pkNames\":[&#013;&#010;        \"op_id\"&#013;&#010;    ],&#013;&#010;    \"sql\":\"\",&#013;&#010;    \"sqlType\":{&#013;&#010;        \"op_id\":4,&#013;&#010;        \"order_id\":4&#013;&#010;    },&#013;&#010;    \"table\":\"order_product\",&#013;&#010;    \"ts\":1595720375837,&#013;&#010;    \"type\":\"INSERT\"&#013;&#010;}&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt;有kafka 中json 数据的样例不？&#013;&#010;&gt;有没有看过 TaskManager 中有没有异常 log 信息？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt;&gt; 你好 测试代码如下&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#013;&#010;&gt;&gt; \" `data` VARCHAR , \" +&#013;&#010;&gt;&gt; \" `table` VARCHAR \" +&#013;&#010;&gt;&gt; \") WITH (\" +&#013;&#010;&gt;&gt; \" 'connector' = 'kafka',\" +&#013;&#010;&gt;&gt; \" 'topic' = 'source_databases',\" +&#013;&#010;&gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#013;&#010;&gt;&gt; \" 'properties.group.id' = 'real1',\" +&#013;&#010;&gt;&gt; \" 'format' = 'json',\" +&#013;&#010;&gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#013;&#010;&gt;&gt; \")\";&#013;&#010;&gt;&gt; public static void main(String[] args) throws Exception {&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; //bink table&#013;&#010;&gt;&gt; StreamExecutionEnvironment bsEnv =&#013;&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt;&gt;     EnvironmentSettings bsSettings =&#013;&#010;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt;&gt;     StreamTableEnvironment bsTableEnv =&#013;&#010;&gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;     TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; tableResult.print();&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;     Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; bsEnv.execute(\"aa\");&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; }&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql binlog&#013;&#010;&gt;&gt; ,order_operation_time&#013;&#010;&gt;&gt; ,inventory_batch_log&#013;&#010;&gt;&gt; ,order_log&#013;&#010;&gt;&gt; ,order_address_book&#013;&#010;&gt;&gt; ,product_inventory&#013;&#010;&gt;&gt; ,order_physical_relation&#013;&#010;&gt;&gt; ,bil_business_attach&#013;&#010;&gt;&gt; ,picking_detail&#013;&#010;&gt;&gt; ,picking_detail&#013;&#010;&gt;&gt; ,orders&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#013;&#010;&gt;&gt; 看到例子都是useOldPlanner 来转table的。&#013;&#010;&gt;&gt; 致谢&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt;&gt; &gt;抱歉，还是没有看到附件。&#013;&#010;&gt;&gt; &gt;如果是文本的话，你可以直接贴到邮件里。&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; 我再上传一次&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; Hi,&#013;&#010;&gt;&gt; &gt;&gt; 你的附件好像没有上传。&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table 不能取到data呢？*&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\"&#013;&#010;&gt;&gt; +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" `data` VARCHAR , \" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" `table` VARCHAR \" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \") WITH (\" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" 'connector' = 'kafka',\" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" 'topic' = 'order_source',\" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" 'format' = 'json',\" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#013;&#010;&gt;&gt; &gt;&gt; &gt;         \")\";&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt; 具体见附件 有打印&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&#013;&#010;",
        "depth": "7",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<30281f72.813f.17394b6669a.Coremail.wangfei23_job@163.com>",
        "from": "air23  &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 09:17:28 GMT",
        "subject": "Re:回复: Re: Re: 解析kafka的mysql binlog问题",
        "content": "你好 使用的是&lt;flink.version&gt;1.11.1&lt;/flink.version&gt;版本的 &#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-28 16:02:30，\"明启 孙\" &lt;374060171@qq.com&gt; 写道：&#010;&gt;你的flink什么版本&#010;&gt;&#010;&gt;发送自 Windows 10 版邮件应用&#010;&gt;&#010;&gt;发件人: air23&#010;&gt;发送时间: 2020年7月28日 15:36&#010;&gt;收件人: user-zh@flink.apache.org&#010;&gt;主题: Re:Re: Re: 解析kafka的mysql binlog问题&#010;&gt;&#010;&gt;格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#010;&gt;&#010;&gt;&#010;&gt;{&#010;&gt;    \"data\":[&#010;&gt;        {&#010;&gt;            \"op_id\":\"97037138\",&#010;&gt;            \"order_id\":\"84172164\"&#010;&gt;        }&#010;&gt;    ],&#010;&gt;    \"database\":\"order_11\",&#010;&gt;    \"es\":1595720375000,&#010;&gt;    \"id\":17469027,&#010;&gt;    \"isDdl\":false,&#010;&gt;    \"mysqlType\":{&#010;&gt;        \"op_id\":\"int(11)\",&#010;&gt;        \"order_id\":\"int(11)\"&#010;&gt;    },&#010;&gt;    \"old\":null,&#010;&gt;    \"pkNames\":[&#010;&gt;        \"op_id\"&#010;&gt;    ],&#010;&gt;    \"sql\":\"\",&#010;&gt;    \"sqlType\":{&#010;&gt;        \"op_id\":4,&#010;&gt;        \"order_id\":4&#010;&gt;    },&#010;&gt;    \"table\":\"order_product\",&#010;&gt;    \"ts\":1595720375837,&#010;&gt;    \"type\":\"INSERT\"&#010;&gt;}&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;有kafka 中json 数据的样例不？&#010;&gt;&gt;有没有看过 TaskManager 中有没有异常 log 信息？&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt;&gt; 你好 测试代码如下&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;&gt;&gt; \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt; \" `table` VARCHAR \" +&#010;&gt;&gt;&gt; \") WITH (\" +&#010;&gt;&gt;&gt; \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt; \" 'topic' = 'source_databases',\" +&#010;&gt;&gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt; \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt; \" 'format' = 'json',\" +&#010;&gt;&gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt; \")\";&#010;&gt;&gt;&gt; public static void main(String[] args) throws Exception {&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; //bink table&#010;&gt;&gt;&gt; StreamExecutionEnvironment bsEnv =&#010;&gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;&gt;     EnvironmentSettings bsSettings =&#010;&gt;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt;&gt;     StreamTableEnvironment bsTableEnv =&#010;&gt;&gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;     TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; tableResult.print();&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;     Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; bsEnv.execute(\"aa\");&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; }&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql binlog&#010;&gt;&gt;&gt; ,order_operation_time&#010;&gt;&gt;&gt; ,inventory_batch_log&#010;&gt;&gt;&gt; ,order_log&#010;&gt;&gt;&gt; ,order_address_book&#010;&gt;&gt;&gt; ,product_inventory&#010;&gt;&gt;&gt; ,order_physical_relation&#010;&gt;&gt;&gt; ,bil_business_attach&#010;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt; ,orders&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;&gt;&gt;&gt; 看到例子都是useOldPlanner 来转table的。&#010;&gt;&gt;&gt; 致谢&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; &gt;抱歉，还是没有看到附件。&#010;&gt;&gt;&gt; &gt;如果是文本的话，你可以直接贴到邮件里。&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; 我再上传一次&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt; Hi,&#010;&gt;&gt;&gt; &gt;&gt; 你的附件好像没有上传。&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table 不能取到data呢？*&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable&#010;(\\n\"&#010;&gt;&gt;&gt; +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" `table` VARCHAR \" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \") WITH (\" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" 'topic' = 'order_source',\" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" 'format' = 'json',\" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt; &gt;&gt; &gt;         \")\";&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt; 具体见附件 有打印&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&#010;",
        "depth": "8",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<DC3373A4-7066-4B39-BB26-55B90D6BAA06@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 08:05:55 GMT",
        "subject": "Re: 解析kafka的mysql binlog问题",
        "content": "data格式不是string，可以定义为ARRAY&lt;ROW&lt; op_id STRING, order_id STRING&gt;&gt;&#010;&#010;&gt; 2020年7月28日 下午3:35，air23 &lt;wangfei23_job@163.com&gt; 写道：&#010;&gt; &#010;&gt; 格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#010;&gt; &#010;&gt; &#010;&gt; {&#010;&gt;    \"data\":[&#010;&gt;        {&#010;&gt;            \"op_id\":\"97037138\",&#010;&gt;            \"order_id\":\"84172164\"&#010;&gt;        }&#010;&gt;    ],&#010;&gt;    \"database\":\"order_11\",&#010;&gt;    \"es\":1595720375000,&#010;&gt;    \"id\":17469027,&#010;&gt;    \"isDdl\":false,&#010;&gt;    \"mysqlType\":{&#010;&gt;        \"op_id\":\"int(11)\",&#010;&gt;        \"order_id\":\"int(11)\"&#010;&gt;    },&#010;&gt;    \"old\":null,&#010;&gt;    \"pkNames\":[&#010;&gt;        \"op_id\"&#010;&gt;    ],&#010;&gt;    \"sql\":\"\",&#010;&gt;    \"sqlType\":{&#010;&gt;        \"op_id\":4,&#010;&gt;        \"order_id\":4&#010;&gt;    },&#010;&gt;    \"table\":\"order_product\",&#010;&gt;    \"ts\":1595720375837,&#010;&gt;    \"type\":\"INSERT\"&#010;&gt; }&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt; 有kafka 中json 数据的样例不？&#010;&gt;&gt; 有没有看过 TaskManager 中有没有异常 log 信息？&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt; &#010;&gt;&gt;&gt; 你好 测试代码如下&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;&gt;&gt; \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt; \" `table` VARCHAR \" +&#010;&gt;&gt;&gt; \") WITH (\" +&#010;&gt;&gt;&gt; \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt; \" 'topic' = 'source_databases',\" +&#010;&gt;&gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt; \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt; \" 'format' = 'json',\" +&#010;&gt;&gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt; \")\";&#010;&gt;&gt;&gt; public static void main(String[] args) throws Exception {&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; //bink table&#010;&gt;&gt;&gt; StreamExecutionEnvironment bsEnv =&#010;&gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;&gt;    EnvironmentSettings bsSettings =&#010;&gt;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt;&gt;    StreamTableEnvironment bsTableEnv =&#010;&gt;&gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; tableResult.print();&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;    Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; bsEnv.execute(\"aa\");&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; }&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql binlog&#010;&gt;&gt;&gt; ,order_operation_time&#010;&gt;&gt;&gt; ,inventory_batch_log&#010;&gt;&gt;&gt; ,order_log&#010;&gt;&gt;&gt; ,order_address_book&#010;&gt;&gt;&gt; ,product_inventory&#010;&gt;&gt;&gt; ,order_physical_relation&#010;&gt;&gt;&gt; ,bil_business_attach&#010;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt; ,orders&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;&gt;&gt;&gt; 看到例子都是useOldPlanner 来转table的。&#010;&gt;&gt;&gt; 致谢&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt; 抱歉，还是没有看到附件。&#010;&gt;&gt;&gt;&gt; 如果是文本的话，你可以直接贴到邮件里。&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 我再上传一次&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; Hi,&#010;&gt;&gt;&gt;&gt;&gt; 你的附件好像没有上传。&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table 不能取到data呢？*&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable&#010;(\\n\"&#010;&gt;&gt;&gt; +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" `table` VARCHAR \" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \") WITH (\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'topic' = 'order_source',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'format' = 'json',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;        \")\";&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 具体见附件 有打印&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&#010;&#010;",
        "depth": "7",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<5c181a39.8111.17394b5d5bb.Coremail.wangfei23_job@163.com>",
        "from": "air23  &lt;wangfei23_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 09:16:51 GMT",
        "subject": "Re:Re: 解析kafka的mysql binlog问题",
        "content": "你好。&#010;我猜测 是有可能是这个问题。但是我这个topic是 读取的一个库的binlog。有很多表&#010;  所以ARRAY&lt;ROW&lt; op_id STRING, order_id STRING&gt;&gt;&#010;这种 里面 不是固定的&#010; 所以我想用datastream 解析 然后在根据表不同 解析成不同的table。但是发现blinkplaner&#010;好像不可以datastream 转换为table。或者是我没有发现这个例子&#010;谢谢&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-28 16:05:55，\"admin\" &lt;17626017841@163.com&gt; 写道：&#010;&gt;data格式不是string，可以定义为ARRAY&lt;ROW&lt; op_id STRING, order_id STRING&gt;&gt;&#010;&gt;&#010;&gt;&gt; 2020年7月28日 下午3:35，air23 &lt;wangfei23_job@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; 格式如下 是canal解析的binlog 格式 data下面是一个数组样式。就是用string取不出来&#010;其他都可以取的出来&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; {&#010;&gt;&gt;    \"data\":[&#010;&gt;&gt;        {&#010;&gt;&gt;            \"op_id\":\"97037138\",&#010;&gt;&gt;            \"order_id\":\"84172164\"&#010;&gt;&gt;        }&#010;&gt;&gt;    ],&#010;&gt;&gt;    \"database\":\"order_11\",&#010;&gt;&gt;    \"es\":1595720375000,&#010;&gt;&gt;    \"id\":17469027,&#010;&gt;&gt;    \"isDdl\":false,&#010;&gt;&gt;    \"mysqlType\":{&#010;&gt;&gt;        \"op_id\":\"int(11)\",&#010;&gt;&gt;        \"order_id\":\"int(11)\"&#010;&gt;&gt;    },&#010;&gt;&gt;    \"old\":null,&#010;&gt;&gt;    \"pkNames\":[&#010;&gt;&gt;        \"op_id\"&#010;&gt;&gt;    ],&#010;&gt;&gt;    \"sql\":\"\",&#010;&gt;&gt;    \"sqlType\":{&#010;&gt;&gt;        \"op_id\":4,&#010;&gt;&gt;        \"order_id\":4&#010;&gt;&gt;    },&#010;&gt;&gt;    \"table\":\"order_product\",&#010;&gt;&gt;    \"ts\":1595720375837,&#010;&gt;&gt;    \"type\":\"INSERT\"&#010;&gt;&gt; }&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 在 2020-07-28 14:44:35，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; 有kafka 中json 数据的样例不？&#010;&gt;&gt;&gt; 有没有看过 TaskManager 中有没有异常 log 信息？&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; On Tue, 28 Jul 2020 at 09:40, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 你好 测试代码如下&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;&gt;&gt;&gt; \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt;&gt; \" `table` VARCHAR \" +&#010;&gt;&gt;&gt;&gt; \") WITH (\" +&#010;&gt;&gt;&gt;&gt; \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt;&gt; \" 'topic' = 'source_databases',\" +&#010;&gt;&gt;&gt;&gt; \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt;&gt; \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt;&gt; \" 'format' = 'json',\" +&#010;&gt;&gt;&gt;&gt; \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt;&gt; \")\";&#010;&gt;&gt;&gt;&gt; public static void main(String[] args) throws Exception {&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; //bink table&#010;&gt;&gt;&gt;&gt; StreamExecutionEnvironment bsEnv =&#010;&gt;&gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;&gt;&gt;    EnvironmentSettings bsSettings =&#010;&gt;&gt;&gt;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;&gt;&gt;&gt;    StreamTableEnvironment bsTableEnv =&#010;&gt;&gt;&gt;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;    TableResult tableResult = bsTableEnv.executeSql(KAFKA_SQL);&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; tableResult.print();&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;    Table table = bsTableEnv.sqlQuery(\"select * from kafkaTable\");&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; bsTableEnv.toAppendStream(table, Row.class).print().setParallelism(1);&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; bsEnv.execute(\"aa\");&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; }&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 输出结果如下  data都是空的。数据格式为canal解析的mysql&#010;binlog&#010;&gt;&gt;&gt;&gt; ,order_operation_time&#010;&gt;&gt;&gt;&gt; ,inventory_batch_log&#010;&gt;&gt;&gt;&gt; ,order_log&#010;&gt;&gt;&gt;&gt; ,order_address_book&#010;&gt;&gt;&gt;&gt; ,product_inventory&#010;&gt;&gt;&gt;&gt; ,order_physical_relation&#010;&gt;&gt;&gt;&gt; ,bil_business_attach&#010;&gt;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt;&gt; ,picking_detail&#010;&gt;&gt;&gt;&gt; ,orders&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 另外再问个问题。1.11版本 blink 不能datastream转table吗？&#010;&gt;&gt;&gt;&gt; 看到例子都是useOldPlanner 来转table的。&#010;&gt;&gt;&gt;&gt; 致谢&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 在 2020-07-27 19:44:10，\"Jark Wu\" &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt;&gt; 抱歉，还是没有看到附件。&#010;&gt;&gt;&gt;&gt;&gt; 如果是文本的话，你可以直接贴到邮件里。&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt; On Mon, 27 Jul 2020 at 19:22, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 我再上传一次&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; 在2020年07月27日 18:55，Jark Wu &lt;imjark@gmail.com&gt; 写道：&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; Hi,&#010;&gt;&gt;&gt;&gt;&gt;&gt; 你的附件好像没有上传。&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt;&#010;wrote:&#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table&#010;数据。 为什么可以取到table 不能取到data呢？*&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable&#010;(\\n\"&#010;&gt;&gt;&gt;&gt; +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" `data` VARCHAR , \" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" `table` VARCHAR \" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \") WITH (\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'connector' = 'kafka',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'topic' = 'order_source',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'properties.group.id' = 'real1',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'format' = 'json',\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt;        \")\";&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 具体见附件 有打印&#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; &#010;",
        "depth": "8",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<79288aa7.66d0.173902c7abf.Coremail.tinyshrimp@163.com>",
        "from": "RS  &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 12:08:20 GMT",
        "subject": "Re:回复：解析kafka的mysql binlog问题",
        "content": "Hi，&#010;附近应该是收不到的，包括图片啥的&#010;只能回复纯文本，贴代码，如果真的需要图片的话，可以上传到其他的网站上，然后给个连接跳转过去&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-27 19:21:51，\"air23\" &lt;wangfei23_job@163.com&gt; 写道：&#010;&#010;我再上传一次 &#010;&#010;&#010;在2020年07月27日 18:55，Jark Wu 写道：&#010;Hi,&#010;你的附件好像没有上传。&#010;&#010;On Mon, 27 Jul 2020 at 18:17, air23 &lt;wangfei23_job@163.com&gt; wrote:&#010;&#010;&gt; *你好。这个是我的解析sql。我想读取binlog的 data数据和table 数据。&#010;为什么可以取到table 不能取到data呢？*&#010;&gt;&#010;&gt; private static final String KAFKA_SQL = \"CREATE TABLE kafkaTable (\\n\" +&#010;&gt;         \" `data` VARCHAR , \" +&#010;&gt;         \" `table` VARCHAR \" +&#010;&gt;         \") WITH (\" +&#010;&gt;         \" 'connector' = 'kafka',\" +&#010;&gt;         \" 'topic' = 'order_source',\" +&#010;&gt;         \" 'properties.bootstrap.servers' = '***',\" +&#010;&gt;         \" 'properties.group.id' = 'real1',\" +&#010;&gt;         \" 'format' = 'json',\" +&#010;&gt;         \" 'scan.startup.mode' = 'earliest-offset'\" +&#010;&gt;         \")\";&#010;&gt;&#010;&gt;&#010;&gt; 具体见附件 有打印&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;",
        "depth": "3",
        "reply": "<72e19718.a447.1738fc6480a.Coremail.wangfei23_job@163.com>"
    },
    {
        "id": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 10:49:22 GMT",
        "subject": "flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "Hi,all：&#013;&#010;         本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#010;catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#013;&#010;        Caused by: java.lang.NullPointerException&#013;&#010;  at java.util.Objects.requireNonNull(Objects.java:203)&#013;&#010;  at org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#013;&#010;  at org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#013;&#010;  at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#013;&#010;  at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#013;&#010;  at org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#013;&#010;  at org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#013;&#010;  at org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#013;&#010;  at org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#013;&#010;  at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#013;&#010;  at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#013;&#010;  at org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#013;&#010;  at org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#013;&#010;  at org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#013;&#010;  at org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#013;&#010;  at org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#013;&#010;  at org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#013;&#010;  at org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#013;&#010;  at org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#013;&#010;  at org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#013;&#010;  at org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#013;&#010;  at org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#013;&#010;  at org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#013;&#010;  at org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#013;&#010;  at org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#013;&#010;  at org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#013;&#010;  at org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#013;&#010;  at org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#013;&#010;  at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#013;&#010;  at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#013;&#010;  at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;  at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#013;&#010;  ... 54 more&#013;&#010;",
        "depth": "0",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB4294CB2F239DFED60599E46DD4720@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 10:55:34 GMT",
        "subject": "回复: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "补充一下，执行的sql如下：&#013;&#010;&#013;&#010;select order_no, order_time from x.ods.ods_binlog_test_trip_create_t_order_1&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#013;&#010;发送时间: 2020年7月27日 18:49&#013;&#010;收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&#013;&#010;Hi,all：&#013;&#010;         本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#010;catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#013;&#010;        Caused by: java.lang.NullPointerException&#013;&#010;  at java.util.Objects.requireNonNull(Objects.java:203)&#013;&#010;  at org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#013;&#010;  at org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#013;&#010;  at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#013;&#010;  at org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#013;&#010;  at org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#013;&#010;  at org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#013;&#010;  at org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#013;&#010;  at org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#013;&#010;  at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#013;&#010;  at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#013;&#010;  at org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;  at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#013;&#010;  at org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#013;&#010;  at org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#013;&#010;  at org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#013;&#010;  at org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#013;&#010;  at org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#013;&#010;  at org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#013;&#010;  at org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#013;&#010;  at org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#013;&#010;  at org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#013;&#010;  at org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#013;&#010;  at org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#013;&#010;  at org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#013;&#010;  at org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#013;&#010;  at org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#013;&#010;  at org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#013;&#010;  at org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#013;&#010;  at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#013;&#010;  at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#013;&#010;  at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;  at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#013;&#010;  at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#013;&#010;  at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#013;&#010;  ... 54 more&#013;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CADQYLGt3nrP=4baTahfmvCmqtNHKxaX+RxeyVdpPPPTyoMm2Mw@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 01:55:17 GMT",
        "subject": "Re: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "hi 能给出详细的schema信息吗？&#010;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一 下午7:02写道：&#010;&#010;&gt; 补充一下，执行的sql如下：&#010;&gt;&#010;&gt; select order_no, order_time from&#010;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#010;&gt;&#010;&gt; ________________________________&#010;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#010;&gt; 发送时间: 2020年7月27日 18:49&#010;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#010;&gt;&#010;&gt; Hi,all：&#010;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#010;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#010;&gt;         Caused by: java.lang.NullPointerException&#010;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#010;&gt;   at&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#010;&gt;   at&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#010;&gt;   at&#010;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#010;&gt;   at&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#010;&gt;   at&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#010;&gt;   at&#010;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#010;&gt;   at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#010;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#010;&gt;   at org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#010;&gt;   at&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#010;&gt;   at&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#010;&gt;   at&#010;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#010;&gt;   at&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#010;&gt;   at&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#010;&gt;   at&#010;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#010;&gt;   at&#010;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#010;&gt;   at org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt;   at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt;   ... 54 more&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CADQYLGv5r-c=mntVcQ19WNWEkHy51pqB_-xzqUvL7MSxxNe0CQ@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 01:58:27 GMT",
        "subject": "Re: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "你们是否在多线程环境下使用 TableEnvironment ?&#010;TableEnvironment 不是线程安全的，多线程情况使用可能出现一些莫名其妙的问题。&#010;&#010;godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月28日周二 上午9:55写道：&#010;&#010;&gt; hi 能给出详细的schema信息吗？&#010;&gt;&#010;&gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一&#010;&gt; 下午7:02写道：&#010;&gt;&#010;&gt;&gt; 补充一下，执行的sql如下：&#010;&gt;&gt;&#010;&gt;&gt; select order_no, order_time from&#010;&gt;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#010;&gt;&gt;&#010;&gt;&gt; ________________________________&#010;&gt;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#010;&gt;&gt; 发送时间: 2020年7月27日 18:49&#010;&gt;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#010;&gt;&gt;&#010;&gt;&gt; Hi,all：&#010;&gt;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#010;&gt;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#010;&gt;&gt;         Caused by: java.lang.NullPointerException&#010;&gt;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#010;&gt;&gt;   at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#010;&gt;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt;&gt;   at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt;&gt;   ... 54 more&#010;&gt;&gt;&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB429469AA4B3DEAEBAA43C025D4730@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 03:02:06 GMT",
        "subject": "回复: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "不是多线程同时操作一个tableEnvironment，每执行一次都会创建一个TableEnvironment&#013;&#010;________________________________&#013;&#010;发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;发送时间: 2020年7月28日 9:58&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&#013;&#010;你们是否在多线程环境下使用 TableEnvironment ?&#013;&#010;TableEnvironment 不是线程安全的，多线程情况使用可能出现一些莫名其妙的问题。&#013;&#010;&#013;&#010;godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月28日周二 上午9:55写道：&#013;&#010;&#013;&#010;&gt; hi 能给出详细的schema信息吗？&#013;&#010;&gt;&#013;&#010;&gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一&#013;&#010;&gt; 下午7:02写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 补充一下，执行的sql如下：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; select order_no, order_time from&#013;&#010;&gt;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; ________________________________&#013;&#010;&gt;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#013;&#010;&gt;&gt; 发送时间: 2020年7月27日 18:49&#013;&#010;&gt;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Hi,all：&#013;&#010;&gt;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#013;&#010;&gt;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#013;&#010;&gt;&gt;         Caused by: java.lang.NullPointerException&#013;&#010;&gt;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#013;&#010;&gt;&gt;   at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#013;&#010;&gt;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#013;&#010;&gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#013;&#010;&gt;&gt;   at&#013;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#013;&#010;&gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#013;&#010;&gt;&gt;   ... 54 more&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CADQYLGvqk4bdRHr_KmHuKR4+rP3TTNXrJ60n-Jzvs=c77om94A@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 05:55:06 GMT",
        "subject": "Re: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "我的怀疑点还是多线程引起的。&#010;你能具体描述一下你们gateway的行为吗？ 是一个web server？&#010;&#010;另外，你可以在table env执行query前加上&#010;RelMetadataQueryBase.THREAD_PROVIDERS&#010;.set(JaninoRelMetadataProvider.of(FlinkDefaultRelMetadataProvider.INSTANCE()));&#010;这句话临时fix。&#010;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月28日周二 上午11:02写道：&#010;&#010;&gt; 不是多线程同时操作一个tableEnvironment，每执行一次都会创建一个TableEnvironment&#010;&gt; ________________________________&#010;&gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#010;&gt; 发送时间: 2020年7月28日 9:58&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt; 主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#010;&gt;&#010;&gt; 你们是否在多线程环境下使用 TableEnvironment ?&#010;&gt; TableEnvironment 不是线程安全的，多线程情况使用可能出现一些莫名其妙的问题。&#010;&gt;&#010;&gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月28日周二 上午9:55写道：&#010;&gt;&#010;&gt; &gt; hi 能给出详细的schema信息吗？&#010;&gt; &gt;&#010;&gt; &gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一&#010;&gt; &gt; 下午7:02写道：&#010;&gt; &gt;&#010;&gt; &gt;&gt; 补充一下，执行的sql如下：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; select order_no, order_time from&#010;&gt; &gt;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; ________________________________&#010;&gt; &gt;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#010;&gt; &gt;&gt; 发送时间: 2020年7月27日 18:49&#010;&gt; &gt;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Hi,all：&#010;&gt; &gt;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#010;&gt; &gt;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#010;&gt; &gt;&gt;         Caused by: java.lang.NullPointerException&#010;&gt; &gt;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#010;&gt; &gt;&gt;   at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#010;&gt; &gt;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt; org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt; org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; &gt;&gt;   at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; &gt;&gt;   ... 54 more&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB4294B229D0B63A97A691E43ED4730@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 06:09:05 GMT",
        "subject": "回复: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "gateway就类似于一个web服务，大概流程是建立连接时会初始化一个session，在session里面初始化TableEnvironment，然后根据sql类型做不同的操作，比如select语句会去执行sqlQuery，具体可查看https://github.com/ververica/flink-sql-gateway。&#013;&#010;&#013;&#010;另外，加了RelMetadataQueryBase.THREAD_PROVIDERS&#013;&#010;.set(JaninoRelMetadataProvider.of(FlinkDefaultRelMetadataProvider.INSTANCE()))后确实不报这个错了。&#013;&#010;&#013;&#010;题外话，个人认为flink不应该将这样的异常抛给用户去解决，除非我去深入研究源码，要不然根本无法搞清楚具体发生了什么，在封装性上还有待改善。&#013;&#010;________________________________&#013;&#010;发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;发送时间: 2020年7月28日 13:55&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&#013;&#010;我的怀疑点还是多线程引起的。&#013;&#010;你能具体描述一下你们gateway的行为吗？ 是一个web server？&#013;&#010;&#013;&#010;另外，你可以在table env执行query前加上&#013;&#010;RelMetadataQueryBase.THREAD_PROVIDERS&#013;&#010;.set(JaninoRelMetadataProvider.of(FlinkDefaultRelMetadataProvider.INSTANCE()));&#013;&#010;这句话临时fix。&#013;&#010;&#013;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月28日周二 上午11:02写道：&#013;&#010;&#013;&#010;&gt; 不是多线程同时操作一个tableEnvironment，每执行一次都会创建一个TableEnvironment&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月28日 9:58&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&gt;&#013;&#010;&gt; 你们是否在多线程环境下使用 TableEnvironment ?&#013;&#010;&gt; TableEnvironment 不是线程安全的，多线程情况使用可能出现一些莫名其妙的问题。&#013;&#010;&gt;&#013;&#010;&gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月28日周二 上午9:55写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi 能给出详细的schema信息吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一&#013;&#010;&gt; &gt; 下午7:02写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; 补充一下，执行的sql如下：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; select order_no, order_time from&#013;&#010;&gt; &gt;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; ________________________________&#013;&#010;&gt; &gt;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#013;&#010;&gt; &gt;&gt; 发送时间: 2020年7月27日 18:49&#013;&#010;&gt; &gt;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Hi,all：&#013;&#010;&gt; &gt;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#013;&#010;&gt; &gt;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#013;&#010;&gt; &gt;&gt;         Caused by: java.lang.NullPointerException&#013;&#010;&gt; &gt;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#013;&#010;&gt; &gt;&gt;   at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#013;&#010;&gt; &gt;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt; org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#013;&#010;&gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt; org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt; &gt;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#013;&#010;&gt; &gt;&gt;   at&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#013;&#010;&gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#013;&#010;&gt; &gt;&gt;   ... 54 more&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CADQYLGv5U47t9X1GiusxhosRd2CBMHkxBi7u2ydm69wdot4ROA@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 06:19:52 GMT",
        "subject": "Re: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "这个问题只能说是使用TableEnvironment不当的问题。ververica的gateway的模式其实就是多线程。&#010;创建TableEnvironment和使用TableEnvironment可能不是一个线程，worker线程是被复用的。&#010;简单来说就是：&#010;当session创建的时候，worker thread1 会创建一个TableEnvironment，&#010;然后当后续其他该session请求过来时候，可能是 worker thread2使用该TableEnvironment执行sql。&#010;&#010;这个其实就是在多线程情况下使用TableEnvironment。不符合TableEnvironment只能在单线程使用的约束。&#010;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月28日周二 下午2:09写道：&#010;&#010;&gt;&#010;&gt; gateway就类似于一个web服务，大概流程是建立连接时会初始化一个session，在session里面初始化TableEnvironment，然后根据sql类型做不同的操作，比如select语句会去执行sqlQuery，具体可查看&#010;&gt; https://github.com/ververica/flink-sql-gateway。&#010;&gt;&#010;&gt; 另外，加了RelMetadataQueryBase.THREAD_PROVIDERS&#010;&gt;&#010;&gt; .set(JaninoRelMetadataProvider.of(FlinkDefaultRelMetadataProvider.INSTANCE()))后确实不报这个错了。&#010;&gt;&#010;&gt; 题外话，个人认为flink不应该将这样的异常抛给用户去解决，除非我去深入研究源码，要不然根本无法搞清楚具体发生了什么，在封装性上还有待改善。&#010;&gt; ________________________________&#010;&gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#010;&gt; 发送时间: 2020年7月28日 13:55&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt; 主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#010;&gt;&#010;&gt; 我的怀疑点还是多线程引起的。&#010;&gt; 你能具体描述一下你们gateway的行为吗？ 是一个web server？&#010;&gt;&#010;&gt; 另外，你可以在table env执行query前加上&#010;&gt; RelMetadataQueryBase.THREAD_PROVIDERS&#010;&gt;&#010;&gt; .set(JaninoRelMetadataProvider.of(FlinkDefaultRelMetadataProvider.INSTANCE()));&#010;&gt; 这句话临时fix。&#010;&gt;&#010;&gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月28日周二&#010;&gt; 上午11:02写道：&#010;&gt;&#010;&gt; &gt; 不是多线程同时操作一个tableEnvironment，每执行一次都会创建一个TableEnvironment&#010;&gt; &gt; ________________________________&#010;&gt; &gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#010;&gt; &gt; 发送时间: 2020年7月28日 9:58&#010;&gt; &gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt; 主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#010;&gt; &gt;&#010;&gt; &gt; 你们是否在多线程环境下使用 TableEnvironment ?&#010;&gt; &gt; TableEnvironment 不是线程安全的，多线程情况使用可能出现一些莫名其妙的问题。&#010;&gt; &gt;&#010;&gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月28日周二 上午9:55写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi 能给出详细的schema信息吗？&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一&#010;&gt; &gt; &gt; 下午7:02写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&gt; 补充一下，执行的sql如下：&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; select order_no, order_time from&#010;&gt; &gt; &gt;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; ________________________________&#010;&gt; &gt; &gt;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#010;&gt; &gt; &gt;&gt; 发送时间: 2020年7月27日 18:49&#010;&gt; &gt; &gt;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&gt; &gt; &gt;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Hi,all：&#010;&gt; &gt; &gt;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#010;&gt; &gt; &gt;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#010;&gt; &gt; &gt;&gt;         Caused by: java.lang.NullPointerException&#010;&gt; &gt; &gt;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#010;&gt; &gt; &gt;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt; org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt; &gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt; org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt; &gt;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; &gt; &gt;&gt;   at&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; &gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; &gt; &gt;&gt;   ... 54 more&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "6",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB429452C751C7B4135D46EE31D4730@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 06:34:33 GMT",
        "subject": "回复: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "嗯，对，创建TableEnvironment和使用不是一个线程。&#013;&#010;________________________________&#013;&#010;发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;发送时间: 2020年7月28日 14:19&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&#013;&#010;这个问题只能说是使用TableEnvironment不当的问题。ververica的gateway的模式其实就是多线程。&#013;&#010;创建TableEnvironment和使用TableEnvironment可能不是一个线程，worker线程是被复用的。&#013;&#010;简单来说就是：&#013;&#010;当session创建的时候，worker thread1 会创建一个TableEnvironment，&#013;&#010;然后当后续其他该session请求过来时候，可能是 worker thread2使用该TableEnvironment执行sql。&#013;&#010;&#013;&#010;这个其实就是在多线程情况下使用TableEnvironment。不符合TableEnvironment只能在单线程使用的约束。&#013;&#010;&#013;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月28日周二 下午2:09写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; gateway就类似于一个web服务，大概流程是建立连接时会初始化一个session，在session里面初始化TableEnvironment，然后根据sql类型做不同的操作，比如select语句会去执行sqlQuery，具体可查看&#013;&#010;&gt; https://github.com/ververica/flink-sql-gateway。&#013;&#010;&gt;&#013;&#010;&gt; 另外，加了RelMetadataQueryBase.THREAD_PROVIDERS&#013;&#010;&gt;&#013;&#010;&gt; .set(JaninoRelMetadataProvider.of(FlinkDefaultRelMetadataProvider.INSTANCE()))后确实不报这个错了。&#013;&#010;&gt;&#013;&#010;&gt; 题外话，个人认为flink不应该将这样的异常抛给用户去解决，除非我去深入研究源码，要不然根本无法搞清楚具体发生了什么，在封装性上还有待改善。&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月28日 13:55&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&gt;&#013;&#010;&gt; 我的怀疑点还是多线程引起的。&#013;&#010;&gt; 你能具体描述一下你们gateway的行为吗？ 是一个web server？&#013;&#010;&gt;&#013;&#010;&gt; 另外，你可以在table env执行query前加上&#013;&#010;&gt; RelMetadataQueryBase.THREAD_PROVIDERS&#013;&#010;&gt;&#013;&#010;&gt; .set(JaninoRelMetadataProvider.of(FlinkDefaultRelMetadataProvider.INSTANCE()));&#013;&#010;&gt; 这句话临时fix。&#013;&#010;&gt;&#013;&#010;&gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月28日周二&#013;&#010;&gt; 上午11:02写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 不是多线程同时操作一个tableEnvironment，每执行一次都会创建一个TableEnvironment&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;&gt; &gt; 发送时间: 2020年7月28日 9:58&#013;&#010;&gt; &gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt; 主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 你们是否在多线程环境下使用 TableEnvironment ?&#013;&#010;&gt; &gt; TableEnvironment 不是线程安全的，多线程情况使用可能出现一些莫名其妙的问题。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月28日周二 上午9:55写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; hi 能给出详细的schema信息吗？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一&#013;&#010;&gt; &gt; &gt; 下午7:02写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; 补充一下，执行的sql如下：&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; select order_no, order_time from&#013;&#010;&gt; &gt; &gt;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; ________________________________&#013;&#010;&gt; &gt; &gt;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#013;&#010;&gt; &gt; &gt;&gt; 发送时间: 2020年7月27日 18:49&#013;&#010;&gt; &gt; &gt;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt; &gt;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; Hi,all：&#013;&#010;&gt; &gt; &gt;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#013;&#010;&gt; &gt; &gt;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#013;&#010;&gt; &gt; &gt;&gt;         Caused by: java.lang.NullPointerException&#013;&#010;&gt; &gt; &gt;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#013;&#010;&gt; &gt; &gt;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt; org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#013;&#010;&gt; &gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt; org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt; &gt; &gt;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#013;&#010;&gt; &gt; &gt;&gt;   at&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#013;&#010;&gt; &gt; &gt;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#013;&#010;&gt; &gt; &gt;&gt;   ... 54 more&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB429480AA1CC9A2EF61E6D4FCD4730@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 03:00:59 GMT",
        "subject": "回复: flink1.11.0 执行sqlQuery时报NullPointException",
        "content": "schema信息如下：&#013;&#010;CREATE TABLE x.ods.ods_binlog_test_trip_create_t_order_1 (&#013;&#010;  _type STRING,&#013;&#010;  order_no STRING,&#013;&#010;  order_time STRING,&#013;&#010;  dt as TO_TIMESTAMP(order_time),&#013;&#010;  proctime as PROCTIME(),&#013;&#010;  WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#013;&#010;) WITH (&#013;&#010;  'connector.type' = 'kafka',&#013;&#010;  'connector.properties.bootstrap.servers' = '*****',&#013;&#010;  'connector.properties.zookeeper.connect' = '*****',&#013;&#010;  'connector.version' = 'universal',&#013;&#010;  'format.type' = 'json',&#013;&#010;  'connector.properties.group.id' = 'testGroup',&#013;&#010;  'connector.startup-mode' = 'group-offsets',&#013;&#010;  'connector.topic' = 'ods-test_trip_create-t_order'&#013;&#010;)&#013;&#010;________________________________&#013;&#010;发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;发送时间: 2020年7月28日 9:55&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&#013;&#010;hi 能给出详细的schema信息吗？&#013;&#010;&#013;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月27日周一 下午7:02写道：&#013;&#010;&#013;&#010;&gt; 补充一下，执行的sql如下：&#013;&#010;&gt;&#013;&#010;&gt; select order_no, order_time from&#013;&#010;&gt; x.ods.ods_binlog_test_trip_create_t_order_1&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月27日 18:49&#013;&#010;&gt; 收件人: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: flink1.11.0 执行sqlQuery时报NullPointException&#013;&#010;&gt;&#013;&#010;&gt; Hi,all：&#013;&#010;&gt;          本人正在为公司之前基于flink1.10的gateway升级flink版本到1.11，用的hive&#013;&#010;&gt; catalog，建表后，执行sqlQuery方法时报NullPointException，希望给出排错建议，具体报错信息如下：&#013;&#010;&gt;         Caused by: java.lang.NullPointerException&#013;&#010;&gt;   at java.util.Objects.requireNonNull(Objects.java:203)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:141)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.rel.metadata.RelMetadataQuery.&lt;init&gt;(RelMetadataQuery.java:106)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.&lt;init&gt;(FlinkRelMetadataQuery.java:73)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.plan.metadata.FlinkRelMetadataQuery.instance(FlinkRelMetadataQuery.java:52)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:39)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$$anon$1.get(FlinkRelOptClusterFactory.scala:38)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.plan.RelOptCluster.getMetadataQuery(RelOptCluster.java:178)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:118)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.rel.logical.LogicalProject.create(LogicalProject.java:111)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.rel.core.RelFactories$ProjectFactoryImpl.createProject(RelFactories.java:180)&#013;&#010;&gt;   at org.apache.calcite.tools.RelBuilder.project_(RelBuilder.java:1462)&#013;&#010;&gt;   at org.apache.calcite.tools.RelBuilder.project(RelBuilder.java:1256)&#013;&#010;&gt;   at org.apache.calcite.tools.RelBuilder.projectNamed(RelBuilder.java:1521)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectList(SqlToRelConverter.java:4125)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:685)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#013;&#010;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNodes(SqlExprToRexConverterImpl.java:81)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.calcite.SqlExprToRexConverterImpl.convertToRexNode(SqlExprToRexConverterImpl.java:73)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parseSqlExpression(ParserImpl.java:93)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolveExpressionDataType(CatalogTableSchemaResolver.java:119)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.api.internal.CatalogTableSchemaResolver.resolve(CatalogTableSchemaResolver.java:83)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.resolveTableSchema(CatalogManager.java:380)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getPermanentTable(CatalogManager.java:408)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.catalog.CatalogManager.getTable(CatalogManager.java:375)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.flink.table.planner.catalog.DatabaseCalciteSchema.getTable(DatabaseCalciteSchema.java:75)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.jdbc.SimpleCalciteSchema.getImplicitTable(SimpleCalciteSchema.java:83)&#013;&#010;&gt;   at org.apache.calcite.jdbc.CalciteSchema.getTable(CalciteSchema.java:289)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolve_(EmptyScope.java:143)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.EmptyScope.resolveTable(EmptyScope.java:99)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.resolveTable(DelegatingScope.java:203)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.resolveImpl(IdentifierNamespace.java:112)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.IdentifierNamespace.validateImpl(IdentifierNamespace.java:184)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#013;&#010;&gt;   at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#013;&#010;&gt;   at&#013;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#013;&#010;&gt;   at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#013;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#013;&#010;&gt;   ... 54 more&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<BYAPR01MB4294B671FA6F19126662C6A0D4720@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<15387299.84e6.1739013f574.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 11:41:33 GMT",
        "subject": "flink1.11.1启动问题",
        "content": "&#010;&#010;首先，flink1.9提交到yarn集群是没有问题的，同等的配置提交flink1.11.1到yarn集群就报下面的错误&#010;2020-07-27 17:08:14,661 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------&#010;2020-07-27 17:08:14,665 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Starting YarnJobClusterEntrypoint (Version: 1.11.1, Scala: 2.11, Rev:7eb514a, Date:2020-07-15T07:02:09+02:00)&#010;2020-07-27 17:08:14,665 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS current user: hadoop&#010;2020-07-27 17:08:15,417 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current Hadoop/Kerberos user: wangty&#010;2020-07-27 17:08:15,418 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM: Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.191-b12&#010;2020-07-27 17:08:15,418 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum heap size: 429 MiBytes&#010;2020-07-27 17:08:15,418 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JAVA_HOME: /usr/local/jdk/&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop version: 2.7.7&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM Options:&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xmx469762048&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Xms469762048&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -XX:MaxMetaspaceSize=268435456&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog.file=/data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configuration=file:log4j.properties&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -     -Dlog4j.configurationFile=file:log4j.properties&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program Arguments: (none)&#010;2020-07-27 17:08:15,419 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Classpath: :lib/flink-csv-1.11.1.jar:lib/flink-json-1.11.1.jar:lib/flink-shaded-zookeeper-3.4.14.jar:lib/flink-table-blink_2.11-1.11.1.jar:lib/flink-table_2.11-1.11.1.jar:lib/log4j-1.2-api-2.12.1.jar:lib/log4j-api-2.12.1.jar:lib/log4j-core-2.12.1.jar:lib/log4j-slf4j-impl-2.12.1.jar:test.jar:flink-dist_2.11-1.11.1.jar:job.graph:flink-conf.yaml::/usr/local/service/hadoop/etc/hadoop:/usr/local/service/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-databind-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-annotations-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/service/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-temrfs-1.0.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/service/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/joda-time-2.9.7.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/spark-2.0.2-yarn-shuffle.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/hadoop-lzo-0.4.20.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar&#010;2020-07-27 17:08:15,420 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - --------------------------------------------------------------------------------&#010;2020-07-27 17:08:15,421 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Registered UNIX signal handlers for [TERM, HUP, INT]&#010;2020-07-27 17:08:15,424 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - YARN daemon is running as: wangty Yarn client user obtainer: wangty&#010;2020-07-27 17:08:15,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m&#010;2020-07-27 17:08:15,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.jobgraph-path, job.graph&#010;2020-07-27 17:08:15,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region&#010;2020-07-27 17:08:15,427 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.cluster-id, application_1568724479991_18850539&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.target, yarn-per-job&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1 gb&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.attached, true&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.cluster.execution-mode, NORMAL&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.shutdown-on-attached-exit, false&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: pipeline.jars, file:/data/rt/jar_version/sql/test.jar&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 3&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: pipeline.classpaths, http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.name, RTC_TEST&#010;2020-07-27 17:08:15,428 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.queue, root.dp.dp_online&#010;2020-07-27 17:08:15,429 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.deployment.config-dir, /data/server/flink-1.11.1/conf&#010;2020-07-27 17:08:15,429 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.yarn.log-config-file, /data/server/flink-1.11.1/conf/log4j.properties&#010;2020-07-27 17:08:15,455 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'web.port' instead of proper key 'rest.bind-port'&#010;2020-07-27 17:08:15,465 INFO  org.apache.flink.runtime.clusterframework.BootstrapTools     [] - Setting directories for temporary files to: /data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data2/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539&#010;2020-07-27 17:08:15,471 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting YarnJobClusterEntrypoint.&#010;2020-07-27 17:08:15,993 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install default filesystem.&#010;2020-07-27 17:08:16,235 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install security context.&#010;2020-07-27 17:08:16,715 INFO  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop user set to wangty (auth:SIMPLE)&#010;2020-07-27 17:08:16,722 INFO  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas file will be created as /data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/jaas-8303363038541870345.conf.&#010;2020-07-27 17:08:16,729 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Initializing cluster services.&#010;2020-07-27 17:08:16,741 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0.&#010;2020-07-27 17:08:18,830 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started&#010;2020-07-27 17:08:19,781 INFO  akka.remote.Remoting                                         [] - Starting remoting&#010;2020-07-27 17:08:19,936 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink@x.x.x.60:36696]&#010;2020-07-27 17:08:20,021 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink@x.x.x.60:36696&#010;2020-07-27 17:08:20,042 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'web.port' instead of proper key 'rest.port'&#010;2020-07-27 17:08:20,049 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory /data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/blobStore-86aff9db-0f30-4688-9e68-b8e5866a93c7&#010;2020-07-27 17:08:20,054 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:56782 - max concurrent requests: 50 - max backlog: 1000&#010;2020-07-27 17:08:20,063 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed/reported.&#010;2020-07-27 17:08:20,066 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0.&#010;2020-07-27 17:08:20,082 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started&#010;2020-07-27 17:08:20,086 INFO  akka.remote.Remoting                                         [] - Starting remoting&#010;2020-07-27 17:08:20,093 INFO  akka.remote.Remoting                                         [] - Remoting started; listening on addresses :[akka.tcp://flink-metrics@x.x.5.60:60801]&#010;2020-07-27 17:08:20,794 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka.tcp://flink-metrics@x.x.5.60:60801&#010;2020-07-27 17:08:20,810 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka://flink-metrics/user/rpc/MetricQueryService .&#010;2020-07-27 17:08:20,856 WARN  org.apache.flink.configuration.Configuration                 [] - Config uses deprecated configuration key 'web.port' instead of proper key 'rest.bind-port'&#010;2020-07-27 17:08:20,858 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Upload directory /tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload does not exist. &#010;2020-07-27 17:08:20,859 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Created directory /tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload for file uploads.&#010;2020-07-27 17:08:20,874 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Starting rest endpoint.&#010;2020-07-27 17:08:21,103 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component log file: /data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;2020-07-27 17:08:21,103 INFO  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Determined location of main cluster component stdout file: /data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.out&#010;2020-07-27 17:08:21,241 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Rest endpoint listening at x.x.5.60:46723&#010;2020-07-27 17:08:21,242 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - http://x.x.5.60:46723 was granted leadership with leaderSessionID=00000000-0000-0000-0000-000000000000&#010;2020-07-27 17:08:21,243 INFO  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Web frontend listening at http://x.x.5.60:46723.&#010;2020-07-27 17:08:21,256 INFO  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is less than its min value 192.000mb (201326592 bytes), min value will be used instead&#010;2020-07-27 17:08:21,304 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.yarn.YarnResourceManager at akka://flink/user/rpc/resourcemanager_0 .&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.memory.process.size, 1728m&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.jobgraph-path, job.graph&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.execution.failover-strategy, region&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: high-availability.cluster-id, application_1568724479991_18850539&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.address, localhost&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.target, yarn-per-job&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.memory.process.size, 1 gb&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: jobmanager.rpc.port, 6123&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.attached, true&#010;2020-07-27 17:08:21,314 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: internal.cluster.execution-mode, NORMAL&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: execution.shutdown-on-attached-exit, false&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: pipeline.jars, file:/data/rt/jar_version/sql/test.jar&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: parallelism.default, 3&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: taskmanager.numberOfTaskSlots, 1&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: pipeline.classpaths, http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.name, RTC_TEST&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: yarn.application.queue, root.dp.dp_online&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.deployment.config-dir, /data/server/flink-1.11.1/conf&#010;2020-07-27 17:08:21,315 INFO  org.apache.flink.configuration.GlobalConfiguration           [] - Loading configuration property: $internal.yarn.log-config-file, /data/server/flink-1.11.1/conf/log4j.properties&#010;2020-07-27 17:08:21,333 INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []&#010;2020-07-27 17:08:21,334 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Cannot get scheduler resource types: This YARN version does not support 'getSchedulerResourceTypes'&#010;2020-07-27 17:08:21,375 INFO  org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcess [] - Start JobDispatcherLeaderProcess.&#010;2020-07-27 17:08:21,379 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.MiniDispatcher at akka://flink/user/rpc/dispatcher_1 .&#010;2020-07-27 17:08:21,408 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .&#010;2020-07-27 17:08:21,414 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:21,437 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=10000) for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.&#010;2020-07-27 17:08:21,488 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 3 pipelined regions in 1 ms&#010;2020-07-27 17:08:21,542 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using application-defined state backend: RocksDBStateBackend{checkpointStreamBackend=File State Backend (checkpoints: 'hdfs://HDFS00000/data/checkpoint-data/wangty/RTC_TEST', savepoints: 'null', asynchronous: UNDEFINED, fileStateThreshold: -1), localRocksDbDirectories=null, enableIncrementalCheckpointing=FALSE, numberOfTransferThreads=-1, writeBatchSize=-1}&#010;2020-07-27 17:08:21,543 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Configuring application-defined state backend with job/cluster config&#010;2020-07-27 17:08:21,568 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using predefined options: DEFAULT.&#010;2020-07-27 17:08:21,569 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using default options factory: DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;2020-07-27 17:08:21,714 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Recovered 0 containers from previous attempts ([]).&#010;2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Register application master response does not contain scheduler resource types, use '$internal.yarn.resourcemanager.enable-vcore-matching'.&#010;2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Container matching strategy: IGNORE_VCORE.&#010;2020-07-27 17:08:21,719 INFO  org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl [] - Upper bound of the thread pool size is 500&#010;2020-07-27 17:08:21,720 INFO  org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy [] - yarn.client.max-nodemanagers-proxies : 500&#010;2020-07-27 17:08:21,723 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - ResourceManager akka.tcp://flink@x.x.5.60:36696/user/rpc/resourcemanager_0 was granted leadership with fencing token 00000000000000000000000000000000&#010;2020-07-27 17:08:21,727 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Starting the SlotManager.&#010;2020-07-27 17:08:22,126 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@c1dab34 for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:22,130 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] - JobManager runner for job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) was granted leadership with session id 00000000-0000-0000-0000-000000000000 at akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2.&#010;2020-07-27 17:08:22,133 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) under job master id 00000000000000000000000000000000.&#010;2020-07-27 17:08:22,135 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;2020-07-27 17:08:22,135 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state CREATED to RUNNING.&#010;2020-07-27 17:08:22,145 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) (8bb9f7b4bcc93895851ec47123d2213a) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:08:22,145 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3) (647da02fb921931e1a35ba4265d95c04) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:08:22,145 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3) (78211c4a866e216b6c821b743b2bf52d) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:08:22,158 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}]&#010;2020-07-27 17:08:22,162 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}]&#010;2020-07-27 17:08:22,162 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}]&#010;2020-07-27 17:08:22,166 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@x.x.5.60:36696/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;2020-07-27 17:08:22,170 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration&#010;2020-07-27 17:08:22,173 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2 for job 9f074e66a0f70274c7a7af42e71525fb.&#010;2020-07-27 17:08:22,177 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2 for job 9f074e66a0f70274c7a7af42e71525fb.&#010;2020-07-27 17:08:22,180 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.&#010;2020-07-27 17:08:22,180 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:08:22,181 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Request slot with profile ResourceProfile{UNKNOWN} for job 9f074e66a0f70274c7a7af42e71525fb with allocation id 4f255670332ee6a2bc934336cc0cee4c.&#010;2020-07-27 17:08:22,181 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:08:22,182 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:08:22,190 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Requesting new TaskExecutor container with resource WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes), taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes), managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this resource is 1.&#010;2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Request slot with profile ResourceProfile{UNKNOWN} for job 9f074e66a0f70274c7a7af42e71525fb with allocation id f29dfd0f0ba7639992b6fe90ca0f3524.&#010;2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Requesting new TaskExecutor container with resource WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes), taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes), managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this resource is 2.&#010;2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Request slot with profile ResourceProfile{UNKNOWN} for job 9f074e66a0f70274c7a7af42e71525fb with allocation id 8a9e009c987a09bf4e36843f7a6eab62.&#010;2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Requesting new TaskExecutor container with resource WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes), taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes), managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this resource is 3.&#010;2020-07-27 17:08:27,253 INFO  org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received new token for : x.x.6.54:5006&#010;2020-07-27 17:08:27,254 INFO  org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received new token for : x.x.4.230:5006&#010;2020-07-27 17:08:27,254 INFO  org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received new token for : x.x.5.229:5006&#010;2020-07-27 17:08:27,256 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Received 3 containers.&#010;2020-07-27 17:08:27,259 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Received 3 containers with resource &lt;memory:2048, vCores:1&gt;, 0 pending container requests.&#010;2020-07-27 17:08:27,261 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Returning excess container container_1568724479991_18850539_01_000002.&#010;2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Returning excess container container_1568724479991_18850539_01_000003.&#010;2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Returning excess container container_1568724479991_18850539_01_000004.&#010;2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Accepted 0 requested containers, returned 3 excess containers, 0 pending container requests of resource &lt;memory:2048, vCores:1&gt;.&#010;2020-07-27 17:08:45,630 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:09:15,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:09:45,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:10:15,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:10:45,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:11:15,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:11:45,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:12:15,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:12:45,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:13:15,629 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:13:22,167 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) (8bb9f7b4bcc93895851ec47123d2213a) switched from SCHEDULED to FAILED on not deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:13:22,177 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 66774974224095ed80fefb1f583d9fb9_0.&#010;2020-07-27 17:13:22,178 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 1 tasks should be restarted to recover the failed task 66774974224095ed80fefb1f583d9fb9_0. &#010;2020-07-27 17:13:22,179 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to RESTARTING.&#010;2020-07-27 17:13:22,181 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 8bb9f7b4bcc93895851ec47123d2213a.&#010;2020-07-27 17:13:22,183 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] timed out.&#010;2020-07-27 17:13:22,185 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3) (647da02fb921931e1a35ba4265d95c04) switched from SCHEDULED to FAILED on not deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:13:22,188 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 66774974224095ed80fefb1f583d9fb9_1.&#010;2020-07-27 17:13:22,188 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 1 tasks should be restarted to recover the failed task 66774974224095ed80fefb1f583d9fb9_1. &#010;2020-07-27 17:13:22,188 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 647da02fb921931e1a35ba4265d95c04.&#010;2020-07-27 17:13:22,189 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] timed out.&#010;2020-07-27 17:13:22,190 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3) (78211c4a866e216b6c821b743b2bf52d) switched from SCHEDULED to FAILED on not deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:13:22,192 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 66774974224095ed80fefb1f583d9fb9_2.&#010;2020-07-27 17:13:22,192 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 1 tasks should be restarted to recover the failed task 66774974224095ed80fefb1f583d9fb9_2. &#010;2020-07-27 17:13:22,192 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 78211c4a866e216b6c821b743b2bf52d.&#010;2020-07-27 17:13:22,193 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] timed out.&#010;2020-07-27 17:13:32,184 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) (de96a2863fbb58f21cf987db0cbe380d) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:13:32,185 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:13:32,185 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Request slot with profile ResourceProfile{UNKNOWN} for job 9f074e66a0f70274c7a7af42e71525fb with allocation id 19be4edc7eadca2c14dd9ad6d1a87dcd.&#010;2020-07-27 17:13:32,189 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3) (c8afff8ed430d1e5caf02d3d8383723c) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:13:32,189 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:13:32,189 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Request slot with profile ResourceProfile{UNKNOWN} for job 9f074e66a0f70274c7a7af42e71525fb with allocation id 60ef8787a8c67ed7cbc198d71838ba22.&#010;2020-07-27 17:13:32,192 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING to RUNNING.&#010;2020-07-27 17:13:32,193 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3) (08889c02dcc65774fe7ea42522f89ca9) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:13:32,193 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:13:32,194 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Request slot with profile ResourceProfile{UNKNOWN} for job 9f074e66a0f70274c7a7af42e71525fb with allocation id 3122df3dc780c5924fbd479a886d435b.&#010;2020-07-27 17:13:57,064 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:14:27,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:14:57,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:15:27,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:15:57,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:16:27,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:16:57,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:17:27,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:17:57,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:18:27,023 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED instead. Aborting checkpoint.&#010;2020-07-27 17:18:32,187 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) (de96a2863fbb58f21cf987db0cbe380d) switched from SCHEDULED to FAILED on not deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,189 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 66774974224095ed80fefb1f583d9fb9_0.&#010;2020-07-27 17:18:32,189 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 1 tasks should be restarted to recover the failed task 66774974224095ed80fefb1f583d9fb9_0. &#010;2020-07-27 17:18:32,189 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to RESTARTING.&#010;2020-07-27 17:18:32,191 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution de96a2863fbb58f21cf987db0cbe380d.&#010;2020-07-27 17:18:32,192 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] timed out.&#010;2020-07-27 17:18:32,193 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3) (c8afff8ed430d1e5caf02d3d8383723c) switched from SCHEDULED to FAILED on not deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,194 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 66774974224095ed80fefb1f583d9fb9_1.&#010;2020-07-27 17:18:32,194 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 1 tasks should be restarted to recover the failed task 66774974224095ed80fefb1f583d9fb9_1. &#010;2020-07-27 17:18:32,199 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution c8afff8ed430d1e5caf02d3d8383723c.&#010;2020-07-27 17:18:32,199 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] timed out.&#010;2020-07-27 17:18:32,200 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: rtsc_test -&gt; Filter -&gt; Map -&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a, b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp, PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3) (08889c02dcc65774fe7ea42522f89ca9) switched from SCHEDULED to FAILED on not deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,203 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task 66774974224095ed80fefb1f583d9fb9_2.&#010;2020-07-27 17:18:32,204 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 1 tasks should be restarted to recover the failed task 66774974224095ed80fefb1f583d9fb9_2. &#010;2020-07-27 17:18:32,204 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING to FAILING.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=10000)&#010;at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;... 45 more&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,208 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state FAILING to FAILED.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5, backoffTimeMS=10000)&#010;at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086) ~[test.jar:?]&#010;at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435) ~[test.jar:?]&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[test.jar:?]&#010;at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[test.jar:?]&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977) ~[?:1.8.0_191]&#010;at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[test.jar:?]&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [test.jar:?]&#010;Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[test.jar:?]&#010;... 45 more&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) ~[?:1.8.0_191]&#010;at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577) ~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,210 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 9f074e66a0f70274c7a7af42e71525fb.&#010;2020-07-27 17:18:32,210 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down&#010;2020-07-27 17:18:32,210 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 08889c02dcc65774fe7ea42522f89ca9.&#010;2020-07-27 17:18:32,211 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] timed out.&#010;2020-07-27 17:18:32,216 INFO  org.apache.flink.runtime.dispatcher.MiniDispatcher           [] - Job 9f074e66a0f70274c7a7af42e71525fb reached globally terminal state FAILED.&#010;2020-07-27 17:18:32,217 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job RTC_TEST(9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:18:32,224 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Suspending SlotPool.&#010;2020-07-27 17:18:32,225 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 4ee5ce2ea08809b1cca2745fa12ee663: JobManager is shutting down..&#010;2020-07-27 17:18:32,225 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping SlotPool.&#010;2020-07-27 17:18:32,225 INFO  org.apache.flink.yarn.YarnResourceManager                    [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2 for job 9f074e66a0f70274c7a7af42e71525fb from the resource manager.&#010;&#010;&#010;&#010;",
        "depth": "0",
        "reply": "<15387299.84e6.1739013f574.Coremail.apache22@163.com>"
    },
    {
        "id": "<CAHsnkPvj2x2fvrVYbxbp2v2Co30-3sU2x9ebuiatQc4uhhwbng@mail.gmail.com>",
        "from": "Xintong Song &lt;tonysong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 02:26:56 GMT",
        "subject": "Re: flink1.11.1启动问题",
        "content": "建议确认一下 Yarn 的配置 “yarn.scheduler.minimum-allocation-mb” 在 Yarn RM 和 Flink JM&#010;这两台机器上是否一致。&#010;&#010;Yarn 会对 container request 做归一化。例如你请求的 TM container 是 1728m&#010;(taskmanager.memory.process.size) ，如果 minimum-allocation-mb 是 1024m，那么实际得到的&#010;container 大小必须是 minimum-allocation-mb 的整数倍，也就是 2048m。Flink 会去获取 Yarn 的配置，计算&#010;container request 实际分到的 container 应该多大，并对分到的 container 进行检查。现在看 JM 日志，分下来的&#010;container 并没有通过这个检查，造成 Flink 认为 container 规格不匹配。这里最可能的原因是 Flink 拿到的&#010;minimum-allocation-mb 和 Yarn RM 实际使用的不一致。&#010;&#010;Thank you~&#010;&#010;Xintong Song&#010;&#010;&#010;&#010;On Mon, Jul 27, 2020 at 7:42 PM 酷酷的浑蛋 &lt;apache22@163.com&gt; wrote:&#010;&#010;&gt;&#010;&gt; 首先，flink1.9提交到yarn集群是没有问题的，同等的配置提交flink1.11.1到yarn集群就报下面的错误&#010;&gt; 2020-07-27 17:08:14,661 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; --------------------------------------------------------------------------------&#010;&gt; 2020-07-27 17:08:14,665 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt;  Starting YarnJobClusterEntrypoint (Version: 1.11.1, Scala: 2.11,&#010;&gt; Rev:7eb514a, Date:2020-07-15T07:02:09+02:00)&#010;&gt; 2020-07-27 17:08:14,665 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS&#010;&gt; current user: hadoop&#010;&gt; 2020-07-27 17:08:15,417 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current&#010;&gt; Hadoop/Kerberos user: wangty&#010;&gt; 2020-07-27 17:08:15,418 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM:&#010;&gt; Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.191-b12&#010;&gt; 2020-07-27 17:08:15,418 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum&#010;&gt; heap size: 429 MiBytes&#010;&gt; 2020-07-27 17:08:15,418 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt;  JAVA_HOME: /usr/local/jdk/&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop&#010;&gt; version: 2.7.7&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM&#010;&gt; Options:&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Xmx469762048&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Xms469762048&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -XX:MaxMetaspaceSize=268435456&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Dlog.file=/data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Dlog4j.configuration=file:log4j.properties&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Dlog4j.configurationFile=file:log4j.properties&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program&#010;&gt; Arguments: (none)&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt;  Classpath:&#010;&gt; :lib/flink-csv-1.11.1.jar:lib/flink-json-1.11.1.jar:lib/flink-shaded-zookeeper-3.4.14.jar:lib/flink-table-blink_2.11-1.11.1.jar:lib/flink-table_2.11-1.11.1.jar:lib/log4j-1.2-api-2.12.1.jar:lib/log4j-api-2.12.1.jar:lib/log4j-core-2.12.1.jar:lib/log4j-slf4j-impl-2.12.1.jar:test.jar:flink-dist_2.11-1.11.1.jar:job.graph:flink-conf.yaml::/usr/local/service/hadoop/etc/hadoop:/usr/local/service/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-databind-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-annotations-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/service/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-temrfs-1.0.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/service/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/joda-time-2.9.7.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/spark-2.0.2-yarn-shuffle.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/hadoop-lzo-0.4.20.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar&#010;&gt; 2020-07-27 17:08:15,420 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; --------------------------------------------------------------------------------&#010;&gt; 2020-07-27 17:08:15,421 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; Registered UNIX signal handlers for [TERM, HUP, INT]&#010;&gt; 2020-07-27 17:08:15,424 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - YARN&#010;&gt; daemon is running as: wangty Yarn client user obtainer: wangty&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.jobgraph-path, job.graph&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: high-availability.cluster-id,&#010;&gt; application_1568724479991_18850539&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.target, yarn-per-job&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.memory.process.size, 1 gb&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.attached, true&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.cluster.execution-mode, NORMAL&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.shutdown-on-attached-exit, false&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.jars,&#010;&gt; file:/data/rt/jar_version/sql/test.jar&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: parallelism.default, 3&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.classpaths,&#010;&gt; http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.name, RTC_TEST&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.queue, root.dp.dp_online&#010;&gt; 2020-07-27 17:08:15,429 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.deployment.config-dir,&#010;&gt; /data/server/flink-1.11.1/conf&#010;&gt; 2020-07-27 17:08:15,429 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.yarn.log-config-file,&#010;&gt; /data/server/flink-1.11.1/conf/log4j.properties&#010;&gt; 2020-07-27 17:08:15,455 WARN  org.apache.flink.configuration.Configuration&#010;&gt;                 [] - Config uses deprecated configuration key 'web.port'&#010;&gt; instead of proper key 'rest.bind-port'&#010;&gt; 2020-07-27 17:08:15,465 INFO&#010;&gt;  org.apache.flink.runtime.clusterframework.BootstrapTools     [] - Setting&#010;&gt; directories for temporary files to:&#010;&gt; /data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data2/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539&#010;&gt; 2020-07-27 17:08:15,471 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting&#010;&gt; YarnJobClusterEntrypoint.&#010;&gt; 2020-07-27 17:08:15,993 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install&#010;&gt; default filesystem.&#010;&gt; 2020-07-27 17:08:16,235 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install&#010;&gt; security context.&#010;&gt; 2020-07-27 17:08:16,715 INFO&#010;&gt;  org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop&#010;&gt; user set to wangty (auth:SIMPLE)&#010;&gt; 2020-07-27 17:08:16,722 INFO&#010;&gt;  org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas&#010;&gt; file will be created as&#010;&gt; /data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/jaas-8303363038541870345.conf.&#010;&gt; 2020-07-27 17:08:16,729 INFO&#010;&gt;  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; Initializing cluster services.&#010;&gt; 2020-07-27 17:08:16,741 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying&#010;&gt; to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0&#010;&gt; .&#010;&gt; 2020-07-27 17:08:18,830 INFO  akka.event.slf4j.Slf4jLogger&#010;&gt;                 [] - Slf4jLogger started&#010;&gt; 2020-07-27 17:08:19,781 INFO  akka.remote.Remoting&#010;&gt;                 [] - Starting remoting&#010;&gt; 2020-07-27 17:08:19,936 INFO  akka.remote.Remoting&#010;&gt;                 [] - Remoting started; listening on addresses&#010;&gt; :[akka.tcp://flink@x.x.x.60:36696]&#010;&gt; 2020-07-27 17:08:20,021 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor&#010;&gt; system started at akka.tcp://flink@x.x.x.60:36696&#010;&gt; 2020-07-27 17:08:20,042 WARN  org.apache.flink.configuration.Configuration&#010;&gt;                 [] - Config uses deprecated configuration key 'web.port'&#010;&gt; instead of proper key 'rest.port'&#010;&gt; 2020-07-27 17:08:20,049 INFO  org.apache.flink.runtime.blob.BlobServer&#010;&gt;                 [] - Created BLOB server storage directory&#010;&gt; /data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/blobStore-86aff9db-0f30-4688-9e68-b8e5866a93c7&#010;&gt; 2020-07-27 17:08:20,054 INFO  org.apache.flink.runtime.blob.BlobServer&#010;&gt;                 [] - Started BLOB server at 0.0.0.0:56782 - max&#010;&gt; concurrent requests: 50 - max backlog: 1000&#010;&gt; 2020-07-27 17:08:20,063 INFO&#010;&gt;  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No&#010;&gt; metrics reporter configured, no metrics will be exposed/reported.&#010;&gt; 2020-07-27 17:08:20,066 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying&#010;&gt; to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0&#010;&gt; .&#010;&gt; 2020-07-27 17:08:20,082 INFO  akka.event.slf4j.Slf4jLogger&#010;&gt;                 [] - Slf4jLogger started&#010;&gt; 2020-07-27 17:08:20,086 INFO  akka.remote.Remoting&#010;&gt;                 [] - Starting remoting&#010;&gt; 2020-07-27 17:08:20,093 INFO  akka.remote.Remoting&#010;&gt;                 [] - Remoting started; listening on addresses&#010;&gt; :[akka.tcp://flink-metrics@x.x.5.60:60801]&#010;&gt; 2020-07-27 17:08:20,794 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor&#010;&gt; system started at akka.tcp://flink-metrics@x.x.5.60:60801&#010;&gt; 2020-07-27 17:08:20,810 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService&#010;&gt; at akka://flink-metrics/user/rpc/MetricQueryService .&#010;&gt; 2020-07-27 17:08:20,856 WARN  org.apache.flink.configuration.Configuration&#010;&gt;                 [] - Config uses deprecated configuration key 'web.port'&#010;&gt; instead of proper key 'rest.bind-port'&#010;&gt; 2020-07-27 17:08:20,858 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Upload&#010;&gt; directory&#010;&gt; /tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload does&#010;&gt; not exist.&#010;&gt; 2020-07-27 17:08:20,859 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Created&#010;&gt; directory&#010;&gt; /tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload for&#010;&gt; file uploads.&#010;&gt; 2020-07-27 17:08:20,874 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] -&#010;&gt; Starting rest endpoint.&#010;&gt; 2020-07-27 17:08:21,103 INFO&#010;&gt;  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] -&#010;&gt; Determined location of main cluster component log file:&#010;&gt; /data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;&gt; 2020-07-27 17:08:21,103 INFO&#010;&gt;  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] -&#010;&gt; Determined location of main cluster component stdout file:&#010;&gt; /data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.out&#010;&gt; 2020-07-27 17:08:21,241 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Rest&#010;&gt; endpoint listening at x.x.5.60:46723&#010;&gt; 2020-07-27 17:08:21,242 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] -&#010;&gt; http://x.x.5.60:46723 was granted leadership with&#010;&gt; leaderSessionID=00000000-0000-0000-0000-000000000000&#010;&gt; 2020-07-27 17:08:21,243 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Web&#010;&gt; frontend listening at http://x.x.5.60:46723.&#010;&gt; 2020-07-27 17:08:21,256 INFO&#010;&gt;  org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;&gt; derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is&#010;&gt; less than its min value 192.000mb (201326592 bytes), min value will be used&#010;&gt; instead&#010;&gt; 2020-07-27 17:08:21,304 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.yarn.YarnResourceManager at&#010;&gt; akka://flink/user/rpc/resourcemanager_0 .&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.jobgraph-path, job.graph&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: high-availability.cluster-id,&#010;&gt; application_1568724479991_18850539&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.target, yarn-per-job&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.memory.process.size, 1 gb&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.attached, true&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.cluster.execution-mode, NORMAL&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.shutdown-on-attached-exit, false&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.jars,&#010;&gt; file:/data/rt/jar_version/sql/test.jar&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: parallelism.default, 3&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.classpaths,&#010;&gt; http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.name, RTC_TEST&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.queue, root.dp.dp_online&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.deployment.config-dir,&#010;&gt; /data/server/flink-1.11.1/conf&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt;  org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.yarn.log-config-file,&#010;&gt; /data/server/flink-1.11.1/conf/log4j.properties&#010;&gt; 2020-07-27 17:08:21,333 INFO&#010;&gt;  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] -&#010;&gt; Enabled external resources: []&#010;&gt; 2020-07-27 17:08:21,334 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Cannot get scheduler resource types: This YARN&#010;&gt; version does not support 'getSchedulerResourceTypes'&#010;&gt; 2020-07-27 17:08:21,375 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcess [] -&#010;&gt; Start JobDispatcherLeaderProcess.&#010;&gt; 2020-07-27 17:08:21,379 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.dispatcher.MiniDispatcher at&#010;&gt; akka://flink/user/rpc/dispatcher_1 .&#010;&gt; 2020-07-27 17:08:21,408 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at&#010;&gt; akka://flink/user/rpc/jobmanager_2 .&#010;&gt; 2020-07-27 17:08:21,414 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Initializing job RTC_TEST&#010;&gt; (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:21,437 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Using restart back off time strategy&#010;&gt; FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;&gt; backoffTimeMS=10000) for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Running initialization on master for job RTC_TEST&#010;&gt; (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Successfully ran initialization on master in 0 ms.&#010;&gt; 2020-07-27 17:08:21,488 INFO&#010;&gt;  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] -&#010;&gt; Built 3 pipelined regions in 1 ms&#010;&gt; 2020-07-27 17:08:21,542 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Using application-defined state backend:&#010;&gt; RocksDBStateBackend{checkpointStreamBackend=File State Backend&#010;&gt; (checkpoints: 'hdfs://HDFS00000/data/checkpoint-data/wangty/RTC_TEST',&#010;&gt; savepoints: 'null', asynchronous: UNDEFINED, fileStateThreshold: -1),&#010;&gt; localRocksDbDirectories=null, enableIncrementalCheckpointing=FALSE,&#010;&gt; numberOfTransferThreads=-1, writeBatchSize=-1}&#010;&gt; 2020-07-27 17:08:21,543 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Configuring application-defined state backend with&#010;&gt; job/cluster config&#010;&gt; 2020-07-27 17:08:21,568 INFO&#010;&gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; predefined options: DEFAULT.&#010;&gt; 2020-07-27 17:08:21,569 INFO&#010;&gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; default options factory:&#010;&gt; DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;&gt; 2020-07-27 17:08:21,714 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Recovered 0 containers from previous attempts ([]).&#010;&gt; 2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Register application master response does not contain&#010;&gt; scheduler resource types, use&#010;&gt; '$internal.yarn.resourcemanager.enable-vcore-matching'.&#010;&gt; 2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Container matching strategy: IGNORE_VCORE.&#010;&gt; 2020-07-27 17:08:21,719 INFO&#010;&gt;  org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl [] - Upper&#010;&gt; bound of the thread pool size is 500&#010;&gt; 2020-07-27 17:08:21,720 INFO&#010;&gt;  org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy []&#010;&gt; - yarn.client.max-nodemanagers-proxies : 500&#010;&gt; 2020-07-27 17:08:21,723 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - ResourceManager akka.tcp://flink@x.x.5.60:36696/user/rpc/resourcemanager_0&#010;&gt; was granted leadership with fencing token 00000000000000000000000000000000&#010;&gt; 2020-07-27 17:08:21,727 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] -&#010;&gt; Starting the SlotManager.&#010;&gt; 2020-07-27 17:08:22,126 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Using failover strategy&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@c1dab34&#010;&gt; for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:22,130 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] -&#010;&gt; JobManager runner for job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) was&#010;&gt; granted leadership with session id 00000000-0000-0000-0000-000000000000 at&#010;&gt; akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2.&#010;&gt; 2020-07-27 17:08:22,133 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Starting execution of job RTC_TEST&#010;&gt; (9f074e66a0f70274c7a7af42e71525fb) under job master id&#010;&gt; 00000000000000000000000000000000.&#010;&gt; 2020-07-27 17:08:22,135 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Starting scheduling with scheduling strategy&#010;&gt; [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;&gt; 2020-07-27 17:08:22,135 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state CREATED to&#010;&gt; RUNNING.&#010;&gt; 2020-07-27 17:08:22,145 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (8bb9f7b4bcc93895851ec47123d2213a) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:08:22,145 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (647da02fb921931e1a35ba4265d95c04) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:08:22,145 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (78211c4a866e216b6c821b743b2bf52d) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:08:22,158 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}]&#010;&gt; 2020-07-27 17:08:22,162 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}]&#010;&gt; 2020-07-27 17:08:22,162 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}]&#010;&gt; 2020-07-27 17:08:22,166 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Connecting to ResourceManager akka.tcp://flink@x.x.5.60&#010;&gt; :36696/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; 2020-07-27 17:08:22,170 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Resolved ResourceManager address, beginning&#010;&gt; registration&#010;&gt; 2020-07-27 17:08:22,173 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Registering job manager&#010;&gt; 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb.&#010;&gt; 2020-07-27 17:08:22,177 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Registered job manager&#010;&gt; 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb.&#010;&gt; 2020-07-27 17:08:22,180 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - JobManager successfully registered at ResourceManager,&#010;&gt; leader id: 00000000000000000000000000000000.&#010;&gt; 2020-07-27 17:08:22,180 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:08:22,181 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 4f255670332ee6a2bc934336cc0cee4c.&#010;&gt; 2020-07-27 17:08:22,181 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:08:22,182 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:08:22,190 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Requesting new TaskExecutor container with resource&#010;&gt; WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;&gt; taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;&gt; managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;&gt; resource is 1.&#010;&gt; 2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; f29dfd0f0ba7639992b6fe90ca0f3524.&#010;&gt; 2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Requesting new TaskExecutor container with resource&#010;&gt; WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;&gt; taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;&gt; managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;&gt; resource is 2.&#010;&gt; 2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 8a9e009c987a09bf4e36843f7a6eab62.&#010;&gt; 2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Requesting new TaskExecutor container with resource&#010;&gt; WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;&gt; taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;&gt; managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;&gt; resource is 3.&#010;&gt; 2020-07-27 17:08:27,253 INFO&#010;&gt;  org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;&gt; new token for : x.x.6.54:5006&#010;&gt; 2020-07-27 17:08:27,254 INFO&#010;&gt;  org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;&gt; new token for : x.x.4.230:5006&#010;&gt; 2020-07-27 17:08:27,254 INFO&#010;&gt;  org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;&gt; new token for : x.x.5.229:5006&#010;&gt; 2020-07-27 17:08:27,256 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Received 3 containers.&#010;&gt; 2020-07-27 17:08:27,259 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Received 3 containers with resource &lt;memory:2048,&#010;&gt; vCores:1&gt;, 0 pending container requests.&#010;&gt; 2020-07-27 17:08:27,261 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Returning excess container&#010;&gt; container_1568724479991_18850539_01_000002.&#010;&gt; 2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Returning excess container&#010;&gt; container_1568724479991_18850539_01_000003.&#010;&gt; 2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Returning excess container&#010;&gt; container_1568724479991_18850539_01_000004.&#010;&gt; 2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Accepted 0 requested containers, returned 3 excess&#010;&gt; containers, 0 pending container requests of resource &lt;memory:2048,&#010;&gt; vCores:1&gt;.&#010;&gt; 2020-07-27 17:08:45,630 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:09:15,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:09:45,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:10:15,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:10:45,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:11:15,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:11:45,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:12:15,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:12:45,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:13:15,629 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:13:22,167 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (8bb9f7b4bcc93895851ec47123d2213a) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:13:22,177 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:13:22,178 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:13:22,179 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to&#010;&gt; RESTARTING.&#010;&gt; 2020-07-27 17:13:22,181 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 8bb9f7b4bcc93895851ec47123d2213a.&#010;&gt; 2020-07-27 17:13:22,183 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] timed out.&#010;&gt; 2020-07-27 17:13:22,185 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (647da02fb921931e1a35ba4265d95c04) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:13:22,188 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:13:22,188 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:13:22,188 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 647da02fb921931e1a35ba4265d95c04.&#010;&gt; 2020-07-27 17:13:22,189 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] timed out.&#010;&gt; 2020-07-27 17:13:22,190 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (78211c4a866e216b6c821b743b2bf52d) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:13:22,192 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:13:22,192 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:13:22,192 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 78211c4a866e216b6c821b743b2bf52d.&#010;&gt; 2020-07-27 17:13:22,193 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] timed out.&#010;&gt; 2020-07-27 17:13:32,184 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (de96a2863fbb58f21cf987db0cbe380d) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:13:32,185 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:13:32,185 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 19be4edc7eadca2c14dd9ad6d1a87dcd.&#010;&gt; 2020-07-27 17:13:32,189 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (c8afff8ed430d1e5caf02d3d8383723c) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:13:32,189 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:13:32,189 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 60ef8787a8c67ed7cbc198d71838ba22.&#010;&gt; 2020-07-27 17:13:32,192 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING&#010;&gt; to RUNNING.&#010;&gt; 2020-07-27 17:13:32,193 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (08889c02dcc65774fe7ea42522f89ca9) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:13:32,193 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:13:32,194 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 3122df3dc780c5924fbd479a886d435b.&#010;&gt; 2020-07-27 17:13:57,064 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:14:27,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:14:57,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:15:27,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:15:57,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:16:27,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:16:57,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:17:27,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:17:57,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:18:27,023 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:18:32,187 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (de96a2863fbb58f21cf987db0cbe380d) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,189 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:18:32,189 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:18:32,189 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to&#010;&gt; RESTARTING.&#010;&gt; 2020-07-27 17:18:32,191 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; de96a2863fbb58f21cf987db0cbe380d.&#010;&gt; 2020-07-27 17:18:32,192 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] timed out.&#010;&gt; 2020-07-27 17:18:32,193 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (c8afff8ed430d1e5caf02d3d8383723c) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,194 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:18:32,194 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:18:32,199 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; c8afff8ed430d1e5caf02d3d8383723c.&#010;&gt; 2020-07-27 17:18:32,199 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] timed out.&#010;&gt; 2020-07-27 17:18:32,200 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (08889c02dcc65774fe7ea42522f89ca9) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,203 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:18:32,204 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:18:32,204 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING&#010;&gt; to FAILING.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;&gt; backoffTimeMS=10000)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,208 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state FAILING to&#010;&gt; FAILED.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;&gt; backoffTimeMS=10000)&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,210 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping&#010;&gt; checkpoint coordinator for job 9f074e66a0f70274c7a7af42e71525fb.&#010;&gt; 2020-07-27 17:18:32,210 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore []&#010;&gt; - Shutting down&#010;&gt; 2020-07-27 17:18:32,210 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 08889c02dcc65774fe7ea42522f89ca9.&#010;&gt; 2020-07-27 17:18:32,211 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] timed out.&#010;&gt; 2020-07-27 17:18:32,216 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.MiniDispatcher           [] - Job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb reached globally terminal state FAILED.&#010;&gt; 2020-07-27 17:18:32,217 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Stopping the JobMaster for job&#010;&gt; RTC_TEST(9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:18:32,224 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Suspending SlotPool.&#010;&gt; 2020-07-27 17:18:32,225 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Close ResourceManager connection&#010;&gt; 4ee5ce2ea08809b1cca2745fa12ee663: JobManager is shutting down..&#010;&gt; 2020-07-27 17:18:32,225 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping&#010;&gt; SlotPool.&#010;&gt; 2020-07-27 17:18:32,225 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt;                  [] - Disconnect job manager&#010;&gt; 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb from the resource manager.&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<15387299.84e6.1739013f574.Coremail.apache22@163.com>"
    },
    {
        "id": "<59b2e427.253f.173942b96cb.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 06:45:51 GMT",
        "subject": "回复： flink1.11.1启动问题",
        "content": "谢谢你，我将flink-conf.yaml的taskmanager.memory.process.size由1728m调成2048m，这个问题解决了, 但是我觉得，这个问题应该很常见才对，怎么偏被我碰上了，flink这个设计是正常的吗？&#010;&#010;&#010;&#010;&#010;在2020年07月28日 10:26，Xintong Song&lt;tonysong820@gmail.com&gt; 写道：&#010;建议确认一下 Yarn 的配置 “yarn.scheduler.minimum-allocation-mb” 在 Yarn RM 和 Flink JM&#010;这两台机器上是否一致。&#010;&#010;Yarn 会对 container request 做归一化。例如你请求的 TM container 是 1728m&#010;(taskmanager.memory.process.size) ，如果 minimum-allocation-mb 是 1024m，那么实际得到的&#010;container 大小必须是 minimum-allocation-mb 的整数倍，也就是 2048m。Flink 会去获取 Yarn 的配置，计算&#010;container request 实际分到的 container 应该多大，并对分到的 container 进行检查。现在看 JM 日志，分下来的&#010;container 并没有通过这个检查，造成 Flink 认为 container 规格不匹配。这里最可能的原因是 Flink 拿到的&#010;minimum-allocation-mb 和 Yarn RM 实际使用的不一致。&#010;&#010;Thank you~&#010;&#010;Xintong Song&#010;&#010;&#010;&#010;On Mon, Jul 27, 2020 at 7:42 PM 酷酷的浑蛋 &lt;apache22@163.com&gt; wrote:&#010;&#010;&#010;首先，flink1.9提交到yarn集群是没有问题的，同等的配置提交flink1.11.1到yarn集群就报下面的错误&#010;2020-07-27 17:08:14,661 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;--------------------------------------------------------------------------------&#010;2020-07-27 17:08:14,665 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;Starting YarnJobClusterEntrypoint (Version: 1.11.1, Scala: 2.11,&#010;Rev:7eb514a, Date:2020-07-15T07:02:09+02:00)&#010;2020-07-27 17:08:14,665 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS&#010;current user: hadoop&#010;2020-07-27 17:08:15,417 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current&#010;Hadoop/Kerberos user: wangty&#010;2020-07-27 17:08:15,418 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM:&#010;Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.191-b12&#010;2020-07-27 17:08:15,418 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum&#010;heap size: 429 MiBytes&#010;2020-07-27 17:08:15,418 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;JAVA_HOME: /usr/local/jdk/&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop&#010;version: 2.7.7&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM&#010;Options:&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;-Xmx469762048&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;-Xms469762048&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;-XX:MaxMetaspaceSize=268435456&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;-Dlog.file=/data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;-Dlog4j.configuration=file:log4j.properties&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;-Dlog4j.configurationFile=file:log4j.properties&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program&#010;Arguments: (none)&#010;2020-07-27 17:08:15,419 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;Classpath:&#010;:lib/flink-csv-1.11.1.jar:lib/flink-json-1.11.1.jar:lib/flink-shaded-zookeeper-3.4.14.jar:lib/flink-table-blink_2.11-1.11.1.jar:lib/flink-table_2.11-1.11.1.jar:lib/log4j-1.2-api-2.12.1.jar:lib/log4j-api-2.12.1.jar:lib/log4j-core-2.12.1.jar:lib/log4j-slf4j-impl-2.12.1.jar:test.jar:flink-dist_2.11-1.11.1.jar:job.graph:flink-conf.yaml::/usr/local/service/hadoop/etc/hadoop:/usr/local/service/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-databind-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-annotations-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/service/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-temrfs-1.0.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/service/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/joda-time-2.9.7.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/spark-2.0.2-yarn-shuffle.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/hadoop-lzo-0.4.20.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar&#010;2020-07-27 17:08:15,420 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;--------------------------------------------------------------------------------&#010;2020-07-27 17:08:15,421 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;Registered UNIX signal handlers for [TERM, HUP, INT]&#010;2020-07-27 17:08:15,424 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - YARN&#010;daemon is running as: wangty Yarn client user obtainer: wangty&#010;2020-07-27 17:08:15,427 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: taskmanager.memory.process.size, 1728m&#010;2020-07-27 17:08:15,427 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: internal.jobgraph-path, job.graph&#010;2020-07-27 17:08:15,427 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.execution.failover-strategy, region&#010;2020-07-27 17:08:15,427 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: high-availability.cluster-id,&#010;application_1568724479991_18850539&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.rpc.address, localhost&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.target, yarn-per-job&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.memory.process.size, 1 gb&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.rpc.port, 6123&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.attached, true&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: internal.cluster.execution-mode, NORMAL&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.shutdown-on-attached-exit, false&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: pipeline.jars,&#010;file:/data/rt/jar_version/sql/test.jar&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: parallelism.default, 3&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: taskmanager.numberOfTaskSlots, 1&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: pipeline.classpaths,&#010;http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: yarn.application.name, RTC_TEST&#010;2020-07-27 17:08:15,428 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: yarn.application.queue, root.dp.dp_online&#010;2020-07-27 17:08:15,429 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: $internal.deployment.config-dir,&#010;/data/server/flink-1.11.1/conf&#010;2020-07-27 17:08:15,429 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: $internal.yarn.log-config-file,&#010;/data/server/flink-1.11.1/conf/log4j.properties&#010;2020-07-27 17:08:15,455 WARN  org.apache.flink.configuration.Configuration&#010;[] - Config uses deprecated configuration key 'web.port'&#010;instead of proper key 'rest.bind-port'&#010;2020-07-27 17:08:15,465 INFO&#010;org.apache.flink.runtime.clusterframework.BootstrapTools     [] - Setting&#010;directories for temporary files to:&#010;/data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data2/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539&#010;2020-07-27 17:08:15,471 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting&#010;YarnJobClusterEntrypoint.&#010;2020-07-27 17:08:15,993 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install&#010;default filesystem.&#010;2020-07-27 17:08:16,235 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install&#010;security context.&#010;2020-07-27 17:08:16,715 INFO&#010;org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop&#010;user set to wangty (auth:SIMPLE)&#010;2020-07-27 17:08:16,722 INFO&#010;org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas&#010;file will be created as&#010;/data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/jaas-8303363038541870345.conf.&#010;2020-07-27 17:08:16,729 INFO&#010;org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;Initializing cluster services.&#010;2020-07-27 17:08:16,741 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying&#010;to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0&#010;.&#010;2020-07-27 17:08:18,830 INFO  akka.event.slf4j.Slf4jLogger&#010;[] - Slf4jLogger started&#010;2020-07-27 17:08:19,781 INFO  akka.remote.Remoting&#010;[] - Starting remoting&#010;2020-07-27 17:08:19,936 INFO  akka.remote.Remoting&#010;[] - Remoting started; listening on addresses&#010;:[akka.tcp://flink@x.x.x.60:36696]&#010;2020-07-27 17:08:20,021 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor&#010;system started at akka.tcp://flink@x.x.x.60:36696&#010;2020-07-27 17:08:20,042 WARN  org.apache.flink.configuration.Configuration&#010;[] - Config uses deprecated configuration key 'web.port'&#010;instead of proper key 'rest.port'&#010;2020-07-27 17:08:20,049 INFO  org.apache.flink.runtime.blob.BlobServer&#010;[] - Created BLOB server storage directory&#010;/data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/blobStore-86aff9db-0f30-4688-9e68-b8e5866a93c7&#010;2020-07-27 17:08:20,054 INFO  org.apache.flink.runtime.blob.BlobServer&#010;[] - Started BLOB server at 0.0.0.0:56782 - max&#010;concurrent requests: 50 - max backlog: 1000&#010;2020-07-27 17:08:20,063 INFO&#010;org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No&#010;metrics reporter configured, no metrics will be exposed/reported.&#010;2020-07-27 17:08:20,066 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying&#010;to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0&#010;.&#010;2020-07-27 17:08:20,082 INFO  akka.event.slf4j.Slf4jLogger&#010;[] - Slf4jLogger started&#010;2020-07-27 17:08:20,086 INFO  akka.remote.Remoting&#010;[] - Starting remoting&#010;2020-07-27 17:08:20,093 INFO  akka.remote.Remoting&#010;[] - Remoting started; listening on addresses&#010;:[akka.tcp://flink-metrics@x.x.5.60:60801]&#010;2020-07-27 17:08:20,794 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor&#010;system started at akka.tcp://flink-metrics@x.x.5.60:60801&#010;2020-07-27 17:08:20,810 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService&#010;at akka://flink-metrics/user/rpc/MetricQueryService .&#010;2020-07-27 17:08:20,856 WARN  org.apache.flink.configuration.Configuration&#010;[] - Config uses deprecated configuration key 'web.port'&#010;instead of proper key 'rest.bind-port'&#010;2020-07-27 17:08:20,858 INFO&#010;org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Upload&#010;directory&#010;/tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload does&#010;not exist.&#010;2020-07-27 17:08:20,859 INFO&#010;org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Created&#010;directory&#010;/tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload for&#010;file uploads.&#010;2020-07-27 17:08:20,874 INFO&#010;org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] -&#010;Starting rest endpoint.&#010;2020-07-27 17:08:21,103 INFO&#010;org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] -&#010;Determined location of main cluster component log file:&#010;/data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;2020-07-27 17:08:21,103 INFO&#010;org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] -&#010;Determined location of main cluster component stdout file:&#010;/data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.out&#010;2020-07-27 17:08:21,241 INFO&#010;org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Rest&#010;endpoint listening at x.x.5.60:46723&#010;2020-07-27 17:08:21,242 INFO&#010;org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] -&#010;http://x.x.5.60:46723 was granted leadership with&#010;leaderSessionID=00000000-0000-0000-0000-000000000000&#010;2020-07-27 17:08:21,243 INFO&#010;org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Web&#010;frontend listening at http://x.x.5.60:46723.&#010;2020-07-27 17:08:21,256 INFO&#010;org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is&#010;less than its min value 192.000mb (201326592 bytes), min value will be used&#010;instead&#010;2020-07-27 17:08:21,304 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;RPC endpoint for org.apache.flink.yarn.YarnResourceManager at&#010;akka://flink/user/rpc/resourcemanager_0 .&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: taskmanager.memory.process.size, 1728m&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: internal.jobgraph-path, job.graph&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.execution.failover-strategy, region&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: high-availability.cluster-id,&#010;application_1568724479991_18850539&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.rpc.address, localhost&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.target, yarn-per-job&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.memory.process.size, 1 gb&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: jobmanager.rpc.port, 6123&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.attached, true&#010;2020-07-27 17:08:21,314 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: internal.cluster.execution-mode, NORMAL&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: execution.shutdown-on-attached-exit, false&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: pipeline.jars,&#010;file:/data/rt/jar_version/sql/test.jar&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: parallelism.default, 3&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: taskmanager.numberOfTaskSlots, 1&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: pipeline.classpaths,&#010;http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: yarn.application.name, RTC_TEST&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: yarn.application.queue, root.dp.dp_online&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: $internal.deployment.config-dir,&#010;/data/server/flink-1.11.1/conf&#010;2020-07-27 17:08:21,315 INFO&#010;org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;configuration property: $internal.yarn.log-config-file,&#010;/data/server/flink-1.11.1/conf/log4j.properties&#010;2020-07-27 17:08:21,333 INFO&#010;org.apache.flink.runtime.externalresource.ExternalResourceUtils [] -&#010;Enabled external resources: []&#010;2020-07-27 17:08:21,334 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Cannot get scheduler resource types: This YARN&#010;version does not support 'getSchedulerResourceTypes'&#010;2020-07-27 17:08:21,375 INFO&#010;org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcess [] -&#010;Start JobDispatcherLeaderProcess.&#010;2020-07-27 17:08:21,379 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;RPC endpoint for org.apache.flink.runtime.dispatcher.MiniDispatcher at&#010;akka://flink/user/rpc/dispatcher_1 .&#010;2020-07-27 17:08:21,408 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at&#010;akka://flink/user/rpc/jobmanager_2 .&#010;2020-07-27 17:08:21,414 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Initializing job RTC_TEST&#010;(9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:21,437 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Using restart back off time strategy&#010;FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;backoffTimeMS=10000) for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Running initialization on master for job RTC_TEST&#010;(9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Successfully ran initialization on master in 0 ms.&#010;2020-07-27 17:08:21,488 INFO&#010;org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] -&#010;Built 3 pipelined regions in 1 ms&#010;2020-07-27 17:08:21,542 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Using application-defined state backend:&#010;RocksDBStateBackend{checkpointStreamBackend=File State Backend&#010;(checkpoints: 'hdfs://HDFS00000/data/checkpoint-data/wangty/RTC_TEST',&#010;savepoints: 'null', asynchronous: UNDEFINED, fileStateThreshold: -1),&#010;localRocksDbDirectories=null, enableIncrementalCheckpointing=FALSE,&#010;numberOfTransferThreads=-1, writeBatchSize=-1}&#010;2020-07-27 17:08:21,543 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Configuring application-defined state backend with&#010;job/cluster config&#010;2020-07-27 17:08:21,568 INFO&#010;org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;predefined options: DEFAULT.&#010;2020-07-27 17:08:21,569 INFO&#010;org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;default options factory:&#010;DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;2020-07-27 17:08:21,714 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Recovered 0 containers from previous attempts ([]).&#010;2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Register application master response does not contain&#010;scheduler resource types, use&#010;'$internal.yarn.resourcemanager.enable-vcore-matching'.&#010;2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Container matching strategy: IGNORE_VCORE.&#010;2020-07-27 17:08:21,719 INFO&#010;org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl [] - Upper&#010;bound of the thread pool size is 500&#010;2020-07-27 17:08:21,720 INFO&#010;org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy []&#010;- yarn.client.max-nodemanagers-proxies : 500&#010;2020-07-27 17:08:21,723 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - ResourceManager akka.tcp://flink@x.x.5.60:36696/user/rpc/resourcemanager_0&#010;was granted leadership with fencing token 00000000000000000000000000000000&#010;2020-07-27 17:08:21,727 INFO&#010;org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] -&#010;Starting the SlotManager.&#010;2020-07-27 17:08:22,126 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Using failover strategy&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@c1dab34&#010;for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:08:22,130 INFO&#010;org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] -&#010;JobManager runner for job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) was&#010;granted leadership with session id 00000000-0000-0000-0000-000000000000 at&#010;akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2.&#010;2020-07-27 17:08:22,133 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Starting execution of job RTC_TEST&#010;(9f074e66a0f70274c7a7af42e71525fb) under job master id&#010;00000000000000000000000000000000.&#010;2020-07-27 17:08:22,135 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Starting scheduling with scheduling strategy&#010;[org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;2020-07-27 17:08:22,135 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state CREATED to&#010;RUNNING.&#010;2020-07-27 17:08:22,145 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;(8bb9f7b4bcc93895851ec47123d2213a) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:08:22,145 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;(647da02fb921931e1a35ba4265d95c04) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:08:22,145 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;(78211c4a866e216b6c821b743b2bf52d) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:08:22,158 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending request&#010;[SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}]&#010;2020-07-27 17:08:22,162 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending request&#010;[SlotRequestId{3ca5a64e992d87a27f207e6020eea047}]&#010;2020-07-27 17:08:22,162 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending request&#010;[SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}]&#010;2020-07-27 17:08:22,166 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Connecting to ResourceManager akka.tcp://flink@x.x.5.60&#010;:36696/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;2020-07-27 17:08:22,170 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Resolved ResourceManager address, beginning&#010;registration&#010;2020-07-27 17:08:22,173 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Registering job manager&#010;00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2&#010;for job 9f074e66a0f70274c7a7af42e71525fb.&#010;2020-07-27 17:08:22,177 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Registered job manager&#010;00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2&#010;for job 9f074e66a0f70274c7a7af42e71525fb.&#010;2020-07-27 17:08:22,180 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - JobManager successfully registered at ResourceManager,&#010;leader id: 00000000000000000000000000000000.&#010;2020-07-27 17:08:22,180 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:08:22,181 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Request slot with profile ResourceProfile{UNKNOWN}&#010;for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;4f255670332ee6a2bc934336cc0cee4c.&#010;2020-07-27 17:08:22,181 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:08:22,182 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:08:22,190 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Requesting new TaskExecutor container with resource&#010;WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;resource is 1.&#010;2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Request slot with profile ResourceProfile{UNKNOWN}&#010;for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;f29dfd0f0ba7639992b6fe90ca0f3524.&#010;2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Requesting new TaskExecutor container with resource&#010;WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;resource is 2.&#010;2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Request slot with profile ResourceProfile{UNKNOWN}&#010;for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;8a9e009c987a09bf4e36843f7a6eab62.&#010;2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Requesting new TaskExecutor container with resource&#010;WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;resource is 3.&#010;2020-07-27 17:08:27,253 INFO&#010;org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;new token for : x.x.6.54:5006&#010;2020-07-27 17:08:27,254 INFO&#010;org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;new token for : x.x.4.230:5006&#010;2020-07-27 17:08:27,254 INFO&#010;org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;new token for : x.x.5.229:5006&#010;2020-07-27 17:08:27,256 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Received 3 containers.&#010;2020-07-27 17:08:27,259 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Received 3 containers with resource &lt;memory:2048,&#010;vCores:1&gt;, 0 pending container requests.&#010;2020-07-27 17:08:27,261 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Returning excess container&#010;container_1568724479991_18850539_01_000002.&#010;2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Returning excess container&#010;container_1568724479991_18850539_01_000003.&#010;2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Returning excess container&#010;container_1568724479991_18850539_01_000004.&#010;2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Accepted 0 requested containers, returned 3 excess&#010;containers, 0 pending container requests of resource &lt;memory:2048,&#010;vCores:1&gt;.&#010;2020-07-27 17:08:45,630 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:09:15,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:09:45,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:10:15,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:10:45,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:11:15,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:11:45,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:12:15,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:12:45,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:13:15,629 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:13:22,167 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;(8bb9f7b4bcc93895851ec47123d2213a) switched from SCHEDULED to FAILED on not&#010;deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:13:22,177 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - Calculating tasks to restart to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_0.&#010;2020-07-27 17:13:22,178 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - 1 tasks should be restarted to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_0.&#010;2020-07-27 17:13:22,179 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to&#010;RESTARTING.&#010;2020-07-27 17:13:22,181 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;8bb9f7b4bcc93895851ec47123d2213a.&#010;2020-07-27 17:13:22,183 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;slot request [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] timed out.&#010;2020-07-27 17:13:22,185 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;(647da02fb921931e1a35ba4265d95c04) switched from SCHEDULED to FAILED on not&#010;deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:13:22,188 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - Calculating tasks to restart to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_1.&#010;2020-07-27 17:13:22,188 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - 1 tasks should be restarted to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_1.&#010;2020-07-27 17:13:22,188 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;647da02fb921931e1a35ba4265d95c04.&#010;2020-07-27 17:13:22,189 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;slot request [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] timed out.&#010;2020-07-27 17:13:22,190 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;(78211c4a866e216b6c821b743b2bf52d) switched from SCHEDULED to FAILED on not&#010;deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:13:22,192 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - Calculating tasks to restart to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_2.&#010;2020-07-27 17:13:22,192 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - 1 tasks should be restarted to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_2.&#010;2020-07-27 17:13:22,192 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;78211c4a866e216b6c821b743b2bf52d.&#010;2020-07-27 17:13:22,193 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;slot request [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] timed out.&#010;2020-07-27 17:13:32,184 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;(de96a2863fbb58f21cf987db0cbe380d) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:13:32,185 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:13:32,185 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Request slot with profile ResourceProfile{UNKNOWN}&#010;for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;19be4edc7eadca2c14dd9ad6d1a87dcd.&#010;2020-07-27 17:13:32,189 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;(c8afff8ed430d1e5caf02d3d8383723c) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:13:32,189 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:13:32,189 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Request slot with profile ResourceProfile{UNKNOWN}&#010;for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;60ef8787a8c67ed7cbc198d71838ba22.&#010;2020-07-27 17:13:32,192 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING&#010;to RUNNING.&#010;2020-07-27 17:13:32,193 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;(08889c02dcc65774fe7ea42522f89ca9) switched from CREATED to SCHEDULED.&#010;2020-07-27 17:13:32,193 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-27 17:13:32,194 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Request slot with profile ResourceProfile{UNKNOWN}&#010;for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;3122df3dc780c5924fbd479a886d435b.&#010;2020-07-27 17:13:57,064 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:14:27,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:14:57,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:15:27,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:15:57,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:16:27,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:16:57,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:17:27,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:17:57,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:18:27,023 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;instead. Aborting checkpoint.&#010;2020-07-27 17:18:32,187 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;(de96a2863fbb58f21cf987db0cbe380d) switched from SCHEDULED to FAILED on not&#010;deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,189 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - Calculating tasks to restart to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_0.&#010;2020-07-27 17:18:32,189 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - 1 tasks should be restarted to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_0.&#010;2020-07-27 17:18:32,189 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to&#010;RESTARTING.&#010;2020-07-27 17:18:32,191 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;de96a2863fbb58f21cf987db0cbe380d.&#010;2020-07-27 17:18:32,192 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;slot request [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] timed out.&#010;2020-07-27 17:18:32,193 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;(c8afff8ed430d1e5caf02d3d8383723c) switched from SCHEDULED to FAILED on not&#010;deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,194 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - Calculating tasks to restart to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_1.&#010;2020-07-27 17:18:32,194 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - 1 tasks should be restarted to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_1.&#010;2020-07-27 17:18:32,199 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;c8afff8ed430d1e5caf02d3d8383723c.&#010;2020-07-27 17:18:32,199 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;slot request [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] timed out.&#010;2020-07-27 17:18:32,200 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;(08889c02dcc65774fe7ea42522f89ca9) switched from SCHEDULED to FAILED on not&#010;deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,203 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - Calculating tasks to restart to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_2.&#010;2020-07-27 17:18:32,204 INFO&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - 1 tasks should be restarted to recover the failed task&#010;66774974224095ed80fefb1f583d9fb9_2.&#010;2020-07-27 17:18:32,204 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING&#010;to FAILING.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;backoffTimeMS=10000)&#010;at&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by:&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;... 45 more&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,208 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state FAILING to&#010;FAILED.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;backoffTimeMS=10000)&#010;at&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[test.jar:?]&#010;at&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_191]&#010;at&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[test.jar:?]&#010;at&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[test.jar:?]&#010;at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[test.jar:?]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[test.jar:?]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[test.jar:?]&#010;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[test.jar:?]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[test.jar:?]&#010;Caused by:&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[test.jar:?]&#010;... 45 more&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;~[?:1.8.0_191]&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;~[?:1.8.0_191]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-27 17:18:32,210 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping&#010;checkpoint coordinator for job 9f074e66a0f70274c7a7af42e71525fb.&#010;2020-07-27 17:18:32,210 INFO&#010;org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore []&#010;- Shutting down&#010;2020-07-27 17:18:32,210 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;08889c02dcc65774fe7ea42522f89ca9.&#010;2020-07-27 17:18:32,211 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;slot request [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] timed out.&#010;2020-07-27 17:18:32,216 INFO&#010;org.apache.flink.runtime.dispatcher.MiniDispatcher           [] - Job&#010;9f074e66a0f70274c7a7af42e71525fb reached globally terminal state FAILED.&#010;2020-07-27 17:18:32,217 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Stopping the JobMaster for job&#010;RTC_TEST(9f074e66a0f70274c7a7af42e71525fb).&#010;2020-07-27 17:18:32,224 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Suspending SlotPool.&#010;2020-07-27 17:18:32,225 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Close ResourceManager connection&#010;4ee5ce2ea08809b1cca2745fa12ee663: JobManager is shutting down..&#010;2020-07-27 17:18:32,225 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping&#010;SlotPool.&#010;2020-07-27 17:18:32,225 INFO  org.apache.flink.yarn.YarnResourceManager&#010;[] - Disconnect job manager&#010;00000000000000000000000000000000@akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2&#010;for job 9f074e66a0f70274c7a7af42e71525fb from the resource manager.&#010;&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<15387299.84e6.1739013f574.Coremail.apache22@163.com>"
    },
    {
        "id": "<CAHsnkPt7u7eLaJcgVeLaehyAaD=Amx5wm5jpkz4gt9yB9tCXjA@mail.gmail.com>",
        "from": "Xintong Song &lt;tonysong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 05:09:10 GMT",
        "subject": "Re: flink1.11.1启动问题",
        "content": "这个是 hadoop 2.x 的已知设计缺陷。&#010;&#010;hadoop 2.x 中，container request 没有唯一的标识，且分配下来的 container&#010;的资源和请求的资源也可能不同，为了将分下来的 container 对应到之前的 request，flink 不得不去进行归一化的计算。如果 yarn&#010;集群 RM 和 AM 机器上的配置不一致，是有潜在风险的。hadoop 3.x 中改进了这一问题，每个 container request 都有一个&#010;id，可以用来将分配到的 container 与之前的 request 对应起来。出于对旧版本 hadoop 的兼容性考虑，flink&#010;仍然采用的是计算资源的方式匹配 container。&#010;&#010;flink 1.9 当中没有遇到这个问题，是因为默认所有 container 都是相同规格的，所以省略了匹配过程。目前 flink&#010;社区正在开发支持申请不同规格 container 调度能力，因此在 1.11 种增加了验证 container 资源的逻辑。&#010;&#010;Thank you~&#010;&#010;Xintong Song&#010;&#010;&#010;&#010;On Tue, Jul 28, 2020 at 2:46 PM 酷酷的浑蛋 &lt;apache22@163.com&gt; wrote:&#010;&#010;&gt; 谢谢你，我将flink-conf.yaml的taskmanager.memory.process.size由1728m调成2048m，这个问题解决了,&#010;&gt; 但是我觉得，这个问题应该很常见才对，怎么偏被我碰上了，flink这个设计是正常的吗？&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在2020年07月28日 10:26，Xintong Song&lt;tonysong820@gmail.com&gt; 写道：&#010;&gt; 建议确认一下 Yarn 的配置 “yarn.scheduler.minimum-allocation-mb” 在 Yarn RM 和 Flink JM&#010;&gt; 这两台机器上是否一致。&#010;&gt;&#010;&gt; Yarn 会对 container request 做归一化。例如你请求的 TM container 是 1728m&#010;&gt; (taskmanager.memory.process.size) ，如果 minimum-allocation-mb 是 1024m，那么实际得到的&#010;&gt; container 大小必须是 minimum-allocation-mb 的整数倍，也就是 2048m。Flink 会去获取 Yarn 的配置，计算&#010;&gt; container request 实际分到的 container 应该多大，并对分到的 container 进行检查。现在看 JM 日志，分下来的&#010;&gt; container 并没有通过这个检查，造成 Flink 认为 container 规格不匹配。这里最可能的原因是 Flink 拿到的&#010;&gt; minimum-allocation-mb 和 Yarn RM 实际使用的不一致。&#010;&gt;&#010;&gt; Thank you~&#010;&gt;&#010;&gt; Xintong Song&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; On Mon, Jul 27, 2020 at 7:42 PM 酷酷的浑蛋 &lt;apache22@163.com&gt; wrote:&#010;&gt;&#010;&gt;&#010;&gt; 首先，flink1.9提交到yarn集群是没有问题的，同等的配置提交flink1.11.1到yarn集群就报下面的错误&#010;&gt; 2020-07-27 17:08:14,661 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt;&#010;&gt; --------------------------------------------------------------------------------&#010;&gt; 2020-07-27 17:08:14,665 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; Starting YarnJobClusterEntrypoint (Version: 1.11.1, Scala: 2.11,&#010;&gt; Rev:7eb514a, Date:2020-07-15T07:02:09+02:00)&#010;&gt; 2020-07-27 17:08:14,665 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  OS&#010;&gt; current user: hadoop&#010;&gt; 2020-07-27 17:08:15,417 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Current&#010;&gt; Hadoop/Kerberos user: wangty&#010;&gt; 2020-07-27 17:08:15,418 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM:&#010;&gt; Java HotSpot(TM) 64-Bit Server VM - Oracle Corporation - 1.8/25.191-b12&#010;&gt; 2020-07-27 17:08:15,418 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Maximum&#010;&gt; heap size: 429 MiBytes&#010;&gt; 2020-07-27 17:08:15,418 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; JAVA_HOME: /usr/local/jdk/&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Hadoop&#010;&gt; version: 2.7.7&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  JVM&#010;&gt; Options:&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Xmx469762048&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Xms469762048&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -XX:MaxMetaspaceSize=268435456&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt;&#010;&gt; -Dlog.file=/data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Dlog4j.configuration=file:log4j.properties&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; -Dlog4j.configurationFile=file:log4j.properties&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -  Program&#010;&gt; Arguments: (none)&#010;&gt; 2020-07-27 17:08:15,419 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; Classpath:&#010;&gt;&#010;&gt; :lib/flink-csv-1.11.1.jar:lib/flink-json-1.11.1.jar:lib/flink-shaded-zookeeper-3.4.14.jar:lib/flink-table-blink_2.11-1.11.1.jar:lib/flink-table_2.11-1.11.1.jar:lib/log4j-1.2-api-2.12.1.jar:lib/log4j-api-2.12.1.jar:lib/log4j-core-2.12.1.jar:lib/log4j-slf4j-impl-2.12.1.jar:test.jar:flink-dist_2.11-1.11.1.jar:job.graph:flink-conf.yaml::/usr/local/service/hadoop/etc/hadoop:/usr/local/service/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/service/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-core-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-databind-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-annotations-2.2.3.jar:/usr/local/service/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/service/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/service/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/common/lib/hadoop-temrfs-1.0.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/service/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/service/hadoop/share/hadoop/common/lib/joda-time-2.9.7.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/service/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/spark-2.0.2-yarn-shuffle.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/hadoop-lzo-0.4.20.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/service/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar&#010;&gt; 2020-07-27 17:08:15,420 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt;&#010;&gt; --------------------------------------------------------------------------------&#010;&gt; 2020-07-27 17:08:15,421 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; Registered UNIX signal handlers for [TERM, HUP, INT]&#010;&gt; 2020-07-27 17:08:15,424 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - YARN&#010;&gt; daemon is running as: wangty Yarn client user obtainer: wangty&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.jobgraph-path, job.graph&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; 2020-07-27 17:08:15,427 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: high-availability.cluster-id,&#010;&gt; application_1568724479991_18850539&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.target, yarn-per-job&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.memory.process.size, 1 gb&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.attached, true&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.cluster.execution-mode, NORMAL&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.shutdown-on-attached-exit, false&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.jars,&#010;&gt; file:/data/rt/jar_version/sql/test.jar&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: parallelism.default, 3&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.classpaths,&#010;&gt; http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.name, RTC_TEST&#010;&gt; 2020-07-27 17:08:15,428 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.queue, root.dp.dp_online&#010;&gt; 2020-07-27 17:08:15,429 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.deployment.config-dir,&#010;&gt; /data/server/flink-1.11.1/conf&#010;&gt; 2020-07-27 17:08:15,429 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.yarn.log-config-file,&#010;&gt; /data/server/flink-1.11.1/conf/log4j.properties&#010;&gt; 2020-07-27 17:08:15,455 WARN  org.apache.flink.configuration.Configuration&#010;&gt; [] - Config uses deprecated configuration key 'web.port'&#010;&gt; instead of proper key 'rest.bind-port'&#010;&gt; 2020-07-27 17:08:15,465 INFO&#010;&gt; org.apache.flink.runtime.clusterframework.BootstrapTools     [] - Setting&#010;&gt; directories for temporary files to:&#010;&gt;&#010;&gt; /data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data2/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539,/data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539&#010;&gt; 2020-07-27 17:08:15,471 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Starting&#010;&gt; YarnJobClusterEntrypoint.&#010;&gt; 2020-07-27 17:08:15,993 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install&#010;&gt; default filesystem.&#010;&gt; 2020-07-27 17:08:16,235 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Install&#010;&gt; security context.&#010;&gt; 2020-07-27 17:08:16,715 INFO&#010;&gt; org.apache.flink.runtime.security.modules.HadoopModule       [] - Hadoop&#010;&gt; user set to wangty (auth:SIMPLE)&#010;&gt; 2020-07-27 17:08:16,722 INFO&#010;&gt; org.apache.flink.runtime.security.modules.JaasModule         [] - Jaas&#010;&gt; file will be created as&#010;&gt;&#010;&gt; /data1/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/jaas-8303363038541870345.conf.&#010;&gt; 2020-07-27 17:08:16,729 INFO&#010;&gt; org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] -&#010;&gt; Initializing cluster services.&#010;&gt; 2020-07-27 17:08:16,741 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying&#010;&gt; to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0&#010;&gt; .&#010;&gt; 2020-07-27 17:08:18,830 INFO  akka.event.slf4j.Slf4jLogger&#010;&gt; [] - Slf4jLogger started&#010;&gt; 2020-07-27 17:08:19,781 INFO  akka.remote.Remoting&#010;&gt; [] - Starting remoting&#010;&gt; 2020-07-27 17:08:19,936 INFO  akka.remote.Remoting&#010;&gt; [] - Remoting started; listening on addresses&#010;&gt; :[akka.tcp://flink@x.x.x.60:36696]&#010;&gt; 2020-07-27 17:08:20,021 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor&#010;&gt; system started at akka.tcp://flink@x.x.x.60:36696&#010;&gt; 2020-07-27 17:08:20,042 WARN  org.apache.flink.configuration.Configuration&#010;&gt; [] - Config uses deprecated configuration key 'web.port'&#010;&gt; instead of proper key 'rest.port'&#010;&gt; 2020-07-27 17:08:20,049 INFO  org.apache.flink.runtime.blob.BlobServer&#010;&gt; [] - Created BLOB server storage directory&#010;&gt;&#010;&gt; /data3/emr/yarn/local/usercache/wangty/appcache/application_1568724479991_18850539/blobStore-86aff9db-0f30-4688-9e68-b8e5866a93c7&#010;&gt; 2020-07-27 17:08:20,054 INFO  org.apache.flink.runtime.blob.BlobServer&#010;&gt; [] - Started BLOB server at 0.0.0.0:56782 - max&#010;&gt; concurrent requests: 50 - max backlog: 1000&#010;&gt; 2020-07-27 17:08:20,063 INFO&#010;&gt; org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No&#010;&gt; metrics reporter configured, no metrics will be exposed/reported.&#010;&gt; 2020-07-27 17:08:20,066 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying&#010;&gt; to start actor system, external address x.x.5.60:0, bind address 0.0.0.0:0&#010;&gt; .&#010;&gt; 2020-07-27 17:08:20,082 INFO  akka.event.slf4j.Slf4jLogger&#010;&gt; [] - Slf4jLogger started&#010;&gt; 2020-07-27 17:08:20,086 INFO  akka.remote.Remoting&#010;&gt; [] - Starting remoting&#010;&gt; 2020-07-27 17:08:20,093 INFO  akka.remote.Remoting&#010;&gt; [] - Remoting started; listening on addresses&#010;&gt; :[akka.tcp://flink-metrics@x.x.5.60:60801]&#010;&gt; 2020-07-27 17:08:20,794 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor&#010;&gt; system started at akka.tcp://flink-metrics@x.x.5.60:60801&#010;&gt; 2020-07-27 17:08:20,810 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService&#010;&gt; at akka://flink-metrics/user/rpc/MetricQueryService .&#010;&gt; 2020-07-27 17:08:20,856 WARN  org.apache.flink.configuration.Configuration&#010;&gt; [] - Config uses deprecated configuration key 'web.port'&#010;&gt; instead of proper key 'rest.bind-port'&#010;&gt; 2020-07-27 17:08:20,858 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Upload&#010;&gt; directory&#010;&gt; /tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload does&#010;&gt; not exist.&#010;&gt; 2020-07-27 17:08:20,859 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Created&#010;&gt; directory&#010;&gt; /tmp/flink-web-f3b225c5-e01d-4dfb-9091-aca7bb8e6192/flink-web-upload for&#010;&gt; file uploads.&#010;&gt; 2020-07-27 17:08:20,874 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] -&#010;&gt; Starting rest endpoint.&#010;&gt; 2020-07-27 17:08:21,103 INFO&#010;&gt; org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] -&#010;&gt; Determined location of main cluster component log file:&#010;&gt;&#010;&gt; /data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.log&#010;&gt; 2020-07-27 17:08:21,103 INFO&#010;&gt; org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] -&#010;&gt; Determined location of main cluster component stdout file:&#010;&gt;&#010;&gt; /data/emr/yarn/logs/application_1568724479991_18850539/container_e25_1568724479991_18850539_01_000001/jobmanager.out&#010;&gt; 2020-07-27 17:08:21,241 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Rest&#010;&gt; endpoint listening at x.x.5.60:46723&#010;&gt; 2020-07-27 17:08:21,242 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] -&#010;&gt; http://x.x.5.60:46723 was granted leadership with&#010;&gt; leaderSessionID=00000000-0000-0000-0000-000000000000&#010;&gt; 2020-07-27 17:08:21,243 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.MiniDispatcherRestEndpoint [] - Web&#010;&gt; frontend listening at http://x.x.5.60:46723.&#010;&gt; 2020-07-27 17:08:21,256 INFO&#010;&gt; org.apache.flink.runtime.util.config.memory.ProcessMemoryUtils [] - The&#010;&gt; derived from fraction jvm overhead memory (172.800mb (181193935 bytes)) is&#010;&gt; less than its min value 192.000mb (201326592 bytes), min value will be used&#010;&gt; instead&#010;&gt; 2020-07-27 17:08:21,304 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.yarn.YarnResourceManager at&#010;&gt; akka://flink/user/rpc/resourcemanager_0 .&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.memory.process.size, 1728m&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.jobgraph-path, job.graph&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.execution.failover-strategy, region&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: high-availability.cluster-id,&#010;&gt; application_1568724479991_18850539&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.address, localhost&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.target, yarn-per-job&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.memory.process.size, 1 gb&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: jobmanager.rpc.port, 6123&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.savepoint.ignore-unclaimed-state, false&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.attached, true&#010;&gt; 2020-07-27 17:08:21,314 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: internal.cluster.execution-mode, NORMAL&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: execution.shutdown-on-attached-exit, false&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.jars,&#010;&gt; file:/data/rt/jar_version/sql/test.jar&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: parallelism.default, 3&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: taskmanager.numberOfTaskSlots, 1&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: pipeline.classpaths,&#010;&gt; http://x.x.32.138:38088/rt/udf/download?udfname=test&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.name, RTC_TEST&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: yarn.application.queue, root.dp.dp_online&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.deployment.config-dir,&#010;&gt; /data/server/flink-1.11.1/conf&#010;&gt; 2020-07-27 17:08:21,315 INFO&#010;&gt; org.apache.flink.configuration.GlobalConfiguration           [] - Loading&#010;&gt; configuration property: $internal.yarn.log-config-file,&#010;&gt; /data/server/flink-1.11.1/conf/log4j.properties&#010;&gt; 2020-07-27 17:08:21,333 INFO&#010;&gt; org.apache.flink.runtime.externalresource.ExternalResourceUtils [] -&#010;&gt; Enabled external resources: []&#010;&gt; 2020-07-27 17:08:21,334 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Cannot get scheduler resource types: This YARN&#010;&gt; version does not support 'getSchedulerResourceTypes'&#010;&gt; 2020-07-27 17:08:21,375 INFO&#010;&gt; org.apache.flink.runtime.dispatcher.runner.JobDispatcherLeaderProcess [] -&#010;&gt; Start JobDispatcherLeaderProcess.&#010;&gt; 2020-07-27 17:08:21,379 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.dispatcher.MiniDispatcher at&#010;&gt; akka://flink/user/rpc/dispatcher_1 .&#010;&gt; 2020-07-27 17:08:21,408 INFO&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at&#010;&gt; akka://flink/user/rpc/jobmanager_2 .&#010;&gt; 2020-07-27 17:08:21,414 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Initializing job RTC_TEST&#010;&gt; (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:21,437 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Using restart back off time strategy&#010;&gt; FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;&gt; backoffTimeMS=10000) for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Running initialization on master for job RTC_TEST&#010;&gt; (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:21,472 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Successfully ran initialization on master in 0 ms.&#010;&gt; 2020-07-27 17:08:21,488 INFO&#010;&gt; org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] -&#010;&gt; Built 3 pipelined regions in 1 ms&#010;&gt; 2020-07-27 17:08:21,542 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Using application-defined state backend:&#010;&gt; RocksDBStateBackend{checkpointStreamBackend=File State Backend&#010;&gt; (checkpoints: 'hdfs://HDFS00000/data/checkpoint-data/wangty/RTC_TEST',&#010;&gt; savepoints: 'null', asynchronous: UNDEFINED, fileStateThreshold: -1),&#010;&gt; localRocksDbDirectories=null, enableIncrementalCheckpointing=FALSE,&#010;&gt; numberOfTransferThreads=-1, writeBatchSize=-1}&#010;&gt; 2020-07-27 17:08:21,543 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Configuring application-defined state backend with&#010;&gt; job/cluster config&#010;&gt; 2020-07-27 17:08:21,568 INFO&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; predefined options: DEFAULT.&#010;&gt; 2020-07-27 17:08:21,569 INFO&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; default options factory:&#010;&gt; DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;&gt; 2020-07-27 17:08:21,714 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Recovered 0 containers from previous attempts ([]).&#010;&gt; 2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Register application master response does not contain&#010;&gt; scheduler resource types, use&#010;&gt; '$internal.yarn.resourcemanager.enable-vcore-matching'.&#010;&gt; 2020-07-27 17:08:21,716 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Container matching strategy: IGNORE_VCORE.&#010;&gt; 2020-07-27 17:08:21,719 INFO&#010;&gt; org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl [] - Upper&#010;&gt; bound of the thread pool size is 500&#010;&gt; 2020-07-27 17:08:21,720 INFO&#010;&gt; org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy []&#010;&gt; - yarn.client.max-nodemanagers-proxies : 500&#010;&gt; 2020-07-27 17:08:21,723 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - ResourceManager akka.tcp://flink@x.x.5.60&#010;&gt; :36696/user/rpc/resourcemanager_0&#010;&gt; was granted leadership with fencing token 00000000000000000000000000000000&#010;&gt; 2020-07-27 17:08:21,727 INFO&#010;&gt; org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] -&#010;&gt; Starting the SlotManager.&#010;&gt; 2020-07-27 17:08:22,126 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Using failover strategy&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@c1dab34&#010;&gt; for RTC_TEST (9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:08:22,130 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] -&#010;&gt; JobManager runner for job RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) was&#010;&gt; granted leadership with session id 00000000-0000-0000-0000-000000000000 at&#010;&gt; akka.tcp://flink@x.x.5.60:36696/user/rpc/jobmanager_2.&#010;&gt; 2020-07-27 17:08:22,133 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Starting execution of job RTC_TEST&#010;&gt; (9f074e66a0f70274c7a7af42e71525fb) under job master id&#010;&gt; 00000000000000000000000000000000.&#010;&gt; 2020-07-27 17:08:22,135 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Starting scheduling with scheduling strategy&#010;&gt; [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;&gt; 2020-07-27 17:08:22,135 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state CREATED to&#010;&gt; RUNNING.&#010;&gt; 2020-07-27 17:08:22,145 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (8bb9f7b4bcc93895851ec47123d2213a) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:08:22,145 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (647da02fb921931e1a35ba4265d95c04) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:08:22,145 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (78211c4a866e216b6c821b743b2bf52d) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:08:22,158 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}]&#010;&gt; 2020-07-27 17:08:22,162 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}]&#010;&gt; 2020-07-27 17:08:22,162 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}]&#010;&gt; 2020-07-27 17:08:22,166 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Connecting to ResourceManager akka.tcp://flink@x.x.5.60&#010;&gt; :36696/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; 2020-07-27 17:08:22,170 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Resolved ResourceManager address, beginning&#010;&gt; registration&#010;&gt; 2020-07-27 17:08:22,173 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Registering job manager&#010;&gt; 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60&#010;&gt; :36696/user/rpc/jobmanager_2&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb.&#010;&gt; 2020-07-27 17:08:22,177 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Registered job manager&#010;&gt; 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60&#010;&gt; :36696/user/rpc/jobmanager_2&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb.&#010;&gt; 2020-07-27 17:08:22,180 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - JobManager successfully registered at ResourceManager,&#010;&gt; leader id: 00000000000000000000000000000000.&#010;&gt; 2020-07-27 17:08:22,180 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:08:22,181 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 4f255670332ee6a2bc934336cc0cee4c.&#010;&gt; 2020-07-27 17:08:22,181 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:08:22,182 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:08:22,190 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Requesting new TaskExecutor container with resource&#010;&gt; WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;&gt; taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;&gt; managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;&gt; resource is 1.&#010;&gt; 2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; f29dfd0f0ba7639992b6fe90ca0f3524.&#010;&gt; 2020-07-27 17:08:22,192 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Requesting new TaskExecutor container with resource&#010;&gt; WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;&gt; taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;&gt; managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;&gt; resource is 2.&#010;&gt; 2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 8a9e009c987a09bf4e36843f7a6eab62.&#010;&gt; 2020-07-27 17:08:22,193 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Requesting new TaskExecutor container with resource&#010;&gt; WorkerResourceSpec {cpuCores=1.0, taskHeapSize=384.000mb (402653174 bytes),&#010;&gt; taskOffHeapSize=0 bytes, networkMemSize=128.000mb (134217730 bytes),&#010;&gt; managedMemSize=512.000mb (536870920 bytes)}. Number pending workers of this&#010;&gt; resource is 3.&#010;&gt; 2020-07-27 17:08:27,253 INFO&#010;&gt; org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;&gt; new token for : x.x.6.54:5006&#010;&gt; 2020-07-27 17:08:27,254 INFO&#010;&gt; org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;&gt; new token for : x.x.4.230:5006&#010;&gt; 2020-07-27 17:08:27,254 INFO&#010;&gt; org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl        [] - Received&#010;&gt; new token for : x.x.5.229:5006&#010;&gt; 2020-07-27 17:08:27,256 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Received 3 containers.&#010;&gt; 2020-07-27 17:08:27,259 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Received 3 containers with resource &lt;memory:2048,&#010;&gt; vCores:1&gt;, 0 pending container requests.&#010;&gt; 2020-07-27 17:08:27,261 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Returning excess container&#010;&gt; container_1568724479991_18850539_01_000002.&#010;&gt; 2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Returning excess container&#010;&gt; container_1568724479991_18850539_01_000003.&#010;&gt; 2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Returning excess container&#010;&gt; container_1568724479991_18850539_01_000004.&#010;&gt; 2020-07-27 17:08:27,262 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Accepted 0 requested containers, returned 3 excess&#010;&gt; containers, 0 pending container requests of resource &lt;memory:2048,&#010;&gt; vCores:1&gt;.&#010;&gt; 2020-07-27 17:08:45,630 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:09:15,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:09:45,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:10:15,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:10:45,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:11:15,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:11:45,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:12:15,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:12:45,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:13:15,629 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:13:22,167 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (8bb9f7b4bcc93895851ec47123d2213a) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:13:22,177 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:13:22,178 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:13:22,179 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to&#010;&gt; RESTARTING.&#010;&gt; 2020-07-27 17:13:22,181 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 8bb9f7b4bcc93895851ec47123d2213a.&#010;&gt; 2020-07-27 17:13:22,183 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{de40e772bd7366814b7ed234f5cdfc53}] timed out.&#010;&gt; 2020-07-27 17:13:22,185 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (647da02fb921931e1a35ba4265d95c04) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:13:22,188 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:13:22,188 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:13:22,188 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 647da02fb921931e1a35ba4265d95c04.&#010;&gt; 2020-07-27 17:13:22,189 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{3ca5a64e992d87a27f207e6020eea047}] timed out.&#010;&gt; 2020-07-27 17:13:22,190 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (78211c4a866e216b6c821b743b2bf52d) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:13:22,192 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:13:22,192 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:13:22,192 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 78211c4a866e216b6c821b743b2bf52d.&#010;&gt; 2020-07-27 17:13:22,193 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{4945a0b0f9dcfe7547cfefab3ee59be7}] timed out.&#010;&gt; 2020-07-27 17:13:32,184 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (de96a2863fbb58f21cf987db0cbe380d) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:13:32,185 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:13:32,185 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 19be4edc7eadca2c14dd9ad6d1a87dcd.&#010;&gt; 2020-07-27 17:13:32,189 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (c8afff8ed430d1e5caf02d3d8383723c) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:13:32,189 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:13:32,189 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 60ef8787a8c67ed7cbc198d71838ba22.&#010;&gt; 2020-07-27 17:13:32,192 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING&#010;&gt; to RUNNING.&#010;&gt; 2020-07-27 17:13:32,193 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (08889c02dcc65774fe7ea42522f89ca9) switched from CREATED to SCHEDULED.&#010;&gt; 2020-07-27 17:13:32,193 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-27 17:13:32,194 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Request slot with profile ResourceProfile{UNKNOWN}&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb with allocation id&#010;&gt; 3122df3dc780c5924fbd479a886d435b.&#010;&gt; 2020-07-27 17:13:57,064 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:14:27,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:14:57,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:15:27,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:15:57,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:16:27,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:16:57,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:17:27,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:17:57,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:18:27,023 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Checkpoint triggering task Source: rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3) of job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb is not in state RUNNING but SCHEDULED&#010;&gt; instead. Aborting checkpoint.&#010;&gt; 2020-07-27 17:18:32,187 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (1/3)&#010;&gt; (de96a2863fbb58f21cf987db0cbe380d) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,189 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:18:32,189 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_0.&#010;&gt; 2020-07-27 17:18:32,189 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RUNNING to&#010;&gt; RESTARTING.&#010;&gt; 2020-07-27 17:18:32,191 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; de96a2863fbb58f21cf987db0cbe380d.&#010;&gt; 2020-07-27 17:18:32,192 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{f8186c1e488ec15065a2f844d97c895c}] timed out.&#010;&gt; 2020-07-27 17:18:32,193 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (2/3)&#010;&gt; (c8afff8ed430d1e5caf02d3d8383723c) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,194 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:18:32,194 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_1.&#010;&gt; 2020-07-27 17:18:32,199 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; c8afff8ed430d1e5caf02d3d8383723c.&#010;&gt; 2020-07-27 17:18:32,199 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{15c4647c462886ed5cfd9e146f9a0562}] timed out.&#010;&gt; 2020-07-27 17:18:32,200 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; rtsc_test -&gt; Filter -&gt; Map -&gt;&#010;&gt; SourceConversion(table=[default_catalog.default_database.test], fields=[a,&#010;&gt; b, record_timestamp, proctime]) -&gt; Calc(select=[a, b, record_timestamp,&#010;&gt; PROCTIME_MATERIALIZE(proctime) AS proctime]) -&gt; SinkConversionToTuple2 -&gt;&#010;&gt; Filter -&gt; Sink: sink kafka topic: rtsc_test2 (3/3)&#010;&gt; (08889c02dcc65774fe7ea42522f89ca9) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,203 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:18:32,204 INFO&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 1 tasks should be restarted to recover the failed task&#010;&gt; 66774974224095ed80fefb1f583d9fb9_2.&#010;&gt; 2020-07-27 17:18:32,204 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state RESTARTING&#010;&gt; to FAILING.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;&gt; backoffTimeMS=10000)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,208 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; RTC_TEST (9f074e66a0f70274c7a7af42e71525fb) switched from state FAILING to&#010;&gt; FAILED.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; FixedDelayRestartBackoffTimeStrategy(maxNumberRestartAttempts=5,&#010;&gt; backoffTimeMS=10000)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:822)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:797)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [test.jar:?]&#010;&gt; at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [test.jar:?]&#010;&gt; at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [test.jar:?]&#010;&gt; at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [test.jar:?]&#010;&gt; at akka.actor.ActorCell.invoke(ActorCell.scala:561) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.run(Mailbox.scala:225) [test.jar:?]&#010;&gt; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [test.jar:?]&#010;&gt; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [test.jar:?]&#010;&gt; at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [test.jar:?]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[test.jar:?]&#010;&gt; ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; at&#010;&gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;&gt; ~[?:1.8.0_191]&#010;&gt; ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; ... 23 more&#010;&gt; 2020-07-27 17:18:32,210 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping&#010;&gt; checkpoint coordinator for job 9f074e66a0f70274c7a7af42e71525fb.&#010;&gt; 2020-07-27 17:18:32,210 INFO&#010;&gt; org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore []&#010;&gt; - Shutting down&#010;&gt; 2020-07-27 17:18:32,210 INFO&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 08889c02dcc65774fe7ea42522f89ca9.&#010;&gt; 2020-07-27 17:18:32,211 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{1ee4efd530c32169b68de993ea4b4460}] timed out.&#010;&gt; 2020-07-27 17:18:32,216 INFO&#010;&gt; org.apache.flink.runtime.dispatcher.MiniDispatcher           [] - Job&#010;&gt; 9f074e66a0f70274c7a7af42e71525fb reached globally terminal state FAILED.&#010;&gt; 2020-07-27 17:18:32,217 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Stopping the JobMaster for job&#010;&gt; RTC_TEST(9f074e66a0f70274c7a7af42e71525fb).&#010;&gt; 2020-07-27 17:18:32,224 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Suspending SlotPool.&#010;&gt; 2020-07-27 17:18:32,225 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; [] - Close ResourceManager connection&#010;&gt; 4ee5ce2ea08809b1cca2745fa12ee663: JobManager is shutting down..&#010;&gt; 2020-07-27 17:18:32,225 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping&#010;&gt; SlotPool.&#010;&gt; 2020-07-27 17:18:32,225 INFO  org.apache.flink.yarn.YarnResourceManager&#010;&gt; [] - Disconnect job manager&#010;&gt; 00000000000000000000000000000000@akka.tcp://flink@x.x.5.60&#010;&gt; :36696/user/rpc/jobmanager_2&#010;&gt; for job 9f074e66a0f70274c7a7af42e71525fb from the resource manager.&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<15387299.84e6.1739013f574.Coremail.apache22@163.com>"
    },
    {
        "id": "<751dd7a2.6e12.17390905df5.Coremail.tinyshrimp@163.com>",
        "from": "RS &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 13:57:27 GMT",
        "subject": "kafka avro格式sink报错，NoClassDefFoundError: Could not initialize class org.apache.avro.SchemaBuilder",
        "content": "Hi,&#010;版本：Flink-1.11.1&#010;任务启动模式：standalone&#010;Flink任务编译的jar的maven中包含了flink-avro，jar-with-dependencies编译的&#010;        &lt;dependency&gt;&#010;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;&#010;            &lt;artifactId&gt;flink-avro&lt;/artifactId&gt;&#010;            &lt;version&gt;1.11.1&lt;/version&gt;&#010;        &lt;/dependency&gt;&#010;编译出来的jar也包含了这个class&#010;&#010;&#010;我看官网上说明 Flink has extensive built-in support for Apache Avro。感觉默认是支持avro的&#010;1. 直接启动的话，会报错  Caused by: org.apache.flink.table.api.ValidationException:&#010;Could not find any factory for identifier 'avro' that implements 'org.apache.flink.table.factories.SerializationFormatFactory'&#010;in the classpath.&#010;Available factory identifiers are:&#010;canal-json&#010;csv&#010;debezium-json&#010;json&#010;2. 下载了一个flink-avro-1.11.1.jar的jar包扔到了flink/lib下，报错 Caused by:&#010;java.lang.ClassNotFoundException: org.apache.avro.generic.GenericRecord&#010;3. 下载了avro-1.10.0.jar放到flink/lib下，报错 Caused by: java.lang.NoClassDefFoundError:&#010;Could not initialize class org.apache.avro.SchemaBuilder&#010;请教下，要支持avro的话，还需要怎么操作下？&#010;&#010;&#010;Thanks&#010;&#010;",
        "depth": "0",
        "reply": "<751dd7a2.6e12.17390905df5.Coremail.tinyshrimp@163.com>"
    },
    {
        "id": "<tencent_00DF501FE9232A0E5AB13005BB00C5F97608@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 14:16:32 GMT",
        "subject": "回复：flink row 类型",
        "content": "哇 这个方式很取巧了 好机智 &amp;nbsp;我之前就是一直索引取值 学习一下&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: Jark Wu &lt;imjark@gmail.com&amp;gt;&#013;&#010;发送时间: 2020年7月24日 12:39&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：flink row 类型&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;你可以看看是不是可以把这个字段声明成 MAP，这样就可以 map['rule_key1']&#010;的方式通过字段名去获取了。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Thu, 23 Jul 2020 at 15:47, Dream-底限 &lt;zhangyu@akulaku.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; hi xiao cai&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我懂你的意思了，这确实是一种解决方式，不过这种方式有一个弊端就是每个这种功能都要开发对应的方法，我还是比较倾向于一个方法适用于一类场景，如果做不到只能每次有需求都重新开发了&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; xiao cai &lt;flinkcx@163.com&amp;gt; 于2020年7月23日周四 下午3:40写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; Hi ，Dream&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 比如你最终拿到的是Row(10)，10表示有10个字段，这些字段的顺序是固定的，那么你可以把每个字段在row里的索引的映射关系保存下来，如下&#013;&#010;&amp;gt; &amp;gt; map&lt;fieldName, fieldIndex&amp;gt; ，然后 row.getField(map.get(fieldName))获取你需要的值&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&amp;nbsp; 原始邮件&#013;&#010;&amp;gt; &amp;gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 收件人: user-zh&lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 发送时间: 2020年7月23日(周四) 14:57&#013;&#010;&amp;gt; &amp;gt; 主题: Re: flink row 类型&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; hi、xiao cai 可以说一下思路吗，我没太懂 》》可以考虑把字段索引值保存下来再获取&#010;Dream-底限 &lt;&#013;&#010;&amp;gt; &amp;gt; zhangyu@akulaku.com&amp;gt; 于2020年7月23日周四 下午2:56写道：&#010;&amp;gt; hi、Jingsong Li &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 我查看了对应的api，并运行了demo测试，通过CallContext我可以拿到对应的字段类型，但是无法拿到对应的字段名称&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; &amp;gt;&amp;gt;在TypeInference中有input的type，这个type应该是包含字段信息的。&#010;&amp;gt; &amp;gt; xiao cai &lt;&#013;&#010;&amp;gt; &amp;gt; flinkcx@163.com&amp;gt; 于2020年7月23日周四 下午2:19写道： &amp;gt;&#010;&amp;gt;&amp;gt; 可以考虑把字段索引值保存下来再获取 &amp;gt;&amp;gt; &amp;gt;&amp;gt;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 原始邮件 &amp;gt;&amp;gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&amp;gt;&#010;&amp;gt;&amp;gt; 收件人: user-zh&lt;&#013;&#010;&amp;gt; &amp;gt; user-zh@flink.apache.org&amp;gt; &amp;gt;&amp;gt; 发送时间: 2020年7月23日(周四) 14:08&#010;&amp;gt;&amp;gt; 主题: Re: flink&#013;&#010;&amp;gt; &amp;gt; row 类型 &amp;gt;&amp;gt; &amp;gt;&amp;gt; &amp;gt;&amp;gt; hi 是的，我们的数据场景比较尴尬，那我想其他方式实现一下&#010;Benchao Li &lt;&#013;&#010;&amp;gt; &amp;gt; libenchao@apache.org&amp;gt; &amp;gt;&amp;gt; 于2020年7月23日周四&#010;下午12:55写道： &amp;gt; 这个应该是做不到的。name只是SQL&#013;&#010;&amp;gt; &amp;gt; plan过程的东西，在运行时它就没有什么实际意义了。 &amp;gt;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#010;&amp;gt; &amp;gt; Dream-底限 &lt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; zhangyu@akulaku.com&amp;gt; 于2020年7月22日周三 下午7:22写道：&#010;&amp;gt; &amp;gt; &amp;gt; hi、 &amp;gt; &amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&amp;gt;，可以通过&#010;&amp;gt; &amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#010;&amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; rule_key 转换为rule_key1,rulekey2 &amp;gt; &amp;gt; 1 &amp;gt; &amp;gt;&#010;2 &amp;gt; &amp;gt; &amp;gt; &amp;gt; &amp;gt; -- &amp;gt; &amp;gt; Best, &amp;gt;&#013;&#010;&amp;gt; Benchao&#013;&#010;&amp;gt; &amp;gt; &amp;gt;&amp;gt; Li &amp;gt; &amp;gt; &amp;gt;&#013;&#010;&amp;gt;",
        "depth": "0",
        "reply": "<tencent_00DF501FE9232A0E5AB13005BB00C5F97608@qq.com>"
    },
    {
        "id": "<CAAiPCh4GOTwgQ2HJw+sbmq+x=zKTaRtqJB19fR0kb3Gkq4w+WA@mail.gmail.com>",
        "from": "shimin huang &lt;huangshimin1...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 03:20:52 GMT",
        "subject": "flink1.11.1使用Table API Hive方言的executSql报错",
        "content": "Hi,all:&#010;  本人基于Flink1.11.1的table API使用Hive方言，调用executSql方法后报错，堆栈信息如下:&#010;org.apache.flink.client.program.ProgramInvocationException: The main method&#010;caused an error: Failed to execute sql&#010;    at org.apache.flink.client.program.PackagedProgram.callMainMethod(&#010;PackagedProgram.java:302) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;    at org.apache.flink.client.program.PackagedProgram&#010;.invokeInteractiveModeForExecution(PackagedProgram.java:198) ~[flink-dist_2.&#010;11-1.11.1.jar:1.11.1]&#010;    at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:&#010;149) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;    at org.apache.flink.client.deployment.application.&#010;DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:78)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;    at org.apache.flink.client.deployment.application.&#010;DetachedApplicationRunner.run(DetachedApplicationRunner.java:67)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;    at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler&#010;.lambda$handleRequest$0(JarRunHandler.java:100) ~[flink-dist_2.11-1.11.1&#010;.jar:1.11.1]&#010;    at java.util.concurrent.CompletableFuture$AsyncSupply.run(&#010;CompletableFuture.java:1604) [?:1.8.0_242]&#010;    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:&#010;511) [?:1.8.0_242]&#010;    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_242&#010;]&#010;    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask&#010;.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242]&#010;    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask&#010;.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_242]&#010;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor&#010;.java:1149) [?:1.8.0_242]&#010;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor&#010;.java:624) [?:1.8.0_242]&#010;    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]&#010;Caused by: org.apache.flink.table.api.TableException: Failed to execute sql&#010;    at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;.executeInternal(TableEnvironmentImpl.java:747) ~[flink-table-blink_2.11-&#010;1.11.1.jar:1.11.1]&#010;    at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;.executeOperation(TableEnvironmentImpl.java:1069) ~[flink-table-blink_2.11-&#010;1.11.1.jar:1.11.1]&#010;    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(&#010;TableEnvironmentImpl.java:690) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]&#010;    at org.forchange.online.etl.h2h.Prod2Poc.main(Prod2Poc.java:46) ~[?:?]&#010;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.&#010;0_242]&#010;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl&#010;.java:62) ~[?:1.8.0_242]&#010;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(&#010;DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]&#010;    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]&#010;    at org.apache.flink.client.program.PackagedProgram.callMainMethod(&#010;PackagedProgram.java:288) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;    ... 13 more&#010;Caused by: java.lang.IllegalArgumentException: Job client must be a&#010;CoordinationRequestGateway. This is a bug.&#010;    at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:&#010;139) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;    at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher&#010;.setJobClient(CollectResultFetcher.java:97) ~[flink-dist_2.11-1.11.1.jar:&#010;1.11.1]&#010;    at org.apache.flink.streaming.api.operators.collect.&#010;CollectResultIterator.setJobClient(CollectResultIterator.java:84)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;    at org.apache.flink.table.planner.sinks.SelectTableSinkBase&#010;.setJobClient(SelectTableSinkBase.java:81) ~[flink-table-blink_2.11-1.11.1&#010;.jar:1.11.1]&#010;    at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;.executeInternal(TableEnvironmentImpl.java:737) ~[flink-table-blink_2.11-&#010;1.11.1.jar:1.11.1]&#010;    at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;.executeOperation(TableEnvironmentImpl.java:1069) ~[flink-table-blink_2.11-&#010;1.11.1.jar:1.11.1]&#010;    at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(&#010;TableEnvironmentImpl.java:690) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]&#010;    at org.forchange.online.etl.h2h.Prod2Poc.main(Prod2Poc.java:46) ~[?:?]&#010;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.&#010;0_242]&#010;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl&#010;.java:62) ~[?:1.8.0_242]&#010;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(&#010;DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]&#010;    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]&#010;    at org.apache.flink.client.program.PackagedProgram.callMainMethod(&#010;PackagedProgram.java:288) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&#010;&#010;* 核心错误&#010;`Job client must be a CoordinationRequestGateway. This is a bug.`&#010;请问这是一个Bug吗？&#010;&#010;",
        "depth": "0",
        "reply": "<CAAiPCh4GOTwgQ2HJw+sbmq+x=zKTaRtqJB19fR0kb3Gkq4w+WA@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGu7-X6Adzfk+8TWd+oOp09KkGpfzTChWiCw-_aaMNpL7A@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 05:41:08 GMT",
        "subject": "Re: flink1.11.1使用Table API Hive方言的executSql报错",
        "content": "你的包是完整的flink-1.11.1的包吗？&#010;例如 check一下 ClusterClientJobClientAdapter 这个类是否继承 CoordinationRequestGateway&#010;？&#010;&#010;shimin huang &lt;huangshimin1996@gmail.com&gt; 于2020年7月28日周二 上午11:21写道：&#010;&#010;&gt; Hi,all:&#010;&gt;   本人基于Flink1.11.1的table API使用Hive方言，调用executSql方法后报错，堆栈信息如下:&#010;&gt; org.apache.flink.client.program.ProgramInvocationException: The main method&#010;&gt; caused an error: Failed to execute sql&#010;&gt;     at org.apache.flink.client.program.PackagedProgram.callMainMethod(&#010;&gt; PackagedProgram.java:302) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.client.program.PackagedProgram&#010;&gt; .invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; ~[flink-dist_2.&#010;&gt; 11-1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:&#010;&gt; 149) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.client.deployment.application.&#010;&gt; DetachedApplicationRunner.tryExecuteJobs(DetachedApplicationRunner.java:78)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.client.deployment.application.&#010;&gt; DetachedApplicationRunner.run(DetachedApplicationRunner.java:67)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.runtime.webmonitor.handlers.JarRunHandler&#010;&gt; .lambda$handleRequest$0(JarRunHandler.java:100) ~[flink-dist_2.11-1.11.1&#010;&gt; .jar:1.11.1]&#010;&gt;     at java.util.concurrent.CompletableFuture$AsyncSupply.run(&#010;&gt; CompletableFuture.java:1604) [?:1.8.0_242]&#010;&gt;     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:&#010;&gt; 511) [?:1.8.0_242]&#010;&gt;     at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;&gt; [?:1.8.0_242&#010;&gt; ]&#010;&gt;     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask&#010;&gt; .access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242]&#010;&gt;     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask&#010;&gt; .run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_242]&#010;&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor&#010;&gt; .java:1149) [?:1.8.0_242]&#010;&gt;     at&#010;&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor&#010;&gt; .java:624) [?:1.8.0_242]&#010;&gt;     at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]&#010;&gt; Caused by: org.apache.flink.table.api.TableException: Failed to execute sql&#010;&gt;     at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;&gt; .executeInternal(TableEnvironmentImpl.java:747) ~[flink-table-blink_2.11-&#010;&gt; 1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;&gt; .executeOperation(TableEnvironmentImpl.java:1069) ~[flink-table-blink_2.11-&#010;&gt; 1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(&#010;&gt; TableEnvironmentImpl.java:690) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at org.forchange.online.etl.h2h.Prod2Poc.main(Prod2Poc.java:46) ~[?:?]&#010;&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.&#010;&gt; 0_242]&#010;&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl&#010;&gt; .java:62) ~[?:1.8.0_242]&#010;&gt;     at sun.reflect.DelegatingMethodAccessorImpl.invoke(&#010;&gt; DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]&#010;&gt;     at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]&#010;&gt;     at org.apache.flink.client.program.PackagedProgram.callMainMethod(&#010;&gt; PackagedProgram.java:288) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;     ... 13 more&#010;&gt; Caused by: java.lang.IllegalArgumentException: Job client must be a&#010;&gt; CoordinationRequestGateway. This is a bug.&#010;&gt;     at&#010;&gt; org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:&#010;&gt; 139) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.collect.CollectResultFetcher&#010;&gt; .setJobClient(CollectResultFetcher.java:97) ~[flink-dist_2.11-1.11.1.jar:&#010;&gt; 1.11.1]&#010;&gt;     at org.apache.flink.streaming.api.operators.collect.&#010;&gt; CollectResultIterator.setJobClient(CollectResultIterator.java:84)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.table.planner.sinks.SelectTableSinkBase&#010;&gt; .setJobClient(SelectTableSinkBase.java:81) ~[flink-table-blink_2.11-1.11.1&#010;&gt; .jar:1.11.1]&#010;&gt;     at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;&gt; .executeInternal(TableEnvironmentImpl.java:737) ~[flink-table-blink_2.11-&#010;&gt; 1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.table.api.internal.TableEnvironmentImpl&#010;&gt; .executeOperation(TableEnvironmentImpl.java:1069) ~[flink-table-blink_2.11-&#010;&gt; 1.11.1.jar:1.11.1]&#010;&gt;     at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(&#010;&gt; TableEnvironmentImpl.java:690) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]&#010;&gt;     at org.forchange.online.etl.h2h.Prod2Poc.main(Prod2Poc.java:46) ~[?:?]&#010;&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.&#010;&gt; 0_242]&#010;&gt;     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl&#010;&gt; .java:62) ~[?:1.8.0_242]&#010;&gt;     at sun.reflect.DelegatingMethodAccessorImpl.invoke(&#010;&gt; DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_242]&#010;&gt;     at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_242]&#010;&gt;     at org.apache.flink.client.program.PackagedProgram.callMainMethod(&#010;&gt; PackagedProgram.java:288) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;&#010;&gt;&#010;&gt; * 核心错误&#010;&gt; `Job client must be a CoordinationRequestGateway. This is a bug.`&#010;&gt; 请问这是一个Bug吗？&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CAAiPCh4GOTwgQ2HJw+sbmq+x=zKTaRtqJB19fR0kb3Gkq4w+WA@mail.gmail.com>"
    },
    {
        "id": "<4fa36dd2.27c3.17393916ec3.Coremail.flinker@126.com>",
        "from": "&quot;Roc Marshal&quot; &lt;flin...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 03:57:28 GMT",
        "subject": "Flink-1.10 on yarn Taskmanager启动参数问题",
        "content": "Hi, all.&#010;&#010;&#010;         请问Flink-1.10 on yarn Taskmanager启动的jvm GC 回收器参数默认信息是G1吗？&#010;         基本集群环境：hadoop-2.7.5、flink-1.10、jdk-1.8_61,其中jvm相关参数均未进行显示设置。&#010;         &#010;&#010;&#010;&#010;&#010;谢谢。&#010;&#010;&#010;&#010;&#010;&#010;&#010;Best,&#010;Roc Marshal.",
        "depth": "0",
        "reply": "<4fa36dd2.27c3.17393916ec3.Coremail.flinker@126.com>"
    },
    {
        "id": "<1595918708360-0.post@n8.nabble.com>",
        "from": "鱼子酱 &lt;384939...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 06:45:08 GMT",
        "subject": "flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "Hi，社区的各位大家好：&#010;我目前生产上面使用的是1.8.2版本，相对稳定&#010;为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#010;截止目前先后研究了1.10.1 1.11.1共2个大版本&#010;&#010;在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#010;状态后端使用的是rocksdb 的增量模式&#010;StateBackend backend =new&#010;RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#010;设置了官网文档中找到的删除策略：&#010;        TableConfig tableConfig = streamTableEnvironment.getConfig();&#010;        tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#010;Time.minutes(7));      &#010;&#010;请问是我使用的方式不对吗？&#010;&#010;通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#010;&#010;&#010;&#010;版本影响：flink1.10.1 flink1.11.1&#010;planner：blink planner&#010;source ： kafka source&#010;时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&#010;&#010;&#009;&#009;&#010;&#010;&#009;&#009;&#010;sql：&#010;insert into  result&#010;    select request_time ,request_id ,request_cnt ,avg_resptime&#010;,stddev_resptime ,terminal_cnt&#010;,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19) from&#010;    (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;        ,commandId as request_id&#010;        ,count(*) as request_cnt&#010;        ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;        ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as stddev_resptime&#010;        from log&#010;        where&#010;        commandId in (104005 ,204005 ,404005)&#010;        and errCode=0 and attr=0&#010;        group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#010;&#010;    &#009;union all&#010;&#010;    &#009;select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;        ,99999999&#010;        ,count(*) as request_cnt&#010;        ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;        ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as stddev_resptime&#010;        from log&#010;        where&#010;        commandId in (104005 ,204005 ,404005)&#010;        and errCode=0 and attr=0&#010;        group by TUMBLE(times, INTERVAL '1' MINUTE)&#010;    )&#010;&#010;&#009;&#010;source：&#010;&#010;    create table log (&#010;      eventTime bigint&#010;      ,times timestamp(3)&#010;&#009;  ……………………&#010;      ,commandId integer&#010;      ,watermark for times as times - interval '5' second&#010;    )&#010;    with(&#010;     'connector' = 'kafka-0.10',&#010;     'topic' = '……',&#010;     'properties.bootstrap.servers' = '……',&#010;     'properties.group.id' = '……',&#010;     'scan.startup.mode' = 'latest-offset',&#010;     'format' = 'json'&#010;    )&#010;&#010;sink1：&#010;create table result (&#010;      request_time varchar&#010;      ,request_id integer&#010;      ,request_cnt bigint&#010;      ,avg_resptime double&#010;      ,stddev_resptime double&#010;      ,insert_time varchar&#010;    ) with (&#010;      'connector' = 'kafka-0.10',&#010;      'topic' = '……',&#010;      'properties.bootstrap.servers' = '……',&#010;      'properties.group.id' = '……',&#010;      'scan.startup.mode' = 'latest-offset',&#010;      'format' = 'json'&#010;    )&#010;&#009;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAA8tFvs=7rdwijOq6d+rwzyeRFGDK4TU_50Hgux9yXtuhsdh2g@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 09:06:00 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "Hi&#013;&#010;    SQL 部分不太熟，根据以往的经验，对于 event time 情况下 window 的某个算子&#010;state 越来越大的情况，或许可以检查下&#013;&#010;watermark[1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/monitoring/debugging_event_time.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;鱼子酱 &lt;384939718@qq.com&gt; 于2020年7月28日周二 下午2:45写道：&#013;&#010;&#013;&#010;&gt; Hi，社区的各位大家好：&#013;&#010;&gt; 我目前生产上面使用的是1.8.2版本，相对稳定&#013;&#010;&gt; 为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#013;&#010;&gt; 截止目前先后研究了1.10.1 1.11.1共2个大版本&#013;&#010;&gt;&#013;&#010;&gt; 在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#013;&#010;&gt; 状态后端使用的是rocksdb 的增量模式&#013;&#010;&gt; StateBackend backend =new&#013;&#010;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#013;&#010;&gt; 设置了官网文档中找到的删除策略：&#013;&#010;&gt;         TableConfig tableConfig = streamTableEnvironment.getConfig();&#013;&#010;&gt;         tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#013;&#010;&gt; Time.minutes(7));&#013;&#010;&gt;&#013;&#010;&gt; 请问是我使用的方式不对吗？&#013;&#010;&gt;&#013;&#010;&gt; 通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 版本影响：flink1.10.1 flink1.11.1&#013;&#010;&gt; planner：blink planner&#013;&#010;&gt; source ： kafka source&#013;&#010;&gt; 时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; sql：&#013;&#010;&gt; insert into  result&#013;&#010;&gt;     select request_time ,request_id ,request_cnt ,avg_resptime&#013;&#010;&gt; ,stddev_resptime ,terminal_cnt&#013;&#010;&gt; ,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19) from&#013;&#010;&gt;     (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#013;&#010;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#013;&#010;&gt;         ,commandId as request_id&#013;&#010;&gt;         ,count(*) as request_cnt&#013;&#010;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#013;&#010;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as stddev_resptime&#013;&#010;&gt;         from log&#013;&#010;&gt;         where&#013;&#010;&gt;         commandId in (104005 ,204005 ,404005)&#013;&#010;&gt;         and errCode=0 and attr=0&#013;&#010;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#013;&#010;&gt;&#013;&#010;&gt;         union all&#013;&#010;&gt;&#013;&#010;&gt;         select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#013;&#010;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#013;&#010;&gt;         ,99999999&#013;&#010;&gt;         ,count(*) as request_cnt&#013;&#010;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#013;&#010;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as stddev_resptime&#013;&#010;&gt;         from log&#013;&#010;&gt;         where&#013;&#010;&gt;         commandId in (104005 ,204005 ,404005)&#013;&#010;&gt;         and errCode=0 and attr=0&#013;&#010;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE)&#013;&#010;&gt;     )&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; source：&#013;&#010;&gt;&#013;&#010;&gt;     create table log (&#013;&#010;&gt;       eventTime bigint&#013;&#010;&gt;       ,times timestamp(3)&#013;&#010;&gt;           ……………………&#013;&#010;&gt;       ,commandId integer&#013;&#010;&gt;       ,watermark for times as times - interval '5' second&#013;&#010;&gt;     )&#013;&#010;&gt;     with(&#013;&#010;&gt;      'connector' = 'kafka-0.10',&#013;&#010;&gt;      'topic' = '……',&#013;&#010;&gt;      'properties.bootstrap.servers' = '……',&#013;&#010;&gt;      'properties.group.id' = '……',&#013;&#010;&gt;      'scan.startup.mode' = 'latest-offset',&#013;&#010;&gt;      'format' = 'json'&#013;&#010;&gt;     )&#013;&#010;&gt;&#013;&#010;&gt; sink1：&#013;&#010;&gt; create table result (&#013;&#010;&gt;       request_time varchar&#013;&#010;&gt;       ,request_id integer&#013;&#010;&gt;       ,request_cnt bigint&#013;&#010;&gt;       ,avg_resptime double&#013;&#010;&gt;       ,stddev_resptime double&#013;&#010;&gt;       ,insert_time varchar&#013;&#010;&gt;     ) with (&#013;&#010;&gt;       'connector' = 'kafka-0.10',&#013;&#010;&gt;       'topic' = '……',&#013;&#010;&gt;       'properties.bootstrap.servers' = '……',&#013;&#010;&gt;       'properties.group.id' = '……',&#013;&#010;&gt;       'scan.startup.mode' = 'latest-offset',&#013;&#010;&gt;       'format' = 'json'&#013;&#010;&gt;     )&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<1595987202313-0.post@n8.nabble.com>",
        "from": "鱼子酱 &lt;384939...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 01:46:42 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "您好：&#010;&#010;我按照您说的试了看了一下watermark，&#010;发现可以 正常更新，相关的计算结果也没发现问题。&#010;1. 刚刚截了图在下面，时间因为时区的问题-8就正常了&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/111.png&gt; &#010;2. checkpoint里面的信息，能看出大小是线性增长的，然后主要集中在2个窗口和group里面。&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/333.png&gt; &#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/222.png&gt; &#010;&#010;&#010;&#010;Congxian Qiu wrote&#010;&gt; Hi&#010;&gt;     SQL 部分不太熟，根据以往的经验，对于 event time 情况下 window&#010;的某个算子 state 越来越大的情况，或许可以检查下&#010;&gt; watermark[1]&#010;&gt; &#010;&gt; [1]&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/monitoring/debugging_event_time.html&#010;&gt; &#010;&gt; Best,&#010;&gt; Congxian&#010;&gt; &#010;&gt; &#010;&gt; 鱼子酱 &lt;&#010;&#010;&gt; 384939718@&#010;&#010;&gt;&gt; 于2020年7月28日周二 下午2:45写道：&#010;&gt; &#010;&gt;&gt; Hi，社区的各位大家好：&#010;&gt;&gt; 我目前生产上面使用的是1.8.2版本，相对稳定&#010;&gt;&gt; 为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#010;&gt;&gt; 截止目前先后研究了1.10.1 1.11.1共2个大版本&#010;&gt;&gt;&#010;&gt;&gt; 在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#010;&gt;&gt; 状态后端使用的是rocksdb 的增量模式&#010;&gt;&gt; StateBackend backend =new&#010;&gt;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#010;&gt;&gt; 设置了官网文档中找到的删除策略：&#010;&gt;&gt;         TableConfig tableConfig = streamTableEnvironment.getConfig();&#010;&gt;&gt;         tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#010;&gt;&gt; Time.minutes(7));&#010;&gt;&gt;&#010;&gt;&gt; 请问是我使用的方式不对吗？&#010;&gt;&gt;&#010;&gt;&gt; 通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 版本影响：flink1.10.1 flink1.11.1&#010;&gt;&gt; planner：blink planner&#010;&gt;&gt; source ： kafka source&#010;&gt;&gt; 时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; sql：&#010;&gt;&gt; insert into  result&#010;&gt;&gt;     select request_time ,request_id ,request_cnt ,avg_resptime&#010;&gt;&gt; ,stddev_resptime ,terminal_cnt&#010;&gt;&gt; ,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19)&#010;&gt;&gt; from&#010;&gt;&gt;     (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt;         ,commandId as request_id&#010;&gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; stddev_resptime&#010;&gt;&gt;         from log&#010;&gt;&gt;         where&#010;&gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#010;&gt;&gt;&#010;&gt;&gt;         union all&#010;&gt;&gt;&#010;&gt;&gt;         select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt;         ,99999999&#010;&gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; stddev_resptime&#010;&gt;&gt;         from log&#010;&gt;&gt;         where&#010;&gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE)&#010;&gt;&gt;     )&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; source：&#010;&gt;&gt;&#010;&gt;&gt;     create table log (&#010;&gt;&gt;       eventTime bigint&#010;&gt;&gt;       ,times timestamp(3)&#010;&gt;&gt;           ……………………&#010;&gt;&gt;       ,commandId integer&#010;&gt;&gt;       ,watermark for times as times - interval '5' second&#010;&gt;&gt;     )&#010;&gt;&gt;     with(&#010;&gt;&gt;      'connector' = 'kafka-0.10',&#010;&gt;&gt;      'topic' = '……',&#010;&gt;&gt;      'properties.bootstrap.servers' = '……',&#010;&gt;&gt;      'properties.group.id' = '……',&#010;&gt;&gt;      'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt;      'format' = 'json'&#010;&gt;&gt;     )&#010;&gt;&gt;&#010;&gt;&gt; sink1：&#010;&gt;&gt; create table result (&#010;&gt;&gt;       request_time varchar&#010;&gt;&gt;       ,request_id integer&#010;&gt;&gt;       ,request_cnt bigint&#010;&gt;&gt;       ,avg_resptime double&#010;&gt;&gt;       ,stddev_resptime double&#010;&gt;&gt;       ,insert_time varchar&#010;&gt;&gt;     ) with (&#010;&gt;&gt;       'connector' = 'kafka-0.10',&#010;&gt;&gt;       'topic' = '……',&#010;&gt;&gt;       'properties.bootstrap.servers' = '……',&#010;&gt;&gt;       'properties.group.id' = '……',&#010;&gt;&gt;       'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt;       'format' = 'json'&#010;&gt;&gt;     )&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt;&#010;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "2",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<CABKuJ_T3KMczp7XDfhPLCgReG5sEDnt6gK-wCLak51dYot9c_w@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 03:07:16 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "这个问题我建议先区分下是SQL operator里面没有清理state，还是state backend本身没有清理state。&#013;&#010;这样你是否可以尝试下其他的state backend，以及非增量模式的rocksdb等？如果在所有state&#010;backend场景下，&#013;&#010;state都是一直上涨的，那有可能某个SQL operator里面对state的清理可能有些问题。&#013;&#010;&#013;&#010;鱼子酱 &lt;384939718@qq.com&gt; 于2020年7月29日周三 上午9:47写道：&#013;&#010;&#013;&#010;&gt; 您好：&#013;&#010;&gt;&#013;&#010;&gt; 我按照您说的试了看了一下watermark，&#013;&#010;&gt; 发现可以 正常更新，相关的计算结果也没发现问题。&#013;&#010;&gt; 1. 刚刚截了图在下面，时间因为时区的问题-8就正常了&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/111.png&gt;&#013;&#010;&gt; 2. checkpoint里面的信息，能看出大小是线性增长的，然后主要集中在2个窗口和group里面。&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/333.png&gt;&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/222.png&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu wrote&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;     SQL 部分不太熟，根据以往的经验，对于 event time 情况下 window&#010;的某个算子 state&#013;&#010;&gt; 越来越大的情况，或许可以检查下&#013;&#010;&gt; &gt; watermark[1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/monitoring/debugging_event_time.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 鱼子酱 &lt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 384939718@&#013;&#010;&gt;&#013;&#010;&gt; &gt;&gt; 于2020年7月28日周二 下午2:45写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi，社区的各位大家好：&#013;&#010;&gt; &gt;&gt; 我目前生产上面使用的是1.8.2版本，相对稳定&#013;&#010;&gt; &gt;&gt; 为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#013;&#010;&gt; &gt;&gt; 截止目前先后研究了1.10.1 1.11.1共2个大版本&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#013;&#010;&gt; &gt;&gt; 状态后端使用的是rocksdb 的增量模式&#013;&#010;&gt; &gt;&gt; StateBackend backend =new&#013;&#010;&gt; &gt;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#013;&#010;&gt; &gt;&gt; 设置了官网文档中找到的删除策略：&#013;&#010;&gt; &gt;&gt;         TableConfig tableConfig = streamTableEnvironment.getConfig();&#013;&#010;&gt; &gt;&gt;         tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#013;&#010;&gt; &gt;&gt; Time.minutes(7));&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 请问是我使用的方式不对吗？&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 版本影响：flink1.10.1 flink1.11.1&#013;&#010;&gt; &gt;&gt; planner：blink planner&#013;&#010;&gt; &gt;&gt; source ： kafka source&#013;&#010;&gt; &gt;&gt; 时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; sql：&#013;&#010;&gt; &gt;&gt; insert into  result&#013;&#010;&gt; &gt;&gt;     select request_time ,request_id ,request_cnt ,avg_resptime&#013;&#010;&gt; &gt;&gt; ,stddev_resptime ,terminal_cnt&#013;&#010;&gt; &gt;&gt; ,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19)&#013;&#010;&gt; &gt;&gt; from&#013;&#010;&gt; &gt;&gt;     (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#013;&#010;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#013;&#010;&gt; &gt;&gt;         ,commandId as request_id&#013;&#010;&gt; &gt;&gt;         ,count(*) as request_cnt&#013;&#010;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#013;&#010;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#013;&#010;&gt; &gt;&gt; stddev_resptime&#013;&#010;&gt; &gt;&gt;         from log&#013;&#010;&gt; &gt;&gt;         where&#013;&#010;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#013;&#010;&gt; &gt;&gt;         and errCode=0 and attr=0&#013;&#010;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;         union all&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;         select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#013;&#010;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#013;&#010;&gt; &gt;&gt;         ,99999999&#013;&#010;&gt; &gt;&gt;         ,count(*) as request_cnt&#013;&#010;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#013;&#010;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#013;&#010;&gt; &gt;&gt; stddev_resptime&#013;&#010;&gt; &gt;&gt;         from log&#013;&#010;&gt; &gt;&gt;         where&#013;&#010;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#013;&#010;&gt; &gt;&gt;         and errCode=0 and attr=0&#013;&#010;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE)&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; source：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;     create table log (&#013;&#010;&gt; &gt;&gt;       eventTime bigint&#013;&#010;&gt; &gt;&gt;       ,times timestamp(3)&#013;&#010;&gt; &gt;&gt;           ……………………&#013;&#010;&gt; &gt;&gt;       ,commandId integer&#013;&#010;&gt; &gt;&gt;       ,watermark for times as times - interval '5' second&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;     with(&#013;&#010;&gt; &gt;&gt;      'connector' = 'kafka-0.10',&#013;&#010;&gt; &gt;&gt;      'topic' = '……',&#013;&#010;&gt; &gt;&gt;      'properties.bootstrap.servers' = '……',&#013;&#010;&gt; &gt;&gt;      'properties.group.id' = '……',&#013;&#010;&gt; &gt;&gt;      'scan.startup.mode' = 'latest-offset',&#013;&#010;&gt; &gt;&gt;      'format' = 'json'&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; sink1：&#013;&#010;&gt; &gt;&gt; create table result (&#013;&#010;&gt; &gt;&gt;       request_time varchar&#013;&#010;&gt; &gt;&gt;       ,request_id integer&#013;&#010;&gt; &gt;&gt;       ,request_cnt bigint&#013;&#010;&gt; &gt;&gt;       ,avg_resptime double&#013;&#010;&gt; &gt;&gt;       ,stddev_resptime double&#013;&#010;&gt; &gt;&gt;       ,insert_time varchar&#013;&#010;&gt; &gt;&gt;     ) with (&#013;&#010;&gt; &gt;&gt;       'connector' = 'kafka-0.10',&#013;&#010;&gt; &gt;&gt;       'topic' = '……',&#013;&#010;&gt; &gt;&gt;       'properties.bootstrap.servers' = '……',&#013;&#010;&gt; &gt;&gt;       'properties.group.id' = '……',&#013;&#010;&gt; &gt;&gt;       'scan.startup.mode' = 'latest-offset',&#013;&#010;&gt; &gt;&gt;       'format' = 'json'&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; --&#013;&#010;&gt; &gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "3",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<1596075746006-0.post@n8.nabble.com>",
        "from": "鱼子酱 &lt;384939...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:22:26 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "感谢！&#010;&#010;flink1.11.1版本里面，我尝试了下面两种backend，目前运行了20多个小时，&#010;能够看到状态的大小在一个区间内波动，没有发现一直增长的情况了。&#010;StateBackend backend =new&#010;RocksDBStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;&#010;        StateBackend backend =new&#010;FsStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;&#010;这样看，有可能是RocksDBStateBackend*增量模式*这边可能存在一些问题。&#010;&#010;下面两种都能成功清理&#010;RocksDBStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/444.png&gt; &#010;&#010;FsStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/555.png&gt; &#010;&#010;&#010;Benchao Li-2 wrote&#010;&gt; 这个问题我建议先区分下是SQL operator里面没有清理state，还是state&#010;backend本身没有清理state。&#010;&gt; 这样你是否可以尝试下其他的state backend，以及非增量模式的rocksdb等？如果在所有state&#010;backend场景下，&#010;&gt; state都是一直上涨的，那有可能某个SQL operator里面对state的清理可能有些问题。&#010;&gt; &#010;&gt; 鱼子酱 &lt;&#010;&#010;&gt; 384939718@&#010;&#010;&gt;&gt; 于2020年7月29日周三 上午9:47写道：&#010;&gt; &#010;&gt;&gt; 您好：&#010;&gt;&gt;&#010;&gt;&gt; 我按照您说的试了看了一下watermark，&#010;&gt;&gt; 发现可以 正常更新，相关的计算结果也没发现问题。&#010;&gt;&gt; 1. 刚刚截了图在下面，时间因为时区的问题-8就正常了&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/111.png&amp;gt;&#010;&gt;&gt; 2. checkpoint里面的信息，能看出大小是线性增长的，然后主要集中在2个窗口和group里面。&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/333.png&amp;gt;&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/222.png&amp;gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Congxian Qiu wrote&#010;&gt;&gt; &gt; Hi&#010;&gt;&gt; &gt;     SQL 部分不太熟，根据以往的经验，对于 event time 情况下&#010;window 的某个算子 state&#010;&gt;&gt; 越来越大的情况，或许可以检查下&#010;&gt;&gt; &gt; watermark[1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; [1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/monitoring/debugging_event_time.html&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Congxian&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 鱼子酱 &lt;&#010;&gt;&gt;&#010;&gt;&gt; &gt; 384939718@&#010;&gt;&gt;&#010;&gt;&gt; &gt;&gt; 于2020年7月28日周二 下午2:45写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; Hi，社区的各位大家好：&#010;&gt;&gt; &gt;&gt; 我目前生产上面使用的是1.8.2版本，相对稳定&#010;&gt;&gt; &gt;&gt; 为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#010;&gt;&gt; &gt;&gt; 截止目前先后研究了1.10.1 1.11.1共2个大版本&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#010;&gt;&gt; &gt;&gt; 状态后端使用的是rocksdb 的增量模式&#010;&gt;&gt; &gt;&gt; StateBackend backend =new&#010;&gt;&gt; &gt;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#010;&gt;&gt; &gt;&gt; 设置了官网文档中找到的删除策略：&#010;&gt;&gt; &gt;&gt;         TableConfig tableConfig = streamTableEnvironment.getConfig();&#010;&gt;&gt; &gt;&gt;         tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#010;&gt;&gt; &gt;&gt; Time.minutes(7));&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 请问是我使用的方式不对吗？&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 版本影响：flink1.10.1 flink1.11.1&#010;&gt;&gt; &gt;&gt; planner：blink planner&#010;&gt;&gt; &gt;&gt; source ： kafka source&#010;&gt;&gt; &gt;&gt; 时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; sql：&#010;&gt;&gt; &gt;&gt; insert into  result&#010;&gt;&gt; &gt;&gt;     select request_time ,request_id ,request_cnt ,avg_resptime&#010;&gt;&gt; &gt;&gt; ,stddev_resptime ,terminal_cnt&#010;&gt;&gt; &gt;&gt; ,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19)&#010;&gt;&gt; &gt;&gt; from&#010;&gt;&gt; &gt;&gt;     (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt; &gt;&gt;         ,commandId as request_id&#010;&gt;&gt; &gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; &gt;&gt; stddev_resptime&#010;&gt;&gt; &gt;&gt;         from log&#010;&gt;&gt; &gt;&gt;         where&#010;&gt;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt; &gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;         union all&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;         select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt; &gt;&gt;         ,99999999&#010;&gt;&gt; &gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; &gt;&gt; stddev_resptime&#010;&gt;&gt; &gt;&gt;         from log&#010;&gt;&gt; &gt;&gt;         where&#010;&gt;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt; &gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE)&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; source：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     create table log (&#010;&gt;&gt; &gt;&gt;       eventTime bigint&#010;&gt;&gt; &gt;&gt;       ,times timestamp(3)&#010;&gt;&gt; &gt;&gt;           ……………………&#010;&gt;&gt; &gt;&gt;       ,commandId integer&#010;&gt;&gt; &gt;&gt;       ,watermark for times as times - interval '5' second&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;     with(&#010;&gt;&gt; &gt;&gt;      'connector' = 'kafka-0.10',&#010;&gt;&gt; &gt;&gt;      'topic' = '……',&#010;&gt;&gt; &gt;&gt;      'properties.bootstrap.servers' = '……',&#010;&gt;&gt; &gt;&gt;      'properties.group.id' = '……',&#010;&gt;&gt; &gt;&gt;      'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;      'format' = 'json'&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; sink1：&#010;&gt;&gt; &gt;&gt; create table result (&#010;&gt;&gt; &gt;&gt;       request_time varchar&#010;&gt;&gt; &gt;&gt;       ,request_id integer&#010;&gt;&gt; &gt;&gt;       ,request_cnt bigint&#010;&gt;&gt; &gt;&gt;       ,avg_resptime double&#010;&gt;&gt; &gt;&gt;       ,stddev_resptime double&#010;&gt;&gt; &gt;&gt;       ,insert_time varchar&#010;&gt;&gt; &gt;&gt;     ) with (&#010;&gt;&gt; &gt;&gt;       'connector' = 'kafka-0.10',&#010;&gt;&gt; &gt;&gt;       'topic' = '……',&#010;&gt;&gt; &gt;&gt;       'properties.bootstrap.servers' = '……',&#010;&gt;&gt; &gt;&gt;       'properties.group.id' = '……',&#010;&gt;&gt; &gt;&gt;       'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;       'format' = 'json'&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; --&#010;&gt;&gt; &gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt;&#010;&gt; &#010;&gt; &#010;&gt; -- &#010;&gt; &#010;&gt; Best,&#010;&gt; Benchao Li&#010;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "4",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<1596076465196-0.post@n8.nabble.com>",
        "from": "鱼子酱 &lt;384939...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:34:25 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "感谢！&#010;&#010;flink1.11.1版本里面，我尝试了下面两种backend，目前运行了20多个小时，&#010;能够看到状态的大小在一个区间内波动，没有发现一直增长的情况了。&#010;StateBackend backend =new&#010;RocksDBStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;&#010;        StateBackend backend =new&#010;FsStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;&#010;这样看，有可能是RocksDBStateBackend*增量模式*这边可能存在一些问题。&#010;&#010;下面两种都能成功清理&#010;RocksDBStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/444.png&gt; &#010;&#010;FsStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/555.png&gt; &#010;&#010;&#010;Benchao Li-2 wrote&#010;&gt; 这个问题我建议先区分下是SQL operator里面没有清理state，还是state&#010;backend本身没有清理state。&#010;&gt; 这样你是否可以尝试下其他的state backend，以及非增量模式的rocksdb等？如果在所有state&#010;backend场景下，&#010;&gt; state都是一直上涨的，那有可能某个SQL operator里面对state的清理可能有些问题。&#010;&gt; &#010;&gt; 鱼子酱 &lt;&#010;&#010;&gt; 384939718@&#010;&#010;&gt;&gt; 于2020年7月29日周三 上午9:47写道：&#010;&gt; &#010;&gt;&gt; 您好：&#010;&gt;&gt;&#010;&gt;&gt; 我按照您说的试了看了一下watermark，&#010;&gt;&gt; 发现可以 正常更新，相关的计算结果也没发现问题。&#010;&gt;&gt; 1. 刚刚截了图在下面，时间因为时区的问题-8就正常了&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/111.png&amp;gt;&#010;&gt;&gt; 2. checkpoint里面的信息，能看出大小是线性增长的，然后主要集中在2个窗口和group里面。&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/333.png&amp;gt;&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/222.png&amp;gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Congxian Qiu wrote&#010;&gt;&gt; &gt; Hi&#010;&gt;&gt; &gt;     SQL 部分不太熟，根据以往的经验，对于 event time 情况下&#010;window 的某个算子 state&#010;&gt;&gt; 越来越大的情况，或许可以检查下&#010;&gt;&gt; &gt; watermark[1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; [1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/monitoring/debugging_event_time.html&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Congxian&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 鱼子酱 &lt;&#010;&gt;&gt;&#010;&gt;&gt; &gt; 384939718@&#010;&gt;&gt;&#010;&gt;&gt; &gt;&gt; 于2020年7月28日周二 下午2:45写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; Hi，社区的各位大家好：&#010;&gt;&gt; &gt;&gt; 我目前生产上面使用的是1.8.2版本，相对稳定&#010;&gt;&gt; &gt;&gt; 为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#010;&gt;&gt; &gt;&gt; 截止目前先后研究了1.10.1 1.11.1共2个大版本&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#010;&gt;&gt; &gt;&gt; 状态后端使用的是rocksdb 的增量模式&#010;&gt;&gt; &gt;&gt; StateBackend backend =new&#010;&gt;&gt; &gt;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#010;&gt;&gt; &gt;&gt; 设置了官网文档中找到的删除策略：&#010;&gt;&gt; &gt;&gt;         TableConfig tableConfig = streamTableEnvironment.getConfig();&#010;&gt;&gt; &gt;&gt;         tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#010;&gt;&gt; &gt;&gt; Time.minutes(7));&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 请问是我使用的方式不对吗？&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 版本影响：flink1.10.1 flink1.11.1&#010;&gt;&gt; &gt;&gt; planner：blink planner&#010;&gt;&gt; &gt;&gt; source ： kafka source&#010;&gt;&gt; &gt;&gt; 时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; sql：&#010;&gt;&gt; &gt;&gt; insert into  result&#010;&gt;&gt; &gt;&gt;     select request_time ,request_id ,request_cnt ,avg_resptime&#010;&gt;&gt; &gt;&gt; ,stddev_resptime ,terminal_cnt&#010;&gt;&gt; &gt;&gt; ,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19)&#010;&gt;&gt; &gt;&gt; from&#010;&gt;&gt; &gt;&gt;     (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt; &gt;&gt;         ,commandId as request_id&#010;&gt;&gt; &gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; &gt;&gt; stddev_resptime&#010;&gt;&gt; &gt;&gt;         from log&#010;&gt;&gt; &gt;&gt;         where&#010;&gt;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt; &gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;         union all&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;         select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt; &gt;&gt;         ,99999999&#010;&gt;&gt; &gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; &gt;&gt; stddev_resptime&#010;&gt;&gt; &gt;&gt;         from log&#010;&gt;&gt; &gt;&gt;         where&#010;&gt;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt; &gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE)&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; source：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     create table log (&#010;&gt;&gt; &gt;&gt;       eventTime bigint&#010;&gt;&gt; &gt;&gt;       ,times timestamp(3)&#010;&gt;&gt; &gt;&gt;           ……………………&#010;&gt;&gt; &gt;&gt;       ,commandId integer&#010;&gt;&gt; &gt;&gt;       ,watermark for times as times - interval '5' second&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;     with(&#010;&gt;&gt; &gt;&gt;      'connector' = 'kafka-0.10',&#010;&gt;&gt; &gt;&gt;      'topic' = '……',&#010;&gt;&gt; &gt;&gt;      'properties.bootstrap.servers' = '……',&#010;&gt;&gt; &gt;&gt;      'properties.group.id' = '……',&#010;&gt;&gt; &gt;&gt;      'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;      'format' = 'json'&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; sink1：&#010;&gt;&gt; &gt;&gt; create table result (&#010;&gt;&gt; &gt;&gt;       request_time varchar&#010;&gt;&gt; &gt;&gt;       ,request_id integer&#010;&gt;&gt; &gt;&gt;       ,request_cnt bigint&#010;&gt;&gt; &gt;&gt;       ,avg_resptime double&#010;&gt;&gt; &gt;&gt;       ,stddev_resptime double&#010;&gt;&gt; &gt;&gt;       ,insert_time varchar&#010;&gt;&gt; &gt;&gt;     ) with (&#010;&gt;&gt; &gt;&gt;       'connector' = 'kafka-0.10',&#010;&gt;&gt; &gt;&gt;       'topic' = '……',&#010;&gt;&gt; &gt;&gt;       'properties.bootstrap.servers' = '……',&#010;&gt;&gt; &gt;&gt;       'properties.group.id' = '……',&#010;&gt;&gt; &gt;&gt;       'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;       'format' = 'json'&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; --&#010;&gt;&gt; &gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt;&#010;&gt; &#010;&gt; &#010;&gt; -- &#010;&gt; &#010;&gt; Best,&#010;&gt; Benchao Li&#010;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "4",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<1596076845319-0.post@n8.nabble.com>",
        "from": "鱼子酱 &lt;384939...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:40:45 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "感谢！&#010;&#010;flink1.11.1版本里面，我尝试了下面两种backend，目前运行了20多个小时，&#010;能够看到状态的大小在一个区间内波动，没有发现一直增长的情况了。&#010;StateBackend backend =new&#010;RocksDBStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;&#010;StateBackend backend =new&#010;FsStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;&#010;这样看，目前是RocksDBStateBackend*增量模式*这边可能存在一些问题。&#010;&#010;下面两种都能成功清理&#010;RocksDBStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/444.png&gt; &#010;&#010;FsStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/555.png&gt; &#010;&#010;&#010;&#010;&#010;&#010;Benchao Li-2 wrote&#010;&gt; 这个问题我建议先区分下是SQL operator里面没有清理state，还是state&#010;backend本身没有清理state。&#010;&gt; 这样你是否可以尝试下其他的state backend，以及非增量模式的rocksdb等？如果在所有state&#010;backend场景下，&#010;&gt; state都是一直上涨的，那有可能某个SQL operator里面对state的清理可能有些问题。&#010;&gt; &#010;&gt; 鱼子酱 &lt;&#010;&#010;&gt; 384939718@&#010;&#010;&gt;&gt; 于2020年7月29日周三 上午9:47写道：&#010;&gt; &#010;&gt;&gt; 您好：&#010;&gt;&gt;&#010;&gt;&gt; 我按照您说的试了看了一下watermark，&#010;&gt;&gt; 发现可以 正常更新，相关的计算结果也没发现问题。&#010;&gt;&gt; 1. 刚刚截了图在下面，时间因为时区的问题-8就正常了&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/111.png&amp;gt;&#010;&gt;&gt; 2. checkpoint里面的信息，能看出大小是线性增长的，然后主要集中在2个窗口和group里面。&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/333.png&amp;gt;&#010;&gt;&gt; &amp;lt;http://apache-flink.147419.n8.nabble.com/file/t793/222.png&amp;gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Congxian Qiu wrote&#010;&gt;&gt; &gt; Hi&#010;&gt;&gt; &gt;     SQL 部分不太熟，根据以往的经验，对于 event time 情况下&#010;window 的某个算子 state&#010;&gt;&gt; 越来越大的情况，或许可以检查下&#010;&gt;&gt; &gt; watermark[1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; [1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/monitoring/debugging_event_time.html&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Congxian&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 鱼子酱 &lt;&#010;&gt;&gt;&#010;&gt;&gt; &gt; 384939718@&#010;&gt;&gt;&#010;&gt;&gt; &gt;&gt; 于2020年7月28日周二 下午2:45写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; Hi，社区的各位大家好：&#010;&gt;&gt; &gt;&gt; 我目前生产上面使用的是1.8.2版本，相对稳定&#010;&gt;&gt; &gt;&gt; 为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#010;&gt;&gt; &gt;&gt; 截止目前先后研究了1.10.1 1.11.1共2个大版本&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#010;&gt;&gt; &gt;&gt; 状态后端使用的是rocksdb 的增量模式&#010;&gt;&gt; &gt;&gt; StateBackend backend =new&#010;&gt;&gt; &gt;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#010;&gt;&gt; &gt;&gt; 设置了官网文档中找到的删除策略：&#010;&gt;&gt; &gt;&gt;         TableConfig tableConfig = streamTableEnvironment.getConfig();&#010;&gt;&gt; &gt;&gt;         tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#010;&gt;&gt; &gt;&gt; Time.minutes(7));&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 请问是我使用的方式不对吗？&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 版本影响：flink1.10.1 flink1.11.1&#010;&gt;&gt; &gt;&gt; planner：blink planner&#010;&gt;&gt; &gt;&gt; source ： kafka source&#010;&gt;&gt; &gt;&gt; 时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; sql：&#010;&gt;&gt; &gt;&gt; insert into  result&#010;&gt;&gt; &gt;&gt;     select request_time ,request_id ,request_cnt ,avg_resptime&#010;&gt;&gt; &gt;&gt; ,stddev_resptime ,terminal_cnt&#010;&gt;&gt; &gt;&gt; ,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19)&#010;&gt;&gt; &gt;&gt; from&#010;&gt;&gt; &gt;&gt;     (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt; &gt;&gt;         ,commandId as request_id&#010;&gt;&gt; &gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; &gt;&gt; stddev_resptime&#010;&gt;&gt; &gt;&gt;         from log&#010;&gt;&gt; &gt;&gt;         where&#010;&gt;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt; &gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;         union all&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;         select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#010;&gt;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#010;&gt;&gt; &gt;&gt;         ,99999999&#010;&gt;&gt; &gt;&gt;         ,count(*) as request_cnt&#010;&gt;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#010;&gt;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#010;&gt;&gt; &gt;&gt; stddev_resptime&#010;&gt;&gt; &gt;&gt;         from log&#010;&gt;&gt; &gt;&gt;         where&#010;&gt;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#010;&gt;&gt; &gt;&gt;         and errCode=0 and attr=0&#010;&gt;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE)&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; source：&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;     create table log (&#010;&gt;&gt; &gt;&gt;       eventTime bigint&#010;&gt;&gt; &gt;&gt;       ,times timestamp(3)&#010;&gt;&gt; &gt;&gt;           ……………………&#010;&gt;&gt; &gt;&gt;       ,commandId integer&#010;&gt;&gt; &gt;&gt;       ,watermark for times as times - interval '5' second&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;     with(&#010;&gt;&gt; &gt;&gt;      'connector' = 'kafka-0.10',&#010;&gt;&gt; &gt;&gt;      'topic' = '……',&#010;&gt;&gt; &gt;&gt;      'properties.bootstrap.servers' = '……',&#010;&gt;&gt; &gt;&gt;      'properties.group.id' = '……',&#010;&gt;&gt; &gt;&gt;      'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;      'format' = 'json'&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; sink1：&#010;&gt;&gt; &gt;&gt; create table result (&#010;&gt;&gt; &gt;&gt;       request_time varchar&#010;&gt;&gt; &gt;&gt;       ,request_id integer&#010;&gt;&gt; &gt;&gt;       ,request_cnt bigint&#010;&gt;&gt; &gt;&gt;       ,avg_resptime double&#010;&gt;&gt; &gt;&gt;       ,stddev_resptime double&#010;&gt;&gt; &gt;&gt;       ,insert_time varchar&#010;&gt;&gt; &gt;&gt;     ) with (&#010;&gt;&gt; &gt;&gt;       'connector' = 'kafka-0.10',&#010;&gt;&gt; &gt;&gt;       'topic' = '……',&#010;&gt;&gt; &gt;&gt;       'properties.bootstrap.servers' = '……',&#010;&gt;&gt; &gt;&gt;       'properties.group.id' = '……',&#010;&gt;&gt; &gt;&gt;       'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt; &gt;&gt;       'format' = 'json'&#010;&gt;&gt; &gt;&gt;     )&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; --&#010;&gt;&gt; &gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt;&#010;&gt; &#010;&gt; &#010;&gt; -- &#010;&gt; &#010;&gt; Best,&#010;&gt; Benchao Li&#010;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "4",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<1596076963797-0.post@n8.nabble.com>",
        "from": "鱼子酱 &lt;384939...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:42:43 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "感谢！&#010;&#010;flink1.11.1版本里面，我尝试了下面两种backend，目前运行了20多个小时，&#010;能够看到状态的大小在一个区间内波动，没有发现一直增长的情况了。&#010;StateBackend backend =new&#010;RocksDBStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;StateBackend backend =new&#010;FsStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#010;&#010;&#010;这样看，有可能是RocksDBStateBackend增量模式这边可能存在一些问题。&#010;RocksDBStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/444.png&gt; &#010;FsStateBackend：&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t793/555.png&gt; &#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "4",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAA8tFvvPZ_VB_P37wVdYgQ8udbo5YZ_60FaSO9umfjJUG9nGMA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 31 Jul 2020 05:41:30 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "Hi   鱼子酱&#013;&#010;    能否把在使用增量 checkpoint 的模式下，截图看一下 checkpoint size 的走势呢？另外可以的话，也麻烦你在每次&#013;&#010;checkpoint 做完之后，到 hdfs 上 ls 一下 checkpoint 目录的大小。&#013;&#010;    另外有一个问题还需要回答一下，你的处理速度大概是多少，state&#010;的更新频率能否评估一下呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;鱼子酱 &lt;384939718@qq.com&gt; 于2020年7月30日周四 上午10:43写道：&#013;&#010;&#013;&#010;&gt; 感谢！&#013;&#010;&gt;&#013;&#010;&gt; flink1.11.1版本里面，我尝试了下面两种backend，目前运行了20多个小时，&#013;&#010;&gt; 能够看到状态的大小在一个区间内波动，没有发现一直增长的情况了。&#013;&#010;&gt; StateBackend backend =new&#013;&#010;&gt;&#013;&#010;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#013;&#010;&gt; StateBackend backend =new&#013;&#010;&gt;&#013;&#010;&gt; FsStateBackend(\"hdfs:///checkpoints-data/\"+yamlReader.getValueByKey(\"jobName\").toString()+\"/\",false);&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 这样看，有可能是RocksDBStateBackend增量模式这边可能存在一些问题。&#013;&#010;&gt; RocksDBStateBackend：&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/444.png&gt;&#013;&#010;&gt; FsStateBackend：&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/555.png&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAEZk043cnZuk7bMYPoCxsvkmDG9AqEc=QD0Df-qczsRCFCs56A@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 03:23:21 GMT",
        "subject": "Re: flink1.10.1/1.11.1 使用sql 进行group 和 时间窗口 操作后 状态越来越大",
        "content": "hi 鱼子酱、&#013;&#010;我当初这样用的时候状态也不清理（子查询+时间窗口+union），后来把时间窗口改成全局group函数，union改成订阅topic列表后，设置状态过期时间状态才清理。。。&#013;&#010;后来看资料有的说分区数据不均衡导致水印不推进的话可能导致这种状态不清理的问题，但是我感觉不是水印导致的，水印导致的窗口应该不触发计算吧，感觉这里面有些bug，需要专业人士定位一下。。。。&#013;&#010;&#013;&#010;鱼子酱 &lt;384939718@qq.com&gt; 于2020年7月29日周三 上午9:53写道：&#013;&#010;&#013;&#010;&gt; 您好：&#013;&#010;&gt;&#013;&#010;&gt; 我按照您说的试了看了一下watermark，&#013;&#010;&gt; 发现可以 正常更新，相关的计算结果也没发现问题。&#013;&#010;&gt; 1. 刚刚截了图在下面，时间因为时区的问题-8就正常了&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/111.png&gt;&#013;&#010;&gt; 2. checkpoint里面的信息，能看出大小是线性增长的，然后主要集中在2个窗口和group里面。&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/333.png&gt;&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t793/222.png&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu wrote&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;     SQL 部分不太熟，根据以往的经验，对于 event time 情况下 window&#010;的某个算子 state&#013;&#010;&gt; 越来越大的情况，或许可以检查下&#013;&#010;&gt; &gt; watermark[1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/monitoring/debugging_event_time.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 鱼子酱 &lt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 384939718@&#013;&#010;&gt;&#013;&#010;&gt; &gt;&gt; 于2020年7月28日周二 下午2:45写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi，社区的各位大家好：&#013;&#010;&gt; &gt;&gt; 我目前生产上面使用的是1.8.2版本，相对稳定&#013;&#010;&gt; &gt;&gt; 为了能够用sql统一所有相关的作业，同时我也一直在跟着flink最新版本进行研究，&#013;&#010;&gt; &gt;&gt; 截止目前先后研究了1.10.1 1.11.1共2个大版本&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在尝试使用的过程中，我发现了通过程序，使用sql进行group操作时，checkpoint中的数据量一直在缓慢增加&#013;&#010;&gt; &gt;&gt; 状态后端使用的是rocksdb 的增量模式&#013;&#010;&gt; &gt;&gt; StateBackend backend =new&#013;&#010;&gt; &gt;&gt; RocksDBStateBackend(\"hdfs:///checkpoints-data/\",true);&#013;&#010;&gt; &gt;&gt; 设置了官网文档中找到的删除策略：&#013;&#010;&gt; &gt;&gt;         TableConfig tableConfig = streamTableEnvironment.getConfig();&#013;&#010;&gt; &gt;&gt;         tableConfig.setIdleStateRetentionTime(Time.minutes(2),&#013;&#010;&gt; &gt;&gt; Time.minutes(7));&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 请问是我使用的方式不对吗？&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 通过WebUI查看详细的checkpoint信息，发现状态大的原因主要集中在group这一Operator&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 版本影响：flink1.10.1 flink1.11.1&#013;&#010;&gt; &gt;&gt; planner：blink planner&#013;&#010;&gt; &gt;&gt; source ： kafka source&#013;&#010;&gt; &gt;&gt; 时间属性： env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; sql：&#013;&#010;&gt; &gt;&gt; insert into  result&#013;&#010;&gt; &gt;&gt;     select request_time ,request_id ,request_cnt ,avg_resptime&#013;&#010;&gt; &gt;&gt; ,stddev_resptime ,terminal_cnt&#013;&#010;&gt; &gt;&gt; ,SUBSTRING(DATE_FORMAT(LOCALTIMESTAMP,'yyyy-MM-dd HH:mm:ss.SSS'),0,19)&#013;&#010;&gt; &gt;&gt; from&#013;&#010;&gt; &gt;&gt;     (   select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#013;&#010;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#013;&#010;&gt; &gt;&gt;         ,commandId as request_id&#013;&#010;&gt; &gt;&gt;         ,count(*) as request_cnt&#013;&#010;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#013;&#010;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#013;&#010;&gt; &gt;&gt; stddev_resptime&#013;&#010;&gt; &gt;&gt;         from log&#013;&#010;&gt; &gt;&gt;         where&#013;&#010;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#013;&#010;&gt; &gt;&gt;         and errCode=0 and attr=0&#013;&#010;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE),commandId&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;         union all&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;         select SUBSTRING(DATE_FORMAT(TUMBLE_START(times, INTERVAL '1'&#013;&#010;&gt; &gt;&gt; MINUTE),'yyyy-MM-dd HH:mm:ss.SSS'),0,21) as request_time&#013;&#010;&gt; &gt;&gt;         ,99999999&#013;&#010;&gt; &gt;&gt;         ,count(*) as request_cnt&#013;&#010;&gt; &gt;&gt;         ,ROUND(avg(CAST(`respTime` as double)),2) as avg_resptime&#013;&#010;&gt; &gt;&gt;         ,ROUND(stddev_pop(CAST(`respTime` as double)),2) as&#013;&#010;&gt; &gt;&gt; stddev_resptime&#013;&#010;&gt; &gt;&gt;         from log&#013;&#010;&gt; &gt;&gt;         where&#013;&#010;&gt; &gt;&gt;         commandId in (104005 ,204005 ,404005)&#013;&#010;&gt; &gt;&gt;         and errCode=0 and attr=0&#013;&#010;&gt; &gt;&gt;         group by TUMBLE(times, INTERVAL '1' MINUTE)&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; source：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;     create table log (&#013;&#010;&gt; &gt;&gt;       eventTime bigint&#013;&#010;&gt; &gt;&gt;       ,times timestamp(3)&#013;&#010;&gt; &gt;&gt;           ……………………&#013;&#010;&gt; &gt;&gt;       ,commandId integer&#013;&#010;&gt; &gt;&gt;       ,watermark for times as times - interval '5' second&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;     with(&#013;&#010;&gt; &gt;&gt;      'connector' = 'kafka-0.10',&#013;&#010;&gt; &gt;&gt;      'topic' = '……',&#013;&#010;&gt; &gt;&gt;      'properties.bootstrap.servers' = '……',&#013;&#010;&gt; &gt;&gt;      'properties.group.id' = '……',&#013;&#010;&gt; &gt;&gt;      'scan.startup.mode' = 'latest-offset',&#013;&#010;&gt; &gt;&gt;      'format' = 'json'&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; sink1：&#013;&#010;&gt; &gt;&gt; create table result (&#013;&#010;&gt; &gt;&gt;       request_time varchar&#013;&#010;&gt; &gt;&gt;       ,request_id integer&#013;&#010;&gt; &gt;&gt;       ,request_cnt bigint&#013;&#010;&gt; &gt;&gt;       ,avg_resptime double&#013;&#010;&gt; &gt;&gt;       ,stddev_resptime double&#013;&#010;&gt; &gt;&gt;       ,insert_time varchar&#013;&#010;&gt; &gt;&gt;     ) with (&#013;&#010;&gt; &gt;&gt;       'connector' = 'kafka-0.10',&#013;&#010;&gt; &gt;&gt;       'topic' = '……',&#013;&#010;&gt; &gt;&gt;       'properties.bootstrap.servers' = '……',&#013;&#010;&gt; &gt;&gt;       'properties.group.id' = '……',&#013;&#010;&gt; &gt;&gt;       'scan.startup.mode' = 'latest-offset',&#013;&#010;&gt; &gt;&gt;       'format' = 'json'&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; --&#013;&#010;&gt; &gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<1595918708360-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_54698D0F69634F0B93B73A22@qq.com>+58A9138D57886129",
        "from": "&quot;taowang&quot;&lt;taow...@deepglint.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 09:04:46 GMT",
        "subject": "flink 1.11 rest api saveppoint接口 异常",
        "content": "在升级了 flink 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints&#010;接口表现有点异常：&#010;在 flink 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint 被触发，/jobs/:jobid/savepoints/:triggerid&#010;返回IN_PROGRESS，等 savepoint 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。&#010;但是在flink 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#010;我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在 flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#010;&#010;&#010;rest api flink docs 链接：https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#010;&#010;&#010;祝好~",
        "depth": "0",
        "reply": "<tencent_54698D0F69634F0B93B73A22@qq.com>+58A9138D57886129"
    },
    {
        "id": "<1595929063128-0.post@n8.nabble.com>",
        "from": "hechuan &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 09:37:43 GMT",
        "subject": "Re: flink作业提交到集群执行异常",
        "content": "Hi，&#010;遇到了同样的问题，请教下是如何解决的？&#010;编译jar包为单独的jar，非jar-with-dependencies的方式，依赖的jar包放到了自定义一个目录lib1&#010;修改了bin/flink脚本，CC_CLASSPATH追加了自定义jar目录lib1，lib1目录下能找到这个类&#010;$ grep org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer *&#010;Binary file flink-sql-connector-kafka_2.12-1.11.1.jar matches&#010;&#010;flink/lib目录和自定义的lib目录里面没有重复的文件&#010;&#010;org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot load&#010;user class: org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer&#010;ClassLoader info: URL ClassLoader:&#010;    file:&#010;'/tmp/blobStore-72796b24-2be1-4bc7-ac86-7acd1fe16b48/job_f532b11a7342424cdc0695126071f96e/blob_p-3dc0956f6379ef65c8f54997d7fe4a4d0918064c-b96f6980c6e06d9618abd63d25c1cee6'&#010;(valid JAR)&#010;Class not resolvable through given classloader.&#010;        at&#010;org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:288)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&gt;(OperatorChain.java:126)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:453)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_251]&#010;Caused by: java.lang.ClassNotFoundException:&#010;org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer&#010;        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)&#010;~[?:1.8.0_251]&#010;        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)&#010;~[?:1.8.0_251]&#010;        at&#010;org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:61)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1595929063128-0.post@n8.nabble.com>"
    },
    {
        "id": "<1595942330237-0.post@n8.nabble.com>",
        "from": "hechuan &lt;tinyshr...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 13:18:50 GMT",
        "subject": "Re: flink作业提交到集群执行异常",
        "content": "定位到问题了，我这里是scala的版本不一致导致的&#013;&#010;部分maven引用的2.11，部分引用的2.12，统一版本后这个报错就不存在了&#013;&#010;&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "2",
        "reply": "<1595929063128-0.post@n8.nabble.com>"
    },
    {
        "id": "<16eb813.7c2f.17394dc478e.Coremail.lydata_jia@163.com>",
        "from": "lydata &lt;lydata_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 09:58:51 GMT",
        "subject": "Flink操作 kafka ，hive遇到kerberos过期",
        "content": "     请问下 Flink操作 kafka ，hive遇到kerberos过期  有什么解决方法吗？",
        "depth": "0",
        "reply": "<16eb813.7c2f.17394dc478e.Coremail.lydata_jia@163.com>"
    },
    {
        "id": "<3d9ee1f1.33fd.1739e77ec60.Coremail.felixzh2020@126.com>",
        "from": "felixzh  &lt;felixzh2...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 06:45:26 GMT",
        "subject": "Re:Flink操作 kafka ，hive遇到kerberos过期",
        "content": "遇到kerberos过期问题，应该是你使用的是ticket cache，而不是keytab文件做认证&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-28 17:58:51，\"lydata\" &lt;lydata_jia@163.com&gt; 写道：&#010;&gt;     请问下 Flink操作 kafka ，hive遇到kerberos过期  有什么解决方法吗？&#010;",
        "depth": "1",
        "reply": "<16eb813.7c2f.17394dc478e.Coremail.lydata_jia@163.com>"
    },
    {
        "id": "<CAA8tFvt56xHmKoScStX+1=gt37=E_4i1sAUUq=xQHSgrEappBQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 10:09:15 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "Hi&#013;&#010;   开启 unalign checkpoint 的情况下，如果有 checkpoint 正在做的话，那么&#010;savepoint 会等待的[1]，但是把&#013;&#010;unaligned checkpoint 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-17342&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;taowang &lt;taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道：&#013;&#010;&#013;&#010;&gt; 在升级了 flink 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints&#010;接口表现有点异常：&#013;&#010;&gt; 在 flink 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint&#013;&#010;&gt; 被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint&#013;&#010;&gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。&#013;&#010;&gt; 但是在flink 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#013;&#010;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到&#013;&#010;&gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#013;&#010;&gt; 我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在&#013;&#010;&gt; flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; rest api flink docs 链接：&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 祝好~&#013;&#010;",
        "depth": "1",
        "reply": "<CAA8tFvt56xHmKoScStX+1=gt37=E_4i1sAUUq=xQHSgrEappBQ@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvsFEMCTFFGg8D4JoD=oWZBPMX+WGqeWN48LpKVD2o-8qw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 05:24:49 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "Hi  taowang&#013;&#010;   感谢你的更新，这个地方应该是 savepoint trigger 的逻辑有问题，现在确实&#013;&#010;setMinPauseBetweenCheckpoints 会影响 savepoint，我创建一个 issue 来跟进一下这个问题&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三 下午12:29写道：&#013;&#010;&#013;&#010;&gt; 我再次确认了一下，可能是因为我设置了checkpoint的setMinPauseBetweenCheckpoints，所以在上一次&#013;&#010;&gt; checkpoint 和这个间隔之间触发 savepoint 不会生效，但是接口返回了IN_PROGRESS&#010;的状态，我觉得这里应该是有点问题的。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;  原始邮件&#013;&#010;&gt; 发件人: taowang&lt;taowang@deepglint.com&gt;&#013;&#010;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 发送时间: 2020年7月28日(周二) 18:53&#013;&#010;&gt; 主题: Re: flink 1.11 rest api saveppoint接口 异常&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 是的，其实无论是否开启了unaligned checkpoint，我在调用这个接口的时候都没有&#010;checkpoint 在做。&#013;&#010;&gt; 而且等待的话，我认为如果有正在做的，那么正在做的 checkpoint执行完成之后新的&#010;savepoint&#013;&#010;&gt; 应该会开始执行吧，但我看到的现象是等了半个小时依旧是 IN_PROGRESS状态，正常状态下，一个&#010;checkpoint 的执行时间也就几秒钟，正常的&#013;&#010;&gt; savpoint 执行完成最多也只需要几分钟。 原始邮件 发件人: Congxian&#010;Qiu&lt;qcx978132955@gmail.com&gt;&#013;&#010;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt; 发送时间: 2020年7月28日(周二) 18:09&#010;主题: Re:&#013;&#010;&gt; flink 1.11 rest api saveppoint接口 异常 Hi 开启 unalign checkpoint 的情况下，如果有&#013;&#010;&gt; checkpoint 正在做的话，那么 savepoint 会等待的[1]，但是把 unaligned checkpoint&#013;&#010;&gt; 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1]&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-17342 Best, Congxian taowang &lt;&#013;&#010;&gt; taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道： &gt; 在升级了&#010;flink&#013;&#010;&gt; 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints 接口表现有点异常：&#010;&gt; 在 flink&#013;&#010;&gt; 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint &gt;&#013;&#010;&gt; 被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint &gt;&#013;&#010;&gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。 &gt; 但是在flink&#013;&#010;&gt; 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;&gt;&#013;&#010;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 &gt;&#013;&#010;&gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#010;&gt;&#013;&#010;&gt; 我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在 &gt;&#013;&#010;&gt; flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#013;&#010;&gt; &gt; &gt; &gt; rest api flink docs 链接： &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#013;&#010;&gt; &gt; &gt; &gt; 祝好~&#013;&#010;",
        "depth": "1",
        "reply": "<CAA8tFvt56xHmKoScStX+1=gt37=E_4i1sAUUq=xQHSgrEappBQ@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvtX0Y2u1bkPSO2Vv9DMTD7dM=ePndvPWjkxFeze2LOCgg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 05:34:13 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "Hi&#013;&#010;   创建了一个 Issue[1] 来跟进这个问题&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18748&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月29日周三 下午1:24写道：&#013;&#010;&#013;&#010;&gt; Hi  taowang&#013;&#010;&gt;    感谢你的更新，这个地方应该是 savepoint trigger 的逻辑有问题，现在确实&#013;&#010;&gt; setMinPauseBetweenCheckpoints 会影响 savepoint，我创建一个 issue 来跟进一下这个问题&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三 下午12:29写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 我再次确认了一下，可能是因为我设置了checkpoint的setMinPauseBetweenCheckpoints，所以在上一次&#013;&#010;&gt;&gt; checkpoint 和这个间隔之间触发 savepoint 不会生效，但是接口返回了IN_PROGRESS&#010;的状态，我觉得这里应该是有点问题的。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;  原始邮件&#013;&#010;&gt;&gt; 发件人: taowang&lt;taowang@deepglint.com&gt;&#013;&#010;&gt;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; 发送时间: 2020年7月28日(周二) 18:53&#013;&#010;&gt;&gt; 主题: Re: flink 1.11 rest api saveppoint接口 异常&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 是的，其实无论是否开启了unaligned checkpoint，我在调用这个接口的时候都没有&#010;checkpoint 在做。&#013;&#010;&gt;&gt; 而且等待的话，我认为如果有正在做的，那么正在做的 checkpoint执行完成之后新的&#010;savepoint&#013;&#010;&gt;&gt; 应该会开始执行吧，但我看到的现象是等了半个小时依旧是 IN_PROGRESS状态，正常状态下，一个&#010;checkpoint 的执行时间也就几秒钟，正常的&#013;&#010;&gt;&gt; savpoint 执行完成最多也只需要几分钟。 原始邮件 发件人: Congxian&#010;Qiu&lt;qcx978132955@gmail.com&gt;&#013;&#010;&gt;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt; 发送时间: 2020年7月28日(周二) 18:09&#010;主题:&#013;&#010;&gt;&gt; Re: flink 1.11 rest api saveppoint接口 异常 Hi 开启 unalign checkpoint 的情况下，如果有&#013;&#010;&gt;&gt; checkpoint 正在做的话，那么 savepoint 会等待的[1]，但是把 unaligned&#010;checkpoint&#013;&#010;&gt;&gt; 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1]&#013;&#010;&gt;&gt; https://issues.apache.org/jira/browse/FLINK-17342 Best, Congxian taowang&#013;&#010;&gt;&gt; &lt;taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道： &gt;&#010;在升级了 flink&#013;&#010;&gt;&gt; 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints 接口表现有点异常：&#010;&gt; 在 flink&#013;&#010;&gt;&gt; 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint &gt;&#013;&#010;&gt;&gt; 被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint&#010;&gt;&#013;&#010;&gt;&gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。 &gt; 但是在flink&#013;&#010;&gt;&gt; 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;&gt;&#013;&#010;&gt;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 &gt;&#013;&#010;&gt;&gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#010;&gt;&#013;&#010;&gt;&gt; 我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在 &gt;&#013;&#010;&gt;&gt; flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#013;&#010;&gt;&gt; &gt; &gt; &gt; rest api flink docs 链接： &gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#013;&#010;&gt;&gt; &gt; &gt; &gt; 祝好~&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAA8tFvt56xHmKoScStX+1=gt37=E_4i1sAUUq=xQHSgrEappBQ@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvvK4QfYv=pthU7SsonYATD6qsYs6sg5t+yZSKoU+M01DQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 12:53:44 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "Hi  taowang&#013;&#010;   FLINK-18748 在 Jiar 侧有一些讨论，这个问题修复起来应该会比较简单，你是否有意愿修复这个问题呢？&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三 下午3:23写道：&#013;&#010;&#013;&#010;&gt; 好哒，我的自动更新逻辑依赖了这个 api，不过现在我用其他方式先处理了。&#013;&#010;&gt; 感谢相应，祝好~&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;  原始邮件&#013;&#010;&gt; 发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt;&#013;&#010;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 发送时间: 2020年7月29日(周三) 13:34&#013;&#010;&gt; 主题: Re: flink 1.11 rest api saveppoint接口 异常&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi 创建了一个 Issue[1] 来跟进这个问题 [1]&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18748 Best, Congxian Congxian&#013;&#010;&gt; Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月29日周三 下午1:24写道： &gt;&#010;Hi taowang &gt;&#013;&#010;&gt; 感谢你的更新，这个地方应该是 savepoint trigger 的逻辑有问题，现在确实&#010;&gt;&#013;&#010;&gt; setMinPauseBetweenCheckpoints 会影响 savepoint，我创建一个 issue 来跟进一下这个问题&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian &gt; &gt; &gt; taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三&#010;下午12:29写道：&#013;&#010;&gt; &gt; &gt;&gt; 我再次确认了一下，可能是因为我设置了checkpoint的setMinPauseBetweenCheckpoints，所以在上一次&#010;&gt;&gt;&#013;&#010;&gt; checkpoint 和这个间隔之间触发 savepoint 不会生效，但是接口返回了IN_PROGRESS&#010;的状态，我觉得这里应该是有点问题的。&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 原始邮件 &gt;&gt; 发件人: taowang&lt;taowang@deepglint.com&gt;&#010;&gt;&gt; 收件人: user-zh&lt;&#013;&#010;&gt; user-zh@flink.apache.org&gt; &gt;&gt; 发送时间: 2020年7月28日(周二) 18:53&#010;&gt;&gt; 主题: Re: flink&#013;&#010;&gt; 1.11 rest api saveppoint接口 异常 &gt;&gt; &gt;&gt; &gt;&gt; 是的，其实无论是否开启了unaligned&#013;&#010;&gt; checkpoint，我在调用这个接口的时候都没有 checkpoint 在做。 &gt;&gt;&#010;而且等待的话，我认为如果有正在做的，那么正在做的&#013;&#010;&gt; checkpoint执行完成之后新的 savepoint &gt;&gt; 应该会开始执行吧，但我看到的现象是等了半个小时依旧是&#013;&#010;&gt; IN_PROGRESS状态，正常状态下，一个 checkpoint 的执行时间也就几秒钟，正常的&#010;&gt;&gt; savpoint 执行完成最多也只需要几分钟。&#013;&#010;&gt; 原始邮件 发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt; &gt;&gt; 收件人:&#010;user-zh&lt;&#013;&#010;&gt; user-zh@flink.apache.org&gt; 发送时间: 2020年7月28日(周二) 18:09 主题: &gt;&gt;&#010;Re: flink&#013;&#010;&gt; 1.11 rest api saveppoint接口 异常 Hi 开启 unalign checkpoint 的情况下，如果有&#010;&gt;&gt;&#013;&#010;&gt; checkpoint 正在做的话，那么 savepoint 会等待的[1]，但是把 unaligned checkpoint&#010;&gt;&gt;&#013;&#010;&gt; 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1] &gt;&gt;&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-17342 Best, Congxian taowang&#013;&#010;&gt; &gt;&gt; &lt;taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道：&#010;&gt; 在升级了 flink &gt;&gt;&#013;&#010;&gt; 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints 接口表现有点异常：&#010;&gt; 在 flink &gt;&gt;&#013;&#010;&gt; 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint &gt; &gt;&gt;&#013;&#010;&gt; 被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint &gt;&#010;&gt;&gt;&#013;&#010;&gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。 &gt; 但是在flink &gt;&gt;&#013;&#010;&gt; 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;&gt; &gt;&gt;&#013;&#010;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 &gt;&#013;&#010;&gt; &gt;&gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#010;&gt; &gt;&gt;&#013;&#010;&gt; 我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在 &gt;&#010;&gt;&gt;&#013;&#010;&gt; flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; rest api flink docs 链接： &gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#013;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 祝好~ &gt; &gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAA8tFvt56xHmKoScStX+1=gt37=E_4i1sAUUq=xQHSgrEappBQ@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvusqtnRZWNmg_nCQRarYR96VZc2kixBt1vzeste6zJivA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 13:06:07 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "Hi  taowang&#013;&#010;   你 Jira ID 是啥，我好像找不到你&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;taowang &lt;taowang@deepglint.com&gt; 于2020年7月30日周四 下午8:58写道：&#013;&#010;&#013;&#010;&gt; 好呀好呀~&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;  原始邮件&#013;&#010;&gt; 发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt;&#013;&#010;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;; taowang&lt;taowang@deepglint.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月30日(周四) 20:53&#013;&#010;&gt; 主题: Re: flink 1.11 rest api saveppoint接口 异常&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hi taowang FLINK-18748 在 Jiar 侧有一些讨论，这个问题修复起来应该会比较简单，你是否有意愿修复这个问题呢？&#010;Best,&#013;&#010;&gt; Congxian taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三 下午3:23写道：&#010;&gt;&#013;&#010;&gt; 好哒，我的自动更新逻辑依赖了这个 api，不过现在我用其他方式先处理了。&#010;&gt; 感谢相应，祝好~ &gt; &gt; &gt; 原始邮件 &gt; 发件人: Congxian&#013;&#010;&gt; Qiu&lt;qcx978132955@gmail.com&gt; &gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;&gt;&#013;&#010;&gt; 发送时间: 2020年7月29日(周三) 13:34 &gt; 主题: Re: flink 1.11 rest api saveppoint接口&#010;异常 &gt;&#013;&#010;&gt; &gt; &gt; Hi 创建了一个 Issue[1] 来跟进这个问题 [1] &gt;&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18748 Best, Congxian Congxian&#013;&#010;&gt; &gt; Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月29日周三 下午1:24写道：&#010;&gt; Hi taowang &gt; &gt;&#013;&#010;&gt; 感谢你的更新，这个地方应该是 savepoint trigger 的逻辑有问题，现在确实&#010;&gt; &gt;&#013;&#010;&gt; setMinPauseBetweenCheckpoints 会影响 savepoint，我创建一个 issue 来跟进一下这个问题&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Congxian &gt; &gt; &gt; taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三&#013;&#010;&gt; 下午12:29写道： &gt; &gt; &gt;&gt;&#013;&#010;&gt; 我再次确认了一下，可能是因为我设置了checkpoint的setMinPauseBetweenCheckpoints，所以在上一次&#010;&gt;&gt; &gt;&#013;&#010;&gt; checkpoint 和这个间隔之间触发 savepoint 不会生效，但是接口返回了IN_PROGRESS&#010;的状态，我觉得这里应该是有点问题的。 &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&gt; 原始邮件 &gt;&gt; 发件人: taowang&lt;taowang@deepglint.com&gt;&#010;&gt;&gt; 收件人: user-zh&lt; &gt;&#013;&#010;&gt; user-zh@flink.apache.org&gt; &gt;&gt; 发送时间: 2020年7月28日(周二) 18:53&#010;&gt;&gt; 主题: Re: flink&#013;&#010;&gt; &gt; 1.11 rest api saveppoint接口 异常 &gt;&gt; &gt;&gt; &gt;&gt; 是的，其实无论是否开启了unaligned&#010;&gt;&#013;&#010;&gt; checkpoint，我在调用这个接口的时候都没有 checkpoint 在做。 &gt;&gt;&#010;而且等待的话，我认为如果有正在做的，那么正在做的 &gt;&#013;&#010;&gt; checkpoint执行完成之后新的 savepoint &gt;&gt; 应该会开始执行吧，但我看到的现象是等了半个小时依旧是&#010;&gt;&#013;&#010;&gt; IN_PROGRESS状态，正常状态下，一个 checkpoint 的执行时间也就几秒钟，正常的&#010;&gt;&gt; savpoint 执行完成最多也只需要几分钟。&#013;&#010;&gt; &gt; 原始邮件 发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt; &gt;&gt; 收件人:&#010;user-zh&lt; &gt;&#013;&#010;&gt; user-zh@flink.apache.org&gt; 发送时间: 2020年7月28日(周二) 18:09 主题: &gt;&gt;&#010;Re: flink &gt;&#013;&#010;&gt; 1.11 rest api saveppoint接口 异常 Hi 开启 unalign checkpoint 的情况下，如果有&#010;&gt;&gt; &gt;&#013;&#010;&gt; checkpoint 正在做的话，那么 savepoint 会等待的[1]，但是把 unaligned checkpoint&#010;&gt;&gt; &gt;&#013;&#010;&gt; 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1] &gt;&gt; &gt;&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-17342 Best, Congxian taowang&#013;&#010;&gt; &gt; &gt;&gt; &lt;taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道：&#010;&gt; 在升级了 flink &gt;&gt; &gt;&#013;&#010;&gt; 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints 接口表现有点异常：&#010;&gt; 在 flink &gt;&gt;&#013;&#010;&gt; &gt; 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint &gt; &gt;&gt;&#010;&gt;&#013;&#010;&gt; 被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint &gt;&#010;&gt;&gt; &gt;&#013;&#010;&gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。 &gt; 但是在flink &gt;&gt;&#010;&gt;&#013;&#010;&gt; 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 &gt;&#013;&#010;&gt; &gt; &gt;&gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#010;&gt;&#013;&#010;&gt; &gt;&gt; &gt; 我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#013;&#010;&gt; &gt; &gt;&gt; &gt; &gt; &gt; rest api flink docs 链接： &gt; &gt;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#013;&#010;&gt; &gt; &gt;&gt; &gt; &gt; &gt; 祝好~ &gt; &gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAA8tFvt56xHmKoScStX+1=gt37=E_4i1sAUUq=xQHSgrEappBQ@mail.gmail.com>"
    },
    {
        "id": "<tencent_F814E517375EE2B1DBEC746B@qq.com>+DB619EFC13762F64",
        "from": "&quot;taowang&quot;&lt;taow...@deepglint.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 10:53:05 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "是的，其实无论是否开启了unaligned checkpoint，我在调用这个接口的时候都没有&#010;checkpoint 在做。&#010;而且等待的话，我认为如果有正在做的，那么正在做的 checkpoint执行完成之后新的&#010;savepoint 应该会开始执行吧，但我看到的现象是等了半个小时依旧是 IN_PROGRESS状态，正常状态下，一个&#010;checkpoint 的执行时间也就几秒钟，正常的 savpoint 执行完成最多也只需要几分钟。&#010;&#010;&#010;&#010;&#010; 原始邮件 &#010;发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月28日(周二) 18:09&#010;主题: Re: flink 1.11 rest api saveppoint接口 异常&#010;&#010;&#010;Hi 开启 unalign checkpoint 的情况下，如果有 checkpoint 正在做的话，那么&#010;savepoint 会等待的[1]，但是把 unaligned checkpoint 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1] https://issues.apache.org/jira/browse/FLINK-17342 Best, Congxian&#010;taowang &lt;taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道： &gt; 在升级了&#010;flink 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints 接口表现有点异常：&#010;&gt; 在 flink 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint &gt;&#010;被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint &gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。&#010;&gt; 但是在flink 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 &gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#010;&gt; 我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在 &gt;&#010;flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#010;&gt; &gt; &gt; rest api flink docs 链接： &gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#010;&gt; &gt; &gt; 祝好~",
        "depth": "1",
        "reply": "<tencent_F814E517375EE2B1DBEC746B@qq.com>+DB619EFC13762F64"
    },
    {
        "id": "<tencent_61B732FBE94FA874622C3C1F@qq.com>+E6F0BBD633F7DC22",
        "from": "&quot;taowang&quot;&lt;taow...@deepglint.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 04:28:45 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "我再次确认了一下，可能是因为我设置了checkpoint的setMinPauseBetweenCheckpoints，所以在上一次&#010;checkpoint 和这个间隔之间触发 savepoint 不会生效，但是接口返回了IN_PROGRESS&#010;的状态，我觉得这里应该是有点问题的。&#010;&#010;&#010; 原始邮件 &#010;发件人: taowang&lt;taowang@deepglint.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月28日(周二) 18:53&#010;主题: Re: flink 1.11 rest api saveppoint接口 异常&#010;&#010;&#010;是的，其实无论是否开启了unaligned checkpoint，我在调用这个接口的时候都没有&#010;checkpoint 在做。 而且等待的话，我认为如果有正在做的，那么正在做的&#010;checkpoint执行完成之后新的 savepoint 应该会开始执行吧，但我看到的现象是等了半个小时依旧是&#010;IN_PROGRESS状态，正常状态下，一个 checkpoint 的执行时间也就几秒钟，正常的&#010;savpoint 执行完成最多也只需要几分钟。 原始邮件 发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt; 发送时间: 2020年7月28日(周二) 18:09&#010;主题: Re: flink 1.11 rest api saveppoint接口 异常 Hi 开启 unalign checkpoint 的情况下，如果有&#010;checkpoint 正在做的话，那么 savepoint 会等待的[1]，但是把 unaligned checkpoint&#010;关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1] https://issues.apache.org/jira/browse/FLINK-17342 Best, Congxian&#010;taowang &lt;taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道： &gt; 在升级了&#010;flink 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints 接口表现有点异常：&#010;&gt; 在 flink 1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint &gt;&#010;被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint &gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。&#010;&gt; 但是在flink 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 &gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid 一直返回IN_PROGRESS。&#010;&gt; 我怀疑这个是不是和我开了 unaligned checkpoint 有关，但是我在 &gt;&#010;flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#010;&gt; &gt; &gt; rest api flink docs 链接： &gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#010;&gt; &gt; &gt; 祝好~",
        "depth": "1",
        "reply": "<tencent_F814E517375EE2B1DBEC746B@qq.com>+DB619EFC13762F64"
    },
    {
        "id": "<tencent_5D64A8FF1DF39A81D3C6ED5E@qq.com>+2D0A82129998DE49",
        "from": "&quot;taowang&quot;&lt;taow...@deepglint.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 07:10:40 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "好哒，我的自动更新逻辑依赖了这个 api，不过现在我用其他方式先处理了。&#010;感谢相应，祝好~&#010;&#010;&#010; 原始邮件 &#010;发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月29日(周三) 13:34&#010;主题: Re: flink 1.11 rest api saveppoint接口 异常&#010;&#010;&#010;Hi 创建了一个 Issue[1] 来跟进这个问题 [1] https://issues.apache.org/jira/browse/FLINK-18748&#010;Best, Congxian Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月29日周三 下午1:24写道：&#010;&gt; Hi taowang &gt; 感谢你的更新，这个地方应该是 savepoint trigger 的逻辑有问题，现在确实&#010;&gt; setMinPauseBetweenCheckpoints 会影响 savepoint，我创建一个 issue 来跟进一下这个问题&#010;&gt; &gt; Best, &gt; Congxian &gt; &gt; &gt; taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三&#010;下午12:29写道： &gt; &gt;&gt; 我再次确认了一下，可能是因为我设置了checkpoint的setMinPauseBetweenCheckpoints，所以在上一次&#010;&gt;&gt; checkpoint 和这个间隔之间触发 savepoint 不会生效，但是接口返回了IN_PROGRESS&#010;的状态，我觉得这里应该是有点问题的。 &gt;&gt; &gt;&gt; &gt;&gt; 原始邮件&#010;&gt;&gt; 发件人: taowang&lt;taowang@deepglint.com&gt; &gt;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; 发送时间: 2020年7月28日(周二) 18:53 &gt;&gt; 主题: Re: flink 1.11 rest&#010;api saveppoint接口 异常 &gt;&gt; &gt;&gt; &gt;&gt; 是的，其实无论是否开启了unaligned&#010;checkpoint，我在调用这个接口的时候都没有 checkpoint 在做。 &gt;&gt; 而且等待的话，我认为如果有正在做的，那么正在做的&#010;checkpoint执行完成之后新的 savepoint &gt;&gt; 应该会开始执行吧，但我看到的现象是等了半个小时依旧是&#010;IN_PROGRESS状态，正常状态下，一个 checkpoint 的执行时间也就几秒钟，正常的&#010;&gt;&gt; savpoint 执行完成最多也只需要几分钟。 原始邮件 发件人: Congxian&#010;Qiu&lt;qcx978132955@gmail.com&gt; &gt;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月28日(周二) 18:09 主题: &gt;&gt; Re: flink 1.11 rest api saveppoint接口&#010;异常 Hi 开启 unalign checkpoint 的情况下，如果有 &gt;&gt; checkpoint 正在做的话，那么&#010;savepoint 会等待的[1]，但是把 unaligned checkpoint &gt;&gt; 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1] &gt;&gt; https://issues.apache.org/jira/browse/FLINK-17342&#010;Best, Congxian taowang &gt;&gt; &lt;taowang@deepglint.com&gt; 于2020年7月28日周二 下午5:05写道：&#010;&gt; 在升级了 flink &gt;&gt; 1.11之后，我在使用的时候发现 rest api 的 /jobs/:jobid/savepoints&#010;接口表现有点异常： &gt; 在 flink &gt;&gt; 1.10 时：当请求该接口后，在&#010;flink ui 可以看到 savepoint &gt; &gt;&gt; 被触发，/jobs/:jobid/savepoints/:triggerid&#010;返回IN_PROGRESS，等 savepoint &gt; &gt;&gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。&#010;&gt; 但是在flink &gt;&gt; 1.11中：经常出现（不是必现，但是概率也不低）&#010;/jobs/:jobid/savepoints &gt; &gt;&gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid&#010;也返回IN_PROGRESS，但是在flink ui 中看不到 &gt; &gt;&gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid&#010;一直返回IN_PROGRESS。 &gt; &gt;&gt; 我怀疑这个是不是和我开了 unaligned checkpoint&#010;有关，但是我在 &gt; &gt;&gt; flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#010;&gt;&gt; &gt; &gt; &gt; rest api flink docs 链接： &gt; &gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#010;&gt;&gt; &gt; &gt; &gt; 祝好~ &gt; &gt;",
        "depth": "1",
        "reply": "<tencent_F814E517375EE2B1DBEC746B@qq.com>+DB619EFC13762F64"
    },
    {
        "id": "<tencent_1971AB1927F1C4112868AF1F@qq.com>+F7A2E09D2EEEF0C7",
        "from": "&quot;taowang&quot;&lt;taow...@deepglint.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 12:57:41 GMT",
        "subject": "Re: flink 1.11 rest api saveppoint接口 异常",
        "content": "好呀好呀~&#010;&#010;&#010; 原始邮件 &#010;发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;; taowang&lt;taowang@deepglint.com&gt;&#010;发送时间: 2020年7月30日(周四) 20:53&#010;主题: Re: flink 1.11 rest api saveppoint接口 异常&#010;&#010;&#010;Hi taowang FLINK-18748 在 Jiar 侧有一些讨论，这个问题修复起来应该会比较简单，你是否有意愿修复这个问题呢？&#010;Best, Congxian taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三 下午3:23写道：&#010;&gt; 好哒，我的自动更新逻辑依赖了这个 api，不过现在我用其他方式先处理了。&#010;&gt; 感谢相应，祝好~ &gt; &gt; &gt; 原始邮件 &gt; 发件人: Congxian Qiu&lt;qcx978132955@gmail.com&gt;&#010;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt; &gt; 发送时间: 2020年7月29日(周三) 13:34&#010;&gt; 主题: Re: flink 1.11 rest api saveppoint接口 异常 &gt; &gt; &gt; Hi 创建了一个&#010;Issue[1] 来跟进这个问题 [1] &gt; https://issues.apache.org/jira/browse/FLINK-18748&#010;Best, Congxian Congxian &gt; Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月29日周三&#010;下午1:24写道： &gt; Hi taowang &gt; &gt; 感谢你的更新，这个地方应该是 savepoint&#010;trigger 的逻辑有问题，现在确实 &gt; &gt; setMinPauseBetweenCheckpoints 会影响&#010;savepoint，我创建一个 issue 来跟进一下这个问题 &gt; &gt; Best, &gt; &gt; Congxian&#010;&gt; &gt; &gt; taowang &lt;taowang@deepglint.com&gt; 于2020年7月29日周三 下午12:29写道：&#010;&gt; &gt; &gt;&gt; 我再次确认了一下，可能是因为我设置了checkpoint的setMinPauseBetweenCheckpoints，所以在上一次&#010;&gt;&gt; &gt; checkpoint 和这个间隔之间触发 savepoint 不会生效，但是接口返回了IN_PROGRESS&#010;的状态，我觉得这里应该是有点问题的。 &gt; &gt;&gt; &gt;&gt; &gt;&gt; 原始邮件&#010;&gt;&gt; 发件人: taowang&lt;taowang@deepglint.com&gt; &gt;&gt; 收件人: user-zh&lt; &gt;&#010;user-zh@flink.apache.org&gt; &gt;&gt; 发送时间: 2020年7月28日(周二) 18:53 &gt;&gt;&#010;主题: Re: flink &gt; 1.11 rest api saveppoint接口 异常 &gt;&gt; &gt;&gt; &gt;&gt; 是的，其实无论是否开启了unaligned&#010;&gt; checkpoint，我在调用这个接口的时候都没有 checkpoint 在做。 &gt;&gt;&#010;而且等待的话，我认为如果有正在做的，那么正在做的 &gt; checkpoint执行完成之后新的&#010;savepoint &gt;&gt; 应该会开始执行吧，但我看到的现象是等了半个小时依旧是&#010;&gt; IN_PROGRESS状态，正常状态下，一个 checkpoint 的执行时间也就几秒钟，正常的&#010;&gt;&gt; savpoint 执行完成最多也只需要几分钟。 &gt; 原始邮件 发件人:&#010;Congxian Qiu&lt;qcx978132955@gmail.com&gt; &gt;&gt; 收件人: user-zh&lt; &gt; user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月28日(周二) 18:09 主题: &gt;&gt; Re: flink &gt; 1.11 rest api&#010;saveppoint接口 异常 Hi 开启 unalign checkpoint 的情况下，如果有 &gt;&gt; &gt;&#010;checkpoint 正在做的话，那么 savepoint 会等待的[1]，但是把 unaligned checkpoint&#010;&gt;&gt; &gt; 关闭之后，还有这个现象看上去不太符合预期。关闭之后这种现象出现的时候，也有&#010;checkpoint 正在做吗？ [1] &gt;&gt; &gt; https://issues.apache.org/jira/browse/FLINK-17342&#010;Best, Congxian taowang &gt; &gt;&gt; &lt;taowang@deepglint.com&gt; 于2020年7月28日周二&#010;下午5:05写道： &gt; 在升级了 flink &gt;&gt; &gt; 1.11之后，我在使用的时候发现&#010;rest api 的 /jobs/:jobid/savepoints 接口表现有点异常： &gt; 在 flink &gt;&gt; &gt;&#010;1.10 时：当请求该接口后，在 flink ui 可以看到 savepoint &gt; &gt;&gt; &gt;&#010;被触发，/jobs/:jobid/savepoints/:triggerid 返回IN_PROGRESS，等 savepoint &gt; &gt;&gt;&#010;&gt; 成功之后jobs/:jobid/savepoints/:triggerid返回COMPLETED。 &gt; 但是在flink &gt;&gt;&#010;&gt; 1.11中：经常出现（不是必现，但是概率也不低） /jobs/:jobid/savepoints&#010;&gt; &gt;&gt; &gt; 接口正常返回，/jobs/:jobid/savepoints/:triggerid 也返回IN_PROGRESS，但是在flink&#010;ui 中看不到 &gt; &gt; &gt;&gt; savepoint 被触发，而且/jobs/:jobid/savepoints/:triggerid&#010;一直返回IN_PROGRESS。 &gt; &gt;&gt; &gt; 我怀疑这个是不是和我开了 unaligned&#010;checkpoint 有关，但是我在 &gt; &gt;&gt; &gt; flink-config.yaml中把execution.checkpointing.unaligned设置为false还是会出现这种问题，请问大家有什么了解吗？&#010;&gt; &gt;&gt; &gt; &gt; &gt; rest api flink docs 链接： &gt; &gt;&gt; &gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/rest_api.html#jobs-jobid-savepoints&#010;&gt; &gt;&gt; &gt; &gt; &gt; 祝好~ &gt; &gt;",
        "depth": "1",
        "reply": "<tencent_F814E517375EE2B1DBEC746B@qq.com>+DB619EFC13762F64"
    },
    {
        "id": "<7c9adeb2.6c1b.17395853903.Coremail.kandy1203@163.com>",
        "from": "&quot;kandy.wang&quot; &lt;kandy1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 13:03:22 GMT",
        "subject": "Flink SQL 解析复杂（嵌套）JSON的问题 以及写入到hive类型映射问题",
        "content": "json格式，如果是一个json array 该如何定义 schema，array里还可能存在嵌套json&#010;array的情况。&#010;&#010;如数据：&#010;{\"user_info\":{\"user_id\":\"0111\",\"name\":\"xxx\"},\"timestam\":1586676835655,\"id\":\"10001\",\"jsonArray\":[{\"name222\":\"xxx\",\"user_id222\":\"0022\"},{\"name333\":\"name3333\",\"user_id222\":\"user3333\"},{\"cc\":\"xxx333\",\"user_id444\":\"user4444\",\"name444\":\"name4444\"}]}&#010;&#010;&#010;参照：https://www.cnblogs.com/Springmoon-venn/p/12664547.html&#010;需要schema这样定义：&#010;user_info 定义成：ROW&lt;user_id STRING, name STRING&gt;&#010;jsonArray 定义成 ： ARRAY&lt;ROW&lt;user_id222 STRING, name222 STRING&gt;&gt;&#010;&#010;&#010;问题是：&#010;如果json array 里还有一个array 也是继续嵌套定义吗？ 这个数据是要写入到hive，该怎么映射，array&#010;&lt;ROW&gt; ,怎么映射成Hive类型，比如映射成array&lt;string&gt;,这种情况的json该如何处理？&#010;有没有什么办法直接把json array，直接映射成array&lt;string&gt;，试了一下发现不行，该如何处理这种复杂类型。",
        "depth": "0",
        "reply": "<7c9adeb2.6c1b.17395853903.Coremail.kandy1203@163.com>"
    },
    {
        "id": "<C7922240-BE25-46C8-9165-73B8AD3B5377@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 28 Jul 2020 13:23:59 GMT",
        "subject": "Re: Flink SQL 解析复杂（嵌套）JSON的问题 以及写入到hive类型映射问题",
        "content": "Hello&#010;&#010;&gt; 问题是：&#010;&gt; 如果json array 里还有一个array 也是继续嵌套定义吗？ 这个数据是要写入到hive，该怎么映射，array&#010;&lt;ROW&gt; ,怎么映射成Hive类型，比如映射成array&lt;string&gt;,这种情况的json该如何处理？&#010;有没有什么办法直接把json array，直接映射成array&lt;string&gt;，试了一下发现不行，该如何处理这种复杂类型。&#010;&#010;&#010;Json format有一个issue在解这个问题[1]，可以把jsonNode强制转成 string, 1.12里会支持，可以看下.&#010;&#010;Best&#010;Leonard&#010;[1] https://issues.apache.org/jira/browse/FLINK-18002 &lt;https://issues.apache.org/jira/browse/FLINK-18002&gt;&#010;",
        "depth": "1",
        "reply": "<7c9adeb2.6c1b.17395853903.Coremail.kandy1203@163.com>"
    },
    {
        "id": "<CAM2Y1LDHz-_Z8_mTzKfPTQe+=Ek=o4qZK4ZFJXZKMaRo=9sz+g@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 01:20:57 GMT",
        "subject": "[DISCUSS] add a '--filename' parameter for sql-client",
        "content": "I want to execute some flink sql batch jobs regularly, such as 'insert into&#010;select .....', but I can't find a suitable method so far, so reference&#010; hive, I changed the source code and add a  '--filename'  parameter  so&#010;that we can execute a sql file.&#010;&#010;like this:&#010;&#010;/home/flink/bin/sql-client.sh embedded -f flink.sql&#010;&#010;what about any ideas or plans for this feature community?&#010;&#010;",
        "depth": "0",
        "reply": "<CAM2Y1LDHz-_Z8_mTzKfPTQe+=Ek=o4qZK4ZFJXZKMaRo=9sz+g@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGvm0mqXOmqhVBi4XMORQ9bLHvWXjK2qHnwi5gHDE4NkEA@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 01:54:48 GMT",
        "subject": "Re: [DISCUSS] add a '--filename' parameter for sql-client",
        "content": "hi Jun,&#010;&#010;Currently, sql client has supported -u option, just like:&#010; ./bin/sql-client.sh embedded -u \"insert_statement\".&#010;&#010;There is already a JIRA [1] that wants to support -f option&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-12828&#010;&#010;Best,&#010;Godfrey&#010;&#010;Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月29日周三 上午9:22写道：&#010;&#010;&gt; I want to execute some flink sql batch jobs regularly, such as 'insert into&#010;&gt; select .....', but I can't find a suitable method so far, so reference&#010;&gt;  hive, I changed the source code and add a  '--filename'  parameter  so&#010;&gt; that we can execute a sql file.&#010;&gt;&#010;&gt; like this:&#010;&gt;&#010;&gt; /home/flink/bin/sql-client.sh embedded -f flink.sql&#010;&gt;&#010;&gt; what about any ideas or plans for this feature community?&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CAM2Y1LDHz-_Z8_mTzKfPTQe+=Ek=o4qZK4ZFJXZKMaRo=9sz+g@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LDmh_iZ9xyyh6uz6-md+4c3AzPs8LhksYbEVu1+8N3N-Q@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 02:17:44 GMT",
        "subject": "Re: [DISCUSS] add a '--filename' parameter for sql-client",
        "content": "hi，godfrey：&#010;Thanks for your reply&#010;&#010;1. I have seen the -u parameter, but my sql file may not only include&#010;'insert into select ....', but also SET, DDL, etc.&#010;&#010;2. I may not have noticed this issue. I took a look at this issue. I think&#010;this issue may have some problems. For example, he finally called the&#010;CliClient.callCommand method.&#010;But I think that many options in callCommand are not completely suitable&#010;for sql files, such as HELP, CLEAR, SELECT, etc. The select operation opens&#010;a window to display the results, obviously this is not suitable for&#010;executing sql files&#010;&#010;godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月29日周三 上午9:56写道：&#010;&#010;&gt; hi Jun,&#010;&gt;&#010;&gt; Currently, sql client has supported -u option, just like:&#010;&gt;  ./bin/sql-client.sh embedded -u \"insert_statement\".&#010;&gt;&#010;&gt; There is already a JIRA [1] that wants to support -f option&#010;&gt;&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-12828&#010;&gt;&#010;&gt; Best,&#010;&gt; Godfrey&#010;&gt;&#010;&gt; Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月29日周三 上午9:22写道：&#010;&gt;&#010;&gt;&gt; I want to execute some flink sql batch jobs regularly, such as 'insert&#010;&gt;&gt; into&#010;&gt;&gt; select .....', but I can't find a suitable method so far, so reference&#010;&gt;&gt;  hive, I changed the source code and add a  '--filename'  parameter  so&#010;&gt;&gt; that we can execute a sql file.&#010;&gt;&gt;&#010;&gt;&gt; like this:&#010;&gt;&gt;&#010;&gt;&gt; /home/flink/bin/sql-client.sh embedded -f flink.sql&#010;&gt;&gt;&#010;&gt;&gt; what about any ideas or plans for this feature community?&#010;&gt;&gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CAM2Y1LDHz-_Z8_mTzKfPTQe+=Ek=o4qZK4ZFJXZKMaRo=9sz+g@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGu+PDyZykpMJF2s_p-KNN4kGOyZSFYjK42QWNFcs_GM+w@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 04:21:11 GMT",
        "subject": "Re: [DISCUSS] add a '--filename' parameter for sql-client",
        "content": "Yes, The pr still needs to be improved.&#010;In most cases, there are more than one statement in the sql file,&#010;so -f option should support multiple statements.&#010;however, a related PR [1] has not completed yet.&#010;&#010;[1] https://github.com/apache/flink/pull/8738&#010;&#010;Best,&#010;Godfrey&#010;&#010;Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月29日周三 上午10:17写道：&#010;&#010;&gt; hi，godfrey：&#010;&gt; Thanks for your reply&#010;&gt;&#010;&gt; 1. I have seen the -u parameter, but my sql file may not only include&#010;&gt; 'insert into select ....', but also SET, DDL, etc.&#010;&gt;&#010;&gt; 2. I may not have noticed this issue. I took a look at this issue. I think&#010;&gt; this issue may have some problems. For example, he finally called the&#010;&gt; CliClient.callCommand method.&#010;&gt; But I think that many options in callCommand are not completely suitable&#010;&gt; for sql files, such as HELP, CLEAR, SELECT, etc. The select operation opens&#010;&gt; a window to display the results, obviously this is not suitable for&#010;&gt; executing sql files&#010;&gt;&#010;&gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月29日周三 上午9:56写道：&#010;&gt;&#010;&gt;&gt; hi Jun,&#010;&gt;&gt;&#010;&gt;&gt; Currently, sql client has supported -u option, just like:&#010;&gt;&gt;  ./bin/sql-client.sh embedded -u \"insert_statement\".&#010;&gt;&gt;&#010;&gt;&gt; There is already a JIRA [1] that wants to support -f option&#010;&gt;&gt;&#010;&gt;&gt; [1] https://issues.apache.org/jira/browse/FLINK-12828&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Godfrey&#010;&gt;&gt;&#010;&gt;&gt; Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月29日周三 上午9:22写道：&#010;&gt;&gt;&#010;&gt;&gt;&gt; I want to execute some flink sql batch jobs regularly, such as 'insert&#010;&gt;&gt;&gt; into&#010;&gt;&gt;&gt; select .....', but I can't find a suitable method so far, so reference&#010;&gt;&gt;&gt;  hive, I changed the source code and add a  '--filename'  parameter  so&#010;&gt;&gt;&gt; that we can execute a sql file.&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; like this:&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; /home/flink/bin/sql-client.sh embedded -f flink.sql&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; what about any ideas or plans for this feature community?&#010;&gt;&gt;&gt;&#010;&gt;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<CAM2Y1LDHz-_Z8_mTzKfPTQe+=Ek=o4qZK4ZFJXZKMaRo=9sz+g@mail.gmail.com>"
    },
    {
        "id": "<e0552d8.1d12.1739848f13b.Coremail.taochanglian@163.com>",
        "from": "111 &lt;taochangl...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 01:56:23 GMT",
        "subject": "flink1.11 可以使用processtime开窗，但是无法使用eventtime开窗",
        "content": "&#010;&#010;&#010;&#010;&#010;&#010;您好，请教一个问题，谢谢：&#010;很简单的json，&#010;{\"num\":100,\"ts\":1595949526874,\"vin\":\"DDDD\"}&#010;{\"num\":200,\"ts\":1595949528874,\"vin\":\"AAAA\"}&#010;{\"num\":200,\"ts\":1595949530880,\"vin\":\"CCCC\"}&#010;{\"num\":300,\"ts\":1595949532883,\"vin\":\"CCCC\"}&#010;{\"num\":100,\"ts\":1595949534888,\"vin\":\"AAAA\"}&#010;{\"num\":300,\"ts\":1595949536892,\"vin\":\"DDDD\"}&#010;我就想使用eventtime开窗，但是亲测使用procetime可以，但是eventtime死活都不行，郁闷，望指教。&#010;public class FlinkKafka {&#010;public static void main(String[] args) throws Exception{&#010;final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;final EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);&#010;&#010;        String kafkaSourceTable = \"CREATE TABLE kafkaSourceTable (\\n\" +&#010;\" ts BIGINT,\\n\" +&#010;\" num INT ,\\n\" +&#010;\" vin STRING ,\\n\" +&#010;\" pts AS PROCTIME() ,  \\n\" +  //处理时间&#010;\" rowtime AS TO_TIMESTAMP(FROM_UNIXTIME(ts / 1000, 'yyyy-MM-dd HH:mm:ss')), \\n \" +&#010;\"  WATERMARK FOR rowtime AS rowtime - INTERVAL '1' SECOND \\n\" +&#010;\") WITH (\\n\" +&#010;\" 'connector' = 'kafka',\\n\" +&#010;\" 'topic' = 'kkb',\\n\" +&#010;\" 'properties.bootstrap.servers' = 'node01:9092,node02:9092,node03:9092',\\n\" +&#010;\" 'properties.group.id' = 'mm',\\n\" +&#010;\" 'format' = 'json',\\n\" +&#010;\" 'scan.startup.mode' = 'latest-offset' \\n\" +&#010;\")\";&#010;        tableEnv.executeSql(kafkaSourceTable);&#010;&#010;        String queryWindowAllDataSql = \"SELECT * from kafkaSourceTable  group by ts,num,vin,pts,rowtime,&#010;TUMBLE(pts, INTERVAL '5' SECOND)\";&#010;final Table windowAllTable = tableEnv.sqlQuery(queryWindowAllDataSql);&#010;&#010;        windowAllTable.printSchema();&#010;        tableEnv.toAppendStream(windowAllTable, Row.class).print();&#010;        System.out.println(\"------------------------------------------------------\");&#010;        env.execute(\"job\");&#010;&#010;    }&#010;&#010;}&#010;&#010;&#010;---------------------------&#010;请看，我这里String queryWindowAllDataSql = \"SELECT * from kafkaSourceTable  group by&#010;ts,num,vin,pts,rowtime, TUMBLE(pts, INTERVAL '5' SECOND)\"&#010;如果使用TUMBLE(pts, INTERVAL '5' SECOND)\"，即使用processtime就没有任何问题，可以每格几秒输出所有的内容。&#010;打印结果：&#010;root&#010; |-- ts: BIGINT&#010; |-- num: INT&#010; |-- vin: STRING&#010; |-- pts: TIMESTAMP(3) NOT NULL *PROCTIME*&#010; |-- rowtime: TIMESTAMP(3) *ROWTIME*&#010;&#010;&#010;------------------------------------------------------&#010;11&gt; 1595949629063,500,AAAA,2020-07-28T15:20:29.066,2020-07-28T23:20:29&#010;7&gt; 1595949627062,500,BBBB,2020-07-28T15:20:27.101,2020-07-28T23:20:27&#010;7&gt; 1595949631067,100,EEEE,2020-07-28T15:20:31.071,2020-07-28T23:20:31&#010;12&gt; 1595949633072,500,BBBB,2020-07-28T15:20:33.077,2020-07-28T23:20:33&#010;11&gt; 1595949637081,400,EEEE,2020-07-28T15:20:37.085,2020-07-28T23:20:37&#010;2&gt; 1595949635077,400,BBBB,2020-07-28T15:20:35.082,2020-07-28T23:20:35&#010;11&gt; 1595949639085,100,EEEE,2020-07-28T15:20:39.089,2020-07-28T23:20:39&#010;1&gt; 1595949643093,200,CCCC,2020-07-28T15:20:43.096,2020-07-28T23:20:43&#010;&#010;&#010;但是如果我使用TUMBLE(rowtime, INTERVAL '5' SECOND)，也就是想使用eventtime开窗，就没有任何的结果输出，一直在等待。&#010;版本是flink1.11.0&#010;&#010;&#010;望指教，谢谢！&#010;&#010;&#010;",
        "depth": "0",
        "reply": "<e0552d8.1d12.1739848f13b.Coremail.taochanglian@163.com>"
    },
    {
        "id": "<363A78D2-D2DB-45E8-AFD8-4F1D11AA66A1@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:34:11 GMT",
        "subject": "Re: flink1.11 可以使用processtime开窗，但是无法使用eventtime开窗",
        "content": "你指定时间语义是EventTime了吗&#010;env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&#010;&#010;&gt; 2020年7月29日 上午9:56，111 &lt;taochanglian@163.com&gt; 写道：&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 您好，请教一个问题，谢谢：&#010;&gt; 很简单的json，&#010;&gt; {\"num\":100,\"ts\":1595949526874,\"vin\":\"DDDD\"}&#010;&gt; {\"num\":200,\"ts\":1595949528874,\"vin\":\"AAAA\"}&#010;&gt; {\"num\":200,\"ts\":1595949530880,\"vin\":\"CCCC\"}&#010;&gt; {\"num\":300,\"ts\":1595949532883,\"vin\":\"CCCC\"}&#010;&gt; {\"num\":100,\"ts\":1595949534888,\"vin\":\"AAAA\"}&#010;&gt; {\"num\":300,\"ts\":1595949536892,\"vin\":\"DDDD\"}&#010;&gt; 我就想使用eventtime开窗，但是亲测使用procetime可以，但是eventtime死活都不行，郁闷，望指教。&#010;&gt; public class FlinkKafka {&#010;&gt; public static void main(String[] args) throws Exception{&#010;&gt; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; final EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);&#010;&gt; &#010;&gt;        String kafkaSourceTable = \"CREATE TABLE kafkaSourceTable (\\n\" +&#010;&gt; \" ts BIGINT,\\n\" +&#010;&gt; \" num INT ,\\n\" +&#010;&gt; \" vin STRING ,\\n\" +&#010;&gt; \" pts AS PROCTIME() ,  \\n\" +  //处理时间&#010;&gt; \" rowtime AS TO_TIMESTAMP(FROM_UNIXTIME(ts / 1000, 'yyyy-MM-dd HH:mm:ss')), \\n \" +&#010;&gt; \"  WATERMARK FOR rowtime AS rowtime - INTERVAL '1' SECOND \\n\" +&#010;&gt; \") WITH (\\n\" +&#010;&gt; \" 'connector' = 'kafka',\\n\" +&#010;&gt; \" 'topic' = 'kkb',\\n\" +&#010;&gt; \" 'properties.bootstrap.servers' = 'node01:9092,node02:9092,node03:9092',\\n\" +&#010;&gt; \" 'properties.group.id' = 'mm',\\n\" +&#010;&gt; \" 'format' = 'json',\\n\" +&#010;&gt; \" 'scan.startup.mode' = 'latest-offset' \\n\" +&#010;&gt; \")\";&#010;&gt;        tableEnv.executeSql(kafkaSourceTable);&#010;&gt; &#010;&gt;        String queryWindowAllDataSql = \"SELECT * from kafkaSourceTable  group by ts,num,vin,pts,rowtime,&#010;TUMBLE(pts, INTERVAL '5' SECOND)\";&#010;&gt; final Table windowAllTable = tableEnv.sqlQuery(queryWindowAllDataSql);&#010;&gt; &#010;&gt;        windowAllTable.printSchema();&#010;&gt;        tableEnv.toAppendStream(windowAllTable, Row.class).print();&#010;&gt;        System.out.println(\"------------------------------------------------------\");&#010;&gt;        env.execute(\"job\");&#010;&gt; &#010;&gt;    }&#010;&gt; &#010;&gt; }&#010;&gt; &#010;&gt; &#010;&gt; ---------------------------&#010;&gt; 请看，我这里String queryWindowAllDataSql = \"SELECT * from kafkaSourceTable  group&#010;by ts,num,vin,pts,rowtime, TUMBLE(pts, INTERVAL '5' SECOND)\"&#010;&gt; 如果使用TUMBLE(pts, INTERVAL '5' SECOND)\"，即使用processtime就没有任何问题，可以每格几秒输出所有的内容。&#010;&gt; 打印结果：&#010;&gt; root&#010;&gt; |-- ts: BIGINT&#010;&gt; |-- num: INT&#010;&gt; |-- vin: STRING&#010;&gt; |-- pts: TIMESTAMP(3) NOT NULL *PROCTIME*&#010;&gt; |-- rowtime: TIMESTAMP(3) *ROWTIME*&#010;&gt; &#010;&gt; &#010;&gt; ------------------------------------------------------&#010;&gt; 11&gt; 1595949629063,500,AAAA,2020-07-28T15:20:29.066,2020-07-28T23:20:29&#010;&gt; 7&gt; 1595949627062,500,BBBB,2020-07-28T15:20:27.101,2020-07-28T23:20:27&#010;&gt; 7&gt; 1595949631067,100,EEEE,2020-07-28T15:20:31.071,2020-07-28T23:20:31&#010;&gt; 12&gt; 1595949633072,500,BBBB,2020-07-28T15:20:33.077,2020-07-28T23:20:33&#010;&gt; 11&gt; 1595949637081,400,EEEE,2020-07-28T15:20:37.085,2020-07-28T23:20:37&#010;&gt; 2&gt; 1595949635077,400,BBBB,2020-07-28T15:20:35.082,2020-07-28T23:20:35&#010;&gt; 11&gt; 1595949639085,100,EEEE,2020-07-28T15:20:39.089,2020-07-28T23:20:39&#010;&gt; 1&gt; 1595949643093,200,CCCC,2020-07-28T15:20:43.096,2020-07-28T23:20:43&#010;&gt; &#010;&gt; &#010;&gt; 但是如果我使用TUMBLE(rowtime, INTERVAL '5' SECOND)，也就是想使用eventtime开窗，就没有任何的结果输出，一直在等待。&#010;&gt; 版本是flink1.11.0&#010;&gt; &#010;&gt; &#010;&gt; 望指教，谢谢！&#010;&gt; &#010;&gt; &#010;&#010;&#010;",
        "depth": "1",
        "reply": "<e0552d8.1d12.1739848f13b.Coremail.taochanglian@163.com>"
    },
    {
        "id": "<tencent_EA99973B2616C038495C670911030CAA6A0A@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 03:51:44 GMT",
        "subject": "Sql往kafka表写聚合数据报错",
        "content": "如下，想用sql直接往kafka写聚合结果，版本是1.11，请问能有什么办法解决，还是只能转换成datastream？？&#013;&#010;&#013;&#010;&#013;&#010;谢谢&#013;&#010;&#013;&#010;&#013;&#010;Exception in thread \"main\" org.apache.flink.table.api.TableException: Table sink 'default_catalog.default_database.mvp_rtdwb_user_business'&#010;doesn't support consuming update changes which is produced by node GroupAggregate(groupBy=[dt,&#010;user_id], select=[dt, user_id, SUM($f2) AS text_feed_count, SUM($f3) AS picture_feed_count,&#010;SUM($f4) AS be_comment_forward_user_count, SUM($f5) AS share_link_count, SUM($f6) AS share_music_count,&#010;SUM($f7) AS share_video_count, SUM($f8) AS follow_count, SUM($f9) AS direct_post_count, SUM($f10)&#010;AS comment_post_count, SUM($f11) AS comment_count, SUM($f12) AS fans_count, MAX(event_time)&#010;AS event_time])",
        "depth": "0",
        "reply": "<tencent_EA99973B2616C038495C670911030CAA6A0A@qq.com>"
    },
    {
        "id": "<CABKuJ_QPY3R8_vJVJL1opVEUnS_C-gpr3MvhBNTRVFBCbBkqfQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 04:51:13 GMT",
        "subject": "Re: Sql往kafka表写聚合数据报错",
        "content": "你可以用Canal或者Debezium format来写入kafka，那样就支持update和delete消息了。&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月29日周三 上午11:59写道：&#013;&#010;&#013;&#010;&gt; 如下，想用sql直接往kafka写聚合结果，版本是1.11，请问能有什么办法解决，还是只能转换成datastream？？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 谢谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Exception in thread \"main\" org.apache.flink.table.api.TableException:&#013;&#010;&gt; Table sink 'default_catalog.default_database.mvp_rtdwb_user_business'&#013;&#010;&gt; doesn't support consuming update changes which is produced by node&#013;&#010;&gt; GroupAggregate(groupBy=[dt, user_id], select=[dt, user_id, SUM($f2) AS&#013;&#010;&gt; text_feed_count, SUM($f3) AS picture_feed_count, SUM($f4) AS&#013;&#010;&gt; be_comment_forward_user_count, SUM($f5) AS share_link_count, SUM($f6) AS&#013;&#010;&gt; share_music_count, SUM($f7) AS share_video_count, SUM($f8) AS follow_count,&#013;&#010;&gt; SUM($f9) AS direct_post_count, SUM($f10) AS comment_post_count, SUM($f11)&#013;&#010;&gt; AS comment_count, SUM($f12) AS fans_count, MAX(event_time) AS event_time])&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_EA99973B2616C038495C670911030CAA6A0A@qq.com>"
    },
    {
        "id": "<CAELO932=uVbSOB3O2pmCed+b0Jx+YE667x1jz1ZRvpg7XfaPvw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:39:01 GMT",
        "subject": "Re: Sql往kafka表写聚合数据报错",
        "content": "抱歉哈，1.11 提供的 Canal 和 Debezium 还不支持 sink 。 预计会在 1.12 中提供。&#013;&#010;&#013;&#010;On Wed, 29 Jul 2020 at 12:51, Benchao Li &lt;libenchao@apache.org&gt; wrote:&#013;&#010;&#013;&#010;&gt; 你可以用Canal或者Debezium format来写入kafka，那样就支持update和delete消息了。&#013;&#010;&gt;&#013;&#010;&gt; op &lt;520075694@qq.com&gt; 于2020年7月29日周三 上午11:59写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 如下，想用sql直接往kafka写聚合结果，版本是1.11，请问能有什么办法解决，还是只能转换成datastream？？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 谢谢&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Exception in thread \"main\" org.apache.flink.table.api.TableException:&#013;&#010;&gt; &gt; Table sink 'default_catalog.default_database.mvp_rtdwb_user_business'&#013;&#010;&gt; &gt; doesn't support consuming update changes which is produced by node&#013;&#010;&gt; &gt; GroupAggregate(groupBy=[dt, user_id], select=[dt, user_id, SUM($f2) AS&#013;&#010;&gt; &gt; text_feed_count, SUM($f3) AS picture_feed_count, SUM($f4) AS&#013;&#010;&gt; &gt; be_comment_forward_user_count, SUM($f5) AS share_link_count, SUM($f6) AS&#013;&#010;&gt; &gt; share_music_count, SUM($f7) AS share_video_count, SUM($f8) AS&#013;&#010;&gt; follow_count,&#013;&#010;&gt; &gt; SUM($f9) AS direct_post_count, SUM($f10) AS comment_post_count, SUM($f11)&#013;&#010;&gt; &gt; AS comment_count, SUM($f12) AS fans_count, MAX(event_time) AS&#013;&#010;&gt; event_time])&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_EA99973B2616C038495C670911030CAA6A0A@qq.com>"
    },
    {
        "id": "<tencent_2D136960A224E8B28489970601B558CADB09@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:40:55 GMT",
        "subject": "回复： Sql往kafka表写聚合数据报错",
        "content": "嗯 谢谢，那能不能像1.10那样自定义connector type&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;imjark@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月30日(星期四) 上午10:39&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: Sql往kafka表写聚合数据报错&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;抱歉哈，1.11 提供的 Canal 和 Debezium 还不支持 sink 。 预计会在 1.12 中提供。&#013;&#010;&#013;&#010;On Wed, 29 Jul 2020 at 12:51, Benchao Li &lt;libenchao@apache.org&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; 你可以用Canal或者Debezium format来写入kafka，那样就支持update和delete消息了。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; op &lt;520075694@qq.com&amp;gt; 于2020年7月29日周三 上午11:59写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 如下，想用sql直接往kafka写聚合结果，版本是1.11，请问能有什么办法解决，还是只能转换成datastream？？&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 谢谢&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; Exception in thread \"main\" org.apache.flink.table.api.TableException:&#013;&#010;&amp;gt; &amp;gt; Table sink 'default_catalog.default_database.mvp_rtdwb_user_business'&#013;&#010;&amp;gt; &amp;gt; doesn't support consuming update changes which is produced by node&#013;&#010;&amp;gt; &amp;gt; GroupAggregate(groupBy=[dt, user_id], select=[dt, user_id, SUM($f2) AS&#013;&#010;&amp;gt; &amp;gt; text_feed_count, SUM($f3) AS picture_feed_count, SUM($f4) AS&#013;&#010;&amp;gt; &amp;gt; be_comment_forward_user_count, SUM($f5) AS share_link_count, SUM($f6) AS&#013;&#010;&amp;gt; &amp;gt; share_music_count, SUM($f7) AS share_video_count, SUM($f8) AS&#013;&#010;&amp;gt; follow_count,&#013;&#010;&amp;gt; &amp;gt; SUM($f9) AS direct_post_count, SUM($f10) AS comment_post_count, SUM($f11)&#013;&#010;&amp;gt; &amp;gt; AS comment_count, SUM($f12) AS fans_count, MAX(event_time) AS&#013;&#010;&amp;gt; event_time])&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; --&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Benchao Li&#013;&#010;&amp;gt;",
        "depth": "3",
        "reply": "<tencent_EA99973B2616C038495C670911030CAA6A0A@qq.com>"
    },
    {
        "id": "<007b01d6655e$d3608e60$7a21ab20$@163.com>",
        "from": "&quot;zhoujibo&quot; &lt;zhoujibo_...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 04:14:59 GMT",
        "subject": "Flink提交任务时可以使用-djs参数，但没有看到相关介绍",
        "content": " &#013;&#010;&#013;&#010;提交任务时可以使用-dis 传递依赖的jar包，如下命令：&#013;&#010;&#013;&#010;./bin/flink run -c com.test.TestData -p 2 -djs $(echo ./test/libs/*.jar | tr&#013;&#010;' ' ',') ./test/test-data-1.0.jar &#013;&#010;&#013;&#010; &#013;&#010;&#013;&#010;但-djs这个参数官网上貌似没有相关介绍，参数有什么文档介绍吗&#013;&#010;&#013;&#010; &#013;&#010;&#013;&#010; &#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<007b01d6655e$d3608e60$7a21ab20$@163.com>"
    },
    {
        "id": "<CANM9E6-1XMbsH9+gqemubRo1WhcubVg9YEX5Ooi1pswwhTWDnQ@mail.gmail.com>",
        "from": "shuwen zhou &lt;jaco...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 06:23:54 GMT",
        "subject": "Flink使用Kafka作为source时checkpoint成功提交offset的机制",
        "content": "大家好，请教一个问题，&#013;&#010;当Flink在使用Kafka作为source，并且checkpoint开启时，提交的offset是在消息被链路最后一个算子处理之后才会把这条offset提交吗？&#013;&#010;还是说这个消息只要从Kafka source读取后，不论链路处理到哪个算子，&#010;checkpoint成功时就会把它的offset提交呢？&#013;&#010;&#013;&#010;另外有大神指路这段代码具体在哪个文件吗？&#013;&#010;谢谢！&#013;&#010;&#013;&#010;-- &#013;&#010;Best Wishes,&#013;&#010;Shuwen Zhou&#013;&#010;",
        "depth": "0",
        "reply": "<CANM9E6-1XMbsH9+gqemubRo1WhcubVg9YEX5Ooi1pswwhTWDnQ@mail.gmail.com>"
    },
    {
        "id": "<006501d66574$8e104150$aa30c3f0$@163.com>",
        "from": "&quot;venn&quot; &lt;wxchunj...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 06:50:32 GMT",
        "subject": "RE: Flink使用Kafka作为source时checkpoint成功提交offset的机制",
        "content": "checkpoint成功时就会把它的offset提交，可以看下这个类:  FlinkKafkaConsumerBase&#010; 的 这个方法： notifyCheckpointComplete&#013;&#010;&#013;&#010;-----Original Message-----&#013;&#010;From: user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org &lt;user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&gt;&#010;On Behalf Of shuwen zhou&#013;&#010;Sent: 2020年7月29日 14:24&#013;&#010;To: user-zh@flink.apache.org&#013;&#010;Subject: Flink使用Kafka作为source时checkpoint成功提交offset的机制&#013;&#010;&#013;&#010;大家好，请教一个问题，&#013;&#010;当Flink在使用Kafka作为source，并且checkpoint开启时，提交的offset是在消息被链路最后一个算子处理之后才会把这条offset提交吗？&#013;&#010;还是说这个消息只要从Kafka source读取后，不论链路处理到哪个算子，&#010;checkpoint成功时就会把它的offset提交呢？&#013;&#010;&#013;&#010;另外有大神指路这段代码具体在哪个文件吗？&#013;&#010;谢谢！&#013;&#010;&#013;&#010;-- &#013;&#010;Best Wishes,&#013;&#010;Shuwen Zhou&#013;&#010;",
        "depth": "1",
        "reply": "<CANM9E6-1XMbsH9+gqemubRo1WhcubVg9YEX5Ooi1pswwhTWDnQ@mail.gmail.com>"
    },
    {
        "id": "<CANM9E6_Xb1S77UYWmbp6tW3YAVTUxp6vYyWA7NjVZpEDvt420g@mail.gmail.com>",
        "from": "shuwen zhou &lt;jaco...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 07:09:32 GMT",
        "subject": "Re: Flink使用Kafka作为source时checkpoint成功提交offset的机制",
        "content": "比如读到一条offset值为100的消息，有Kafka Consumer source，一个Process算子和一个Sink算子&#013;&#010;那么在这条消息到达Process算子，还未到达Sink算子时，提交offset是100吗？&#013;&#010;&#013;&#010;On Wed, 29 Jul 2020 at 14:51, venn &lt;wxchunjhyy@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; checkpoint成功时就会把它的offset提交，可以看下这个类:  FlinkKafkaConsumerBase&#010; 的 这个方法：&#013;&#010;&gt; notifyCheckpointComplete&#013;&#010;&gt;&#013;&#010;&gt; -----Original Message-----&#013;&#010;&gt; From: user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&#013;&#010;&gt; &lt;user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&gt; On Behalf Of&#013;&#010;&gt; shuwen zhou&#013;&#010;&gt; Sent: 2020年7月29日 14:24&#013;&#010;&gt; To: user-zh@flink.apache.org&#013;&#010;&gt; Subject: Flink使用Kafka作为source时checkpoint成功提交offset的机制&#013;&#010;&gt;&#013;&#010;&gt; 大家好，请教一个问题，&#013;&#010;&gt;&#013;&#010;&gt; 当Flink在使用Kafka作为source，并且checkpoint开启时，提交的offset是在消息被链路最后一个算子处理之后才会把这条offset提交吗？&#013;&#010;&gt; 还是说这个消息只要从Kafka source读取后，不论链路处理到哪个算子，&#010;checkpoint成功时就会把它的offset提交呢？&#013;&#010;&gt;&#013;&#010;&gt; 另外有大神指路这段代码具体在哪个文件吗？&#013;&#010;&gt; 谢谢！&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best Wishes,&#013;&#010;&gt; Shuwen Zhou&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best Wishes,&#013;&#010;Shuwen Zhou &lt;http://www.linkedin.com/pub/shuwen-zhou/57/55b/599/&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CANM9E6-1XMbsH9+gqemubRo1WhcubVg9YEX5Ooi1pswwhTWDnQ@mail.gmail.com>"
    },
    {
        "id": "<007801d6657d$9c9f6ee0$d5de4ca0$@163.com>",
        "from": "&quot;venn&quot; &lt;wxchunj...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 07:55:20 GMT",
        "subject": "RE: Flink使用Kafka作为source时checkpoint成功提交offset的机制",
        "content": "可以这样理解，实际上souce 算子只知道这条数据发出去了，不知道这条数据执行到哪里的&#013;&#010;&#013;&#010;-----Original Message-----&#013;&#010;From: user-zh-return-5981-wxchunjhyy=163.com@flink.apache.org &lt;user-zh-return-5981-wxchunjhyy=163.com@flink.apache.org&gt;&#010;On Behalf Of shuwen zhou&#013;&#010;Sent: 2020年7月29日 15:10&#013;&#010;To: user-zh@flink.apache.org&#013;&#010;Subject: Re: Flink使用Kafka作为source时checkpoint成功提交offset的机制&#013;&#010;&#013;&#010;比如读到一条offset值为100的消息，有Kafka Consumer source，一个Process算子和一个Sink算子&#013;&#010;那么在这条消息到达Process算子，还未到达Sink算子时，提交offset是100吗？&#013;&#010;&#013;&#010;On Wed, 29 Jul 2020 at 14:51, venn &lt;wxchunjhyy@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; checkpoint成功时就会把它的offset提交，可以看下这个类:  FlinkKafkaConsumerBase&#010; 的 这个方法：&#013;&#010;&gt; notifyCheckpointComplete&#013;&#010;&gt;&#013;&#010;&gt; -----Original Message-----&#013;&#010;&gt; From: user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&#013;&#010;&gt; &lt;user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&gt; On Behalf Of &#013;&#010;&gt; shuwen zhou&#013;&#010;&gt; Sent: 2020年7月29日 14:24&#013;&#010;&gt; To: user-zh@flink.apache.org&#013;&#010;&gt; Subject: Flink使用Kafka作为source时checkpoint成功提交offset的机制&#013;&#010;&gt;&#013;&#010;&gt; 大家好，请教一个问题，&#013;&#010;&gt;&#013;&#010;&gt; 当Flink在使用Kafka作为source，并且checkpoint开启时，提交的offset是在消息被链路最后一个算子处理之后才会把这条&#013;&#010;&gt; offset提交吗？&#013;&#010;&gt; 还是说这个消息只要从Kafka source读取后，不论链路处理到哪个算子，&#010;checkpoint成功时就会把它的offset提交呢？&#013;&#010;&gt;&#013;&#010;&gt; 另外有大神指路这段代码具体在哪个文件吗？&#013;&#010;&gt; 谢谢！&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best Wishes,&#013;&#010;&gt; Shuwen Zhou&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;--&#013;&#010;Best Wishes,&#013;&#010;Shuwen Zhou &lt;http://www.linkedin.com/pub/shuwen-zhou/57/55b/599/&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CANM9E6-1XMbsH9+gqemubRo1WhcubVg9YEX5Ooi1pswwhTWDnQ@mail.gmail.com>"
    },
    {
        "id": "<df102d6.96c0.1739995ea30.Coremail.hk__lrzy@163.com>",
        "from": "hk__lrzy &lt;hk__l...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 08:00:05 GMT",
        "subject": "回复： Flink使用Kafka作为source时checkpoint成功提交offset的机制",
        "content": "你是说emit之后的offset commit么？可以看下&#010;`Kafka09Fetcher`的runFetchLoop方法&#010;&#010;&#010;在2020年07月29日 15:09，shuwen zhou&lt;jacobc3@gmail.com&gt; 写道：&#010;比如读到一条offset值为100的消息，有Kafka Consumer source，一个Process算子和一个Sink算子&#010;那么在这条消息到达Process算子，还未到达Sink算子时，提交offset是100吗？&#010;&#010;On Wed, 29 Jul 2020 at 14:51, venn &lt;wxchunjhyy@163.com&gt; wrote:&#010;&#010;checkpoint成功时就会把它的offset提交，可以看下这个类:  FlinkKafkaConsumerBase&#010; 的 这个方法：&#010;notifyCheckpointComplete&#010;&#010;-----Original Message-----&#010;From: user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&#010;&lt;user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&gt; On Behalf Of&#010;shuwen zhou&#010;Sent: 2020年7月29日 14:24&#010;To: user-zh@flink.apache.org&#010;Subject: Flink使用Kafka作为source时checkpoint成功提交offset的机制&#010;&#010;大家好，请教一个问题，&#010;&#010;当Flink在使用Kafka作为source，并且checkpoint开启时，提交的offset是在消息被链路最后一个算子处理之后才会把这条offset提交吗？&#010;还是说这个消息只要从Kafka source读取后，不论链路处理到哪个算子，&#010;checkpoint成功时就会把它的offset提交呢？&#010;&#010;另外有大神指路这段代码具体在哪个文件吗？&#010;谢谢！&#010;&#010;--&#010;Best Wishes,&#010;Shuwen Zhou&#010;&#010;&#010;&#010;--&#010;Best Wishes,&#010;Shuwen Zhou &lt;http://www.linkedin.com/pub/shuwen-zhou/57/55b/599/&gt;&#010;",
        "depth": "3",
        "reply": "<CANM9E6-1XMbsH9+gqemubRo1WhcubVg9YEX5Ooi1pswwhTWDnQ@mail.gmail.com>"
    },
    {
        "id": "<CAOMLN=a3f8VVFN0KgcWD9KE9epW-8zM_Q6ZVgwwdmxasqbJ4yQ@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 10:00:33 GMT",
        "subject": "Re: Flink使用Kafka作为source时checkpoint成功提交offset的机制",
        "content": "似乎楼主一开始说的checkpoint成功是指source&#013;&#010;算子的checkpoint成功？但notifyCheckpointComplete函数要求的是整个链路的chk成功。&#013;&#010;这个时候offset为100的消息必然已经被sink算子处理完成了，因为触发chk的屏障消息必然在offset100的消息之后到达sink算子。&#013;&#010;&#013;&#010;hk__lrzy &lt;hk__lrzy@163.com&gt; 于2020年7月29日周三 下午5:53写道：&#013;&#010;&#013;&#010;&gt; 你是说emit之后的offset commit么？可以看下&#013;&#010;&gt; `Kafka09Fetcher`的runFetchLoop方法&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月29日 15:09，shuwen zhou&lt;jacobc3@gmail.com&gt; 写道：&#013;&#010;&gt; 比如读到一条offset值为100的消息，有Kafka Consumer source，一个Process算子和一个Sink算子&#013;&#010;&gt; 那么在这条消息到达Process算子，还未到达Sink算子时，提交offset是100吗？&#013;&#010;&gt;&#013;&#010;&gt; On Wed, 29 Jul 2020 at 14:51, venn &lt;wxchunjhyy@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; checkpoint成功时就会把它的offset提交，可以看下这个类:  FlinkKafkaConsumerBase&#010; 的 这个方法：&#013;&#010;&gt; notifyCheckpointComplete&#013;&#010;&gt;&#013;&#010;&gt; -----Original Message-----&#013;&#010;&gt; From: user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&#013;&#010;&gt; &lt;user-zh-return-5976-wxchunjhyy=163.com@flink.apache.org&gt; On Behalf Of&#013;&#010;&gt; shuwen zhou&#013;&#010;&gt; Sent: 2020年7月29日 14:24&#013;&#010;&gt; To: user-zh@flink.apache.org&#013;&#010;&gt; Subject: Flink使用Kafka作为source时checkpoint成功提交offset的机制&#013;&#010;&gt;&#013;&#010;&gt; 大家好，请教一个问题，&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 当Flink在使用Kafka作为source，并且checkpoint开启时，提交的offset是在消息被链路最后一个算子处理之后才会把这条offset提交吗？&#013;&#010;&gt; 还是说这个消息只要从Kafka source读取后，不论链路处理到哪个算子，&#010;checkpoint成功时就会把它的offset提交呢？&#013;&#010;&gt;&#013;&#010;&gt; 另外有大神指路这段代码具体在哪个文件吗？&#013;&#010;&gt; 谢谢！&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best Wishes,&#013;&#010;&gt; Shuwen Zhou&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best Wishes,&#013;&#010;&gt; Shuwen Zhou &lt;http://www.linkedin.com/pub/shuwen-zhou/57/55b/599/&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CANM9E6-1XMbsH9+gqemubRo1WhcubVg9YEX5Ooi1pswwhTWDnQ@mail.gmail.com>"
    },
    {
        "id": "<43c901fe.31a7.173994141dc.Coremail.felixzh2020@126.com>",
        "from": "felixzh &lt;felixzh2...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 06:27:37 GMT",
        "subject": "FLink1.11.1整合hadoop3.0.0",
        "content": "以 -m yarn-clsuter运行flink1.11.1的examples/streaming/SocketWindowWordCount.jar&#010;任务正常。ncat -l 输入数据可以在taskmanager.out日志中看到。&#010;但是，yarn 中ApplicationMaster（也就是Tracking UI）打不开flink集群的页面&#010;提示：{\"errors\":[\"Unable to load requested file /index.html\"]}&#010;大佬遇到过吗？",
        "depth": "0",
        "reply": "<43c901fe.31a7.173994141dc.Coremail.felixzh2020@126.com>"
    },
    {
        "id": "<tencent_EFDFF3DBE9836DA4CFE125F6A5801A802E05@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 07:07:26 GMT",
        "subject": "flink-1.11 hive-1.2.1 ddl 无法写入数据",
        "content": "确认数据源有数据，全部代码如下，但是hive就是没有数据&#013;&#010;&#013;&#010;package com.hive;&#013;&#010;&#013;&#010;import org.apache.flink.runtime.state.filesystem.FsStateBackend;&#013;&#010;import org.apache.flink.streaming.api.CheckpointingMode;&#013;&#010;import org.apache.flink.streaming.api.TimeCharacteristic;&#013;&#010;import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;&#013;&#010;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#013;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#013;&#010;import org.apache.flink.table.api.SqlDialect;&#013;&#010;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;&#013;&#010;import org.apache.flink.table.catalog.hive.HiveCatalog;&#013;&#010;&#013;&#010;import java.time.Duration;&#013;&#010;&#013;&#010;public class HiveTest {&#013;&#010;    private static final String path = \"hdfs_path\";&#013;&#010;    public static void main(String []args)  {&#013;&#010;        System.setProperty(\"HADOOP_USER_NAME\", \"work\");&#013;&#010;        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;        env.setParallelism(1);&#013;&#010;        env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);&#013;&#010;        // 同一时间只允许进行一个检查点&#013;&#010;        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);&#013;&#010;&#013;&#010;        env.setStateBackend(new FsStateBackend(path));&#013;&#010;        EnvironmentSettings tableEnvSettings = EnvironmentSettings.newInstance()&#013;&#010;                .useBlinkPlanner()&#013;&#010;                .inStreamingMode()&#013;&#010;                .build();&#013;&#010;        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env,tableEnvSettings);&#013;&#010;        tableEnv.getConfig().getConfiguration().set(ExecutionCheckpointingOptions.CHECKPOINTING_MODE,&#010;CheckpointingMode.EXACTLY_ONCE);&#013;&#010;        tableEnv.getConfig().getConfiguration().set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,&#010;Duration.ofSeconds(20));&#013;&#010;&#013;&#010;        String name            = \"myhive\";&#013;&#010;        String defaultDatabase = \"situation\";&#013;&#010;        String hiveConfDir     = \"/load/data/hive/hive-conf\"; // a local path&#013;&#010;        String version         = \"1.2.1\";&#013;&#010;&#013;&#010;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase, hiveConfDir, version);&#013;&#010;        tableEnv.registerCatalog(\"myhive\", hive);&#013;&#010;&#013;&#010;// set the HiveCatalog as the current catalog of the session&#013;&#010;        tableEnv.useCatalog(\"myhive\");&#013;&#010;        tableEnv.executeSql(\"CREATE DATABASE IF NOT EXISTS situation\");&#013;&#010;        tableEnv.executeSql(\"DROP TABLE IF EXISTS situation.source_table\");&#013;&#010;&#013;&#010;&#013;&#010;        tableEnv.executeSql(\"CREATE TABLE situation.source_table (\\n\" +&#013;&#010;                \"\\thost STRING,\\n\" +&#013;&#010;                \"\\turl STRING,\\n\" +&#013;&#010;                \"\\tpublic_date STRING\\n\" +&#013;&#010;                \") WITH (\\n\" +&#013;&#010;                \"\\t'connector.type' = 'kafka',\\n\" +&#013;&#010;                \"\\t'connector.version' = 'universal',\\n\" +&#013;&#010;                \"\\t'connector.startup-mode' = 'latest-offset',\\n\" +&#013;&#010;                \"\\t'connector.topic' = 'sendMessage',\\n\" +&#013;&#010;                \"\\t'connector.properties.group.id' = 'domain_testGroup',\\n\" +&#013;&#010;                \"\\t'connector.properties.zookeeper.connect' = '127.0.0.1:2181',\\n\" +&#013;&#010;                \"\\t'connector.properties.bootstrap.servers' = '127.0.0.1:9092',\\n\" +&#013;&#010;                \"\\t'update-mode' = 'append',\\n\" +&#013;&#010;                \"\\t'format.type' = 'json',\\n\" +&#013;&#010;                \"\\t'format.derive-schema' = 'true'\\n\" +&#013;&#010;                \")\");&#013;&#010;&#013;&#010;        tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#013;&#010;        tableEnv.executeSql(\"DROP TABLE IF EXISTS situation.fs_table\");&#013;&#010;&#013;&#010;        String hiveSql = \"\\n\" +&#013;&#010;                \"  CREATE TABLE situation.fs_table (\\n\" +&#013;&#010;                \" \\n\" +&#013;&#010;                \"    host STRING,\\n\" +&#013;&#010;                \"    url STRING,\\n\" +&#013;&#010;                \"    public_date STRING\\n\" +&#013;&#010;                \"  \\n\" +&#013;&#010;                \"  ) PARTITIONED BY (\\n\" +&#013;&#010;                \"    ts_date STRING,\\n\" +&#013;&#010;                \"    ts_hour STRING,\\n\" +&#013;&#010;                \"    ts_minute STRING\\n\" +&#013;&#010;                \"  ) STORED AS PARQUET\\n\" +&#013;&#010;                \"  TBLPROPERTIES (\\n\" +&#013;&#010;                \"    'sink.partition-commit.trigger' = 'process time',\\n\" +&#013;&#010;                \"    'sink.partition-commit.delay' = '1 min',\\n\" +&#013;&#010;                \"    'sink.partition-commit.policy.kind' = 'metastore,success-file',\\n\" +&#013;&#010;                \"    'partition.time-extractor.timestamp-pattern' = '$ts_date $ts_hour:$ts_minute:00'\\n\"&#010;+&#013;&#010;                \"  )\\n\" +&#013;&#010;                \"  \";&#013;&#010;        tableEnv.executeSql(hiveSql);&#013;&#010;&#013;&#010;        tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#013;&#010;&#013;&#010;        tableEnv.executeSql(\"INSERT INTO  situation.fs_table SELECT host, url,public_date,\"&#010;+&#013;&#010;                \" DATE_FORMAT(public_date,'yyyy-MM-dd') ,DATE_FORMAT(public_date,'HH') ,DATE_FORMAT(public_date,'mm')&#010; FROM situation.source_table\");&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;    }&#013;&#010;}",
        "depth": "0",
        "reply": "<tencent_EFDFF3DBE9836DA4CFE125F6A5801A802E05@qq.com>"
    },
    {
        "id": "<55d18189.111b4.1739b59e624.Coremail.18868816710@163.com>",
        "from": "hailongwang  &lt;18868816...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 16:13:46 GMT",
        "subject": "Re:flink-1.11 hive-1.2.1 ddl 无法写入数据",
        "content": "&#010;&#010;&#010;有什么异常信息吗&#010;&#010;&#010;&#010;&#010;在 2020-07-29 14:07:26，\"kcz\" &lt;573693104@qq.com&gt; 写道：&#010;&gt;确认数据源有数据，全部代码如下，但是hive就是没有数据&#010;&gt;&#010;&gt;package com.hive;&#010;&gt;&#010;&gt;import org.apache.flink.runtime.state.filesystem.FsStateBackend;&#010;&gt;import org.apache.flink.streaming.api.CheckpointingMode;&#010;&gt;import org.apache.flink.streaming.api.TimeCharacteristic;&#010;&gt;import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;&#010;&gt;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt;import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt;import org.apache.flink.table.api.SqlDialect;&#010;&gt;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;&#010;&gt;import org.apache.flink.table.catalog.hive.HiveCatalog;&#010;&gt;&#010;&gt;import java.time.Duration;&#010;&gt;&#010;&gt;public class HiveTest {&#010;&gt;    private static final String path = \"hdfs_path\";&#010;&gt;    public static void main(String []args)  {&#010;&gt;        System.setProperty(\"HADOOP_USER_NAME\", \"work\");&#010;&gt;        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;        env.setParallelism(1);&#010;&gt;        env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);&#010;&gt;        // 同一时间只允许进行一个检查点&#010;&gt;        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);&#010;&gt;&#010;&gt;        env.setStateBackend(new FsStateBackend(path));&#010;&gt;        EnvironmentSettings tableEnvSettings = EnvironmentSettings.newInstance()&#010;&gt;                .useBlinkPlanner()&#010;&gt;                .inStreamingMode()&#010;&gt;                .build();&#010;&gt;        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env,tableEnvSettings);&#010;&gt;        tableEnv.getConfig().getConfiguration().set(ExecutionCheckpointingOptions.CHECKPOINTING_MODE,&#010;CheckpointingMode.EXACTLY_ONCE);&#010;&gt;        tableEnv.getConfig().getConfiguration().set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,&#010;Duration.ofSeconds(20));&#010;&gt;&#010;&gt;        String name            = \"myhive\";&#010;&gt;        String defaultDatabase = \"situation\";&#010;&gt;        String hiveConfDir     = \"/load/data/hive/hive-conf\"; // a local path&#010;&gt;        String version         = \"1.2.1\";&#010;&gt;&#010;&gt;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase, hiveConfDir, version);&#010;&gt;        tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt;&#010;&gt;// set the HiveCatalog as the current catalog of the session&#010;&gt;        tableEnv.useCatalog(\"myhive\");&#010;&gt;        tableEnv.executeSql(\"CREATE DATABASE IF NOT EXISTS situation\");&#010;&gt;        tableEnv.executeSql(\"DROP TABLE IF EXISTS situation.source_table\");&#010;&gt;&#010;&gt;&#010;&gt;        tableEnv.executeSql(\"CREATE TABLE situation.source_table (\\n\" +&#010;&gt;                \"\\thost STRING,\\n\" +&#010;&gt;                \"\\turl STRING,\\n\" +&#010;&gt;                \"\\tpublic_date STRING\\n\" +&#010;&gt;                \") WITH (\\n\" +&#010;&gt;                \"\\t'connector.type' = 'kafka',\\n\" +&#010;&gt;                \"\\t'connector.version' = 'universal',\\n\" +&#010;&gt;                \"\\t'connector.startup-mode' = 'latest-offset',\\n\" +&#010;&gt;                \"\\t'connector.topic' = 'sendMessage',\\n\" +&#010;&gt;                \"\\t'connector.properties.group.id' = 'domain_testGroup',\\n\" +&#010;&gt;                \"\\t'connector.properties.zookeeper.connect' = '127.0.0.1:2181',\\n\" +&#010;&gt;                \"\\t'connector.properties.bootstrap.servers' = '127.0.0.1:9092',\\n\" +&#010;&gt;                \"\\t'update-mode' = 'append',\\n\" +&#010;&gt;                \"\\t'format.type' = 'json',\\n\" +&#010;&gt;                \"\\t'format.derive-schema' = 'true'\\n\" +&#010;&gt;                \")\");&#010;&gt;&#010;&gt;        tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt;        tableEnv.executeSql(\"DROP TABLE IF EXISTS situation.fs_table\");&#010;&gt;&#010;&gt;        String hiveSql = \"\\n\" +&#010;&gt;                \"  CREATE TABLE situation.fs_table (\\n\" +&#010;&gt;                \" \\n\" +&#010;&gt;                \"    host STRING,\\n\" +&#010;&gt;                \"    url STRING,\\n\" +&#010;&gt;                \"    public_date STRING\\n\" +&#010;&gt;                \"  \\n\" +&#010;&gt;                \"  ) PARTITIONED BY (\\n\" +&#010;&gt;                \"    ts_date STRING,\\n\" +&#010;&gt;                \"    ts_hour STRING,\\n\" +&#010;&gt;                \"    ts_minute STRING\\n\" +&#010;&gt;                \"  ) STORED AS PARQUET\\n\" +&#010;&gt;                \"  TBLPROPERTIES (\\n\" +&#010;&gt;                \"    'sink.partition-commit.trigger' = 'process time',\\n\" +&#010;&gt;                \"    'sink.partition-commit.delay' = '1 min',\\n\" +&#010;&gt;                \"    'sink.partition-commit.policy.kind' = 'metastore,success-file',\\n\"&#010;+&#010;&gt;                \"    'partition.time-extractor.timestamp-pattern' = '$ts_date $ts_hour:$ts_minute:00'\\n\"&#010;+&#010;&gt;                \"  )\\n\" +&#010;&gt;                \"  \";&#010;&gt;        tableEnv.executeSql(hiveSql);&#010;&gt;&#010;&gt;        tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt;&#010;&gt;        tableEnv.executeSql(\"INSERT INTO  situation.fs_table SELECT host, url,public_date,\"&#010;+&#010;&gt;                \" DATE_FORMAT(public_date,'yyyy-MM-dd') ,DATE_FORMAT(public_date,'HH')&#010;,DATE_FORMAT(public_date,'mm')  FROM situation.source_table\");&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;    }&#010;&gt;}&#010;",
        "depth": "1",
        "reply": "<tencent_EFDFF3DBE9836DA4CFE125F6A5801A802E05@qq.com>"
    },
    {
        "id": "<37E3D75F-F65D-423E-8522-7B628F6C5B17@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 16:14:08 GMT",
        "subject": "Re: flink-1.11 hive-1.2.1 ddl 无法写入数据",
        "content": "Hi, kcz&#010;&#010;看connector的properties还是1.10的格式，你换成1.11试试[1].&#010;&#010;&#010;&gt; 在 2020年7月29日，15:07，kcz &lt;573693104@qq.com&gt; 写道：&#010;&gt; &#010;&gt;  tableEnv.executeSql(\"CREATE TABLE situation.source_table (\\n\" +&#010;&gt;                \"\\thost STRING,\\n\" +&#010;&gt;                \"\\turl STRING,\\n\" +&#010;&gt;                \"\\tpublic_date STRING\\n\" +&#010;&gt;                \") WITH (\\n\" +&#010;&gt;                \"\\t'connector.type' = 'kafka',\\n\" +&#010;&gt;                \"\\t'connector.version' = 'universal',\\n\" +&#010;&gt;                \"\\t'connector.startup-mode' = 'latest-offset',\\n\" +&#010;&gt;                \"\\t'connector.topic' = 'sendMessage',\\n\" +&#010;&gt;                \"\\t'connector.properties.group.id &lt;http://connector.properties.group.id/&gt;'&#010;= 'domain_testGroup',\\n\" +&#010;&gt;                \"\\t'connector.properties.zookeeper.connect' = '127.0.0.1:2181',\\n\" +&#010;&gt;                \"\\t'connector.properties.bootstrap.servers' = '127.0.0.1:9092',\\n\" +&#010;&gt;                \"\\t'update-mode' = 'append',\\n\" +&#010;&gt;                \"\\t'format.type' = 'json',\\n\" +&#010;&gt;                \"\\t'format.derive-schema' = 'true'\\n\" +&#010;&gt;                \")\");&#010;&#010;&#010;Best&#010;Leonard&#010;[1]https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options&gt;&#010;",
        "depth": "1",
        "reply": "<tencent_EFDFF3DBE9836DA4CFE125F6A5801A802E05@qq.com>"
    },
    {
        "id": "<tencent_E28F7CD88D483888C6C617C00374E57B7E0A@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 03:16:19 GMT",
        "subject": "回复： flink-1.11 hive-1.2.1 ddl 无法写入数据",
        "content": "sorry,我把idea的log4j弄坏了，没有出现错误提示，我下面的process-time 写成了&#010;process time。改了log提示之后，有清楚的提示了。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月30日(星期四) 凌晨0:14&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink-1.11 hive-1.2.1 ddl 无法写入数据&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi, kcz&#013;&#010;&#013;&#010;看connector的properties还是1.10的格式，你换成1.11试试[1].&#013;&#010;&#013;&#010;&#013;&#010;&amp;gt; 在 2020年7月29日，15:07，kcz &lt;573693104@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt;&amp;nbsp; tableEnv.executeSql(\"CREATE TABLE situation.source_table (\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\thost STRING,\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\turl STRING,\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\tpublic_date STRING\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\") WITH (\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'connector.type' = 'kafka',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'connector.version' = 'universal',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'connector.startup-mode' = 'latest-offset',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'connector.topic' = 'sendMessage',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'connector.properties.group.id &lt;http://connector.properties.group.id/&amp;gt;' = 'domain_testGroup',\\n\"&#010;+&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'connector.properties.zookeeper.connect' = '127.0.0.1:2181',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'connector.properties.bootstrap.servers' = '127.0.0.1:9092',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'update-mode' = 'append',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'format.type' = 'json',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"\\t'format.derive-schema' = 'true'\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\")\");&#013;&#010;&#013;&#010;&#013;&#010;Best&#013;&#010;Leonard&#013;&#010;[1]https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/kafka.html#connector-options&amp;gt;",
        "depth": "2",
        "reply": "<tencent_EFDFF3DBE9836DA4CFE125F6A5801A802E05@qq.com>"
    },
    {
        "id": "<CANM9E6-+t7Ld_31J=Oz3QtYKB0_LQBfL1JConU9OuUFM8JUq7g@mail.gmail.com>",
        "from": "shuwen zhou &lt;jaco...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 08:17:20 GMT",
        "subject": "关于window过程中barrier的问题",
        "content": "大家好，想请教一个关于barrier的问题&#013;&#010;如果我有如下算子&#013;&#010;.window()&#013;&#010;.reduce()&#013;&#010;假设barrier和元素顺序是&#013;&#010;tuple 和 barrier&#013;&#010;当tuple流经窗口期，barrier是会阻塞等待tuple从窗口流出之后再达到下一个算子，还是不在窗口阻塞，直接流到下一个算子呢？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Best Wishes,&#013;&#010;Shuwen Zhou&#013;&#010;",
        "depth": "0",
        "reply": "<CANM9E6-+t7Ld_31J=Oz3QtYKB0_LQBfL1JConU9OuUFM8JUq7g@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvuxuzzR5R_0+BBK8pyZpAF7-Ywf6Ghgs5KmOwQ0NUqRcQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:57:03 GMT",
        "subject": "Re: 关于window过程中barrier的问题",
        "content": "Hi Shuwen&#013;&#010;    barrier 是和 checkpoint 相关的逻辑，用来触发 checkpoint 的，你可以认为&#010;barrier&#013;&#010;和数据的顺序必须是严格保证的，不然没法保证 exactlyonce 的语义。&#013;&#010;&#013;&#010;   假设某个算子 B 有两个上游 A1 和 A2，那么 A1 和 A2 的 barrier 都发送的&#010;B 之后，B 才会开始做&#013;&#010;checkpoint，假设 A1 的 barrier 在 10:00 到了，A2 的 barrier 在 10:01 才到，那么&#010;10:00 -&#013;&#010;10:01 这段时间内，A1 发送到 B 的数据是否会被处理取决于是 exactlyonce，还是&#010;at least once。如果是 exactly&#013;&#010;once 语义，则不会处理（堆积在 B 这里），如果是 at least once 语义则会处理并且发送到下游。&#013;&#010;&#013;&#010;   另外也可以阅读一下社区相关的文档[1]&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/learn-flink/fault_tolerance.html#how-does-state-snapshotting-work&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;shuwen zhou &lt;jacobc3@gmail.com&gt; 于2020年7月29日周三 下午4:55写道：&#013;&#010;&#013;&#010;&gt; 大家好，想请教一个关于barrier的问题&#013;&#010;&gt; 如果我有如下算子&#013;&#010;&gt; .window()&#013;&#010;&gt; .reduce()&#013;&#010;&gt; 假设barrier和元素顺序是&#013;&#010;&gt; tuple 和 barrier&#013;&#010;&gt; 当tuple流经窗口期，barrier是会阻塞等待tuple从窗口流出之后再达到下一个算子，还是不在窗口阻塞，直接流到下一个算子呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best Wishes,&#013;&#010;&gt; Shuwen Zhou&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CANM9E6-+t7Ld_31J=Oz3QtYKB0_LQBfL1JConU9OuUFM8JUq7g@mail.gmail.com>"
    },
    {
        "id": "<tencent_D68DAD81CD4F405B5C63CB2C821028E10D06@qq.com>",
        "from": "&quot;Asahi Lee&quot; &lt;978466...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 08:35:30 GMT",
        "subject": "使用datagen connector生成无界数据，一秒时间窗口的聚合操作，一直没有数据输出打印",
        "content": "以下程序运行，控制台一直没有数据输出1. 程序package kafka;&#013;&#010;&#013;&#010;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#013;&#010;import org.apache.flink.table.api.EnvironmentSettings;&#013;&#010;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;&#013;&#010;&#013;&#010;public class DataGenTest {&#013;&#010;&#013;&#010;    public static void main(String[] args) {&#013;&#010;&#013;&#010;        StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;        EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;        StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#013;&#010;&#013;&#010;        String sourceTableDDL = \"CREATE TABLE datagen ( \" +&#013;&#010;                \" f_random INT, \" +&#013;&#010;                \" f_random_str STRING, \" +&#013;&#010;                \" ts AS localtimestamp, \" +&#013;&#010;                \" WATERMARK FOR ts AS ts \" +&#013;&#010;                \") WITH ( \" +&#013;&#010;                \" 'connector' = 'datagen', \" +&#013;&#010;                \" 'rows-per-second'='20', \" +&#013;&#010;                \" 'fields.f_random.min'='1', \" +&#013;&#010;                \" 'fields.f_random.max'='10', \" +&#013;&#010;                \" 'fields.f_random_str.length'='10' \" +&#013;&#010;                \")\";&#013;&#010;&#013;&#010;        bsTableEnv.executeSql(sourceTableDDL);&#013;&#010;&#013;&#010;        bsTableEnv.executeSql(\"SELECT f_random, count(1) \" +&#013;&#010;                \"FROM datagen \" +&#013;&#010;                \"GROUP BY TUMBLE(ts, INTERVAL '1' second), f_random\").print();&#013;&#010;&#013;&#010;    }&#013;&#010;&#013;&#010;}2. 控制台，log4j:WARN No appenders could be found for logger (org.apache.flink.table.module.ModuleManager).&#010;log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig&#010;for more info. +-------------+----------------------+ |    f_random |               EXPR$1&#010;| +-------------+----------------------+",
        "depth": "0",
        "reply": "<tencent_D68DAD81CD4F405B5C63CB2C821028E10D06@qq.com>"
    },
    {
        "id": "<293cfbb5.10fd6.1739b4167b0.Coremail.18868816710@163.com>",
        "from": "hailongwang  &lt;18868816...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 15:47:01 GMT",
        "subject": "Re:使用datagen connector生成无界数据，一秒时间窗口的聚合操作，一直没有数据输出打印",
        "content": "Hi Asahi Lee:&#010; 我在 master 上的 flink-sql-client 模块中建了一个类，复制你的代码控制台是有输出的，你使用的版本是什么的？&#010;Best,&#010;Hailong Wang&#010;&#010;&#010;&#010;&#010;在 2020-07-29 15:35:30，\"Asahi Lee\" &lt;978466273@qq.com&gt; 写道：&#010;&gt;以下程序运行，控制台一直没有数据输出1. 程序package kafka;&#010;&gt;&#010;&gt;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;&#010;&gt;import org.apache.flink.table.api.EnvironmentSettings;&#010;&gt;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;&#010;&gt;&#010;&gt;public class DataGenTest {&#010;&gt;&#010;&gt;    public static void main(String[] args) {&#010;&gt;&#010;&gt;        StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;        EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;        StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;&#010;&gt;        String sourceTableDDL = \"CREATE TABLE datagen ( \" +&#010;&gt;                \" f_random INT, \" +&#010;&gt;                \" f_random_str STRING, \" +&#010;&gt;                \" ts AS localtimestamp, \" +&#010;&gt;                \" WATERMARK FOR ts AS ts \" +&#010;&gt;                \") WITH ( \" +&#010;&gt;                \" 'connector' = 'datagen', \" +&#010;&gt;                \" 'rows-per-second'='20', \" +&#010;&gt;                \" 'fields.f_random.min'='1', \" +&#010;&gt;                \" 'fields.f_random.max'='10', \" +&#010;&gt;                \" 'fields.f_random_str.length'='10' \" +&#010;&gt;                \")\";&#010;&gt;&#010;&gt;        bsTableEnv.executeSql(sourceTableDDL);&#010;&gt;&#010;&gt;        bsTableEnv.executeSql(\"SELECT f_random, count(1) \" +&#010;&gt;                \"FROM datagen \" +&#010;&gt;                \"GROUP BY TUMBLE(ts, INTERVAL '1' second), f_random\").print();&#010;&gt;&#010;&gt;    }&#010;&gt;&#010;&gt;}2. 控制台，log4j:WARN No appenders could be found for logger (org.apache.flink.table.module.ModuleManager).&#010;log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig&#010;for more info. +-------------+----------------------+ |    f_random |               EXPR$1&#010;| +-------------+----------------------+&#010;",
        "depth": "1",
        "reply": "<tencent_D68DAD81CD4F405B5C63CB2C821028E10D06@qq.com>"
    },
    {
        "id": "<D85C5A78-10F5-4CFE-9323-6AA70B9ADA1C@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 16:42:43 GMT",
        "subject": "Re: 使用datagen connector生成无界数据，一秒时间窗口的聚合操作，一直没有数据输出打印",
        "content": "Hi&#010;&#010;&gt; &#010;&gt;        bsTableEnv.executeSql(\"SELECT f_random, count(1) \" +&#010;&gt;                \"FROM datagen \" +&#010;&gt;                \"GROUP BY TUMBLE(ts, INTERVAL '1' second), f_random\").print();&#010;&#010;TableResult.print() 方法目前只支持了 exactly-once 语义，在 streaming 模式下必须设置checkpoint才能work，&#010;你配置下checkpoint之后再试下，支持 At Least Once 的方法在1.12里应该会支持，支持后可以不用设置&#010;checkpoint。&#010;&#010;祝好&#010;Leonard&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_D68DAD81CD4F405B5C63CB2C821028E10D06@qq.com>"
    },
    {
        "id": "<1352f5e8.b213.17399bc4380.Coremail.18668118272@163.com>",
        "from": "Ryiyi &lt;18668118...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 08:41:58 GMT",
        "subject": "近期使用flinksql(1.11.0)发现的一些问题 ",
        "content": "1. create table语句不支持create talbe if not exists：&#010;    不支持if not exists语法在实际使用时特别麻烦，每次重新执行SQL都需要先删除上次执行创建的table。&#010;    Q1： CREATE TABLE IF NOT EXITS语法个人理解实现并不特别麻烦，社区为什么还没实现？&#010;2. flink1.11创建kafka sink表时不再支持update-mode属性：&#010;    创建kafka sink表时报不支持udpate-mode属性的语法检验错误。但查看flink1.11源码中仍存在多个测试类在使用update-mode属性。&#010;    Q2：从flink1.11开始，kafka数据源的建表语句是否明确不再支持update-mode？(官网示例中已删除，但源码测试类中仍存在)&#010;3. update-mode属性校验失败，任务推出，但该表却已经在catalog中生成&#010;    Q3：建表语句的属性校验失败，任务异常退出，但该表却已经在catalog中生成。感觉不太合理&#010;&#010;&#010;麻烦了解的大佬抽空解答下，多谢！",
        "depth": "0",
        "reply": "<1352f5e8.b213.17399bc4380.Coremail.18668118272@163.com>"
    },
    {
        "id": "<CAELO933VSBOV1tW5QyhCe9CXC9HZjxBxJSXDA=PR9c-xyAS-Pg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:36:58 GMT",
        "subject": "Re: 近期使用flinksql(1.11.0)发现的一些问题",
        "content": "Hi,&#013;&#010;&#013;&#010;1. 这个功能应该是漏加了，我建了个 issue 去支持这个功能：&#013;&#010;https://issues.apache.org/jira/browse/FLINK-18756&#013;&#010;2. update-mode 属性对于 kafka 来说一直都是没有用的（因为只支持 append-only）。所以自&#010;1.10&#013;&#010;，这个属性就变成可选了，文档中也不再标识出来。&#013;&#010;    1.11 中新版的 connector 的实现中，也没有这个属性。&#013;&#010;3. 目前 DDL 建表语句将表元信息存到 catalog 中，是不会走校验逻辑的。校验逻辑现在发生在&#010;query 编译期。 这个确实和&#013;&#010;fail-fast 原则有点相悖。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Wed, 29 Jul 2020 at 17:31, Ryiyi &lt;18668118272@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 1. create table语句不支持create talbe if not exists：&#013;&#010;&gt;     不支持if not exists语法在实际使用时特别麻烦，每次重新执行SQL都需要先删除上次执行创建的table。&#013;&#010;&gt;     Q1： CREATE TABLE IF NOT EXITS语法个人理解实现并不特别麻烦，社区为什么还没实现？&#013;&#010;&gt; 2. flink1.11创建kafka sink表时不再支持update-mode属性：&#013;&#010;&gt;     创建kafka&#013;&#010;&gt; sink表时报不支持udpate-mode属性的语法检验错误。但查看flink1.11源码中仍存在多个测试类在使用update-mode属性。&#013;&#010;&gt;     Q2：从flink1.11开始，kafka数据源的建表语句是否明确不再支持update-mode？(官网示例中已删除，但源码测试类中仍存在)&#013;&#010;&gt; 3. update-mode属性校验失败，任务推出，但该表却已经在catalog中生成&#013;&#010;&gt;     Q3：建表语句的属性校验失败，任务异常退出，但该表却已经在catalog中生成。感觉不太合理&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 麻烦了解的大佬抽空解答下，多谢！&#013;&#010;",
        "depth": "1",
        "reply": "<1352f5e8.b213.17399bc4380.Coremail.18668118272@163.com>"
    },
    {
        "id": "<1596076826963-0.post@n8.nabble.com>",
        "from": "wxpcc &lt;wxp4...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:40:26 GMT",
        "subject": "Re: 近期使用flinksql(1.11.0)发现的一些问题",
        "content": "Q1, 可以使用 DROP TABLE IF EXISTS table;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1352f5e8.b213.17399bc4380.Coremail.18668118272@163.com>"
    },
    {
        "id": "<CAK=PWPYR7fOmsmph-8S1uj2rnEGeO3D_NnMWeNeTa3CJx2pXqw@mail.gmail.com>",
        "from": "张锴 &lt;zk357794...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 09:02:24 GMT",
        "subject": "flink on yarn 读取 hbase数据时 ，Task失败，具体描述如下",
        "content": "flink获取Hbase数据并计算&#013;&#010;在本地测试没问题，提交到Yarn上出现Task任务失败，无相关日志输出，task任务一直重启。任务失败的地方在数据计算部分。&#013;&#010;语言：Scala，无堆栈信息输出&#013;&#010;",
        "depth": "0",
        "reply": "<CAK=PWPYR7fOmsmph-8S1uj2rnEGeO3D_NnMWeNeTa3CJx2pXqw@mail.gmail.com>"
    },
    {
        "id": "<75F16B40-E2D1-463C-80B0-23494F83E2F5@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 16:45:29 GMT",
        "subject": "Re: flink on yarn 读取 hbase数据时 ，Task失败，具体描述如下",
        "content": "Hi，张锴&#013;&#010;&#013;&#010;这个描述看起来没有用的信息呢，既然有任务失败，失败的日志和异常信息可以贴出来看看。或者贴一个可以复现这个失败的case.&#013;&#010;&#013;&#010;&gt; 在 2020年7月29日，17:02，张锴 &lt;zk357794239@gmail.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; flink获取Hbase数据并计算&#013;&#010;&gt; 在本地测试没问题，提交到Yarn上出现Task任务失败，无相关日志输出，task任务一直重启。任务失败的地方在数据计算部分。&#013;&#010;&gt; 语言：Scala，无堆栈信息输出&#013;&#010;&#013;&#010;Best&#013;&#010;Leonard",
        "depth": "1",
        "reply": "<CAK=PWPYR7fOmsmph-8S1uj2rnEGeO3D_NnMWeNeTa3CJx2pXqw@mail.gmail.com>"
    },
    {
        "id": "<1596017675326-0.post@n8.nabble.com>",
        "from": "bradyMk &lt;zhbm...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 10:14:35 GMT",
        "subject": "flink1.9.1 在WebUI中查看committedOffsets指标为负值",
        "content": "flink1.9.1&#010;在WebUI中查看Source__Custom_Source.KafkaConsumer.topic.geek-event-target.partition.3.committedOffsets指标为负值，查看官网释义：对于每个分区，最后一次成功提交到Kafka的偏移量。&#010;但我这里为什么是负值呢？&#010;&lt;http://apache-flink.147419.n8.nabble.com/file/t802/%E6%8D%95%E8%8E%B711.png&gt; &#010;&#010;希望能得到指导，万分感谢~&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1596017675326-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAOMLN=YAkg1Tr_JvP_qAYqUV=i=YQ+N1nMWWWWFW_h4WAgoX6g@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 10:06:13 GMT",
        "subject": "Re: flink1.9.1 在WebUI中查看committedOffsets指标为负值",
        "content": "可以检查下在patition3上有没有成功提交过offsets。负值可能是没有提交过情况下的默认值（我猜这是个不变值）。&#013;&#010;&#013;&#010;bradyMk &lt;zhbmeng@126.com&gt; 于2020年7月29日周三 下午6:36写道：&#013;&#010;&#013;&#010;&gt; flink1.9.1&#013;&#010;&gt;&#013;&#010;&gt; 在WebUI中查看Source__Custom_Source.KafkaConsumer.topic.geek-event-target.partition.3.committedOffsets指标为负值，查看官网释义：对于每个分区，最后一次成功提交到Kafka的偏移量。&#013;&#010;&gt; 但我这里为什么是负值呢？&#013;&#010;&gt; &lt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/file/t802/%E6%8D%95%E8%8E%B711.png&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 希望能得到指导，万分感谢~&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<1596017675326-0.post@n8.nabble.com>"
    },
    {
        "id": "<1596109065306-0.post@n8.nabble.com>",
        "from": "bradyMk &lt;zhbm...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 11:37:45 GMT",
        "subject": "Re: flink1.9.1 在WebUI中查看committedOffsets指标为负值",
        "content": "谢谢解答~&#013;&#010;这个确实是个不变的值，应该是没有成功提交；而且我发现了，只要是没有设置ck的任务，该指标都会显示这个值，如果设置了ck，就会正常；但是我不懂为什么会这样，请问您知道详细的原因么？&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "2",
        "reply": "<1596017675326-0.post@n8.nabble.com>"
    },
    {
        "id": "<3d95b715.e132.173a004797a.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 13:58:34 GMT",
        "subject": "回复：flink1.9.1 在WebUI中查看committedOffsets指标为负值",
        "content": "hi&#010;提交offset到Kafka是在ck成功之后 如果没有开启ck的话 需要设置自动提交提交offset&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月30日 19:37，bradyMk 写道：&#010;谢谢解答~&#010;这个确实是个不变的值，应该是没有成功提交；而且我发现了，只要是没有设置ck的任务，该指标都会显示这个值，如果设置了ck，就会正常；但是我不懂为什么会这样，请问您知道详细的原因么？&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "3",
        "reply": "<1596017675326-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAOMLN=Y_AJyNA0-9nHy_vrujsZOwWEuTuzU26V9VVhsgFD7HJg@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 31 Jul 2020 03:27:47 GMT",
        "subject": "Re: flink1.9.1 在WebUI中查看committedOffsets指标为负值",
        "content": "如JasonLee所说，你可以在FlinkKafkaConsumerBase的notifyCheckpointComplete方法中看到提交offset的逻辑。&#013;&#010;值得注意的是，此处的chk完成指的是整个链路上的chk完成，而不是kafka&#010;source的chk完成。&#013;&#010;&#013;&#010;&#013;&#010;JasonLee &lt;17610775726@163.com&gt; 于2020年7月30日周四 下午9:59写道：&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt; 提交offset到Kafka是在ck成功之后 如果没有开启ck的话 需要设置自动提交提交offset&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; JasonLee&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：17610775726@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; Signature is customized by Netease Mail Master&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月30日 19:37，bradyMk 写道：&#013;&#010;&gt; 谢谢解答~&#013;&#010;&gt;&#013;&#010;&gt; 这个确实是个不变的值，应该是没有成功提交；而且我发现了，只要是没有设置ck的任务，该指标都会显示这个值，如果设置了ck，就会正常；但是我不懂为什么会这样，请问您知道详细的原因么？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;",
        "depth": "4",
        "reply": "<1596017675326-0.post@n8.nabble.com>"
    },
    {
        "id": "<BYAPR01MB4294B95F8BE6BBE746C20821D4700@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 29 Jul 2020 10:39:00 GMT",
        "subject": "flink1.11 sql 发布到yarn session时找不到hbase相关的类",
        "content": "Hi,all：&#013;&#010;最近在升级flink1.11，sql中用到hbase connctor，发布到yarn-session时，报如下异常：&#013;&#010;2020-07-29 11:49:55&#013;&#010;org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.NoClassDefFoundError: Could not initialize&#010;class org.apache.hadoop.hbase.util.ByteStringer&#013;&#010;at org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:248)&#013;&#010;at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:221)&#013;&#010;at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:388)&#013;&#010;at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:362)&#013;&#010;at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:142)&#013;&#010;at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)&#013;&#010;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#013;&#010;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#013;&#010;at java.lang.Thread.run(Thread.java:748)&#013;&#010;Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ByteStringer&#013;&#010;at org.apache.hadoop.hbase.protobuf.RequestConverter.buildRegionSpecifier(RequestConverter.java:1053)&#013;&#010;at org.apache.hadoop.hbase.protobuf.RequestConverter.buildScanRequest(RequestConverter.java:496)&#013;&#010;at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:402)&#013;&#010;at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:274)&#013;&#010;at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)&#013;&#010;at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)&#013;&#010;... 7 more&#013;&#010;&#013;&#010;注意到官方文档有Note:&#013;&#010;Note: To use HBase connector in SQL Client or Flink cluster, it’s highly recommended to&#010;add HBase dependency jars to Hadoop classpath. Flink will load all jars under Hadoop classpath&#010;automatically, please refer to HBase, MapReduce, and the CLASSPATH&lt;https://hbase.apache.org/book.html#hbase.mapreduce.classpath&gt;&#010;about how to add HBase dependency jars to Hadoop classpath.&#013;&#010;          但是在yarn session下怎么设置classpath呢&#013;&#010;",
        "depth": "0",
        "reply": "<BYAPR01MB4294B95F8BE6BBE746C20821D4700@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CAHsnkPsK7Xm6D9GodMJ02ZLtwGiW-XAfegBiSGs7Amm1wEDj=A@mail.gmail.com>",
        "from": "Xintong Song &lt;tonysong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:28:34 GMT",
        "subject": "Re: flink1.11 sql 发布到yarn session时找不到hbase相关的类",
        "content": "export HADOOP_CLASSPATH 就可以了&#010;&#010;&#010;Thank you~&#010;&#010;Xintong Song&#010;&#010;&#010;&#010;On Wed, Jul 29, 2020 at 6:43 PM wind.fly.vip@outlook.com &lt;&#010;wind.fly.vip@outlook.com&gt; wrote:&#010;&#010;&gt; Hi,all：&#010;&gt; 最近在升级flink1.11，sql中用到hbase connctor，发布到yarn-session时，报如下异常：&#010;&gt; 2020-07-29 11:49:55&#010;&gt; org.apache.hadoop.hbase.DoNotRetryIOException:&#010;&gt; java.lang.NoClassDefFoundError: Could not initialize class&#010;&gt; org.apache.hadoop.hbase.util.ByteStringer&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:248)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:221)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:388)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:362)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:142)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)&#010;&gt; at&#010;&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#010;&gt; at&#010;&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#010;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: java.lang.NoClassDefFoundError: Could not initialize class&#010;&gt; org.apache.hadoop.hbase.util.ByteStringer&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.protobuf.RequestConverter.buildRegionSpecifier(RequestConverter.java:1053)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.protobuf.RequestConverter.buildScanRequest(RequestConverter.java:496)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:402)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:274)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)&#010;&gt; at&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)&#010;&gt; ... 7 more&#010;&gt;&#010;&gt; 注意到官方文档有Note:&#010;&gt; Note: To use HBase connector in SQL Client or Flink cluster, it’s highly&#010;&gt; recommended to add HBase dependency jars to Hadoop classpath. Flink will&#010;&gt; load all jars under Hadoop classpath automatically, please refer to HBase,&#010;&gt; MapReduce, and the CLASSPATH&lt;&#010;&gt; https://hbase.apache.org/book.html#hbase.mapreduce.classpath&gt; about how&#010;&gt; to add HBase dependency jars to Hadoop classpath.&#010;&gt;           但是在yarn session下怎么设置classpath呢&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB4294B95F8BE6BBE746C20821D4700@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB42949071832F5838F272A331D4710@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 02:32:08 GMT",
        "subject": "回复: flink1.11 sql 发布到yarn session时找不到hbase相关的类",
        "content": "Hi，Xintong：&#013;&#010;是把相关的jar配到hadoop_classpath然后再export吗？&#013;&#010;&#013;&#010;Best,&#013;&#010;Junbao Zhang&#013;&#010;________________________________&#013;&#010;发件人: Xintong Song &lt;tonysong820@gmail.com&gt;&#013;&#010;发送时间: 2020年7月30日 10:28&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink1.11 sql 发布到yarn session时找不到hbase相关的类&#013;&#010;&#013;&#010;export HADOOP_CLASSPATH 就可以了&#013;&#010;&#013;&#010;&#013;&#010;Thank you~&#013;&#010;&#013;&#010;Xintong Song&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;On Wed, Jul 29, 2020 at 6:43 PM wind.fly.vip@outlook.com &lt;&#013;&#010;wind.fly.vip@outlook.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi,all：&#013;&#010;&gt; 最近在升级flink1.11，sql中用到hbase connctor，发布到yarn-session时，报如下异常：&#013;&#010;&gt; 2020-07-29 11:49:55&#013;&#010;&gt; org.apache.hadoop.hbase.DoNotRetryIOException:&#013;&#010;&gt; java.lang.NoClassDefFoundError: Could not initialize class&#013;&#010;&gt; org.apache.hadoop.hbase.util.ByteStringer&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:248)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:221)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:388)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:362)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:142)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)&#013;&#010;&gt; at&#013;&#010;&gt; java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#013;&#010;&gt; at&#013;&#010;&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#013;&#010;&gt; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&gt; Caused by: java.lang.NoClassDefFoundError: Could not initialize class&#013;&#010;&gt; org.apache.hadoop.hbase.util.ByteStringer&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.protobuf.RequestConverter.buildRegionSpecifier(RequestConverter.java:1053)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.protobuf.RequestConverter.buildScanRequest(RequestConverter.java:496)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:402)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:274)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)&#013;&#010;&gt; at&#013;&#010;&gt; org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)&#013;&#010;&gt; ... 7 more&#013;&#010;&gt;&#013;&#010;&gt; 注意到官方文档有Note:&#013;&#010;&gt; Note: To use HBase connector in SQL Client or Flink cluster, it’s highly&#013;&#010;&gt; recommended to add HBase dependency jars to Hadoop classpath. Flink will&#013;&#010;&gt; load all jars under Hadoop classpath automatically, please refer to HBase,&#013;&#010;&gt; MapReduce, and the CLASSPATH&lt;&#013;&#010;&gt; https://hbase.apache.org/book.html#hbase.mapreduce.classpath&gt; about how&#013;&#010;&gt; to add HBase dependency jars to Hadoop classpath.&#013;&#010;&gt;           但是在yarn session下怎么设置classpath呢&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB4294B95F8BE6BBE746C20821D4700@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CABFgzE6WEWYqjv1W_WAMwDJpY=EZAx7bSN1vDaJc0Ayo1iGO+Q@mail.gmail.com>",
        "from": "王松 &lt;sdlcwangson...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 30 Jul 2020 01:51:36 GMT",
        "subject": "Flink Weekly | 每周社区动态更新 - 2020/07/30",
        "content": "大家好，本文为 Flink Weekly 的第二十四期，由王松整理，李本超Review。&#013;&#010;&#013;&#010;本期主要内容包括：近期社区开发进展、邮件问题答疑、Flink 最新社区动态及技术文章推荐等。&#013;&#010;&#013;&#010;&#013;&#010;社区开发进展 Release&#013;&#010;&#013;&#010;[releases] Flink 1.11.1 正式发布！&#013;&#010;&#013;&#010;具体信息参考：&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/ANNOUNCE-Apache-Flink-1-11-1-released-td43335.html&#013;&#010;Vote&#013;&#010;&#013;&#010;[vote] 伍翀发起Refactor Descriptor API to register connectors in Table API的投票&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-FLIP-129-Refactor-Descriptor-API-to-register-connector-in-Table-API-td43420.html&#013;&#010;&#013;&#010;[vote] Shuiqiang Chen发起支持 Python DataStream API (Stateless part) 的投票&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-FLIP-130-Support-for-Python-DataStream-API-Stateless-Part-td43424.html&#013;&#010;Discuss&#013;&#010;&#013;&#010;[connector] 李本超发起了关于对齐 InputFormat#nextRecord 返回为空值语义的讨论。目前还没有明确的相关&#010;java&#013;&#010;doc，flink 中通常有三种处理方式：&#013;&#010;&#013;&#010;   1.&#013;&#010;&#013;&#010;   将 null 作为输入的结尾&#013;&#010;   2.&#013;&#010;&#013;&#010;   跳过 null&#013;&#010;   3.&#013;&#010;&#013;&#010;   假定 InputFormat#nextRecord 中的值不能为 null&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Align-the-semantic-of-returning-null-from-InputFormat-nextRecord-td43379.html&#013;&#010;&#013;&#010;[releases] Robert Metzger发起了关于发布 Flink 1.12 计划的讨论，并决定在9月底之前冻结master的功能。&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Planning-Flink-1-12-td43348.html#a43383&#013;&#010;&#013;&#010;[connector] Israel Ekpo发起了关于在 DataStream、Table 和 SQL Connectors 中支持&#010;Azure&#013;&#010;平台的讨论，并列出了相关的issue，来跟踪这些 connectors&#013;&#010;&#013;&#010;对 Azure 平台做出的贡献，目前在用户邮件列表中已经有大约50个 Azure&#010;相关的主题，这也证明了 Flink 在Azure平台上的使用度&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Adding-Azure-Platform-Support-in-DataStream-Table-and-SQL-Connectors-td43342.html&#013;&#010;&#013;&#010;[connector] Seth Wiesman 发起了关于使用 LIKE 子句创建的 DataGen 表中的时间戳处理问题，目前&#010;DataGen&#013;&#010;表只支持 FLINK SQL 的部分字段类型，&#013;&#010;&#013;&#010;比如 TIMESTAMP(3) 就不支持，文档中建议是使用计算列创建事件时间属性。在&#010;DataGen 表中使用 LIKE 子句时，如果物理表是&#013;&#010;kafka 表就会报错。&#013;&#010;&#013;&#010;Seth Wiesman 给出了两种解决方式：&#013;&#010;&#013;&#010;   1.&#013;&#010;&#013;&#010;   在datagen表中支持TIMESTAMP&#013;&#010;   2.&#013;&#010;&#013;&#010;   放宽 LIKE 子句的约束，允许使用计算列覆盖物理列&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/Discuss-Handling-of-Timestamp-in-DataGen-table-created-via-LIKE-td43433.html&#013;&#010;&#013;&#010;[release] Robert Metzger&#013;&#010;发起了关于过时blockers和不稳定build的讨论，希望将此作为长期的讨论组，定期同步过时的blocker和不稳定的build，并列出了一些test&#013;&#010;&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Release-1-12-Stale-blockers-and-build-instabilities-td43477.html&#013;&#010;用户问题&#013;&#010;&#013;&#010;[sql] 刘首维 提问如果基于Flink1.11新的API去开发的话，如何获取到DataStream？并列举了几个使用场景:&#013;&#010;&#013;&#010;   1.&#013;&#010;&#013;&#010;   我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;   会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，&#013;&#010;   2.&#013;&#010;&#013;&#010;   用于处理这种情况，这样对用户是透明的。&#013;&#010;   3.&#013;&#010;&#013;&#010;   我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;   4.&#013;&#010;&#013;&#010;   调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;   5.&#013;&#010;&#013;&#010;   对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&#013;&#010;云邪进行了回答，&#013;&#010;&#013;&#010;   1.&#013;&#010;&#013;&#010;   场景1建议做在 DeserializationSchema。&#013;&#010;   2.&#013;&#010;&#013;&#010;   场景2建议封装在 SinkFunction。&#013;&#010;   3.&#013;&#010;&#013;&#010;   场景3社区有计划在 FLIP-95 之上支持，会提供并发（或者分区）推断的能力。&#013;&#010;   4.&#013;&#010;&#013;&#010;   场景4可以通过引入类似 SupportsPartitioning 的接口实现。&#013;&#010;&#013;&#010;并建了一个issue来跟进 [https://issues.apache.org/jira/browse/FLINK-18674]&#013;&#010;&#013;&#010;http://apache-flink.147419.n8.nabble.com/1-11Flink-SQL-API-td5261.html#a5275&#013;&#010;&#013;&#010;[sql] Dream-底限 提问如何在eval()方法中传递Row类型？&#013;&#010;&#013;&#010;godfrey he、云邪和李本超进行了回答，可以参考[&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#013;&#010;]进行实现，&#013;&#010;&#013;&#010;但是Dream-底限需要的是 Row[] 作为eval参数，目前并不支持，社区有 issue[&#013;&#010;https://issues.apache.org/jira/browse/FLINK-17855] 正在跟进解决，针对 Dream-底限&#013;&#010;打平array的具体场景，&#013;&#010;&#013;&#010;本超提出参考 [&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins]&#013;&#010;的”Expanding arrays into a relation“部分使用flink内置方法解决&#013;&#010;&#013;&#010;http://apache-flink.147419.n8.nabble.com/flink1-11-tablefunction-td5229.html&#013;&#010;&#013;&#010;[sql] junbaozhang 提出flink 1.11 executeSql查询kafka表print没有输出？&#013;&#010;&#013;&#010;godfrey he进行了回答，1.11的 TableResult.collect() 和 TableResult.print() 方法在流模式下，&#013;&#010;&#013;&#010;都是exactly once语义，需要配置checkpoint才能得到结果。&#013;&#010;&#013;&#010;http://apache-flink.147419.n8.nabble.com/flink-1-11-executeSql-kafka-print-td5367.html#a5370&#013;&#010;活动博客文章及其他&#013;&#010;&#013;&#010;共享很重要 —— Flink SQL 中的 catalogs&#013;&#010;&#013;&#010;https://flink.apache.org/2020/07/23/catalogs.html&#013;&#010;&#013;&#010;Flink 1.11 SQL 使用攻略&#013;&#010;&#013;&#010;https://mp.weixin.qq.com/s/BBRw3sR323d-jaxxONYknQ&#013;&#010;&#013;&#010;高能预警！Apache Flink Meetup · 上海站返场啦&#013;&#010;&#013;&#010;https://mp.weixin.qq.com/s/2k4os3FakPde8IGPtSvglA&#013;&#010;&#013;&#010;你与30W奖金只差一个 Apache Flink 极客挑战赛的报名&#013;&#010;&#013;&#010;https://mp.weixin.qq.com/s/IW6VKWVTrzO1lTDZxJfPXQ&#013;&#010;",
        "depth": "0",
        "reply": "<CABFgzE6WEWYqjv1W_WAMwDJpY=EZAx7bSN1vDaJc0Ayo1iGO+Q@mail.gmail.com>"
    }
]