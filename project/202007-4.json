[
    {
        "id": "<7300709.c102.17356e078cf.Coremail.gp1907971839@163.com>",
        "from": "李国鹏 &lt;gp1907971...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 09:06:58 GMT",
        "subject": "来自李国鹏的邮件",
        "content": "退订",
        "depth": "0",
        "reply": "<7300709.c102.17356e078cf.Coremail.gp1907971839@163.com>"
    },
    {
        "id": "<CAMSfv0sTcU=5WerLx3sMT893G+DY1oH+QzUr8SVLRy4Zx5aBmQ@mail.gmail.com>",
        "from": "李佳宸 &lt;lijiachen...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 10:03:10 GMT",
        "subject": "Flink 1.11 Hive Streaming Write的问题",
        "content": "想请教下大家 hive streaming write需要有哪些配置，不知道为什么我的作业能够跑起来，但是没有数据写入hive。&#010;批量的hive写入，流环境的读取是正常的。&#010;&#010;附代码，很简短：&#010;&#010;public class KafkaToHiveStreaming {&#010;    public static void main(String[] arg) throws Exception{&#010;        StreamExecutionEnvironment bsEnv =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;        EnvironmentSettings bsSettings =&#010;EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;        StreamTableEnvironment bsTableEnv =&#010;StreamTableEnvironment.create(bsEnv, bsSettings);&#010;        String name            = \"myhive\";&#010;        String defaultDatabase = \"default\";&#010;        String hiveConfDir     =&#010;\"/Users/uzi/Downloads/Hadoop/apache-hive-3.1.2-bin/conf/\"; // a local&#010;path&#010;        String version         = \"3.1.2\";&#010;&#010;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;hiveConfDir, version);&#010;        bsTableEnv.registerCatalog(\"myhive\", hive);&#010;        bsTableEnv.useCatalog(\"myhive\");&#010;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;        bsTableEnv.executeSql(\"CREATE TABLE topic_products (\" +&#010;                \"  id BIGINT ,\" +&#010;                \"  order_id STRING,\" +&#010;                \"  amount DECIMAL(10, 2),\" +&#010;                \"  create_time TIMESTAMP \" +&#010;                \") WITH (\" +&#010;                \" 'connector' = 'kafka',\" +&#010;                \" 'topic' = 'order.test',\" +&#010;                \" 'properties.bootstrap.servers' = 'localhost:9092',\" +&#010;                \" 'properties.group.id' = 'testGroup',\" +&#010;                \" 'scan.startup.mode' = 'earliest-offset', \" +&#010;                \" 'format' = 'json'  \" +&#010;                \")\");&#010;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&#010;        bsTableEnv.executeSql(\"CREATE TABLE hive_sink_table_streaming (\" +&#010;                \"  id BIGINT ,\" +&#010;                \"  order_id STRING,\" +&#010;                \"  amount DECIMAL(10, 2)\" +&#010;                \"  )\");&#010;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;        bsTableEnv.executeSql(\"CREATE TABLE print_table WITH&#010;('connector' = 'print')\" +&#010;                \"LIKE INSERT INTO hive_sink_table_streaming (EXCLUDING ALL)\");&#010;&#010;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;        bsTableEnv.executeSql(\"INSERT INTO hive_sink_table_streaming SELECT \" +&#010;                \"id, \" +&#010;                \"order_id, \" +&#010;                \"amount \" +&#010;                \"FROM topic_products\");&#010;&#010;        Table table1 = bsTableEnv.from(\"hive_sink_table_streaming\");&#010;        table1.executeInsert(\"print_table\");&#010;    }&#010;}&#010;&#010;",
        "depth": "0",
        "reply": "<CAMSfv0sTcU=5WerLx3sMT893G+DY1oH+QzUr8SVLRy4Zx5aBmQ@mail.gmail.com>"
    },
    {
        "id": "<3ec68482.47cd.17357935410.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:22:19 GMT",
        "subject": "回复：Flink 1.11 Hive Streaming Write的问题",
        "content": "hi&#010;需要开启checkpoint&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月16日 18:03，李佳宸 写道：&#010;想请教下大家 hive streaming write需要有哪些配置，不知道为什么我的作业能够跑起来，但是没有数据写入hive。&#010;批量的hive写入，流环境的读取是正常的。&#010;&#010;附代码，很简短：&#010;&#010;public class KafkaToHiveStreaming {&#010;   public static void main(String[] arg) throws Exception{&#010;       StreamExecutionEnvironment bsEnv =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;       EnvironmentSettings bsSettings =&#010;EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;       StreamTableEnvironment bsTableEnv =&#010;StreamTableEnvironment.create(bsEnv, bsSettings);&#010;       String name            = \"myhive\";&#010;       String defaultDatabase = \"default\";&#010;       String hiveConfDir     =&#010;\"/Users/uzi/Downloads/Hadoop/apache-hive-3.1.2-bin/conf/\"; // a local&#010;path&#010;       String version         = \"3.1.2\";&#010;&#010;       HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;hiveConfDir, version);&#010;       bsTableEnv.registerCatalog(\"myhive\", hive);&#010;       bsTableEnv.useCatalog(\"myhive\");&#010;       bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;       bsTableEnv.executeSql(\"CREATE TABLE topic_products (\" +&#010;               \"  id BIGINT ,\" +&#010;               \"  order_id STRING,\" +&#010;               \"  amount DECIMAL(10, 2),\" +&#010;               \"  create_time TIMESTAMP \" +&#010;               \") WITH (\" +&#010;               \" 'connector' = 'kafka',\" +&#010;               \" 'topic' = 'order.test',\" +&#010;               \" 'properties.bootstrap.servers' = 'localhost:9092',\" +&#010;               \" 'properties.group.id' = 'testGroup',\" +&#010;               \" 'scan.startup.mode' = 'earliest-offset', \" +&#010;               \" 'format' = 'json'  \" +&#010;               \")\");&#010;       bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&#010;       bsTableEnv.executeSql(\"CREATE TABLE hive_sink_table_streaming (\" +&#010;               \"  id BIGINT ,\" +&#010;               \"  order_id STRING,\" +&#010;               \"  amount DECIMAL(10, 2)\" +&#010;               \"  )\");&#010;       bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;       bsTableEnv.executeSql(\"CREATE TABLE print_table WITH&#010;('connector' = 'print')\" +&#010;               \"LIKE INSERT INTO hive_sink_table_streaming (EXCLUDING ALL)\");&#010;&#010;       bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;       bsTableEnv.executeSql(\"INSERT INTO hive_sink_table_streaming SELECT \" +&#010;               \"id, \" +&#010;               \"order_id, \" +&#010;               \"amount \" +&#010;               \"FROM topic_products\");&#010;&#010;       Table table1 = bsTableEnv.from(\"hive_sink_table_streaming\");&#010;       table1.executeInsert(\"print_table\");&#010;   }&#010;}&#010;",
        "depth": "1",
        "reply": "<CAMSfv0sTcU=5WerLx3sMT893G+DY1oH+QzUr8SVLRy4Zx5aBmQ@mail.gmail.com>"
    },
    {
        "id": "<CAMSfv0uSpqTVhtq66DUw4vPQ8RW3zehLYmMqj4oFC=s0_AOUWA@mail.gmail.com>",
        "from": "李佳宸 &lt;lijiachen...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 14:39:14 GMT",
        "subject": "Re: Flink 1.11 Hive Streaming Write的问题",
        "content": "好的，谢谢～～～&#010;&#010;JasonLee &lt;17610775726@163.com&gt; 于2020年7月16日周四 下午8:22写道：&#010;&#010;&gt; hi&#010;&gt; 需要开启checkpoint&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; JasonLee&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：17610775726@163.com&#010;&gt; |&#010;&gt;&#010;&gt; Signature is customized by Netease Mail Master&#010;&gt;&#010;&gt; 在2020年07月16日 18:03，李佳宸 写道：&#010;&gt; 想请教下大家 hive streaming write需要有哪些配置，不知道为什么我的作业能够跑起来，但是没有数据写入hive。&#010;&gt; 批量的hive写入，流环境的读取是正常的。&#010;&gt;&#010;&gt; 附代码，很简短：&#010;&gt;&#010;&gt; public class KafkaToHiveStreaming {&#010;&gt;    public static void main(String[] arg) throws Exception{&#010;&gt;        StreamExecutionEnvironment bsEnv =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;        EnvironmentSettings bsSettings =&#010;&gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt;        StreamTableEnvironment bsTableEnv =&#010;&gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt;        String name            = \"myhive\";&#010;&gt;        String defaultDatabase = \"default\";&#010;&gt;        String hiveConfDir     =&#010;&gt; \"/Users/uzi/Downloads/Hadoop/apache-hive-3.1.2-bin/conf/\"; // a local&#010;&gt; path&#010;&gt;        String version         = \"3.1.2\";&#010;&gt;&#010;&gt;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; hiveConfDir, version);&#010;&gt;        bsTableEnv.registerCatalog(\"myhive\", hive);&#010;&gt;        bsTableEnv.useCatalog(\"myhive\");&#010;&gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt;        bsTableEnv.executeSql(\"CREATE TABLE topic_products (\" +&#010;&gt;                \"  id BIGINT ,\" +&#010;&gt;                \"  order_id STRING,\" +&#010;&gt;                \"  amount DECIMAL(10, 2),\" +&#010;&gt;                \"  create_time TIMESTAMP \" +&#010;&gt;                \") WITH (\" +&#010;&gt;                \" 'connector' = 'kafka',\" +&#010;&gt;                \" 'topic' = 'order.test',\" +&#010;&gt;                \" 'properties.bootstrap.servers' = 'localhost:9092',\" +&#010;&gt;                \" 'properties.group.id' = 'testGroup',\" +&#010;&gt;                \" 'scan.startup.mode' = 'earliest-offset', \" +&#010;&gt;                \" 'format' = 'json'  \" +&#010;&gt;                \")\");&#010;&gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt;&#010;&gt;        bsTableEnv.executeSql(\"CREATE TABLE hive_sink_table_streaming (\" +&#010;&gt;                \"  id BIGINT ,\" +&#010;&gt;                \"  order_id STRING,\" +&#010;&gt;                \"  amount DECIMAL(10, 2)\" +&#010;&gt;                \"  )\");&#010;&gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt;        bsTableEnv.executeSql(\"CREATE TABLE print_table WITH&#010;&gt; ('connector' = 'print')\" +&#010;&gt;                \"LIKE INSERT INTO hive_sink_table_streaming (EXCLUDING&#010;&gt; ALL)\");&#010;&gt;&#010;&gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt;        bsTableEnv.executeSql(\"INSERT INTO hive_sink_table_streaming SELECT&#010;&gt; \" +&#010;&gt;                \"id, \" +&#010;&gt;                \"order_id, \" +&#010;&gt;                \"amount \" +&#010;&gt;                \"FROM topic_products\");&#010;&gt;&#010;&gt;        Table table1 = bsTableEnv.from(\"hive_sink_table_streaming\");&#010;&gt;        table1.executeInsert(\"print_table\");&#010;&gt;    }&#010;&gt; }&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CAMSfv0sTcU=5WerLx3sMT893G+DY1oH+QzUr8SVLRy4Zx5aBmQ@mail.gmail.com>"
    },
    {
        "id": "<CAEZk043D_-9i-YrULqx0o-1iXWvM+oJtKpGkaL6S3RUksNZGCA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 09:39:49 GMT",
        "subject": "Re: Flink 1.11 Hive Streaming Write的问题",
        "content": "hi、&#010;请问这个问题最后怎么解决了，数据能滚动写入hive了嘛，我这面开启了checkpoint之后hive也是没数据&#010;&#010;李佳宸 &lt;lijiachen218@gmail.com&gt; 于2020年7月16日周四 下午10:39写道：&#010;&#010;&gt; 好的，谢谢～～～&#010;&gt;&#010;&gt; JasonLee &lt;17610775726@163.com&gt; 于2020年7月16日周四 下午8:22写道：&#010;&gt;&#010;&gt; &gt; hi&#010;&gt; &gt; 需要开启checkpoint&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; | |&#010;&gt; &gt; JasonLee&#010;&gt; &gt; |&#010;&gt; &gt; |&#010;&gt; &gt; 邮箱：17610775726@163.com&#010;&gt; &gt; |&#010;&gt; &gt;&#010;&gt; &gt; Signature is customized by Netease Mail Master&#010;&gt; &gt;&#010;&gt; &gt; 在2020年07月16日 18:03，李佳宸 写道：&#010;&gt; &gt; 想请教下大家 hive streaming write需要有哪些配置，不知道为什么我的作业能够跑起来，但是没有数据写入hive。&#010;&gt; &gt; 批量的hive写入，流环境的读取是正常的。&#010;&gt; &gt;&#010;&gt; &gt; 附代码，很简短：&#010;&gt; &gt;&#010;&gt; &gt; public class KafkaToHiveStreaming {&#010;&gt; &gt;    public static void main(String[] arg) throws Exception{&#010;&gt; &gt;        StreamExecutionEnvironment bsEnv =&#010;&gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt;        EnvironmentSettings bsSettings =&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; &gt;        StreamTableEnvironment bsTableEnv =&#010;&gt; &gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt; &gt;        String name            = \"myhive\";&#010;&gt; &gt;        String defaultDatabase = \"default\";&#010;&gt; &gt;        String hiveConfDir     =&#010;&gt; &gt; \"/Users/uzi/Downloads/Hadoop/apache-hive-3.1.2-bin/conf/\"; // a local&#010;&gt; &gt; path&#010;&gt; &gt;        String version         = \"3.1.2\";&#010;&gt; &gt;&#010;&gt; &gt;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; hiveConfDir, version);&#010;&gt; &gt;        bsTableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt;        bsTableEnv.useCatalog(\"myhive\");&#010;&gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt;        bsTableEnv.executeSql(\"CREATE TABLE topic_products (\" +&#010;&gt; &gt;                \"  id BIGINT ,\" +&#010;&gt; &gt;                \"  order_id STRING,\" +&#010;&gt; &gt;                \"  amount DECIMAL(10, 2),\" +&#010;&gt; &gt;                \"  create_time TIMESTAMP \" +&#010;&gt; &gt;                \") WITH (\" +&#010;&gt; &gt;                \" 'connector' = 'kafka',\" +&#010;&gt; &gt;                \" 'topic' = 'order.test',\" +&#010;&gt; &gt;                \" 'properties.bootstrap.servers' = 'localhost:9092',\" +&#010;&gt; &gt;                \" 'properties.group.id' = 'testGroup',\" +&#010;&gt; &gt;                \" 'scan.startup.mode' = 'earliest-offset', \" +&#010;&gt; &gt;                \" 'format' = 'json'  \" +&#010;&gt; &gt;                \")\");&#010;&gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt;&#010;&gt; &gt;        bsTableEnv.executeSql(\"CREATE TABLE hive_sink_table_streaming (\" +&#010;&gt; &gt;                \"  id BIGINT ,\" +&#010;&gt; &gt;                \"  order_id STRING,\" +&#010;&gt; &gt;                \"  amount DECIMAL(10, 2)\" +&#010;&gt; &gt;                \"  )\");&#010;&gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt;        bsTableEnv.executeSql(\"CREATE TABLE print_table WITH&#010;&gt; &gt; ('connector' = 'print')\" +&#010;&gt; &gt;                \"LIKE INSERT INTO hive_sink_table_streaming (EXCLUDING&#010;&gt; &gt; ALL)\");&#010;&gt; &gt;&#010;&gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt;        bsTableEnv.executeSql(\"INSERT INTO hive_sink_table_streaming&#010;&gt; SELECT&#010;&gt; &gt; \" +&#010;&gt; &gt;                \"id, \" +&#010;&gt; &gt;                \"order_id, \" +&#010;&gt; &gt;                \"amount \" +&#010;&gt; &gt;                \"FROM topic_products\");&#010;&gt; &gt;&#010;&gt; &gt;        Table table1 = bsTableEnv.from(\"hive_sink_table_streaming\");&#010;&gt; &gt;        table1.executeInsert(\"print_table\");&#010;&gt; &gt;    }&#010;&gt; &gt; }&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<CAMSfv0sTcU=5WerLx3sMT893G+DY1oH+QzUr8SVLRy4Zx5aBmQ@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jRf-1aWRwj4insseyy+1LL0=GFH7QwFLRGHH7wSsqWehw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 10:09:25 GMT",
        "subject": "Re: Flink 1.11 Hive Streaming Write的问题",
        "content": "Hi Dream,&#010;&#010;可以详述下你的测试场景吗？&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 20, 2020 at 5:40 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&#010;&gt; hi、&#010;&gt; 请问这个问题最后怎么解决了，数据能滚动写入hive了嘛，我这面开启了checkpoint之后hive也是没数据&#010;&gt;&#010;&gt; 李佳宸 &lt;lijiachen218@gmail.com&gt; 于2020年7月16日周四 下午10:39写道：&#010;&gt;&#010;&gt; &gt; 好的，谢谢～～～&#010;&gt; &gt;&#010;&gt; &gt; JasonLee &lt;17610775726@163.com&gt; 于2020年7月16日周四 下午8:22写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi&#010;&gt; &gt; &gt; 需要开启checkpoint&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; | |&#010;&gt; &gt; &gt; JasonLee&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt; 邮箱：17610775726@163.com&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Signature is customized by Netease Mail Master&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 在2020年07月16日 18:03，李佳宸 写道：&#010;&gt; &gt; &gt; 想请教下大家 hive streaming write需要有哪些配置，不知道为什么我的作业能够跑起来，但是没有数据写入hive。&#010;&gt; &gt; &gt; 批量的hive写入，流环境的读取是正常的。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 附代码，很简短：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; public class KafkaToHiveStreaming {&#010;&gt; &gt; &gt;    public static void main(String[] arg) throws Exception{&#010;&gt; &gt; &gt;        StreamExecutionEnvironment bsEnv =&#010;&gt; &gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt; &gt;        EnvironmentSettings bsSettings =&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; &gt; &gt;        StreamTableEnvironment bsTableEnv =&#010;&gt; &gt; &gt; StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt; &gt; &gt;        String name            = \"myhive\";&#010;&gt; &gt; &gt;        String defaultDatabase = \"default\";&#010;&gt; &gt; &gt;        String hiveConfDir     =&#010;&gt; &gt; &gt; \"/Users/uzi/Downloads/Hadoop/apache-hive-3.1.2-bin/conf/\"; // a local&#010;&gt; &gt; &gt; path&#010;&gt; &gt; &gt;        String version         = \"3.1.2\";&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; &gt; hiveConfDir, version);&#010;&gt; &gt; &gt;        bsTableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt; &gt;        bsTableEnv.useCatalog(\"myhive\");&#010;&gt; &gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt; &gt;        bsTableEnv.executeSql(\"CREATE TABLE topic_products (\" +&#010;&gt; &gt; &gt;                \"  id BIGINT ,\" +&#010;&gt; &gt; &gt;                \"  order_id STRING,\" +&#010;&gt; &gt; &gt;                \"  amount DECIMAL(10, 2),\" +&#010;&gt; &gt; &gt;                \"  create_time TIMESTAMP \" +&#010;&gt; &gt; &gt;                \") WITH (\" +&#010;&gt; &gt; &gt;                \" 'connector' = 'kafka',\" +&#010;&gt; &gt; &gt;                \" 'topic' = 'order.test',\" +&#010;&gt; &gt; &gt;                \" 'properties.bootstrap.servers' = 'localhost:9092',\" +&#010;&gt; &gt; &gt;                \" 'properties.group.id' = 'testGroup',\" +&#010;&gt; &gt; &gt;                \" 'scan.startup.mode' = 'earliest-offset', \" +&#010;&gt; &gt; &gt;                \" 'format' = 'json'  \" +&#010;&gt; &gt; &gt;                \")\");&#010;&gt; &gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;        bsTableEnv.executeSql(\"CREATE TABLE hive_sink_table_streaming&#010;&gt; (\" +&#010;&gt; &gt; &gt;                \"  id BIGINT ,\" +&#010;&gt; &gt; &gt;                \"  order_id STRING,\" +&#010;&gt; &gt; &gt;                \"  amount DECIMAL(10, 2)\" +&#010;&gt; &gt; &gt;                \"  )\");&#010;&gt; &gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt; &gt;        bsTableEnv.executeSql(\"CREATE TABLE print_table WITH&#010;&gt; &gt; &gt; ('connector' = 'print')\" +&#010;&gt; &gt; &gt;                \"LIKE INSERT INTO hive_sink_table_streaming (EXCLUDING&#010;&gt; &gt; &gt; ALL)\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;        bsTableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt; &gt;        bsTableEnv.executeSql(\"INSERT INTO hive_sink_table_streaming&#010;&gt; &gt; SELECT&#010;&gt; &gt; &gt; \" +&#010;&gt; &gt; &gt;                \"id, \" +&#010;&gt; &gt; &gt;                \"order_id, \" +&#010;&gt; &gt; &gt;                \"amount \" +&#010;&gt; &gt; &gt;                \"FROM topic_products\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;        Table table1 = bsTableEnv.from(\"hive_sink_table_streaming\");&#010;&gt; &gt; &gt;        table1.executeInsert(\"print_table\");&#010;&gt; &gt; &gt;    }&#010;&gt; &gt; &gt; }&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "4",
        "reply": "<CAMSfv0sTcU=5WerLx3sMT893G+DY1oH+QzUr8SVLRy4Zx5aBmQ@mail.gmail.com>"
    },
    {
        "id": "<CACaQKu47pY=MJ0rKzW_97Z9GBCJpZ24LeL=CyKVkf15+00Hcuw@mail.gmail.com>",
        "from": "LakeShen &lt;shenleifight...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 10:03:11 GMT",
        "subject": "Flink on k8s 中，Jar 任务 avatica-core 依赖和 flink-table jar 冲突问题",
        "content": "Hi 社区，&#013;&#010;&#013;&#010;我现在正在迁移任务到 k8s ,目前版本为 Flink 1.6 版本，k8s 上面作业运行模式为&#010;standalone per job.&#013;&#010;&#013;&#010;现在遇到一个问题，业务方 Flink jar 任务使用了 org.apache.calcite.avatica&#010;依赖，也就是下面依赖：&#013;&#010;&lt;dependency&gt;&#013;&#010;            &lt;groupId&gt;org.apache.calcite.avatica&lt;/groupId&gt;&#013;&#010;            &lt;artifactId&gt;avatica-core&lt;/artifactId&gt;&#013;&#010;            &lt;version&gt;${avatica.version}&lt;/version&gt;&#013;&#010;        &lt;/dependency&gt;&#013;&#010;&#013;&#010;但是这个依赖其实在 flink-table 模块中，也有这个依赖：&#013;&#010;[image: image.png]&#013;&#010;&#013;&#010;由于 flink on k8s  standalone per job 模式，会把 Flink 任务 jar 包放入到 flink&#010;本身的lib&#013;&#010;包中，我在任务启动的时候，就会报：&#013;&#010;Caused by: java.lang.NoClassDefFoundError: Could not initialize class&#013;&#010;org.apache.calcite.avatica.ConnectionPropertiesImpl 错误。&#013;&#010;&#013;&#010;按照我的理解，由于 Flink jar 任务包中有 avatica-core 依赖，同时在 flink&#010;lib&#013;&#010;目录下面，flink-table_2.11-1.6-RELEASE.jar 中也有这个依赖，这两个都在 lib&#010;目录下，然后就出现了类冲突问题。&#013;&#010;&#013;&#010;请问怎么解决这个问题呢，非常期待你的回复。&#013;&#010;&#013;&#010;Best,&#013;&#010;LakeShen&#013;&#010;",
        "depth": "0",
        "reply": "<CACaQKu47pY=MJ0rKzW_97Z9GBCJpZ24LeL=CyKVkf15+00Hcuw@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvunychdKkCJ+VAG1LfNeTMqoJgingf+36hd_ZnGOc4_Qw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:19:08 GMT",
        "subject": "Re: Flink on k8s 中，Jar 任务 avatica-core 依赖和 flink-table jar 冲突问题",
        "content": "Hi&#013;&#010;&#013;&#010;你的图挂了，如果单纯想解决 jar 包冲突的问题，那么 maven shade plugin[1]&#010;或许对你有用&#013;&#010;&#013;&#010;[1]&#013;&#010;https://maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation.html&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月16日周四 下午6:03写道：&#013;&#010;&#013;&#010;&gt; Hi 社区，&#013;&#010;&gt;&#013;&#010;&gt; 我现在正在迁移任务到 k8s ,目前版本为 Flink 1.6 版本，k8s 上面作业运行模式为&#010;standalone per job.&#013;&#010;&gt;&#013;&#010;&gt; 现在遇到一个问题，业务方 Flink jar 任务使用了 org.apache.calcite.avatica&#010;依赖，也就是下面依赖：&#013;&#010;&gt; &lt;dependency&gt;&#013;&#010;&gt;             &lt;groupId&gt;org.apache.calcite.avatica&lt;/groupId&gt;&#013;&#010;&gt;             &lt;artifactId&gt;avatica-core&lt;/artifactId&gt;&#013;&#010;&gt;             &lt;version&gt;${avatica.version}&lt;/version&gt;&#013;&#010;&gt;         &lt;/dependency&gt;&#013;&#010;&gt;&#013;&#010;&gt; 但是这个依赖其实在 flink-table 模块中，也有这个依赖：&#013;&#010;&gt; [image: image.png]&#013;&#010;&gt;&#013;&#010;&gt; 由于 flink on k8s  standalone per job 模式，会把 Flink 任务 jar 包放入到&#010;flink 本身的lib&#013;&#010;&gt; 包中，我在任务启动的时候，就会报：&#013;&#010;&gt; Caused by: java.lang.NoClassDefFoundError: Could not initialize class&#013;&#010;&gt; org.apache.calcite.avatica.ConnectionPropertiesImpl 错误。&#013;&#010;&gt;&#013;&#010;&gt; 按照我的理解，由于 Flink jar 任务包中有 avatica-core 依赖，同时在&#010;flink lib&#013;&#010;&gt; 目录下面，flink-table_2.11-1.6-RELEASE.jar 中也有这个依赖，这两个都在&#010;lib 目录下，然后就出现了类冲突问题。&#013;&#010;&gt;&#013;&#010;&gt; 请问怎么解决这个问题呢，非常期待你的回复。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; LakeShen&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CACaQKu47pY=MJ0rKzW_97Z9GBCJpZ24LeL=CyKVkf15+00Hcuw@mail.gmail.com>"
    },
    {
        "id": "<CACaQKu7ygroKjeT6BbygGV=0fh=00-5_FJRZcJ5CKTzWtjMoBA@mail.gmail.com>",
        "from": "LakeShen &lt;shenleifight...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:02:17 GMT",
        "subject": "Re: Flink on k8s 中，Jar 任务 avatica-core 依赖和 flink-table jar 冲突问题",
        "content": "嗯嗯，Congxian,感谢你的回复，我通过 Maven Shaded 解决问题😁。&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月16日周四 下午8:19写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 你的图挂了，如果单纯想解决 jar 包冲突的问题，那么 maven shade plugin[1]&#010;或许对你有用&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation.html&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月16日周四 下午6:03写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi 社区，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我现在正在迁移任务到 k8s ,目前版本为 Flink 1.6 版本，k8s 上面作业运行模式为&#010;standalone per job.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 现在遇到一个问题，业务方 Flink jar 任务使用了 org.apache.calcite.avatica&#010;依赖，也就是下面依赖：&#013;&#010;&gt; &gt; &lt;dependency&gt;&#013;&#010;&gt; &gt;             &lt;groupId&gt;org.apache.calcite.avatica&lt;/groupId&gt;&#013;&#010;&gt; &gt;             &lt;artifactId&gt;avatica-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt;             &lt;version&gt;${avatica.version}&lt;/version&gt;&#013;&#010;&gt; &gt;         &lt;/dependency&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 但是这个依赖其实在 flink-table 模块中，也有这个依赖：&#013;&#010;&gt; &gt; [image: image.png]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 由于 flink on k8s  standalone per job 模式，会把 Flink 任务 jar 包放入到&#010;flink 本身的lib&#013;&#010;&gt; &gt; 包中，我在任务启动的时候，就会报：&#013;&#010;&gt; &gt; Caused by: java.lang.NoClassDefFoundError: Could not initialize class&#013;&#010;&gt; &gt; org.apache.calcite.avatica.ConnectionPropertiesImpl 错误。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 按照我的理解，由于 Flink jar 任务包中有 avatica-core 依赖，同时在&#010;flink lib&#013;&#010;&gt; &gt; 目录下面，flink-table_2.11-1.6-RELEASE.jar 中也有这个依赖，这两个都在&#010;lib 目录下，然后就出现了类冲突问题。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 请问怎么解决这个问题呢，非常期待你的回复。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; LakeShen&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CACaQKu47pY=MJ0rKzW_97Z9GBCJpZ24LeL=CyKVkf15+00Hcuw@mail.gmail.com>"
    },
    {
        "id": "<CAP+gf36icRu06Wqi11LcTzmUwxyNfDsboZd+hjKUupy8P3jf4Q@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:18:07 GMT",
        "subject": "Re: Flink on k8s 中，Jar 任务 avatica-core 依赖和 flink-table jar 冲突问题",
        "content": "Flink从1.10开始是支持用user classloader来加载用户jar的，包括Standalone perjob&#013;&#010;你需要将jar包放到$FLINK_HOME/usrlib目录下，如果放到lib下就会用框架的classloader&#013;&#010;来加载&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月17日周五 上午10:02写道：&#013;&#010;&#013;&#010;&gt; 嗯嗯，Congxian,感谢你的回复，我通过 Maven Shaded 解决问题😁。&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月16日周四 下午8:19写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 你的图挂了，如果单纯想解决 jar 包冲突的问题，那么 maven shade&#010;plugin[1] 或许对你有用&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation.html&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; LakeShen &lt;shenleifighting@gmail.com&gt; 于2020年7月16日周四 下午6:03写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi 社区，&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 我现在正在迁移任务到 k8s ,目前版本为 Flink 1.6 版本，k8s&#010;上面作业运行模式为 standalone per job.&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 现在遇到一个问题，业务方 Flink jar 任务使用了 org.apache.calcite.avatica&#010;依赖，也就是下面依赖：&#013;&#010;&gt; &gt; &gt; &lt;dependency&gt;&#013;&#010;&gt; &gt; &gt;             &lt;groupId&gt;org.apache.calcite.avatica&lt;/groupId&gt;&#013;&#010;&gt; &gt; &gt;             &lt;artifactId&gt;avatica-core&lt;/artifactId&gt;&#013;&#010;&gt; &gt; &gt;             &lt;version&gt;${avatica.version}&lt;/version&gt;&#013;&#010;&gt; &gt; &gt;         &lt;/dependency&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 但是这个依赖其实在 flink-table 模块中，也有这个依赖：&#013;&#010;&gt; &gt; &gt; [image: image.png]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 由于 flink on k8s  standalone per job 模式，会把 Flink 任务 jar 包放入到&#010;flink&#013;&#010;&gt; 本身的lib&#013;&#010;&gt; &gt; &gt; 包中，我在任务启动的时候，就会报：&#013;&#010;&gt; &gt; &gt; Caused by: java.lang.NoClassDefFoundError: Could not initialize class&#013;&#010;&gt; &gt; &gt; org.apache.calcite.avatica.ConnectionPropertiesImpl 错误。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 按照我的理解，由于 Flink jar 任务包中有 avatica-core 依赖，同时在&#010;flink lib&#013;&#010;&gt; &gt; &gt; 目录下面，flink-table_2.11-1.6-RELEASE.jar 中也有这个依赖，这两个都在&#010;lib&#013;&#010;&gt; 目录下，然后就出现了类冲突问题。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 请问怎么解决这个问题呢，非常期待你的回复。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; LakeShen&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CACaQKu47pY=MJ0rKzW_97Z9GBCJpZ24LeL=CyKVkf15+00Hcuw@mail.gmail.com>"
    },
    {
        "id": "<tencent_A102A9D2E3D31E32955A7C7454B611B33409@qq.com>",
        "from": "&quot;sun&quot; &lt;1392427...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 10:16:45 GMT",
        "subject": "state无法从checkpoint中恢复",
        "content": "配置代码env.enableCheckpointing(1000);env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;//作业失败后不重启&#013;&#010;env.setRestartStrategy(RestartStrategies.noRestart());&#013;&#010;env.getCheckpointConfig().setCheckpointTimeout(500);&#013;&#010;env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#013;&#010;env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;env.setStateBackend(new RocksDBStateBackend(\"file:///opt/flink/flink-1.7.2/checkpoints\"));&#010;           使用状态的代码private transient ListState&lt;String&amp;gt; counts;&#013;&#010;&#013;&#010;&#013;&#010;@Override&#013;&#010;public void open(Configuration parameters) throws Exception {&#013;&#010;    StateTtlConfig ttlConfig = StateTtlConfig&#013;&#010;            .newBuilder(Time.minutes(30))&#013;&#010;            .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;            .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;            .build();&#013;&#010;&#013;&#010;    ListStateDescriptor&lt;String&amp;gt; lastUserLogin = new ListStateDescriptor&lt;&amp;gt;(\"lastUserLogin\",&#010;String.class);&#013;&#010;    lastUserLogin.enableTimeToLive(ttlConfig);&#013;&#010;    counts = getRuntimeContext().getListState(lastUserLogin);&#013;&#010;}&#013;&#010;我重启了task managers 后。发现  counts  里面的数据都丢失了",
        "depth": "0",
        "reply": "<tencent_A102A9D2E3D31E32955A7C7454B611B33409@qq.com>"
    },
    {
        "id": "<CAA8tFvvB5pku1i9SnP7Lk1GwJ13aLtX=BoRSJZMv4biUaKEkog@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:16:48 GMT",
        "subject": "Re: state无法从checkpoint中恢复",
        "content": "Hi&#013;&#010;&#013;&#010;1 counts 的数据丢失了能否详细描述一下呢？你预期是什么，看到什么现象&#013;&#010;2 能否把你关于 counts 的其他代码也贴一下&#013;&#010;3. 你的作业是否从 checkpoint 恢复了呢？这个可以从 JM log 来查看&#013;&#010;4. 如果你确定是数据有丢失的话，或许你可以使用 state-process-api[1] 看一下是序列化出去有问题，还是&#010;restore 回来有问题&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/state_processor_api.html&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;sun &lt;1392427699@qq.com&gt; 于2020年7月16日周四 下午6:16写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 配置代码env.enableCheckpointing(1000);env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;&gt; //作业失败后不重启&#013;&#010;&gt; env.setRestartStrategy(RestartStrategies.noRestart());&#013;&#010;&gt; env.getCheckpointConfig().setCheckpointTimeout(500);&#013;&#010;&gt;&#013;&#010;&gt; env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#013;&#010;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; env.setStateBackend(new&#013;&#010;&gt; RocksDBStateBackend(\"file:///opt/flink/flink-1.7.2/checkpoints\"));&#013;&#010;&gt;   使用状态的代码private transient ListState&lt;String&amp;gt; counts;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; @Override&#013;&#010;&gt; public void open(Configuration parameters) throws Exception {&#013;&#010;&gt;     StateTtlConfig ttlConfig = StateTtlConfig&#013;&#010;&gt;             .newBuilder(Time.minutes(30))&#013;&#010;&gt;             .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;&gt;&#013;&#010;&gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;&gt;             .build();&#013;&#010;&gt;&#013;&#010;&gt;     ListStateDescriptor&lt;String&amp;gt; lastUserLogin = new&#013;&#010;&gt; ListStateDescriptor&lt;&amp;gt;(\"lastUserLogin\", String.class);&#013;&#010;&gt;     lastUserLogin.enableTimeToLive(ttlConfig);&#013;&#010;&gt;     counts = getRuntimeContext().getListState(lastUserLogin);&#013;&#010;&gt; }&#013;&#010;&gt; 我重启了task managers 后。发现  counts  里面的数据都丢失了&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_A102A9D2E3D31E32955A7C7454B611B33409@qq.com>"
    },
    {
        "id": "<tencent_6E992C9F40A574D4B4984CC3966A1D94ED07@qq.com>",
        "from": "&quot;sun&quot; &lt;1392427...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:21:41 GMT",
        "subject": "回复： state无法从checkpoint中恢复",
        "content": "你好：counts 的数据 我是在下面打印出来了 List&lt;String&amp;gt; list = Lists.newArrayList(counts.get())&#010;;&#013;&#010;            for(String ss : list){&#013;&#010;                System.out.println(\"!!!\" + ss);&#013;&#010;                log.info(\"!!!\" + ss);&#013;&#010;            }，但是我重启服务之后，之前存的那些内容打印不出来了。&#013;&#010;@Slf4j&#013;&#010;public class FlatMapTestState extends RichFlatMapFunction&lt;String, Test222&amp;gt; {&#013;&#010;&#013;&#010;&#013;&#010;    private transient ListState&lt;String&amp;gt; counts;&#013;&#010;&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void open(Configuration parameters) throws Exception {&#013;&#010;        StateTtlConfig ttlConfig = StateTtlConfig&#013;&#010;                .newBuilder(Time.minutes(30))&#013;&#010;                .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;                .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;                .build();&#013;&#010;&#013;&#010;        ListStateDescriptor&lt;String&amp;gt; lastUserLogin = new ListStateDescriptor&lt;&amp;gt;(\"lastUserLogin\",&#010;String.class);&#013;&#010;        lastUserLogin.enableTimeToLive(ttlConfig);&#013;&#010;        counts = getRuntimeContext().getListState(lastUserLogin);&#013;&#010;    }&#013;&#010;&#013;&#010;&#013;&#010;    @Override&#013;&#010;    public void flatMap(String s, Collector&lt;Test222&amp;gt; collector) throws Exception&#010;{&#013;&#010;            Test222 message = JSONUtil.toObject(s, new TypeReference&lt;Test222&amp;gt;()&#010;{&#013;&#010;            });&#013;&#010;&#013;&#010;            System.out.println(DateUtil.toLongDateString(new Date()));&#013;&#010;            log.info(DateUtil.toLongDateString(new Date()));&#013;&#010;            counts.add(message.getId());&#013;&#010;            List&lt;String&amp;gt; list = Lists.newArrayList(counts.get()) ;&#013;&#010;            for(String ss : list){&#013;&#010;                System.out.println(\"!!!\" + ss);&#013;&#010;                log.info(\"!!!\" + ss);&#013;&#010;            }&#013;&#010;              log.info(DateUtil.toLongDateString(new Date()));&#013;&#010;            System.out.println(DateUtil.toLongDateString(new Date()));&#013;&#010;    }&#013;&#010;}&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月16日(星期四) 晚上8:16&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: state无法从checkpoint中恢复&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;1 counts 的数据丢失了能否详细描述一下呢？你预期是什么，看到什么现象&#013;&#010;2 能否把你关于 counts 的其他代码也贴一下&#013;&#010;3. 你的作业是否从 checkpoint 恢复了呢？这个可以从 JM log 来查看&#013;&#010;4. 如果你确定是数据有丢失的话，或许你可以使用 state-process-api[1] 看一下是序列化出去有问题，还是&#010;restore 回来有问题&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/state_processor_api.html&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;sun &lt;1392427699@qq.com&amp;gt; 于2020年7月16日周四 下午6:16写道：&#013;&#010;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 配置代码env.enableCheckpointing(1000);env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;&amp;gt; //作业失败后不重启&#013;&#010;&amp;gt; env.setRestartStrategy(RestartStrategies.noRestart());&#013;&#010;&amp;gt; env.getCheckpointConfig().setCheckpointTimeout(500);&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&amp;gt; env.setStateBackend(new&#013;&#010;&amp;gt; RocksDBStateBackend(\"file:///opt/flink/flink-1.7.2/checkpoints\"));&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp; 使用状态的代码private transient ListState&lt;String&amp;amp;gt;&#010;counts;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; @Override&#013;&#010;&amp;gt; public void open(Configuration parameters) throws Exception {&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; StateTtlConfig ttlConfig = StateTtlConfig&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.newBuilder(Time.minutes(30))&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.build();&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ListStateDescriptor&lt;String&amp;amp;gt;&#010;lastUserLogin = new&#013;&#010;&amp;gt; ListStateDescriptor&lt;&amp;amp;gt;(\"lastUserLogin\", String.class);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; lastUserLogin.enableTimeToLive(ttlConfig);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; counts = getRuntimeContext().getListState(lastUserLogin);&#013;&#010;&amp;gt; }&#013;&#010;&amp;gt; 我重启了task managers 后。发现&amp;nbsp; counts&amp;nbsp; 里面的数据都丢失了",
        "depth": "2",
        "reply": "<tencent_A102A9D2E3D31E32955A7C7454B611B33409@qq.com>"
    },
    {
        "id": "<CAA8tFvtfLPy1wHu8pzta_fo7eUM2F=PHBGt0hqubNq4qjbi0pg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 14:58:17 GMT",
        "subject": "Re: state无法从checkpoint中恢复",
        "content": "Hi&#010;&#010;1 你需要回复一下我之前问你的问题：你可以从 JM log 看一下是否从 checkpoint&#010;恢复了&#010;2. 这里没有打印只是表明当前处理的 key 没有 state 数据，并不能表示&#010;state 没有恢复回来，state 值是绑定到某个 key&#010;上的（keyby 的 key）&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;sun &lt;1392427699@qq.com&gt; 于2020年7月17日周五 下午5:22写道：&#010;&#010;&gt; 你好：counts 的数据 我是在下面打印出来了 List&lt;String&amp;gt; list&#010;=&#010;&gt; Lists.newArrayList(counts.get()) ;&#010;&gt;             for(String ss : list){&#010;&gt;                 System.out.println(\"!!!\" + ss);&#010;&gt;                 log.info(\"!!!\" + ss);&#010;&gt;             }，但是我重启服务之后，之前存的那些内容打印不出来了。&#010;&gt; @Slf4j&#010;&gt; public class FlatMapTestState extends RichFlatMapFunction&lt;String,&#010;&gt; Test222&amp;gt; {&#010;&gt;&#010;&gt;&#010;&gt;     private transient ListState&lt;String&amp;gt; counts;&#010;&gt;&#010;&gt;&#010;&gt;     @Override&#010;&gt;     public void open(Configuration parameters) throws Exception {&#010;&gt;         StateTtlConfig ttlConfig = StateTtlConfig&#010;&gt;                 .newBuilder(Time.minutes(30))&#010;&gt;                 .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#010;&gt;&#010;&gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#010;&gt;                 .build();&#010;&gt;&#010;&gt;         ListStateDescriptor&lt;String&amp;gt; lastUserLogin = new&#010;&gt; ListStateDescriptor&lt;&amp;gt;(\"lastUserLogin\", String.class);&#010;&gt;         lastUserLogin.enableTimeToLive(ttlConfig);&#010;&gt;         counts = getRuntimeContext().getListState(lastUserLogin);&#010;&gt;     }&#010;&gt;&#010;&gt;&#010;&gt;     @Override&#010;&gt;     public void flatMap(String s, Collector&lt;Test222&amp;gt; collector) throws&#010;&gt; Exception {&#010;&gt;             Test222 message = JSONUtil.toObject(s, new&#010;&gt; TypeReference&lt;Test222&amp;gt;() {&#010;&gt;             });&#010;&gt;&#010;&gt;             System.out.println(DateUtil.toLongDateString(new Date()));&#010;&gt;             log.info(DateUtil.toLongDateString(new Date()));&#010;&gt;             counts.add(message.getId());&#010;&gt;             List&lt;String&amp;gt; list = Lists.newArrayList(counts.get()) ;&#010;&gt;             for(String ss : list){&#010;&gt;                 System.out.println(\"!!!\" + ss);&#010;&gt;                 log.info(\"!!!\" + ss);&#010;&gt;             }&#010;&gt;               log.info(DateUtil.toLongDateString(new Date()));&#010;&gt;             System.out.println(DateUtil.toLongDateString(new Date()));&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; qcx978132955@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月16日(星期四) 晚上8:16&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: state无法从checkpoint中恢复&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Hi&#010;&gt;&#010;&gt; 1 counts 的数据丢失了能否详细描述一下呢？你预期是什么，看到什么现象&#010;&gt; 2 能否把你关于 counts 的其他代码也贴一下&#010;&gt; 3. 你的作业是否从 checkpoint 恢复了呢？这个可以从 JM log 来查看&#010;&gt; 4. 如果你确定是数据有丢失的话，或许你可以使用 state-process-api[1]&#010;看一下是序列化出去有问题，还是 restore 回来有问题&#010;&gt;&#010;&gt; [1]&#010;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/state_processor_api.html&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; sun &lt;1392427699@qq.com&amp;gt; 于2020年7月16日周四 下午6:16写道：&#010;&gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; 配置代码env.enableCheckpointing(1000);env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&gt; &amp;gt; //作业失败后不重启&#010;&gt; &amp;gt; env.setRestartStrategy(RestartStrategies.noRestart());&#010;&gt; &amp;gt; env.getCheckpointConfig().setCheckpointTimeout(500);&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;&gt; &amp;gt; env.setStateBackend(new&#010;&gt; &amp;gt; RocksDBStateBackend(\"file:///opt/flink/flink-1.7.2/checkpoints\"));&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp; 使用状态的代码private transient ListState&lt;String&amp;amp;gt;&#010;counts;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; @Override&#010;&gt; &amp;gt; public void open(Configuration parameters) throws Exception {&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; StateTtlConfig ttlConfig = StateTtlConfig&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; .newBuilder(Time.minutes(30))&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; .build();&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ListStateDescriptor&lt;String&amp;amp;gt;&#010;&gt; lastUserLogin = new&#010;&gt; &amp;gt; ListStateDescriptor&lt;&amp;amp;gt;(\"lastUserLogin\", String.class);&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; lastUserLogin.enableTimeToLive(ttlConfig);&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; counts =&#010;&gt; getRuntimeContext().getListState(lastUserLogin);&#010;&gt; &amp;gt; }&#010;&gt; &amp;gt; 我重启了task managers 后。发现&amp;nbsp; counts&amp;nbsp; 里面的数据都丢失了&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_A102A9D2E3D31E32955A7C7454B611B33409@qq.com>"
    },
    {
        "id": "<tencent_A3A5CFA5D56636B4C4E8E86CD366D905860A@qq.com>",
        "from": "&quot;sun&quot; &lt;1392427...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 10:13:32 GMT",
        "subject": "回复： state无法从checkpoint中恢复",
        "content": "JM日志有点不熟悉，不知道是否从 checkpoint 恢复了&#013;&#010;&#013;&#010;&#013;&#010;18:08:07.615 [Checkpoint Timer] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Triggering checkpoint 116 @ 1595239687615 for job acd456ff6f2f9f59ee89b126503c20f0.&#013;&#010;18:08:07.628 [flink-akka.actor.default-dispatcher-420] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Completed checkpoint 116 for job acd456ff6f2f9f59ee89b126503c20f0 (74305 bytes in 13 ms).&#013;&#010;18:08:08.615 [Checkpoint Timer] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Triggering checkpoint 117 @ 1595239688615 for job acd456ff6f2f9f59ee89b126503c20f0.&#013;&#010;18:08:08.626 [flink-akka.actor.default-dispatcher-420] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Completed checkpoint 117 for job acd456ff6f2f9f59ee89b126503c20f0 (74305 bytes in 11 ms).&#013;&#010;18:08:09.354 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Job ty-bi-flink (acd456ff6f2f9f59ee89b126503c20f0) switched from state RUNNING to CANCELLING.&#013;&#010;18:08:09.354 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (1/4) (4d8a61b0a71ff37d1e7d7da578878e55) switched from RUNNING to&#010;CANCELING.&#013;&#010;18:08:09.354 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (2/4) (97909ed1fcf34f658a3b6d9b3e8ee412) switched from RUNNING to&#010;CANCELING.&#013;&#010;18:08:09.354 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (3/4) (7be70346e0c7fc8f2b2224ca3a0907f0) switched from RUNNING to&#010;CANCELING.&#013;&#010;18:08:09.354 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (4/4) (4df2905ee56b06d9fc384e4beb228015) switched from RUNNING to&#010;CANCELING.&#013;&#010;18:08:09.355 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (1/4) (87d4c7af7d5fb5f81bae48aae77de473)&#010;switched from RUNNING to CANCELING.&#013;&#010;18:08:09.355 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (2/4) (7dfdd54faf11bc364fb6afc3dfdfb4dd)&#010;switched from RUNNING to CANCELING.&#013;&#010;18:08:09.355 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (3/4) (9035e059e465b8c520edf37ec734b43e)&#010;switched from RUNNING to CANCELING.&#013;&#010;18:08:09.355 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (4/4) (e6ff47b0da505b2aa4d775d7821b8356)&#010;switched from RUNNING to CANCELING.&#013;&#010;18:08:09.377 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (4/4) (e6ff47b0da505b2aa4d775d7821b8356)&#010;switched from CANCELING to CANCELED.&#013;&#010;18:08:09.377 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (3/4) (9035e059e465b8c520edf37ec734b43e)&#010;switched from CANCELING to CANCELED.&#013;&#010;18:08:09.378 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (2/4) (7dfdd54faf11bc364fb6afc3dfdfb4dd)&#010;switched from CANCELING to CANCELED.&#013;&#010;18:08:09.378 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (1/4) (87d4c7af7d5fb5f81bae48aae77de473)&#010;switched from CANCELING to CANCELED.&#013;&#010;18:08:09.378 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (1/4) (4d8a61b0a71ff37d1e7d7da578878e55) switched from CANCELING to&#010;CANCELED.&#013;&#010;18:08:09.379 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (2/4) (97909ed1fcf34f658a3b6d9b3e8ee412) switched from CANCELING to&#010;CANCELED.&#013;&#010;18:08:09.379 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (3/4) (7be70346e0c7fc8f2b2224ca3a0907f0) switched from CANCELING to&#010;CANCELED.&#013;&#010;18:08:09.381 [flink-akka.actor.default-dispatcher-416] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (4/4) (4df2905ee56b06d9fc384e4beb228015) switched from CANCELING to&#010;CANCELED.&#013;&#010;18:08:09.381 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Job ty-bi-flink (acd456ff6f2f9f59ee89b126503c20f0) switched from state CANCELLING to CANCELED.&#013;&#010;18:08:09.381 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Stopping checkpoint coordinator for job acd456ff6f2f9f59ee89b126503c20f0.&#013;&#010;18:08:09.381 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; o.a.f.runtime.checkpoint.StandaloneCompletedCheckpointStore&amp;nbsp;&#010;- Shutting down&#013;&#010;18:08:09.381 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CompletedCheckpoint&amp;nbsp;&#010;- Checkpoint with ID 117 at 'file:/opt/flink/flink-1.7.2/checkpoints/acd456ff6f2f9f59ee89b126503c20f0/chk-117'&#010;not discarded.&#013;&#010;18:08:09.382 [flink-akka.actor.default-dispatcher-427] INFO&amp;nbsp; org.apache.flink.runtime.dispatcher.StandaloneDispatcher&amp;nbsp;&#010;- Job acd456ff6f2f9f59ee89b126503c20f0 reached globally terminal state CANCELED.&#013;&#010;18:08:09.384 [flink-akka.actor.default-dispatcher-416] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Stopping the JobMaster for job ty-bi-flink(acd456ff6f2f9f59ee89b126503c20f0).&#013;&#010;18:08:09.385 [flink-akka.actor.default-dispatcher-416] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Close ResourceManager connection 7f7791cdc957a13cfaf639062c495fb9: JobManager is shutting&#010;down..&#013;&#010;18:08:09.385 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Suspending SlotPool.&#013;&#010;18:08:09.385 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Stopping SlotPool.&#013;&#010;18:08:09.385 [flink-akka.actor.default-dispatcher-416] INFO&amp;nbsp; o.a.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@rcx51101:6123/user/jobmanager_4&#010;for job acd456ff6f2f9f59ee89b126503c20f0 from the resource manager.&#013;&#010;18:08:09.385 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobManagerRunner&amp;nbsp;&#010;- JobManagerRunner already shutdown.&#013;&#010;18:08:33.384 [flink-rest-server-netty-worker-thread-4] WARN&amp;nbsp; org.apache.flink.runtime.webmonitor.handlers.JarRunHandler&amp;nbsp;&#010;- Configuring the job submission via query parameters is deprecated. Please migrate to submitting&#010;a JSON request instead.&#013;&#010;18:08:34.205 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.dispatcher.StandaloneDispatcher&amp;nbsp;&#010;- Submitting job 6dbecb3e4f536c2c92ca7931cba54fd2 (ty-bi-flink).&#013;&#010;18:08:34.205 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.rpc.akka.AkkaRpcService&amp;nbsp;&#010;- Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/jobmanager_5&#010;.&#013;&#010;18:08:34.206 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Initializing job ty-bi-flink (6dbecb3e4f536c2c92ca7931cba54fd2).&#013;&#010;18:08:34.206 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Using restart strategy NoRestartStrategy for ty-bi-flink (6dbecb3e4f536c2c92ca7931cba54fd2).&#013;&#010;18:08:34.206 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.rpc.akka.AkkaRpcService&amp;nbsp;&#010;- Starting RPC endpoint for org.apache.flink.runtime.jobmaster.slotpool.SlotPool at akka://flink/user/c8a89ca4-afcc-41c0-b121-bbfe4354e502&#010;.&#013;&#010;18:08:34.206 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Job recovers via failover strategy: full graph restart&#013;&#010;18:08:34.206 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Running initialization on master for job ty-bi-flink (6dbecb3e4f536c2c92ca7931cba54fd2).&#013;&#010;18:08:34.206 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Successfully ran initialization on master in 0 ms.&#013;&#010;18:08:34.207 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Using application-defined state backend: RocksDBStateBackend{checkpointStreamBackend=File&#010;State Backend (checkpoints: 'file:/opt/flink/flink-1.7.2/checkpoints', savepoints: 'null',&#010;asynchronous: UNDEFINED, fileStateThreshold: -1), localRocksDbDirectories=null, enableIncrementalCheckpointing=UNDEFINED}&#013;&#010;18:08:34.207 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Configuring application-defined state backend with job/cluster config&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobManagerRunner&amp;nbsp;&#010;- JobManager runner for job ty-bi-flink (6dbecb3e4f536c2c92ca7931cba54fd2) was granted leadership&#010;with session id 00000000-0000-0000-0000-000000000000 at akka.tcp://flink@rcx51101:6123/user/jobmanager_5.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Starting execution of job ty-bi-flink (6dbecb3e4f536c2c92ca7931cba54fd2)&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Job ty-bi-flink (6dbecb3e4f536c2c92ca7931cba54fd2) switched from state CREATED to RUNNING.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (1/4) (c06f0e753f644bdbcfe50cc8d2364cf6) switched from CREATED to&#010;SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (2/4) (5436dd5759d18472fcf171f5df9d9bc9) switched from CREATED to&#010;SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (3/4) (2a8cf04be945d59a70a3d82f50b38cd6) switched from CREATED to&#010;SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (4/4) (9797fe0ec397922dff0c8bde4fb89ba2) switched from CREATED to&#010;SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (1/4) (4127cdcd8ad7bd2011b7f8a8330663b9)&#010;switched from CREATED to SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (2/4) (c0e50cbfbab0b29973cc517056f3f561)&#010;switched from CREATED to SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (3/4) (989517f5535736062e6ce870e30742ee)&#010;switched from CREATED to SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (4/4) (eaf47a632d3e735f1341e1d6d4ec7b7f)&#010;switched from CREATED to SCHEDULED.&#013;&#010;18:08:34.208 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Connecting to ResourceManager akka.tcp://flink@rcx51101:6123/user/resourcemanager(00000000000000000000000000000000)&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{092c50cc73f659cbca805205e07b239c}]&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{16e0d6c68cbbf62c056758903c129661}]&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{f727b58cc9c5abe1627216c5973f98b5}]&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{bb8a60407b3fa9329ccc1ae8454bf239}]&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Resolved ResourceManager address, beginning registration&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- Registration at ResourceManager attempt 1 (timeout=100ms)&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; o.a.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Registering job manager 00000000000000000000000000000000@akka.tcp://flink@rcx51101:6123/user/jobmanager_5&#010;for job 6dbecb3e4f536c2c92ca7931cba54fd2.&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; o.a.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Registered job manager 00000000000000000000000000000000@akka.tcp://flink@rcx51101:6123/user/jobmanager_5&#010;for job 6dbecb3e4f536c2c92ca7931cba54fd2.&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-420] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.JobMaster&amp;nbsp;&#010;- JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.&#013;&#010;18:08:34.209 [flink-akka.actor.default-dispatcher-420] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Requesting new slot [SlotRequestId{bb8a60407b3fa9329ccc1ae8454bf239}] and profile ResourceProfile{cpuCores=-1.0,&#010;heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource&#010;manager.&#013;&#010;18:08:34.210 [flink-akka.actor.default-dispatcher-420] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Requesting new slot [SlotRequestId{f727b58cc9c5abe1627216c5973f98b5}] and profile ResourceProfile{cpuCores=-1.0,&#010;heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource&#010;manager.&#013;&#010;18:08:34.210 [flink-akka.actor.default-dispatcher-415] INFO&amp;nbsp; o.a.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0,&#010;nativeMemoryInMB=0, networkMemoryInMB=0} for job 6dbecb3e4f536c2c92ca7931cba54fd2 with allocation&#010;id AllocationID{db118025945481bba66b8ffa734e4202}.&#013;&#010;18:08:34.210 [flink-akka.actor.default-dispatcher-420] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Requesting new slot [SlotRequestId{16e0d6c68cbbf62c056758903c129661}] and profile ResourceProfile{cpuCores=-1.0,&#010;heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource&#010;manager.&#013;&#010;18:08:34.210 [flink-akka.actor.default-dispatcher-420] INFO&amp;nbsp; org.apache.flink.runtime.jobmaster.slotpool.SlotPool&amp;nbsp;&#010;- Requesting new slot [SlotRequestId{092c50cc73f659cbca805205e07b239c}] and profile ResourceProfile{cpuCores=-1.0,&#010;heapMemoryInMB=-1, directMemoryInMB=0, nativeMemoryInMB=0, networkMemoryInMB=0} from resource&#010;manager.&#013;&#010;18:08:34.210 [flink-akka.actor.default-dispatcher-415] INFO&amp;nbsp; o.a.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0,&#010;nativeMemoryInMB=0, networkMemoryInMB=0} for job 6dbecb3e4f536c2c92ca7931cba54fd2 with allocation&#010;id AllocationID{d5147bcc731a51f09bdb32e366d93b02}.&#013;&#010;18:08:34.210 [flink-akka.actor.default-dispatcher-415] INFO&amp;nbsp; o.a.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0,&#010;nativeMemoryInMB=0, networkMemoryInMB=0} for job 6dbecb3e4f536c2c92ca7931cba54fd2 with allocation&#010;id AllocationID{14001529dd0d04ebbd169241cb59f918}.&#013;&#010;18:08:34.210 [flink-akka.actor.default-dispatcher-415] INFO&amp;nbsp; o.a.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Request slot with profile ResourceProfile{cpuCores=-1.0, heapMemoryInMB=-1, directMemoryInMB=0,&#010;nativeMemoryInMB=0, networkMemoryInMB=0} for job 6dbecb3e4f536c2c92ca7931cba54fd2 with allocation&#010;id AllocationID{bb02373b91c626c6fde666512d5b62ed}.&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (1/4) (c06f0e753f644bdbcfe50cc8d2364cf6) switched from SCHEDULED to&#010;DEPLOYING.&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying Source: Custom Source (1/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (3/4) (2a8cf04be945d59a70a3d82f50b38cd6) switched from SCHEDULED to&#010;DEPLOYING.&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying Source: Custom Source (3/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (2/4) (5436dd5759d18472fcf171f5df9d9bc9) switched from SCHEDULED to&#010;DEPLOYING.&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying Source: Custom Source (2/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (4/4) (9797fe0ec397922dff0c8bde4fb89ba2) switched from SCHEDULED to&#010;DEPLOYING.&#013;&#010;18:08:34.219 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying Source: Custom Source (4/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.220 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (4/4) (eaf47a632d3e735f1341e1d6d4ec7b7f)&#010;switched from SCHEDULED to DEPLOYING.&#013;&#010;18:08:34.220 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (4/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.222 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (3/4) (989517f5535736062e6ce870e30742ee)&#010;switched from SCHEDULED to DEPLOYING.&#013;&#010;18:08:34.222 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (3/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.222 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (2/4) (c0e50cbfbab0b29973cc517056f3f561)&#010;switched from SCHEDULED to DEPLOYING.&#013;&#010;18:08:34.222 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (2/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.222 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (1/4) (4127cdcd8ad7bd2011b7f8a8330663b9)&#010;switched from SCHEDULED to DEPLOYING.&#013;&#010;18:08:34.222 [flink-akka.actor.default-dispatcher-378] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Deploying map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (1/4) (attempt #0) to rcx51102&#013;&#010;18:08:34.506 [Checkpoint Timer] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Checkpoint triggering task Source: Custom Source (1/4) of job 6dbecb3e4f536c2c92ca7931cba54fd2&#010;is not in state RUNNING but DEPLOYING instead. Aborting checkpoint.&#013;&#010;18:08:35.036 [flink-akka.actor.default-dispatcher-430] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (3/4) (989517f5535736062e6ce870e30742ee)&#010;switched from DEPLOYING to RUNNING.&#013;&#010;18:08:35.037 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (2/4) (c0e50cbfbab0b29973cc517056f3f561)&#010;switched from DEPLOYING to RUNNING.&#013;&#010;18:08:35.057 [flink-akka.actor.default-dispatcher-430] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (4/4) (eaf47a632d3e735f1341e1d6d4ec7b7f)&#010;switched from DEPLOYING to RUNNING.&#013;&#010;18:08:35.058 [flink-akka.actor.default-dispatcher-430] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- map_sub_order_detail -&amp;gt; Sink: Print to Std. Out (1/4) (4127cdcd8ad7bd2011b7f8a8330663b9)&#010;switched from DEPLOYING to RUNNING.&#013;&#010;18:08:35.069 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (4/4) (9797fe0ec397922dff0c8bde4fb89ba2) switched from DEPLOYING to&#010;RUNNING.&#013;&#010;18:08:35.070 [flink-akka.actor.default-dispatcher-430] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (1/4) (c06f0e753f644bdbcfe50cc8d2364cf6) switched from DEPLOYING to&#010;RUNNING.&#013;&#010;18:08:35.076 [flink-akka.actor.default-dispatcher-418] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (3/4) (2a8cf04be945d59a70a3d82f50b38cd6) switched from DEPLOYING to&#010;RUNNING.&#013;&#010;18:08:35.076 [flink-akka.actor.default-dispatcher-430] INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;- Source: Custom Source (2/4) (5436dd5759d18472fcf171f5df9d9bc9) switched from DEPLOYING to&#010;RUNNING.&#013;&#010;18:08:35.506 [Checkpoint Timer] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Triggering checkpoint 1 @ 1595239715506 for job 6dbecb3e4f536c2c92ca7931cba54fd2.&#013;&#010;18:08:35.530 [flink-akka.actor.default-dispatcher-430] INFO&amp;nbsp; org.apache.flink.runtime.checkpoint.CheckpointCoordinator&amp;nbsp;&#010;- Completed checkpoint 1 for job 6dbecb3e4f536c2c92ca7931cba54fd2 (74134 bytes in 24 ms).&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月17日(星期五) 晚上10:58&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: state无法从checkpoint中恢复&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;1 你需要回复一下我之前问你的问题：你可以从 JM log 看一下是否从 checkpoint&#010;恢复了&#013;&#010;2. 这里没有打印只是表明当前处理的 key 没有 state 数据，并不能表示&#010;state 没有恢复回来，state 值是绑定到某个 key&#013;&#010;上的（keyby 的 key）&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;sun &lt;1392427699@qq.com&amp;gt; 于2020年7月17日周五 下午5:22写道：&#013;&#010;&#013;&#010;&amp;gt; 你好：counts 的数据 我是在下面打印出来了 List&lt;String&amp;amp;gt;&#010;list =&#013;&#010;&amp;gt; Lists.newArrayList(counts.get()) ;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;for(String ss : list){&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;System.out.println(\"!!!\" + ss);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;log.info(\"!!!\" + ss);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;}，但是我重启服务之后，之前存的那些内容打印不出来了。&#013;&#010;&amp;gt; @Slf4j&#013;&#010;&amp;gt; public class FlatMapTestState extends RichFlatMapFunction&lt;String,&#013;&#010;&amp;gt; Test222&amp;amp;gt; {&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; private transient ListState&lt;String&amp;amp;gt;&#010;counts;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void open(Configuration parameters)&#010;throws Exception {&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; StateTtlConfig&#010;ttlConfig = StateTtlConfig&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.newBuilder(Time.minutes(30))&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.build();&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ListStateDescriptor&lt;String&amp;amp;gt;&#010;lastUserLogin = new&#013;&#010;&amp;gt; ListStateDescriptor&lt;&amp;amp;gt;(\"lastUserLogin\", String.class);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; lastUserLogin.enableTimeToLive(ttlConfig);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; counts&#010;= getRuntimeContext().getListState(lastUserLogin);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; @Override&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void flatMap(String s, Collector&lt;Test222&amp;amp;gt;&#010;collector) throws&#013;&#010;&amp;gt; Exception {&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;Test222 message = JSONUtil.toObject(s, new&#013;&#010;&amp;gt; TypeReference&lt;Test222&amp;amp;gt;() {&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;});&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;System.out.println(DateUtil.toLongDateString(new Date()));&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;log.info(DateUtil.toLongDateString(new Date()));&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;counts.add(message.getId());&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;List&lt;String&amp;amp;gt; list = Lists.newArrayList(counts.get()) ;&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;for(String ss : list){&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;System.out.println(\"!!!\" + ss);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;log.info(\"!!!\" + ss);&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;}&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;log.info(DateUtil.toLongDateString(new Date()));&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;System.out.println(DateUtil.toLongDateString(new Date()));&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&#013;&#010;&amp;gt; }&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月16日(星期四) 晚上8:16&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: state无法从checkpoint中恢复&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Hi&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 1 counts 的数据丢失了能否详细描述一下呢？你预期是什么，看到什么现象&#013;&#010;&amp;gt; 2 能否把你关于 counts 的其他代码也贴一下&#013;&#010;&amp;gt; 3. 你的作业是否从 checkpoint 恢复了呢？这个可以从 JM log 来查看&#013;&#010;&amp;gt; 4. 如果你确定是数据有丢失的话，或许你可以使用 state-process-api[1]&#010;看一下是序列化出去有问题，还是 restore 回来有问题&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; [1]&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/state_processor_api.html&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Congxian&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; sun &lt;1392427699@qq.com&amp;amp;gt; 于2020年7月16日周四 下午6:16写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 配置代码env.enableCheckpointing(1000);env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#013;&#010;&amp;gt; &amp;amp;gt; //作业失败后不重启&#013;&#010;&amp;gt; &amp;amp;gt; env.setRestartStrategy(RestartStrategies.noRestart());&#013;&#010;&amp;gt; &amp;amp;gt; env.getCheckpointConfig().setCheckpointTimeout(500);&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&amp;gt; &amp;amp;gt; env.setStateBackend(new&#013;&#010;&amp;gt; &amp;amp;gt; RocksDBStateBackend(\"file:///opt/flink/flink-1.7.2/checkpoints\"));&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp; 使用状态的代码private transient ListState&lt;String&amp;amp;amp;gt;&#010;counts;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; @Override&#013;&#010;&amp;gt; &amp;amp;gt; public void open(Configuration parameters) throws Exception {&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; StateTtlConfig&#010;ttlConfig = StateTtlConfig&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; .newBuilder(Time.minutes(30))&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; .build();&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; ListStateDescriptor&lt;String&amp;amp;amp;gt;&#013;&#010;&amp;gt; lastUserLogin = new&#013;&#010;&amp;gt; &amp;amp;gt; ListStateDescriptor&lt;&amp;amp;amp;gt;(\"lastUserLogin\", String.class);&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; lastUserLogin.enableTimeToLive(ttlConfig);&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; counts =&#013;&#010;&amp;gt; getRuntimeContext().getListState(lastUserLogin);&#013;&#010;&amp;gt; &amp;amp;gt; }&#013;&#010;&amp;gt; &amp;amp;gt; 我重启了task managers 后。发现&amp;amp;nbsp; counts&amp;amp;nbsp;&#010;里面的数据都丢失了",
        "depth": "4",
        "reply": "<tencent_A102A9D2E3D31E32955A7C7454B611B33409@qq.com>"
    },
    {
        "id": "<1df8caff.a185.1735f71eec4.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 18 Jul 2020 01:02:49 GMT",
        "subject": "回复：state无法从checkpoint中恢复",
        "content": "hi&#010;你在UI上checkpoint那里可以看到是否从上一次成功的checkpoint恢复了 先确定一下这个问题&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月17日 17:21，sun 写道：&#010;你好：counts 的数据 我是在下面打印出来了 List&lt;String&amp;gt; list = Lists.newArrayList(counts.get())&#010;;&#010;           for(String ss : list){&#010;               System.out.println(\"!!!\" + ss);&#010;               log.info(\"!!!\" + ss);&#010;           }，但是我重启服务之后，之前存的那些内容打印不出来了。&#010;@Slf4j&#010;public class FlatMapTestState extends RichFlatMapFunction&lt;String, Test222&amp;gt; {&#010;&#010;&#010;   private transient ListState&lt;String&amp;gt; counts;&#010;&#010;&#010;   @Override&#010;   public void open(Configuration parameters) throws Exception {&#010;       StateTtlConfig ttlConfig = StateTtlConfig&#010;               .newBuilder(Time.minutes(30))&#010;               .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#010;               .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#010;               .build();&#010;&#010;       ListStateDescriptor&lt;String&amp;gt; lastUserLogin = new ListStateDescriptor&lt;&amp;gt;(\"lastUserLogin\",&#010;String.class);&#010;       lastUserLogin.enableTimeToLive(ttlConfig);&#010;       counts = getRuntimeContext().getListState(lastUserLogin);&#010;   }&#010;&#010;&#010;   @Override&#010;   public void flatMap(String s, Collector&lt;Test222&amp;gt; collector) throws Exception&#010;{&#010;           Test222 message = JSONUtil.toObject(s, new TypeReference&lt;Test222&amp;gt;() {&#010;           });&#010;&#010;           System.out.println(DateUtil.toLongDateString(new Date()));&#010;           log.info(DateUtil.toLongDateString(new Date()));&#010;           counts.add(message.getId());&#010;           List&lt;String&amp;gt; list = Lists.newArrayList(counts.get()) ;&#010;           for(String ss : list){&#010;               System.out.println(\"!!!\" + ss);&#010;               log.info(\"!!!\" + ss);&#010;           }&#010;             log.info(DateUtil.toLongDateString(new Date()));&#010;           System.out.println(DateUtil.toLongDateString(new Date()));&#010;   }&#010;}&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;qcx978132955@gmail.com&amp;gt;;&#010;发送时间:&amp;nbsp;2020年7月16日(星期四) 晚上8:16&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&#010;主题:&amp;nbsp;Re: state无法从checkpoint中恢复&#010;&#010;&#010;&#010;Hi&#010;&#010;1 counts 的数据丢失了能否详细描述一下呢？你预期是什么，看到什么现象&#010;2 能否把你关于 counts 的其他代码也贴一下&#010;3. 你的作业是否从 checkpoint 恢复了呢？这个可以从 JM log 来查看&#010;4. 如果你确定是数据有丢失的话，或许你可以使用 state-process-api[1] 看一下是序列化出去有问题，还是&#010;restore 回来有问题&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/state_processor_api.html&#010;Best,&#010;Congxian&#010;&#010;&#010;sun &lt;1392427699@qq.com&amp;gt; 于2020年7月16日周四 下午6:16写道：&#010;&#010;&amp;gt;&#010;&amp;gt; 配置代码env.enableCheckpointing(1000);env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);&#010;&amp;gt; //作业失败后不重启&#010;&amp;gt; env.setRestartStrategy(RestartStrategies.noRestart());&#010;&amp;gt; env.getCheckpointConfig().setCheckpointTimeout(500);&#010;&amp;gt;&#010;&amp;gt; env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&amp;gt;&#010;&amp;gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;&amp;gt; env.setStateBackend(new&#010;&amp;gt; RocksDBStateBackend(\"file:///opt/flink/flink-1.7.2/checkpoints\"));&#010;&amp;gt;&amp;nbsp;&amp;nbsp; 使用状态的代码private transient ListState&lt;String&amp;amp;gt;&#010;counts;&#010;&amp;gt;&#010;&amp;gt;&#010;&amp;gt; @Override&#010;&amp;gt; public void open(Configuration parameters) throws Exception {&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; StateTtlConfig ttlConfig = StateTtlConfig&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.newBuilder(Time.minutes(30))&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)&#010;&amp;gt;&#010;&amp;gt; .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;.build();&#010;&amp;gt;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ListStateDescriptor&lt;String&amp;amp;gt;&#010;lastUserLogin = new&#010;&amp;gt; ListStateDescriptor&lt;&amp;amp;gt;(\"lastUserLogin\", String.class);&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; lastUserLogin.enableTimeToLive(ttlConfig);&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; counts = getRuntimeContext().getListState(lastUserLogin);&#010;&amp;gt; }&#010;&amp;gt; 我重启了task managers 后。发现&amp;nbsp; counts&amp;nbsp; 里面的数据都丢失了",
        "depth": "3",
        "reply": "<tencent_A102A9D2E3D31E32955A7C7454B611B33409@qq.com>"
    },
    {
        "id": "<33333f3f.4b95.1735732e0b7.Coremail.xiayongquan1011@163.com>",
        "from": "xyq &lt;xiayongquan1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 10:36:58 GMT",
        "subject": "flink1.10升级到flink1.11 jar冲突",
        "content": "hello  大家好&#010; 我在flink由1.10升级到1.11过程中遇到如下问题，请问是哪个包冲突了（本地可跑，上测试环境就报错），谢谢：&#010;&#010;&#010;Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.&#010;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:329)&#010;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:535)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:301)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;... 11 more&#010;Caused by: java.io.IOException: Could not find class 'org.apache.flink.table.runtime.typeutils.BaseRowSerializer$BaseRowSerializerSnapshot'&#010;in classpath.&#010;at org.apache.flink.util.InstantiationUtil.resolveClassByName(InstantiationUtil.java:721)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.readAndInstantiateSnapshotClass(TypeSerializerSnapshotSerializationUtil.java:84)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.readVersionedSnapshot(TypeSerializerSnapshot.java:163)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.deserializeV2(TypeSerializerSnapshotSerializationUtil.java:179)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.read(TypeSerializerSnapshotSerializationUtil.java:150)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.readSerializerSnapshot(TypeSerializerSnapshotSerializationUtil.java:76)&#010;at org.apache.flink.runtime.state.KeyedBackendSerializationProxy.read(KeyedBackendSerializationProxy.java:145)&#010;at org.apache.flink.contrib.streaming.state.restore.AbstractRocksDBRestoreOperation.readMetaData(AbstractRocksDBRestoreOperation.java:187)&#010;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateMetaData(RocksDBFullRestoreOperation.java:180)&#010;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:167)&#010;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:270)&#010;... 15 more&#010;Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.runtime.typeutils.BaseRowSerializer$BaseRowSerializerSnapshot&#010;at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:61)&#010;at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:48)&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;at java.lang.Class.forName0(Native Method)&#010;at java.lang.Class.forName(Class.java:348)&#010;at org.apache.flink.util.InstantiationUtil.resolveClassByName(InstantiationUtil.java:718)&#010;&#010;",
        "depth": "0",
        "reply": "<33333f3f.4b95.1735732e0b7.Coremail.xiayongquan1011@163.com>"
    },
    {
        "id": "<50538910.2f38.1735b8023c6.Coremail.xiayongquan1011@163.com>",
        "from": "xyq  &lt;xiayongquan1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 06:39:51 GMT",
        "subject": "Re:flink1.10升级到flink1.11 jar冲突",
        "content": "hello  大家好  flink由1.10升级到1.11&#010;从savepoint处恢复数据报错（这个报错的是flink sql双流join的，带状态，其他的stream的单流程序都已经照常恢复）&#010;请大家帮忙指导一下，谢谢。&#010;&#010;&#010;报错如下：&#010;&#010;org.apache.flink.client.program.ProgramInvocationException: The main method caused an error:&#010;org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete the operation.&#010;Number of retries has been exhausted.&#010;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;at java.security.AccessController.doPrivileged(Native Method)&#010;at javax.security.auth.Subject.doAs(Subject.java:422)&#010;at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1692)&#010;at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;Caused by: java.util.concurrent.ExecutionException: org.apache.flink.runtime.concurrent.FutureUtils$RetryException:&#010;Could not complete the operation. Number of retries has been exhausted.&#010;at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)&#010;at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)&#010;at org.apache.flink.client.program.StreamContextEnvironment.getJobExecutionResult(StreamContextEnvironment.java:116)&#010;at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:80)&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1699)&#010;at com.zhidao.bigdata.plat.dophin.streaming.warehouse.dwd.iotbind.etl.DwdBaseCarlifeTIotBindGeoManage.main(DwdBaseCarlifeTIotBindGeoManage.java:282)&#010;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;at java.lang.reflect.Method.invoke(Method.java:498)&#010;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;... 11 more&#010;Caused by: org.apache.flink.runtime.concurrent.FutureUtils$RetryException: Could not complete&#010;the operation. Number of retries has been exhausted.&#010;at org.apache.flink.runtime.concurrent.FutureUtils.lambda$retryOperationWithDelay$8(FutureUtils.java:302)&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)&#010;at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$1(RestClient.java:342)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:500)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:493)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:472)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:413)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:538)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:531)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:111)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:323)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:339)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)&#010;at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)&#010;at java.lang.Thread.run(Thread.java:748)&#010;Caused by: java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException:&#010;Connection refused: vm-9-72-centos/10.2.9.72:40620&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)&#010;at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;... 19 more&#010;Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException:&#010;Connection refused: vm-9-72-centos/10.2.9.72:40620&#010;Caused by: java.net.ConnectException: Connection refused&#010;at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)&#010;at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:336)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:685)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:632)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:549)&#010;at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:511)&#010;at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)&#010;at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)&#010;at java.lang.Thread.run(Thread.java:748)&#010;========================================================================&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-16 18:36:58，\"xyq\" &lt;xiayongquan1011@163.com&gt; 写道：&#010;&#010;hello  大家好&#010; 我在flink由1.10升级到1.11过程中遇到如下问题，请问是哪个包冲突了（本地可跑，上测试环境就报错），谢谢：&#010;&#010;&#010;Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.&#010;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:329)&#010;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:535)&#010;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:301)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;... 11 more&#010;Caused by: java.io.IOException: Could not find class 'org.apache.flink.table.runtime.typeutils.BaseRowSerializer$BaseRowSerializerSnapshot'&#010;in classpath.&#010;at org.apache.flink.util.InstantiationUtil.resolveClassByName(InstantiationUtil.java:721)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.readAndInstantiateSnapshotClass(TypeSerializerSnapshotSerializationUtil.java:84)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshot.readVersionedSnapshot(TypeSerializerSnapshot.java:163)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.deserializeV2(TypeSerializerSnapshotSerializationUtil.java:179)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil$TypeSerializerSnapshotSerializationProxy.read(TypeSerializerSnapshotSerializationUtil.java:150)&#010;at org.apache.flink.api.common.typeutils.TypeSerializerSnapshotSerializationUtil.readSerializerSnapshot(TypeSerializerSnapshotSerializationUtil.java:76)&#010;at org.apache.flink.runtime.state.KeyedBackendSerializationProxy.read(KeyedBackendSerializationProxy.java:145)&#010;at org.apache.flink.contrib.streaming.state.restore.AbstractRocksDBRestoreOperation.readMetaData(AbstractRocksDBRestoreOperation.java:187)&#010;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateMetaData(RocksDBFullRestoreOperation.java:180)&#010;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:167)&#010;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:270)&#010;... 15 more&#010;Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.runtime.typeutils.BaseRowSerializer$BaseRowSerializerSnapshot&#010;at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;at org.apache.flink.util.FlinkUserCodeClassLoader.loadClassWithoutExceptionHandling(FlinkUserCodeClassLoader.java:61)&#010;at org.apache.flink.util.FlinkUserCodeClassLoader.loadClass(FlinkUserCodeClassLoader.java:48)&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;at java.lang.Class.forName0(Native Method)&#010;at java.lang.Class.forName(Class.java:348)&#010;at org.apache.flink.util.InstantiationUtil.resolveClassByName(InstantiationUtil.java:718)&#010;&#010;&#010;&#010;&#010;&#010;&#010; ",
        "depth": "1",
        "reply": "<33333f3f.4b95.1735732e0b7.Coremail.xiayongquan1011@163.com>"
    },
    {
        "id": "<tencent_34880706EFB502CA3B1289616C2514B27209@qq.com>",
        "from": "&quot;op&quot; &lt;520075...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 11:16:00 GMT",
        "subject": "flink state问题",
        "content": "大家好&amp;nbsp;&#013;&#010;我有一个去重的需求，想节省内存用的bloomfilter，代码如下：&#013;&#010; .keyBy(_._1).process(new KeyedProcessFunction[String,(String,String),String]() {&#013;&#010;  var state:ValueState[BloomFilter[CharSequence",
        "depth": "0",
        "reply": "<tencent_34880706EFB502CA3B1289616C2514B27209@qq.com>"
    },
    {
        "id": "<CAA8tFvuJZPivvna_TpbaVQJUJ2hwJSgQXfj78XnTy74ZGkkHGg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:09:39 GMT",
        "subject": "Re: flink state问题",
        "content": "Hi&#013;&#010;&#013;&#010;你可以尝试用 state-process-api[1] 看一下 savepoint 中 state 的内容，先缩小一下问题的范围，如果&#013;&#010;savepoint 中就没有了，那就是序列化到 savepoint 的时候出错了，savepoitn&#010;是有的，那么就是恢复的时候出错了。&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/state_processor_api.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;op &lt;520075694@qq.com&gt; 于2020年7月16日周四 下午7:16写道：&#013;&#010;&#013;&#010;&gt; 大家好&amp;nbsp;&#013;&#010;&gt; 我有一个去重的需求，想节省内存用的bloomfilter，代码如下：&#013;&#010;&gt;  .keyBy(_._1).process(new&#013;&#010;&gt; KeyedProcessFunction[String,(String,String),String]() {&#013;&#010;&gt;   var state:ValueState[BloomFilter[CharSequence",
        "depth": "1",
        "reply": "<tencent_34880706EFB502CA3B1289616C2514B27209@qq.com>"
    },
    {
        "id": "<CAHOPYeXWtwz+WkSAogV6h8hL4=TLe+qT40nYaBDN=_1gN9yfXA@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 11:43:18 GMT",
        "subject": "[sql-client] 如何绕过交互式模式去做ddl",
        "content": "hi flink users&#013;&#010;&#013;&#010;众所周知，sql-client.sh的非交互模式下的-u是不支持ddl的，现在我们是用代码来调用sql-client.sh来做ddl，&#013;&#010;这样在交互模式如何去做。 通过hack sql client代码可以实现，但是不改代码的情况下有没有什么最佳实践。谢谢！&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "0",
        "reply": "<CAHOPYeXWtwz+WkSAogV6h8hL4=TLe+qT40nYaBDN=_1gN9yfXA@mail.gmail.com>"
    },
    {
        "id": "<CAELO931uUbQN3DT3ozSAg3BEFLH9oMnNny+HWZj6b+rwyHOAcw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 10:40:31 GMT",
        "subject": "Re: [sql-client] 如何绕过交互式模式去做ddl",
        "content": "Hi,&#013;&#010;&#013;&#010;你想要的是类似于 sql-client.sh -u 的功能，直接通过命令行去执行 ddl 是么？非常抱歉，目前这是不支持的。&#013;&#010;社区的e2e测试目前也是通过 Java 代码来调用 sql-client.sh 来实现执行 ddl&#010;的。&#013;&#010;不过社区是有计划支持 sql-client.sh 执行一个 sql 文件的，可以关注下FLINK-12828.&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Thu, 16 Jul 2020 at 19:43, Harold.Miao &lt;miaohonghit@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi flink users&#013;&#010;&gt;&#013;&#010;&gt; 众所周知，sql-client.sh的非交互模式下的-u是不支持ddl的，现在我们是用代码来调用sql-client.sh来做ddl，&#013;&#010;&gt; 这样在交互模式如何去做。 通过hack sql client代码可以实现，但是不改代码的情况下有没有什么最佳实践。谢谢！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best Regards,&#013;&#010;&gt; Harold Miao&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAHOPYeXWtwz+WkSAogV6h8hL4=TLe+qT40nYaBDN=_1gN9yfXA@mail.gmail.com>"
    },
    {
        "id": "<CAHOPYeV11ZU6xE6EUhNO8MqigSJrhD+iTnYVZpd3eTnLS1uYhg@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 12:34:57 GMT",
        "subject": "Re: [sql-client] 如何绕过交互式模式去做ddl",
        "content": "谢谢  我暂时这样改了一下&#013;&#010;&#013;&#010;public boolean submitUpdate(String statement) {&#013;&#010;   terminal.writer().println(CliStrings.messageInfo(CliStrings.MESSAGE_WILL_EXECUTE).toAnsi());&#013;&#010;   terminal.writer().println(new AttributedString(statement).toString());&#013;&#010;   terminal.flush();&#013;&#010;&#013;&#010;   final Optional&lt;SqlCommandCall&gt; parsedStatement = parseCommand(statement);&#013;&#010;   // only support INSERT INTO/OVERWRITE&#013;&#010;   return parsedStatement.map(cmdCall -&gt; {&#013;&#010;      switch (cmdCall.command) {&#013;&#010;         case INSERT_INTO:&#013;&#010;         case INSERT_OVERWRITE:&#013;&#010;            return callInsert(cmdCall);&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;*         case CREATE_TABLE:            callDdl(cmdCall.operands[0],&#013;&#010;CliStrings.MESSAGE_TABLE_CREATED);            return true;*&#013;&#010;default:&#013;&#010;            printError(CliStrings.MESSAGE_UNSUPPORTED_SQL);&#013;&#010;            return false;&#013;&#010;      }&#013;&#010;   }).orElse(false);&#013;&#010;}&#013;&#010;&#013;&#010;&#013;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月21日周二 下午6:41写道：&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 你想要的是类似于 sql-client.sh -u 的功能，直接通过命令行去执行&#010;ddl 是么？非常抱歉，目前这是不支持的。&#013;&#010;&gt; 社区的e2e测试目前也是通过 Java 代码来调用 sql-client.sh 来实现执行&#010;ddl 的。&#013;&#010;&gt; 不过社区是有计划支持 sql-client.sh 执行一个 sql 文件的，可以关注下FLINK-12828.&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Thu, 16 Jul 2020 at 19:43, Harold.Miao &lt;miaohonghit@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi flink users&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 众所周知，sql-client.sh的非交互模式下的-u是不支持ddl的，现在我们是用代码来调用sql-client.sh来做ddl，&#013;&#010;&gt; &gt; 这样在交互模式如何去做。 通过hack sql client代码可以实现，但是不改代码的情况下有没有什么最佳实践。谢谢！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best Regards,&#013;&#010;&gt; &gt; Harold Miao&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "2",
        "reply": "<CAHOPYeXWtwz+WkSAogV6h8hL4=TLe+qT40nYaBDN=_1gN9yfXA@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGt8RTFtnw3gr2FwE9muDT2yGToj796SjPEXHeKpVUesew@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 12:35:01 GMT",
        "subject": "Re: [sql-client] 如何绕过交互式模式去做ddl",
        "content": "sql-client.sh的-u是指update语句，目前只支持insert。&#013;&#010;&#013;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月21日周二 下午6:47写道：&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 你想要的是类似于 sql-client.sh -u 的功能，直接通过命令行去执行&#010;ddl 是么？非常抱歉，目前这是不支持的。&#013;&#010;&gt; 社区的e2e测试目前也是通过 Java 代码来调用 sql-client.sh 来实现执行&#010;ddl 的。&#013;&#010;&gt; 不过社区是有计划支持 sql-client.sh 执行一个 sql 文件的，可以关注下FLINK-12828.&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Thu, 16 Jul 2020 at 19:43, Harold.Miao &lt;miaohonghit@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi flink users&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 众所周知，sql-client.sh的非交互模式下的-u是不支持ddl的，现在我们是用代码来调用sql-client.sh来做ddl，&#013;&#010;&gt; &gt; 这样在交互模式如何去做。 通过hack sql client代码可以实现，但是不改代码的情况下有没有什么最佳实践。谢谢！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best Regards,&#013;&#010;&gt; &gt; Harold Miao&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAHOPYeXWtwz+WkSAogV6h8hL4=TLe+qT40nYaBDN=_1gN9yfXA@mail.gmail.com>"
    },
    {
        "id": "<5fc1b77b.6099.173717fe87c.Coremail.zhanglianzhg@163.com>",
        "from": "zhanglianzhg  &lt;zhanglian...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 13:11:14 GMT",
        "subject": "Re:Re: [sql-client] 如何绕过交互式模式去做ddl",
        "content": "看下CliClient.java源码 open接口，&#010;final Optional&lt;SqlCommandCall&gt; cmdCall = parseCommand(line);&#010;cmdCall.ifPresent(this::callCommand);&#010;可以看出解析字符串后执行响应命令。&#010;目前我们这边一个项目也在做相似的，可以界面写好slq，以分号作为分隔符表示ddl或则DMl作为分隔符。&#010;然后以文件方式保存(可以作为日志等用作)。&#010;然后自己实现一个excutor类包装了tableEnvironment，主要功能用作string命令解析以及命令执行，可以简单的把flink的解析以及&#010;callCommand拿过来，然后加以改造，内部支持ddl、dml、函数注册等。&#010;&#010;这样不管做什么table操作，创建表或者注册函数、执行操作命令一个接口搞定。&#010;其主要改动是：扩展callCommand以及SqlCommandCall&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-21 20:35:01，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#010;&gt;sql-client.sh的-u是指update语句，目前只支持insert。&#010;&gt;&#010;&gt;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月21日周二 下午6:47写道：&#010;&gt;&#010;&gt;&gt; Hi,&#010;&gt;&gt;&#010;&gt;&gt; 你想要的是类似于 sql-client.sh -u 的功能，直接通过命令行去执行&#010;ddl 是么？非常抱歉，目前这是不支持的。&#010;&gt;&gt; 社区的e2e测试目前也是通过 Java 代码来调用 sql-client.sh 来实现执行&#010;ddl 的。&#010;&gt;&gt; 不过社区是有计划支持 sql-client.sh 执行一个 sql 文件的，可以关注下FLINK-12828.&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Jark&#010;&gt;&gt;&#010;&gt;&gt; On Thu, 16 Jul 2020 at 19:43, Harold.Miao &lt;miaohonghit@gmail.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt; &gt; hi flink users&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 众所周知，sql-client.sh的非交互模式下的-u是不支持ddl的，现在我们是用代码来调用sql-client.sh来做ddl，&#010;&gt;&gt; &gt; 这样在交互模式如何去做。 通过hack sql client代码可以实现，但是不改代码的情况下有没有什么最佳实践。谢谢！&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; --&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best Regards,&#010;&gt;&gt; &gt; Harold Miao&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;",
        "depth": "3",
        "reply": "<CAHOPYeXWtwz+WkSAogV6h8hL4=TLe+qT40nYaBDN=_1gN9yfXA@mail.gmail.com>"
    },
    {
        "id": "<5D396283-1D2F-4AD2-BBF1-C5720758CAED@gmail.com>",
        "from": "徐粟 &lt;kevinbrandon202...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:01:41 GMT",
        "subject": "Fwd: 【求助】flink打包到集群运行问题",
        "content": "&#013;&#010;&#013;&#010;&gt; 下面是被转发的邮件：&#013;&#010;&gt; &#013;&#010;&gt; 发件人: 徐粟 &lt;kevinbrandon2020xu@gmail.com&gt;&#013;&#010;&gt; 主题: 【求助】flink打包到集群运行问题&#013;&#010;&gt; 日期: 2020年7月16日 GMT+8 下午7:51:06&#013;&#010;&gt; 收件人: user-zh@flink.apache.org&#013;&#010;&gt; &#013;&#010;&gt; hi ，please help me&#013;&#010;&gt; 我打包到集群之后，产生了如下图错误。&#013;&#010;&gt; flink版本是1.10.1 jar包是flnk-1.10.1-bin-scala_2.12.taz&#013;&#010;&gt; 命令在图片里面。thanks&#013;&#010;",
        "depth": "1",
        "reply": "<5D396283-1D2F-4AD2-BBF1-C5720758CAED@gmail.com>"
    },
    {
        "id": "<CAA8tFvtk05DafGzOzp8qkKLMASx+pHLcseqEi3vN+uAfNq31yg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:13:09 GMT",
        "subject": "Re: 【求助】flink打包到集群运行问题",
        "content": "Hi&#013;&#010;&#013;&#010;图片的文字太小了，可以看一下这个邮件[1]，应该是一个问题，按理在&#010;google 能够搜索到这个邮件列表的&#013;&#010;&#013;&#010;[1]&#013;&#010;http://apache-flink.147419.n8.nabble.com/Could-not-find-a-suitable-table-factory-for-TableSourceFactory-td3287.html&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;徐粟 &lt;kevinbrandon2020xu@gmail.com&gt; 于2020年7月16日周四 下午8:02写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 下面是被转发的邮件：&#013;&#010;&gt;&#013;&#010;&gt; *发件人: *徐粟 &lt;kevinbrandon2020xu@gmail.com&gt;&#013;&#010;&gt; *主题: **【求助】flink打包到集群运行问题*&#013;&#010;&gt; *日期: *2020年7月16日 GMT+8 下午7:51:06&#013;&#010;&gt; *收件人: *user-zh@flink.apache.org&#013;&#010;&gt;&#013;&#010;&gt; hi ，please help me&#013;&#010;&gt; 我打包到集群之后，产生了如下图错误。&#013;&#010;&gt; flink版本是1.10.1 jar包是flnk-1.10.1-bin-scala_2.12.taz&#013;&#010;&gt; 命令在图片里面。thanks&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<5D396283-1D2F-4AD2-BBF1-C5720758CAED@gmail.com>"
    },
    {
        "id": "<1594901056880-0.post@n8.nabble.com>",
        "from": "曹武 &lt;14701319...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:04:16 GMT",
        "subject": "flink 1.11 checkpoint使用",
        "content": "我在使用flink 1.11.0中得ddl 部分 采用debezium-json做cdc得时候&#010;从checkpoint恢复以后,新来op=d的数据会删除失败&#010;重启命令:./bin/flink run -m yarn-cluster  /root/bigdata-flink-1.0.jar -s&#010;hdfs://prehadoop01:8020/flink/checkpoints/4cc5df8b96e90c1c2a4d3719a77f51d1/chk-819/_metadata&#010;代码:   EnvironmentSettings settings = EnvironmentSettings.newInstance()&#010;                .useBlinkPlanner()&#010;                .inStreamingMode()&#010;                .build();&#010;&#010;        StreamExecutionEnvironment env =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;&#010;        env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#010;        env.getCheckpointConfig().setCheckpointTimeout(6000L); // 超时时间&#010;        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); //&#010;最大允许同时出现几个CheckPoint&#010;        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(10L); //&#010;最小得间隔时间&#010;        env.getCheckpointConfig().setPreferCheckpointForRecovery(true); //&#010;是否倾向于用CheckPoint做故障恢复&#010;        env.getCheckpointConfig().setTolerableCheckpointFailureNumber(1); //&#010;容忍多少次CheckPoint失败&#010;        //Checkpoint文件清理策略&#010;       &#010;env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;        //Checkpoint外部文件路径&#010;        env.setStateBackend(new FsStateBackend(new&#010;URI(\"hdfs://172.22.20.205:8020/flink/checkpoints\"), false));&#010;TimeUnit.MINUTES), Time.of(10, TimeUnit.SECONDS)));&#010;        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#010;settings);&#010;        String sourceDDL = String.format(&#010;                \"CREATE TABLE debezium_source (\" +&#010;                        \" id INT NOT NULL,\" +&#010;                        \" name STRING,\" +&#010;                        \" description STRING,\" +&#010;                        \" weight Double\" +&#010;                        \") WITH (\" +&#010;                        \" 'connector' = 'kafka-0.11',\" +&#010;                        \" 'topic' = '%s',\" +&#010;                        \" 'properties.bootstrap.servers' = '%s',\" +&#010;                        \" 'scan.startup.mode' = 'group-offsets',\" +&#010;                        \" 'format' = 'debezium-json'\" +&#010;                        \")\", \"ddd\", \" 172.22.20.206:9092\");&#010;        String sinkDDL = \"CREATE TABLE sink (\" +&#010;                \" id INT NOT NULL,\" +&#010;                \" name STRING,\" +&#010;                \" description STRING,\" +&#010;                \" weight Double,\" +&#010;                \" PRIMARY KEY (id,name, description,weight) NOT ENFORCED \" +&#010;                \") WITH (\" +&#010;                \" 'connector' = 'jdbc',\" +&#010;                \" 'url' =&#010;'jdbc:mysql://172.27.4.22:3306/test?autoReconnect=true',\" +&#010;                \" 'table-name' = 'products',\" +&#010;                \" 'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;                \" 'username'='DataPip',\" +&#010;                \" 'password'='DataPip'\" +&#010;                \")\";&#010;        String dml = \"INSERT INTO sink SELECT  id,name ,description, weight&#010;FROM debezium_source GROUP BY id,name ,description, weight\";&#010;        tEnv.executeSql(sourceDDL);&#010;        tEnv.executeSql(sinkDDL);&#010;        tEnv.executeSql(dml);&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1594901056880-0.post@n8.nabble.com>"
    },
    {
        "id": "<CADQYLGs6VbVNa5intYfA79=jtSdNp7CR4tnr=2VvJpZGW02kLA@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 13:55:30 GMT",
        "subject": "Re: flink 1.11 checkpoint使用",
        "content": "为什么要 GROUP BY id,name ,description, weight ？&#010;直接 \"INSERT INTO sink SELECT  id,name ,description, weight FROM&#010;debezium_source\" 不能满足需求？&#010;&#010;曹武 &lt;14701319164@163.com&gt; 于2020年7月16日周四 下午9:30写道：&#010;&#010;&gt; 我在使用flink 1.11.0中得ddl 部分 采用debezium-json做cdc得时候&#010;&gt; 从checkpoint恢复以后,新来op=d的数据会删除失败&#010;&gt; 重启命令:./bin/flink run -m yarn-cluster  /root/bigdata-flink-1.0.jar -s&#010;&gt;&#010;&gt; hdfs://prehadoop01:8020/flink/checkpoints/4cc5df8b96e90c1c2a4d3719a77f51d1/chk-819/_metadata&#010;&gt; 代码:   EnvironmentSettings settings = EnvironmentSettings.newInstance()&#010;&gt;                 .useBlinkPlanner()&#010;&gt;                 .inStreamingMode()&#010;&gt;                 .build();&#010;&gt;&#010;&gt;         StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&#010;&gt;         env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#010;&gt;         env.getCheckpointConfig().setCheckpointTimeout(6000L); // 超时时间&#010;&gt;         env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); //&#010;&gt; 最大允许同时出现几个CheckPoint&#010;&gt;         env.getCheckpointConfig().setMinPauseBetweenCheckpoints(10L); //&#010;&gt; 最小得间隔时间&#010;&gt;         env.getCheckpointConfig().setPreferCheckpointForRecovery(true); //&#010;&gt; 是否倾向于用CheckPoint做故障恢复&#010;&gt;         env.getCheckpointConfig().setTolerableCheckpointFailureNumber(1);&#010;&gt; //&#010;&gt; 容忍多少次CheckPoint失败&#010;&gt;         //Checkpoint文件清理策略&#010;&gt;&#010;&gt;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;&gt;         //Checkpoint外部文件路径&#010;&gt;         env.setStateBackend(new FsStateBackend(new&#010;&gt; URI(\"hdfs://172.22.20.205:8020/flink/checkpoints\"), false));&#010;&gt; TimeUnit.MINUTES), Time.of(10, TimeUnit.SECONDS)));&#010;&gt;         StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#010;&gt; settings);&#010;&gt;         String sourceDDL = String.format(&#010;&gt;                 \"CREATE TABLE debezium_source (\" +&#010;&gt;                         \" id INT NOT NULL,\" +&#010;&gt;                         \" name STRING,\" +&#010;&gt;                         \" description STRING,\" +&#010;&gt;                         \" weight Double\" +&#010;&gt;                         \") WITH (\" +&#010;&gt;                         \" 'connector' = 'kafka-0.11',\" +&#010;&gt;                         \" 'topic' = '%s',\" +&#010;&gt;                         \" 'properties.bootstrap.servers' = '%s',\" +&#010;&gt;                         \" 'scan.startup.mode' = 'group-offsets',\" +&#010;&gt;                         \" 'format' = 'debezium-json'\" +&#010;&gt;                         \")\", \"ddd\", \" 172.22.20.206:9092\");&#010;&gt;         String sinkDDL = \"CREATE TABLE sink (\" +&#010;&gt;                 \" id INT NOT NULL,\" +&#010;&gt;                 \" name STRING,\" +&#010;&gt;                 \" description STRING,\" +&#010;&gt;                 \" weight Double,\" +&#010;&gt;                 \" PRIMARY KEY (id,name, description,weight) NOT ENFORCED \"&#010;&gt; +&#010;&gt;                 \") WITH (\" +&#010;&gt;                 \" 'connector' = 'jdbc',\" +&#010;&gt;                 \" 'url' =&#010;&gt; 'jdbc:mysql://172.27.4.22:3306/test?autoReconnect=true',\" +&#010;&gt;                 \" 'table-name' = 'products',\" +&#010;&gt;                 \" 'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;&gt;                 \" 'username'='DataPip',\" +&#010;&gt;                 \" 'password'='DataPip'\" +&#010;&gt;                 \")\";&#010;&gt;         String dml = \"INSERT INTO sink SELECT  id,name ,description, weight&#010;&gt; FROM debezium_source GROUP BY id,name ,description, weight\";&#010;&gt;         tEnv.executeSql(sourceDDL);&#010;&gt;         tEnv.executeSql(sinkDDL);&#010;&gt;         tEnv.executeSql(dml);&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<1594901056880-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO930paSUy0dTSocZO6O7qZ-CdC9EX58Mf3sENupCw7FVfDg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:39:03 GMT",
        "subject": "Re: flink 1.11 checkpoint使用",
        "content": "Hi,&#010;&#010;能确认一下 kafka 中有完整的全量数据吗？ 也就是 这个 DELETE 消息之前，有对应的&#010;INSERT 消息吗？&#010;如果没有的话，是可能会发生这个现象的（DELETE 在 group by 节点会被认为脏数据而丢掉）。&#010;当然也可以像 godfrey 建议的那样，不 groupby，直接全部字段 INSERT INTO&#010;sink，DELETE 就不会被丢弃掉。&#010;&#010;Best,&#010;Jark&#010;&#010;On Thu, 16 Jul 2020 at 21:56, godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#010;&#010;&gt; 为什么要 GROUP BY id,name ,description, weight ？&#010;&gt; 直接 \"INSERT INTO sink SELECT  id,name ,description, weight FROM&#010;&gt; debezium_source\" 不能满足需求？&#010;&gt;&#010;&gt; 曹武 &lt;14701319164@163.com&gt; 于2020年7月16日周四 下午9:30写道：&#010;&gt;&#010;&gt; &gt; 我在使用flink 1.11.0中得ddl 部分 采用debezium-json做cdc得时候&#010;&gt; &gt; 从checkpoint恢复以后,新来op=d的数据会删除失败&#010;&gt; &gt; 重启命令:./bin/flink run -m yarn-cluster  /root/bigdata-flink-1.0.jar -s&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; hdfs://prehadoop01:8020/flink/checkpoints/4cc5df8b96e90c1c2a4d3719a77f51d1/chk-819/_metadata&#010;&gt; &gt; 代码:   EnvironmentSettings settings = EnvironmentSettings.newInstance()&#010;&gt; &gt;                 .useBlinkPlanner()&#010;&gt; &gt;                 .inStreamingMode()&#010;&gt; &gt;                 .build();&#010;&gt; &gt;&#010;&gt; &gt;         StreamExecutionEnvironment env =&#010;&gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt;&#010;&gt; &gt;         env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &gt;         env.getCheckpointConfig().setCheckpointTimeout(6000L); // 超时时间&#010;&gt; &gt;         env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); //&#010;&gt; &gt; 最大允许同时出现几个CheckPoint&#010;&gt; &gt;         env.getCheckpointConfig().setMinPauseBetweenCheckpoints(10L); //&#010;&gt; &gt; 最小得间隔时间&#010;&gt; &gt;         env.getCheckpointConfig().setPreferCheckpointForRecovery(true);&#010;&gt; //&#010;&gt; &gt; 是否倾向于用CheckPoint做故障恢复&#010;&gt; &gt;         env.getCheckpointConfig().setTolerableCheckpointFailureNumber(1);&#010;&gt; &gt; //&#010;&gt; &gt; 容忍多少次CheckPoint失败&#010;&gt; &gt;         //Checkpoint文件清理策略&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;&gt; &gt;         //Checkpoint外部文件路径&#010;&gt; &gt;         env.setStateBackend(new FsStateBackend(new&#010;&gt; &gt; URI(\"hdfs://172.22.20.205:8020/flink/checkpoints\"), false));&#010;&gt; &gt; TimeUnit.MINUTES), Time.of(10, TimeUnit.SECONDS)));&#010;&gt; &gt;         StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#010;&gt; &gt; settings);&#010;&gt; &gt;         String sourceDDL = String.format(&#010;&gt; &gt;                 \"CREATE TABLE debezium_source (\" +&#010;&gt; &gt;                         \" id INT NOT NULL,\" +&#010;&gt; &gt;                         \" name STRING,\" +&#010;&gt; &gt;                         \" description STRING,\" +&#010;&gt; &gt;                         \" weight Double\" +&#010;&gt; &gt;                         \") WITH (\" +&#010;&gt; &gt;                         \" 'connector' = 'kafka-0.11',\" +&#010;&gt; &gt;                         \" 'topic' = '%s',\" +&#010;&gt; &gt;                         \" 'properties.bootstrap.servers' = '%s',\" +&#010;&gt; &gt;                         \" 'scan.startup.mode' = 'group-offsets',\" +&#010;&gt; &gt;                         \" 'format' = 'debezium-json'\" +&#010;&gt; &gt;                         \")\", \"ddd\", \" 172.22.20.206:9092\");&#010;&gt; &gt;         String sinkDDL = \"CREATE TABLE sink (\" +&#010;&gt; &gt;                 \" id INT NOT NULL,\" +&#010;&gt; &gt;                 \" name STRING,\" +&#010;&gt; &gt;                 \" description STRING,\" +&#010;&gt; &gt;                 \" weight Double,\" +&#010;&gt; &gt;                 \" PRIMARY KEY (id,name, description,weight) NOT ENFORCED&#010;&gt; \"&#010;&gt; &gt; +&#010;&gt; &gt;                 \") WITH (\" +&#010;&gt; &gt;                 \" 'connector' = 'jdbc',\" +&#010;&gt; &gt;                 \" 'url' =&#010;&gt; &gt; 'jdbc:mysql://172.27.4.22:3306/test?autoReconnect=true',\" +&#010;&gt; &gt;                 \" 'table-name' = 'products',\" +&#010;&gt; &gt;                 \" 'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;&gt; &gt;                 \" 'username'='DataPip',\" +&#010;&gt; &gt;                 \" 'password'='DataPip'\" +&#010;&gt; &gt;                 \")\";&#010;&gt; &gt;         String dml = \"INSERT INTO sink SELECT  id,name ,description,&#010;&gt; weight&#010;&gt; &gt; FROM debezium_source GROUP BY id,name ,description, weight\";&#010;&gt; &gt;         tEnv.executeSql(sourceDDL);&#010;&gt; &gt;         tEnv.executeSql(sinkDDL);&#010;&gt; &gt;         tEnv.executeSql(dml);&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<1594901056880-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594976934452-0.post@n8.nabble.com>",
        "from": "曹武 &lt;14701319...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:08:54 GMT",
        "subject": "Re: flink 1.11 checkpoint使用",
        "content": "如果去掉group by会抛出异常,请问有没有关这个异常的解决方式:&#010;Exception in thread \"main\" org.apache.flink.table.api.TableException:&#010;Provided trait [BEFORE_AND_AFTER] can't satisfy required trait&#010;[ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue.&#010;Current node is TableSourceScan(table=[[default_catalog, default_database,&#010;ddd",
        "depth": "3",
        "reply": "<1594901056880-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594976982887-0.post@n8.nabble.com>",
        "from": "曹武 &lt;14701319...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:09:42 GMT",
        "subject": "Re: flink 1.11 checkpoint使用",
        "content": "如果去掉group by会抛出异常,请问有没有关这个异常的解决方式:&#010;Exception in thread \"main\" org.apache.flink.table.api.TableException:&#010;Provided trait [BEFORE_AND_AFTER] can't satisfy required trait&#010;[ONLY_UPDATE_AFTER]. This is a bug in planner, please file an issue.&#010;Current node is TableSourceScan(table=[[default_catalog, default_database,&#010;ddd",
        "depth": "2",
        "reply": "<1594901056880-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594977167653-0.post@n8.nabble.com>",
        "from": "曹武 &lt;14701319...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:12:47 GMT",
        "subject": "Re: flink 1.11 checkpoint使用",
        "content": "感觉好像是应为从checkpoint启动失败或者是checkpiont文件里面不包含groupby的中间结果,这个怎么排查呀!&#010;&#010;godfrey he wrote&#010;&gt; 为什么要 GROUP BY id,name ,description, weight ？&#010;&gt; 直接 \"INSERT INTO sink SELECT  id,name ,description, weight FROM&#010;&gt; debezium_source\" 不能满足需求？&#010;&gt; &#010;&gt; 曹武 &lt;&#010;&#010;&gt; 14701319164@&#010;&#010;&gt;&gt; 于2020年7月16日周四 下午9:30写道：&#010;&gt; &#010;&gt;&gt; 我在使用flink 1.11.0中得ddl 部分 采用debezium-json做cdc得时候&#010;&gt;&gt; 从checkpoint恢复以后,新来op=d的数据会删除失败&#010;&gt;&gt; 重启命令:./bin/flink run -m yarn-cluster  /root/bigdata-flink-1.0.jar -s&#010;&gt;&gt;&#010;&gt;&gt; hdfs://prehadoop01:8020/flink/checkpoints/4cc5df8b96e90c1c2a4d3719a77f51d1/chk-819/_metadata&#010;&gt;&gt; 代码:   EnvironmentSettings settings = EnvironmentSettings.newInstance()&#010;&gt;&gt;                 .useBlinkPlanner()&#010;&gt;&gt;                 .inStreamingMode()&#010;&gt;&gt;                 .build();&#010;&gt;&gt;&#010;&gt;&gt;         StreamExecutionEnvironment env =&#010;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;&#010;&gt;&gt;         env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#010;&gt;&gt;         env.getCheckpointConfig().setCheckpointTimeout(6000L); // 超时时间&#010;&gt;&gt;         env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); //&#010;&gt;&gt; 最大允许同时出现几个CheckPoint&#010;&gt;&gt;         env.getCheckpointConfig().setMinPauseBetweenCheckpoints(10L); //&#010;&gt;&gt; 最小得间隔时间&#010;&gt;&gt;         env.getCheckpointConfig().setPreferCheckpointForRecovery(true);&#010;&gt;&gt; //&#010;&gt;&gt; 是否倾向于用CheckPoint做故障恢复&#010;&gt;&gt;         env.getCheckpointConfig().setTolerableCheckpointFailureNumber(1);&#010;&gt;&gt; //&#010;&gt;&gt; 容忍多少次CheckPoint失败&#010;&gt;&gt;         //Checkpoint文件清理策略&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;&gt;&gt;         //Checkpoint外部文件路径&#010;&gt;&gt;         env.setStateBackend(new FsStateBackend(new&#010;&gt;&gt; URI(\"hdfs://172.22.20.205:8020/flink/checkpoints\"), false));&#010;&gt;&gt; TimeUnit.MINUTES), Time.of(10, TimeUnit.SECONDS)));&#010;&gt;&gt;         StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#010;&gt;&gt; settings);&#010;&gt;&gt;         String sourceDDL = String.format(&#010;&gt;&gt;                 \"CREATE TABLE debezium_source (\" +&#010;&gt;&gt;                         \" id INT NOT NULL,\" +&#010;&gt;&gt;                         \" name STRING,\" +&#010;&gt;&gt;                         \" description STRING,\" +&#010;&gt;&gt;                         \" weight Double\" +&#010;&gt;&gt;                         \") WITH (\" +&#010;&gt;&gt;                         \" 'connector' = 'kafka-0.11',\" +&#010;&gt;&gt;                         \" 'topic' = '%s',\" +&#010;&gt;&gt;                         \" 'properties.bootstrap.servers' = '%s',\" +&#010;&gt;&gt;                         \" 'scan.startup.mode' = 'group-offsets',\" +&#010;&gt;&gt;                         \" 'format' = 'debezium-json'\" +&#010;&gt;&gt;                         \")\", \"ddd\", \" 172.22.20.206:9092\");&#010;&gt;&gt;         String sinkDDL = \"CREATE TABLE sink (\" +&#010;&gt;&gt;                 \" id INT NOT NULL,\" +&#010;&gt;&gt;                 \" name STRING,\" +&#010;&gt;&gt;                 \" description STRING,\" +&#010;&gt;&gt;                 \" weight Double,\" +&#010;&gt;&gt;                 \" PRIMARY KEY (id,name, description,weight) NOT ENFORCED&#010;&gt;&gt; \"&#010;&gt;&gt; +&#010;&gt;&gt;                 \") WITH (\" +&#010;&gt;&gt;                 \" 'connector' = 'jdbc',\" +&#010;&gt;&gt;                 \" 'url' =&#010;&gt;&gt; 'jdbc:mysql://172.27.4.22:3306/test?autoReconnect=true',\" +&#010;&gt;&gt;                 \" 'table-name' = 'products',\" +&#010;&gt;&gt;                 \" 'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;&gt;&gt;                 \" 'username'='DataPip',\" +&#010;&gt;&gt;                 \" 'password'='DataPip'\" +&#010;&gt;&gt;                 \")\";&#010;&gt;&gt;         String dml = \"INSERT INTO sink SELECT  id,name ,description,&#010;&gt;&gt; weight&#010;&gt;&gt; FROM debezium_source GROUP BY id,name ,description, weight\";&#010;&gt;&gt;         tEnv.executeSql(sourceDDL);&#010;&gt;&gt;         tEnv.executeSql(sinkDDL);&#010;&gt;&gt;         tEnv.executeSql(dml);&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt;&#010;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "2",
        "reply": "<1594901056880-0.post@n8.nabble.com>"
    },
    {
        "id": "<30C72674-A29D-46AE-AE12-CBE134F1D02B@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 11:37:13 GMT",
        "subject": "Re: flink 1.11 checkpoint使用",
        "content": "Hi， 曹武&#010;&#010;这是一个已知bug，这个在1.11.1和1.12.0里已经修复，&#010;&#010;如果着急使用，可以自己编译下release-1.11分支。&#010;&#010;祝好 &#010;Leonard Xu&#010;&#010;https://issues.apache.org/jira/browse/FLINK-18461 &lt;https://issues.apache.org/jira/browse/FLINK-18461&gt;&#010;&#010;&gt; 在 2020年7月17日，17:12，曹武 &lt;14701319164@163.com&gt; 写道：&#010;&gt; &#010;&gt; 感觉好像是应为从checkpoint启动失败或者是checkpiont文件里面不包含groupby的中间结果,这个怎么排查呀!&#010;&gt; &#010;&gt; godfrey he wrote&#010;&gt;&gt; 为什么要 GROUP BY id,name ,description, weight ？&#010;&gt;&gt; 直接 \"INSERT INTO sink SELECT  id,name ,description, weight FROM&#010;&gt;&gt; debezium_source\" 不能满足需求？&#010;&gt;&gt; &#010;&gt;&gt; 曹武 &lt;&#010;&gt; &#010;&gt;&gt; 14701319164@&#010;&gt; &#010;&gt;&gt;&gt; 于2020年7月16日周四 下午9:30写道：&#010;&gt;&gt; &#010;&gt;&gt;&gt; 我在使用flink 1.11.0中得ddl 部分 采用debezium-json做cdc得时候&#010;&gt;&gt;&gt; 从checkpoint恢复以后,新来op=d的数据会删除失败&#010;&gt;&gt;&gt; 重启命令:./bin/flink run -m yarn-cluster  /root/bigdata-flink-1.0.jar -s&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; hdfs://prehadoop01:8020/flink/checkpoints/4cc5df8b96e90c1c2a4d3719a77f51d1/chk-819/_metadata&#010;&gt;&gt;&gt; 代码:   EnvironmentSettings settings = EnvironmentSettings.newInstance()&#010;&gt;&gt;&gt;                .useBlinkPlanner()&#010;&gt;&gt;&gt;                .inStreamingMode()&#010;&gt;&gt;&gt;                .build();&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        StreamExecutionEnvironment env =&#010;&gt;&gt;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;        env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#010;&gt;&gt;&gt;        env.getCheckpointConfig().setCheckpointTimeout(6000L); // 超时时间&#010;&gt;&gt;&gt;        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); //&#010;&gt;&gt;&gt; 最大允许同时出现几个CheckPoint&#010;&gt;&gt;&gt;        env.getCheckpointConfig().setMinPauseBetweenCheckpoints(10L); //&#010;&gt;&gt;&gt; 最小得间隔时间&#010;&gt;&gt;&gt;        env.getCheckpointConfig().setPreferCheckpointForRecovery(true);&#010;&gt;&gt;&gt; //&#010;&gt;&gt;&gt; 是否倾向于用CheckPoint做故障恢复&#010;&gt;&gt;&gt;        env.getCheckpointConfig().setTolerableCheckpointFailureNumber(1);&#010;&gt;&gt;&gt; //&#010;&gt;&gt;&gt; 容忍多少次CheckPoint失败&#010;&gt;&gt;&gt;        //Checkpoint文件清理策略&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;&gt;&gt;&gt;        //Checkpoint外部文件路径&#010;&gt;&gt;&gt;        env.setStateBackend(new FsStateBackend(new&#010;&gt;&gt;&gt; URI(\"hdfs://172.22.20.205:8020/flink/checkpoints\"), false));&#010;&gt;&gt;&gt; TimeUnit.MINUTES), Time.of(10, TimeUnit.SECONDS)));&#010;&gt;&gt;&gt;        StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#010;&gt;&gt;&gt; settings);&#010;&gt;&gt;&gt;        String sourceDDL = String.format(&#010;&gt;&gt;&gt;                \"CREATE TABLE debezium_source (\" +&#010;&gt;&gt;&gt;                        \" id INT NOT NULL,\" +&#010;&gt;&gt;&gt;                        \" name STRING,\" +&#010;&gt;&gt;&gt;                        \" description STRING,\" +&#010;&gt;&gt;&gt;                        \" weight Double\" +&#010;&gt;&gt;&gt;                        \") WITH (\" +&#010;&gt;&gt;&gt;                        \" 'connector' = 'kafka-0.11',\" +&#010;&gt;&gt;&gt;                        \" 'topic' = '%s',\" +&#010;&gt;&gt;&gt;                        \" 'properties.bootstrap.servers' = '%s',\" +&#010;&gt;&gt;&gt;                        \" 'scan.startup.mode' = 'group-offsets',\" +&#010;&gt;&gt;&gt;                        \" 'format' = 'debezium-json'\" +&#010;&gt;&gt;&gt;                        \")\", \"ddd\", \" 172.22.20.206:9092\");&#010;&gt;&gt;&gt;        String sinkDDL = \"CREATE TABLE sink (\" +&#010;&gt;&gt;&gt;                \" id INT NOT NULL,\" +&#010;&gt;&gt;&gt;                \" name STRING,\" +&#010;&gt;&gt;&gt;                \" description STRING,\" +&#010;&gt;&gt;&gt;                \" weight Double,\" +&#010;&gt;&gt;&gt;                \" PRIMARY KEY (id,name, description,weight) NOT ENFORCED&#010;&gt;&gt;&gt; \"&#010;&gt;&gt;&gt; +&#010;&gt;&gt;&gt;                \") WITH (\" +&#010;&gt;&gt;&gt;                \" 'connector' = 'jdbc',\" +&#010;&gt;&gt;&gt;                \" 'url' =&#010;&gt;&gt;&gt; 'jdbc:mysql://172.27.4.22:3306/test?autoReconnect=true',\" +&#010;&gt;&gt;&gt;                \" 'table-name' = 'products',\" +&#010;&gt;&gt;&gt;                \" 'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;&gt;&gt;&gt;                \" 'username'='DataPip',\" +&#010;&gt;&gt;&gt;                \" 'password'='DataPip'\" +&#010;&gt;&gt;&gt;                \")\";&#010;&gt;&gt;&gt;        String dml = \"INSERT INTO sink SELECT  id,name ,description,&#010;&gt;&gt;&gt; weight&#010;&gt;&gt;&gt; FROM debezium_source GROUP BY id,name ,description, weight\";&#010;&gt;&gt;&gt;        tEnv.executeSql(sourceDDL);&#010;&gt;&gt;&gt;        tEnv.executeSql(sinkDDL);&#010;&gt;&gt;&gt;        tEnv.executeSql(dml);&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; --&#010;&gt;&gt;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&gt;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/ &lt;http://apache-flink.147419.n8.nabble.com/&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<1594901056880-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_B3C761B6785686327061644D99A78E74EC05@qq.com>",
        "from": "&quot;沈阳&quot; &lt;122969...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 14:46:02 GMT",
        "subject": "退订",
        "content": "退订",
        "depth": "0",
        "reply": "<tencent_B3C761B6785686327061644D99A78E74EC05@qq.com>"
    },
    {
        "id": "<9CB39241-6082-4AE1-8E09-40525F71C4A0@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 16:12:57 GMT",
        "subject": "Re: 退订",
        "content": "Hi,&#010;是指取消订阅邮件吗？&#010;可以发送任意内容的邮件到  user-zh-unsubscribe@flink.apache.org &lt;mailto:user-zh-unsubscribe@flink.apache.org&gt;&#010; 取消订阅来自 user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt; 邮件组的邮件&#010;&#010;邮件组的订阅管理，可以参考[1]&#010;&#010;祝好，&#010;Leonard Xu&#010;https://flink.apache.org/community.html#how-to-subscribe-to-a-mailing-list &lt;https://flink.apache.org/community.html#how-to-subscribe-to-a-mailing-list&gt;&#010;&#010;&gt; 在 2020年7月16日，22:46，沈阳 &lt;122969720@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 退订&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_B3C761B6785686327061644D99A78E74EC05@qq.com>"
    },
    {
        "id": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 15:08:03 GMT",
        "subject": "flink 1.11任务提交的问题",
        "content": "hi，&#010;请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#010;通过StreamExecutionEnvironment.execute提交，yarn per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。",
        "depth": "0",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<001B4BE4-EB68-4CB2-BCC6-B2703C8EDA74@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 16:12:14 GMT",
        "subject": "Re: flink 1.11任务提交的问题",
        "content": "Hi，&#013;&#010;&#013;&#010;我理解目前好像做不到， cc: godfrey 大佬看看&#013;&#010;&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&gt; 在 2020年7月16日，23:08，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; hi，&#013;&#010;&gt; 请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#013;&#010;&gt; 通过StreamExecutionEnvironment.execute提交，yarn per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CADQYLGsGhTzbE-Ug=mVMPEVkt=Wv-MYyL+ErKT47+=nT0nd8Mw@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 04:09:20 GMT",
        "subject": "Re: flink 1.11任务提交的问题",
        "content": "hi sunfulin,&#013;&#010;目前这个做不到。executeSQL 和 table to DataStream 是分别优化和提交作业的。&#013;&#010;即使在1.11 之前，table to DataStream 也不会和 sqlUpdate 或者 insertInto 的语句一起优化，&#013;&#010;虽然只提交了一个job，但是是两个独立的pipeline，也没有计算复用，和两个job没啥差别。&#013;&#010;&#013;&#010;Best，&#013;&#010;Godfrey&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月17日周五 上午12:12写道：&#013;&#010;&#013;&#010;&gt; Hi，&#013;&#010;&gt;&#013;&#010;&gt; 我理解目前好像做不到， cc: godfrey 大佬看看&#013;&#010;&gt;&#013;&#010;&gt; 祝好，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月16日，23:08，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hi，&#013;&#010;&gt; &gt; 请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql&#013;&#010;&gt; dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#013;&#010;&gt; &gt; 通过StreamExecutionEnvironment.execute提交，yarn&#013;&#010;&gt; per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<74065d5.3c7f.1735b5768ef.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 05:55:21 GMT",
        "subject": "Re:Re: flink 1.11任务提交的问题",
        "content": "&#010;&#010;&#010;hi,&#010;感谢回复。这个机制我理解了。想了解下，有办法在1.11里仍然使用1.10版本的作业提交机制么？我现在虽然把代码回滚到1.10版本的逻辑，但是提交作业仍然有问题：比如我如果不执行env.execute，那么table&#010;to DataStream的语句不会生成拓扑。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-17 12:09:20，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#010;&gt;hi sunfulin,&#010;&gt;目前这个做不到。executeSQL 和 table to DataStream 是分别优化和提交作业的。&#010;&gt;即使在1.11 之前，table to DataStream 也不会和 sqlUpdate 或者 insertInto 的语句一起优化，&#010;&gt;虽然只提交了一个job，但是是两个独立的pipeline，也没有计算复用，和两个job没啥差别。&#010;&gt;&#010;&gt;Best，&#010;&gt;Godfrey&#010;&gt;&#010;&gt;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月17日周五 上午12:12写道：&#010;&gt;&#010;&gt;&gt; Hi，&#010;&gt;&gt;&#010;&gt;&gt; 我理解目前好像做不到， cc: godfrey 大佬看看&#010;&gt;&gt;&#010;&gt;&gt; 祝好，&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt;&#010;&gt;&gt; &gt; 在 2020年7月16日，23:08，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; hi，&#010;&gt;&gt; &gt; 请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql&#010;&gt;&gt; dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#010;&gt;&gt; &gt; 通过StreamExecutionEnvironment.execute提交，yarn&#010;&gt;&gt; per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "3",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<788cf01f.3d22.1735b5bf659.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 06:00:20 GMT",
        "subject": "Re:Re:Re: flink 1.11任务提交的问题",
        "content": "hi，&#010;补充一下，1.10版本的代码使用sqlUpdate + table2datastream，并通过StreamExecutionEnvironment.execute来提交。我回滚到1.10版本的代码后，因为我看1.11版本里如果使用sqlUpdate执行insertInto，必须使用StreamTableEnvironment.execute来提交。现在我的问题就是这个：我想通过一个job来提交。现在有机制可以做不？在1.11版本里执行。因为之前的job逻辑较为复杂，做拆分的话还有点麻烦。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-17 13:55:21，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&#010;&#010;&#010;&#010;hi,&#010;感谢回复。这个机制我理解了。想了解下，有办法在1.11里仍然使用1.10版本的作业提交机制么？我现在虽然把代码回滚到1.10版本的逻辑，但是提交作业仍然有问题：比如我如果不执行env.execute，那么table&#010;to DataStream的语句不会生成拓扑。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-17 12:09:20，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#010;&gt;hi sunfulin,&#010;&gt;目前这个做不到。executeSQL 和 table to DataStream 是分别优化和提交作业的。&#010;&gt;即使在1.11 之前，table to DataStream 也不会和 sqlUpdate 或者 insertInto 的语句一起优化，&#010;&gt;虽然只提交了一个job，但是是两个独立的pipeline，也没有计算复用，和两个job没啥差别。&#010;&gt;&#010;&gt;Best，&#010;&gt;Godfrey&#010;&gt;&#010;&gt;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月17日周五 上午12:12写道：&#010;&gt;&#010;&gt;&gt; Hi，&#010;&gt;&gt;&#010;&gt;&gt; 我理解目前好像做不到， cc: godfrey 大佬看看&#010;&gt;&gt;&#010;&gt;&gt; 祝好，&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt;&#010;&gt;&gt; &gt; 在 2020年7月16日，23:08，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; hi，&#010;&gt;&gt; &gt; 请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql&#010;&gt;&gt; dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#010;&gt;&gt; &gt; 通过StreamExecutionEnvironment.execute提交，yarn&#010;&gt;&gt; per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。&#010;&gt;&gt;&#010;&gt;&gt;&#010;&#010;&#010;&#010;&#010;&#010; ",
        "depth": "4",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CADQYLGsn_ebb=ebUMqpBwmxae=jdMZs19MtQtKOmtKNrn1oFrA@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 06:36:19 GMT",
        "subject": "Re: Re:Re: flink 1.11任务提交的问题",
        "content": "做不到，1.11里把 StreamExecutionEnvironment.execute 和&#013;&#010;StreamTableEnvironment.execute 的逻辑已经切分干净了。&#013;&#010;有个改动比较小的方案可以参考：可以在原来的逻辑的基础上，把两种提交job的方式放到两个不同的类中，其他的逻辑放到另外一个类共性。&#013;&#010;&#013;&#010;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月17日周五 下午2:00写道：&#013;&#010;&#013;&#010;&gt; hi，&#013;&#010;&gt; 补充一下，1.10版本的代码使用sqlUpdate +&#013;&#010;&gt; table2datastream，并通过StreamExecutionEnvironment.execute来提交。我回滚到1.10版本的代码后，因为我看1.11版本里如果使用sqlUpdate执行insertInto，必须使用StreamTableEnvironment.execute来提交。现在我的问题就是这个：我想通过一个job来提交。现在有机制可以做不？在1.11版本里执行。因为之前的job逻辑较为复杂，做拆分的话还有点麻烦。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-17 13:55:21，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi,&#013;&#010;&gt; 感谢回复。这个机制我理解了。想了解下，有办法在1.11里仍然使用1.10版本的作业提交机制么？我现在虽然把代码回滚到1.10版本的逻辑，但是提交作业仍然有问题：比如我如果不执行env.execute，那么table&#013;&#010;&gt; to DataStream的语句不会生成拓扑。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-17 12:09:20，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;hi sunfulin,&#013;&#010;&gt; &gt;目前这个做不到。executeSQL 和 table to DataStream 是分别优化和提交作业的。&#013;&#010;&gt; &gt;即使在1.11 之前，table to DataStream 也不会和 sqlUpdate 或者 insertInto&#010;的语句一起优化，&#013;&#010;&gt; &gt;虽然只提交了一个job，但是是两个独立的pipeline，也没有计算复用，和两个job没啥差别。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best，&#013;&#010;&gt; &gt;Godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月17日周五 上午12:12写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 我理解目前好像做不到， cc: godfrey 大佬看看&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 祝好，&#013;&#010;&gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt; 在 2020年7月16日，23:08，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; hi，&#013;&#010;&gt; &gt;&gt; &gt; 请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql&#013;&#010;&gt; &gt;&gt; dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#013;&#010;&gt; &gt;&gt; &gt; 通过StreamExecutionEnvironment.execute提交，yarn&#013;&#010;&gt; &gt;&gt; per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<7ebbc904.4c00.1735b9cc25c.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 07:11:06 GMT",
        "subject": "Re:Re: Re:Re: flink 1.11任务提交的问题",
        "content": "&#010;&#010;&#010;hi,&#010;再问下，这个方案还是会提交两个job吧？&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-17 14:36:19，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#010;&gt;做不到，1.11里把 StreamExecutionEnvironment.execute 和&#010;&gt;StreamTableEnvironment.execute 的逻辑已经切分干净了。&#010;&gt;有个改动比较小的方案可以参考：可以在原来的逻辑的基础上，把两种提交job的方式放到两个不同的类中，其他的逻辑放到另外一个类共性。&#010;&gt;&#010;&gt;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月17日周五 下午2:00写道：&#010;&gt;&#010;&gt;&gt; hi，&#010;&gt;&gt; 补充一下，1.10版本的代码使用sqlUpdate +&#010;&gt;&gt; table2datastream，并通过StreamExecutionEnvironment.execute来提交。我回滚到1.10版本的代码后，因为我看1.11版本里如果使用sqlUpdate执行insertInto，必须使用StreamTableEnvironment.execute来提交。现在我的问题就是这个：我想通过一个job来提交。现在有机制可以做不？在1.11版本里执行。因为之前的job逻辑较为复杂，做拆分的话还有点麻烦。&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-17 13:55:21，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; hi,&#010;&gt;&gt; 感谢回复。这个机制我理解了。想了解下，有办法在1.11里仍然使用1.10版本的作业提交机制么？我现在虽然把代码回滚到1.10版本的逻辑，但是提交作业仍然有问题：比如我如果不执行env.execute，那么table&#010;&gt;&gt; to DataStream的语句不会生成拓扑。&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 在 2020-07-17 12:09:20，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#010;&gt;&gt; &gt;hi sunfulin,&#010;&gt;&gt; &gt;目前这个做不到。executeSQL 和 table to DataStream 是分别优化和提交作业的。&#010;&gt;&gt; &gt;即使在1.11 之前，table to DataStream 也不会和 sqlUpdate 或者 insertInto&#010;的语句一起优化，&#010;&gt;&gt; &gt;虽然只提交了一个job，但是是两个独立的pipeline，也没有计算复用，和两个job没啥差别。&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Best，&#010;&gt;&gt; &gt;Godfrey&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月17日周五 上午12:12写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; Hi，&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 我理解目前好像做不到， cc: godfrey 大佬看看&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; 祝好，&#010;&gt;&gt; &gt;&gt; Leonard Xu&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt; &gt; 在 2020年7月16日，23:08，sunfulin &lt;sunfulin0321@163.com&gt;&#010;写道：&#010;&gt;&gt; &gt;&gt; &gt;&#010;&gt;&gt; &gt;&gt; &gt; hi，&#010;&gt;&gt; &gt;&gt; &gt; 请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql&#010;&gt;&gt; &gt;&gt; dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#010;&gt;&gt; &gt;&gt; &gt; 通过StreamExecutionEnvironment.execute提交，yarn&#010;&gt;&gt; &gt;&gt; per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt; &gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "6",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CADQYLGvzeftPO08ihp8oL27BfLZL-55LhBSYJ23+7HmzFadRZg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 07:29:48 GMT",
        "subject": "Re: Re: Re:Re: flink 1.11任务提交的问题",
        "content": "是的。目前按照你的写法做不到只提交一个job了&#013;&#010;&#013;&#010;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月17日周五 下午3:11写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi,&#013;&#010;&gt; 再问下，这个方案还是会提交两个job吧？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-17 14:36:19，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;做不到，1.11里把 StreamExecutionEnvironment.execute 和&#013;&#010;&gt; &gt;StreamTableEnvironment.execute 的逻辑已经切分干净了。&#013;&#010;&gt; &gt;有个改动比较小的方案可以参考：可以在原来的逻辑的基础上，把两种提交job的方式放到两个不同的类中，其他的逻辑放到另外一个类共性。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;sunfulin &lt;sunfulin0321@163.com&gt; 于2020年7月17日周五 下午2:00写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; hi，&#013;&#010;&gt; &gt;&gt; 补充一下，1.10版本的代码使用sqlUpdate +&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; table2datastream，并通过StreamExecutionEnvironment.execute来提交。我回滚到1.10版本的代码后，因为我看1.11版本里如果使用sqlUpdate执行insertInto，必须使用StreamTableEnvironment.execute来提交。现在我的问题就是这个：我想通过一个job来提交。现在有机制可以做不？在1.11版本里执行。因为之前的job逻辑较为复杂，做拆分的话还有点麻烦。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在 2020-07-17 13:55:21，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; hi,&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; 感谢回复。这个机制我理解了。想了解下，有办法在1.11里仍然使用1.10版本的作业提交机制么？我现在虽然把代码回滚到1.10版本的逻辑，但是提交作业仍然有问题：比如我如果不执行env.execute，那么table&#013;&#010;&gt; &gt;&gt; to DataStream的语句不会生成拓扑。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在 2020-07-17 12:09:20，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;&gt; &gt;hi sunfulin,&#013;&#010;&gt; &gt;&gt; &gt;目前这个做不到。executeSQL 和 table to DataStream 是分别优化和提交作业的。&#013;&#010;&gt; &gt;&gt; &gt;即使在1.11 之前，table to DataStream 也不会和 sqlUpdate 或者&#010;insertInto 的语句一起优化，&#013;&#010;&gt; &gt;&gt; &gt;虽然只提交了一个job，但是是两个独立的pipeline，也没有计算复用，和两个job没啥差别。&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;Best，&#013;&#010;&gt; &gt;&gt; &gt;Godfrey&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月17日周五 上午12:12写道：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; Hi，&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; 我理解目前好像做不到， cc: godfrey 大佬看看&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; 祝好，&#013;&#010;&gt; &gt;&gt; &gt;&gt; Leonard Xu&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; 在 2020年7月16日，23:08，sunfulin &lt;sunfulin0321@163.com&gt;&#010;写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; hi，&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; 请教下flink 1.11任务提交的问题。如果我的一个作业里既有sql&#013;&#010;&gt; &gt;&gt; &gt;&gt; dml提交（executeSQL执行），又通过DataStream.addSink来写出，&#013;&#010;&gt; &gt;&gt; &gt;&gt; &gt; 通过StreamExecutionEnvironment.execute提交，yarn&#013;&#010;&gt; &gt;&gt; &gt;&gt; per-job貌似会提交两个作业。这种情况下，我该如何处理呢？只想提交一个作业。&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<42395c4a.796e.173582b0e2a.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAEZk042OK8ZcSgkGKB6_y8yzb1Z4znNwksPst2afWDfv3UMJxQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:12:31 GMT",
        "subject": "flink1.9写权限认证的es6",
        "content": "hi：&#013;&#010;请问flink如何将数据写入到权限认证的es集群哪，没找到配置用户名密码的地方，哪位大佬帮忙解答一下。。。。&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk042OK8ZcSgkGKB6_y8yzb1Z4znNwksPst2afWDfv3UMJxQ@mail.gmail.com>"
    },
    {
        "id": "<80371ff2-ea0a-431e-8336-b9e8b38b426a.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:49:48 GMT",
        "subject": "回复：flink1.9写权限认证的es6",
        "content": "你好,请问是FlinkSQL么&#010;FLinkSQL可以参考下这份邮件&#010;http://apache-flink.147419.n8.nabble.com/ddl-es-td2094.html&#010;DataStream可以尝试自定义ElasticsearchSink实现权限认证&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;发送时间：2020年7月17日(星期五) 10:12&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：flink1.9写权限认证的es6&#010;&#010;hi：&#010;请问flink如何将数据写入到权限认证的es集群哪，没找到配置用户名密码的地方，哪位大佬帮忙解答一下。。。。&#010;",
        "depth": "1",
        "reply": "<CAEZk042OK8ZcSgkGKB6_y8yzb1Z4znNwksPst2afWDfv3UMJxQ@mail.gmail.com>"
    },
    {
        "id": "<CAFTKPZpG5PQ-kGZhQrDeX4M-2Ux6dgMagEGouGi=XkMjJRTaSQ@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 05:38:35 GMT",
        "subject": "Re: flink1.9写权限认证的es6",
        "content": "Hi,&#013;&#010;&#013;&#010;SQL添加认证的逻辑已经在FLINK-18361[1] 中完成了，1.12版本会支持这个功能&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18361&#013;&#010;&#013;&#010;Best,&#013;&#010;Yangze Guo&#013;&#010;&#013;&#010;On Fri, Jul 17, 2020 at 10:12 AM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; hi：&#013;&#010;&gt; 请问flink如何将数据写入到权限认证的es集群哪，没找到配置用户名密码的地方，哪位大佬帮忙解答一下。。。。&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk042OK8ZcSgkGKB6_y8yzb1Z4znNwksPst2afWDfv3UMJxQ@mail.gmail.com>"
    },
    {
        "id": "<12099057-288b-4d58-b57d-161b28976755.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 05:57:06 GMT",
        "subject": "回复： flink1.9写权限认证的es6",
        "content": "get到了&#010;&#010;&#010;&#010;&#010;&#010;来自钉钉专属商务邮箱------------------------------------------------------------------&#010;发件人：Yangze Guo&lt;karmagyz@gmail.com&gt;&#010;日　期：2020年07月17日 13:38:35&#010;收件人：user-zh&lt;user-zh@flink.apache.org&gt;&#010;主　题：Re: flink1.9写权限认证的es6&#010;&#010;Hi,&#010;&#010;SQL添加认证的逻辑已经在FLINK-18361[1] 中完成了，1.12版本会支持这个功能&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18361&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Fri, Jul 17, 2020 at 10:12 AM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt;&#010;&gt; hi：&#010;&gt; 请问flink如何将数据写入到权限认证的es集群哪，没找到配置用户名密码的地方，哪位大佬帮忙解答一下。。。。&#010;&#010;",
        "depth": "1",
        "reply": "<CAEZk042OK8ZcSgkGKB6_y8yzb1Z4znNwksPst2afWDfv3UMJxQ@mail.gmail.com>"
    },
    {
        "id": "<fa6b4f0.5c1b.1735aa6962d.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:42:13 GMT",
        "subject": "flink connector formats问题",
        "content": "请问flink可以自定义format吗，目前提供的format必须要进行一次数据过滤为规则数据才行，可不可以自定义format实现自己的数据格式source呢？&#010;目前flink支持的：&#010;| 格式 | 支持的连接器 |&#010;| CSV | Apache Kafka, Filesystem |&#010;| JSON | Apache Kafka, Filesystem, Elasticsearch |&#010;| Apache Avro | Apache Kafka, Filesystem |&#010;| Debezium CDC | Apache Kafka |&#010;| Canal CDC | Apache Kafka |&#010;| Apache Parquet | Filesystem |&#010;| Apache ORC | Filesystem |",
        "depth": "0",
        "reply": "<fa6b4f0.5c1b.1735aa6962d.Coremail.apache22@163.com>"
    },
    {
        "id": "<1da74e7d-4640-49a4-82eb-37fdaf80f0f5.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:47:36 GMT",
        "subject": "回复：flink connector formats问题",
        "content": "你好,这个是可以进行自定义的&#010;参考https://jxeditor.github.io/2020/06/11/FlinkSQL%E8%87%AA%E5%AE%9A%E4%B9%89FORMAT_TYPE/&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：酷酷的浑蛋 &lt;apache22@163.com&gt;&#010;发送时间：2020年7月17日(星期五) 10:42&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：flink connector formats问题&#010;&#010;请问flink可以自定义format吗，目前提供的format必须要进行一次数据过滤为规则数据才行，可不可以自定义format实现自己的数据格式source呢？&#010;目前flink支持的：&#010;| 格式 | 支持的连接器 |&#010;| CSV | Apache Kafka, Filesystem |&#010;| JSON | Apache Kafka, Filesystem, Elasticsearch |&#010;| Apache Avro | Apache Kafka, Filesystem |&#010;| Debezium CDC | Apache Kafka |&#010;| Canal CDC | Apache Kafka |&#010;| Apache Parquet | Filesystem |&#010;| Apache ORC | Filesystem |",
        "depth": "1",
        "reply": "<fa6b4f0.5c1b.1735aa6962d.Coremail.apache22@163.com>"
    },
    {
        "id": "<3bb142b8.5de8.1735ab4862b.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:57:27 GMT",
        "subject": "回复：flink connector formats问题",
        "content": "&#010;&#010;我看您写了'format.type' = ‘custom'，这个custom 是跟哪里关联的呢？ 还是说这里要写类路径？&#010;&#010;&#010;在2020年07月17日 10:47，夏帅&lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;你好,这个是可以进行自定义的&#010;参考https://jxeditor.github.io/2020/06/11/FlinkSQL%E8%87%AA%E5%AE%9A%E4%B9%89FORMAT_TYPE/&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：酷酷的浑蛋 &lt;apache22@163.com&gt;&#010;发送时间：2020年7月17日(星期五) 10:42&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：flink connector formats问题&#010;&#010;请问flink可以自定义format吗，目前提供的format必须要进行一次数据过滤为规则数据才行，可不可以自定义format实现自己的数据格式source呢？&#010;目前flink支持的：&#010;| 格式 | 支持的连接器 |&#010;| CSV | Apache Kafka, Filesystem |&#010;| JSON | Apache Kafka, Filesystem, Elasticsearch |&#010;| Apache Avro | Apache Kafka, Filesystem |&#010;| Debezium CDC | Apache Kafka |&#010;| Canal CDC | Apache Kafka |&#010;| Apache Parquet | Filesystem |&#010;| Apache ORC | Filesystem |",
        "depth": "2",
        "reply": "<fa6b4f0.5c1b.1735aa6962d.Coremail.apache22@163.com>"
    },
    {
        "id": "<23626b48.603a.1735ac73ca0.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 03:17:53 GMT",
        "subject": "回复：flink connector formats问题",
        "content": "找到了，谢谢&#010;&#010;&#010;| |&#010;apache22&#010;|&#010;|&#010;apache22@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;&#010;&#010;在2020年07月17日 10:57，酷酷的浑蛋&lt;apache22@163.com&gt; 写道：&#010;&#010;&#010;我看您写了'format.type' = ‘custom'，这个custom 是跟哪里关联的呢？ 还是说这里要写类路径？&#010;&#010;&#010;在2020年07月17日 10:47，夏帅&lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;你好,这个是可以进行自定义的&#010;参考https://jxeditor.github.io/2020/06/11/FlinkSQL%E8%87%AA%E5%AE%9A%E4%B9%89FORMAT_TYPE/&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：酷酷的浑蛋 &lt;apache22@163.com&gt;&#010;发送时间：2020年7月17日(星期五) 10:42&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;&#010;主　题：flink connector formats问题&#010;&#010;请问flink可以自定义format吗，目前提供的format必须要进行一次数据过滤为规则数据才行，可不可以自定义format实现自己的数据格式source呢？&#010;目前flink支持的：&#010;| 格式 | 支持的连接器 |&#010;| CSV | Apache Kafka, Filesystem |&#010;| JSON | Apache Kafka, Filesystem, Elasticsearch |&#010;| Apache Avro | Apache Kafka, Filesystem |&#010;| Debezium CDC | Apache Kafka |&#010;| Canal CDC | Apache Kafka |&#010;| Apache Parquet | Filesystem |&#010;| Apache ORC | Filesystem |",
        "depth": "3",
        "reply": "<fa6b4f0.5c1b.1735aa6962d.Coremail.apache22@163.com>"
    },
    {
        "id": "<202007171301599643296@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 05:02:00 GMT",
        "subject": "flink-1.11  KafkaDynamicTableSouce groupBy 结果怎样发送到 kafka",
        "content": "&#013;&#010; INSERT INTO kafka_dws_artemis_out_order select warehouse_id, count(*) from kafka_ods_artemis_out_order&#010;group by warehouse_id;&#013;&#010;[ERROR] Could not execute SQL statement. Reason:&#013;&#010;org.apache.flink.table.api.TableException: Table sink 'myhive.wanglei.kafka_dws_artemis_out_order'&#010;doesn't support consuming update changes which is produced by node GroupAggregate(groupBy=[warehouse_id],&#010;select=[warehouse_id, COUNT(*) AS EXPR$1])&#013;&#010;&#013;&#010;在 Flink-1.10 中可以更改 KafkaTableSinkBase 让它 implements RetractStream 实现。&#013;&#010; &#013;&#010;我看现在 Flink-1.11 中是用了  KafkaDynamicSource， KafkaDynamicSink，这样怎样改动才能让&#010;GroupBy 的结果也发送到 Kafka 呢？&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊 &#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<202007171301599643296@geekplus.com.cn>"
    },
    {
        "id": "<CABKuJ_Q52Ky4s4zX_FQC_YB_ktwokaeriNjYjXTY+2P3rqD-gQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 12:18:14 GMT",
        "subject": "Re: flink-1.11 KafkaDynamicTableSouce groupBy 结果怎样发送到 kafka",
        "content": "DynamicTableSink有一个方法是getChangelogMode，可以通过这个方法来指定这个sink接收什么种类的数据&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月17日周五 下午1:02写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;  INSERT INTO kafka_dws_artemis_out_order select warehouse_id, count(*)&#013;&#010;&gt; from kafka_ods_artemis_out_order group by warehouse_id;&#013;&#010;&gt; [ERROR] Could not execute SQL statement. Reason:&#013;&#010;&gt; org.apache.flink.table.api.TableException: Table sink&#013;&#010;&gt; 'myhive.wanglei.kafka_dws_artemis_out_order' doesn't support consuming&#013;&#010;&gt; update changes which is produced by node&#013;&#010;&gt; GroupAggregate(groupBy=[warehouse_id], select=[warehouse_id, COUNT(*) AS&#013;&#010;&gt; EXPR$1])&#013;&#010;&gt;&#013;&#010;&gt; 在 Flink-1.10 中可以更改 KafkaTableSinkBase 让它 implements RetractStream 实现。&#013;&#010;&gt;&#013;&#010;&gt; 我看现在 Flink-1.11 中是用了  KafkaDynamicSource， KafkaDynamicSink，这样怎样改动才能让&#013;&#010;&gt; GroupBy 的结果也发送到 Kafka 呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<202007171301599643296@geekplus.com.cn>"
    },
    {
        "id": "<2020072011084378469119@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:08:44 GMT",
        "subject": "Re: Re: flink-1.11 KafkaDynamicTableSouce groupBy 结果怎样发送到 kafka",
        "content": "&#013;&#010;谢谢，我直接更改了 KafkaDynamicSinkBase 的 getChangelogMode 方法, 是可以实现目的的。&#013;&#010;&#013;&#010;更改前：&#013;&#010;public ChangelogMode getChangelogMode(ChangelogMode requestedMode) {&#013;&#010;    return this.encodingFormat.getChangelogMode();&#013;&#010;}&#013;&#010;更改后:public ChangelogMode getChangelogMode(ChangelogMode requestedMode) {&#013;&#010;      return ChangelogMode.newBuilder()&#013;&#010;         .addContainedKind(RowKind.INSERT)&#013;&#010;         .addContainedKind(RowKind.UPDATE_AFTER)&#013;&#010;         .build();&#013;&#010;   }而且这样更改以后 UPDATE_BEFORE 的记录被过滤掉了，没有被发送到 Kafka&#013;&#010;谢谢，王磊&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010; &#013;&#010;Sender: Benchao Li&#013;&#010;Send Time: 2020-07-17 20:18&#013;&#010;Receiver: user-zh&#013;&#010;Subject: Re: flink-1.11 KafkaDynamicTableSouce groupBy 结果怎样发送到 kafka&#013;&#010;DynamicTableSink有一个方法是getChangelogMode，可以通过这个方法来指定这个sink接收什么种类的数据&#013;&#010; &#013;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月17日周五 下午1:02写道：&#013;&#010; &#013;&#010;&gt;&#013;&#010;&gt;  INSERT INTO kafka_dws_artemis_out_order select warehouse_id, count(*)&#013;&#010;&gt; from kafka_ods_artemis_out_order group by warehouse_id;&#013;&#010;&gt; [ERROR] Could not execute SQL statement. Reason:&#013;&#010;&gt; org.apache.flink.table.api.TableException: Table sink&#013;&#010;&gt; 'myhive.wanglei.kafka_dws_artemis_out_order' doesn't support consuming&#013;&#010;&gt; update changes which is produced by node&#013;&#010;&gt; GroupAggregate(groupBy=[warehouse_id], select=[warehouse_id, COUNT(*) AS&#013;&#010;&gt; EXPR$1])&#013;&#010;&gt;&#013;&#010;&gt; 在 Flink-1.10 中可以更改 KafkaTableSinkBase 让它 implements RetractStream 实现。&#013;&#010;&gt;&#013;&#010;&gt; 我看现在 Flink-1.11 中是用了  KafkaDynamicSource， KafkaDynamicSink，这样怎样改动才能让&#013;&#010;&gt; GroupBy 的结果也发送到 Kafka 呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010; &#013;&#010;-- &#013;&#010; &#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<202007171301599643296@geekplus.com.cn>"
    },
    {
        "id": "<tencent_806F808F82910E078458CC820180887CBC07@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 05:32:18 GMT",
        "subject": "flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function",
        "content": "standalone&amp;nbsp;&#013;&#010;lib&amp;nbsp; jar包如下&#013;&#010;flink-connector-hive_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; flink-json-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; flink-sql-connector-kafka_2.12-1.11.0.jar&amp;nbsp; log4j-api-2.12.1.jar&#013;&#010;flink-csv-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; flink-parquet_2.11-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; flink-table_2.11-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; log4j-core-2.12.1.jar&#013;&#010;flink-dist_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; flink-shaded-hadoop-2-uber-2.7.2.11-9.0.jar&amp;nbsp; flink-table-blink_2.11-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; log4j-slf4j-impl-2.12.1.jar&#013;&#010;flink-hadoop-compatibility_2.11-1.11.0.jar&amp;nbsp; flink-shaded-zookeeper-3.4.14.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; log4j-1.2-api-2.12.1.jar&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;代码如下：idea下不报错&#013;&#010;StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#013;&#010;env.setParallelism(1);&#013;&#010;env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#013;&#010;// 同一时间只允许进行一个检查点&#013;&#010;env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);;&#013;&#010;env.setStateBackend(new FsStateBackend(path));&#013;&#010;&#013;&#010;tableEnv.executeSql(\"CREATE TABLE source_table (\\n\" +&#013;&#010;        \"\\thost STRING,\\n\" +&#013;&#010;        \"\\turl STRING,\\n\" +&#013;&#010;        \"\\tpublic_date STRING\\n\" +&#013;&#010;        \") WITH (\\n\" +&#013;&#010;        \"\\t'connector.type' = 'kafka',\\n\" +&#013;&#010;        \"\\t'connector.version' = 'universal',\\n\" +&#013;&#010;        \"\\t'connector.startup-mode' = 'latest-offset',\\n\" +&#013;&#010;        \"\\t'connector.topic' = 'test_flink_1.11',\\n\" +&#013;&#010;        \"\\t'connector.properties.group.id' = 'domain_testGroup',\\n\" +&#013;&#010;        \"\\t'connector.properties.zookeeper.connect' = '127.0.0.1:2181',\\n\" +&#013;&#010;        \"\\t'connector.properties.bootstrap.servers' = '127.0.0.1:9092',\\n\" +&#013;&#010;        \"\\t'update-mode' = 'append',\\n\" +&#013;&#010;        \"\\t'format.type' = 'json',\\n\" +&#013;&#010;        \"\\t'format.derive-schema' = 'true'\\n\" +&#013;&#010;        \")\");&#013;&#010;&#013;&#010;tableEnv.executeSql(\"CREATE TABLE fs_table (\\n\" +&#013;&#010;        \"  host STRING,\\n\" +&#013;&#010;        \"  url STRING,\\n\" +&#013;&#010;        \"  public_date STRING\\n\" +&#013;&#010;        \") PARTITIONED BY (public_date) WITH (\\n\" +&#013;&#010;        \"  'connector'='filesystem',\\n\" +&#013;&#010;        \"  'path'='path',\\n\" +&#013;&#010;        \"  'format'='json',\\n\" +&#013;&#010;        \"  'sink.partition-commit.delay'='0s',\\n\" +&#013;&#010;        \"  'sink.partition-commit.policy.kind'='success-file'\\n\" +&#013;&#010;        \")\");&#013;&#010;&#013;&#010;tableEnv.executeSql(\"INSERT INTO  fs_table SELECT host, url, DATE_FORMAT(public_date, 'yyyy-MM-dd')&#010;FROM source_table\");&#013;&#010;TableResult result = tableEnv.executeSql(\"SELECT * FROM fs_table \");&#013;&#010;result.print();&#013;&#010;报错如下&#013;&#010;org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function.&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&amp;gt;(OperatorChain.java:126)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:453)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;at java.lang.Thread.run(Thread.java:745)&#013;&#010;Caused by: java.lang.ClassCastException: cannot assign instance of org.apache.commons.collections.map.LinkedMap&#010;to field &#013;&#010;&#013;&#010;&#013;&#010;第二个bug sink到hdfs时候，采用parquet时候，lib下面有parquet包，pom里面是provided，但是会提示这个error，也试过pom里面不是provided，还是不OK",
        "depth": "0",
        "reply": "<tencent_806F808F82910E078458CC820180887CBC07@qq.com>"
    },
    {
        "id": "<tencent_F60E4AB13605A90023F965DF5F1F0B231508@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 06:17:01 GMT",
        "subject": "回复：flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function",
        "content": "第一个bug提示只需要&#013;&#010;classloader.resolve-order: parent-first&#013;&#010;第二个bug采用了parquet还没解决&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"kcz\"                                            &#010;                                       &lt;573693104@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月17日(星期五) 中午1:32&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;standalone &#013;&#010;lib&amp;nbsp; jar包如下&#013;&#010;flink-connector-hive_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; flink-json-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; flink-sql-connector-kafka_2.12-1.11.0.jar&amp;nbsp; log4j-api-2.12.1.jar&#013;&#010;flink-csv-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; flink-parquet_2.11-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; flink-table_2.11-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; log4j-core-2.12.1.jar&#013;&#010;flink-dist_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; flink-shaded-hadoop-2-uber-2.7.2.11-9.0.jar&amp;nbsp; flink-table-blink_2.11-1.11.0.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; log4j-slf4j-impl-2.12.1.jar&#013;&#010;flink-hadoop-compatibility_2.11-1.11.0.jar&amp;nbsp; flink-shaded-zookeeper-3.4.14.jar&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; log4j-1.2-api-2.12.1.jar&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;代码如下：idea下不报错&#013;&#010;StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#013;&#010;env.setParallelism(1);&#013;&#010;env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#013;&#010;// 同一时间只允许进行一个检查点&#013;&#010;env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);;&#013;&#010;env.setStateBackend(new FsStateBackend(path));&#013;&#010;&#013;&#010;tableEnv.executeSql(\"CREATE TABLE source_table (\\n\" +&#013;&#010;        \"\\thost STRING,\\n\" +&#013;&#010;        \"\\turl STRING,\\n\" +&#013;&#010;        \"\\tpublic_date STRING\\n\" +&#013;&#010;        \") WITH (\\n\" +&#013;&#010;        \"\\t'connector.type' = 'kafka',\\n\" +&#013;&#010;        \"\\t'connector.version' = 'universal',\\n\" +&#013;&#010;        \"\\t'connector.startup-mode' = 'latest-offset',\\n\" +&#013;&#010;        \"\\t'connector.topic' = 'test_flink_1.11',\\n\" +&#013;&#010;        \"\\t'connector.properties.group.id' = 'domain_testGroup',\\n\" +&#013;&#010;        \"\\t'connector.properties.zookeeper.connect' = '127.0.0.1:2181',\\n\" +&#013;&#010;        \"\\t'connector.properties.bootstrap.servers' = '127.0.0.1:9092',\\n\" +&#013;&#010;        \"\\t'update-mode' = 'append',\\n\" +&#013;&#010;        \"\\t'format.type' = 'json',\\n\" +&#013;&#010;        \"\\t'format.derive-schema' = 'true'\\n\" +&#013;&#010;        \")\");&#013;&#010;&#013;&#010;tableEnv.executeSql(\"CREATE TABLE fs_table (\\n\" +&#013;&#010;        \"  host STRING,\\n\" +&#013;&#010;        \"  url STRING,\\n\" +&#013;&#010;        \"  public_date STRING\\n\" +&#013;&#010;        \") PARTITIONED BY (public_date) WITH (\\n\" +&#013;&#010;        \"  'connector'='filesystem',\\n\" +&#013;&#010;        \"  'path'='path',\\n\" +&#013;&#010;        \"  'format'='json',\\n\" +&#013;&#010;        \"  'sink.partition-commit.delay'='0s',\\n\" +&#013;&#010;        \"  'sink.partition-commit.policy.kind'='success-file'\\n\" +&#013;&#010;        \")\");&#013;&#010;&#013;&#010;tableEnv.executeSql(\"INSERT INTO  fs_table SELECT host, url, DATE_FORMAT(public_date, 'yyyy-MM-dd')&#010;FROM source_table\");&#013;&#010;TableResult result = tableEnv.executeSql(\"SELECT * FROM fs_table \");&#013;&#010;result.print();&#013;&#010;报错如下&#013;&#010;org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot instantiate user function.&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&amp;gt;(OperatorChain.java:126)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:453)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:745)&#013;&#010;Caused by: java.lang.ClassCastException: cannot assign instance of org.apache.commons.collections.map.LinkedMap&#010;to field &#013;&#010;&#013;&#010;&#013;&#010;第二个bug sink到hdfs时候，采用parquet时候，lib下面有parquet包，pom里面是provided，但是会提示这个error，也试过pom里面不是provided，还是不OK",
        "depth": "1",
        "reply": "<tencent_806F808F82910E078458CC820180887CBC07@qq.com>"
    },
    {
        "id": "<CADQYLGvJmcB-EUnWV4GenPQ+08vRX58ruvsmmvRfLcYyo01d1A@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 07:29:11 GMT",
        "subject": "Re: flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function",
        "content": "第二个问题的异常栈是啥？&#010;&#010;kcz &lt;573693104@qq.com&gt; 于2020年7月17日周五 下午2:17写道：&#010;&#010;&gt; 第一个bug提示只需要&#010;&gt; classloader.resolve-order: parent-first&#010;&gt; 第二个bug采用了parquet还没解决&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"kcz\"&#010;&gt;                                                                 &lt;&#010;&gt; 573693104@qq.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月17日(星期五) 中午1:32&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; standalone&#010;&gt; lib&amp;nbsp; jar包如下&#010;&gt; flink-connector-hive_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; flink-json-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; flink-sql-connector-kafka_2.12-1.11.0.jar&amp;nbsp; log4j-api-2.12.1.jar&#010;&gt; flink-csv-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; flink-parquet_2.11-1.11.0.jar&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; flink-table_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; log4j-core-2.12.1.jar&#010;&gt; flink-dist_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; flink-shaded-hadoop-2-uber-2.7.2.11-9.0.jar&amp;nbsp;&#010;&gt; flink-table-blink_2.11-1.11.0.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; log4j-slf4j-impl-2.12.1.jar&#010;&gt; flink-hadoop-compatibility_2.11-1.11.0.jar&amp;nbsp;&#010;&gt; flink-shaded-zookeeper-3.4.14.jar&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; log4j-1.2-api-2.12.1.jar&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 代码如下：idea下不报错&#010;&gt; StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#010;&gt; env.setParallelism(1);&#010;&gt; env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#010;&gt; // 同一时间只允许进行一个检查点&#010;&gt; env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);;&#010;&gt; env.setStateBackend(new FsStateBackend(path));&#010;&gt;&#010;&gt; tableEnv.executeSql(\"CREATE TABLE source_table (\\n\" +&#010;&gt;         \"\\thost STRING,\\n\" +&#010;&gt;         \"\\turl STRING,\\n\" +&#010;&gt;         \"\\tpublic_date STRING\\n\" +&#010;&gt;         \") WITH (\\n\" +&#010;&gt;         \"\\t'connector.type' = 'kafka',\\n\" +&#010;&gt;         \"\\t'connector.version' = 'universal',\\n\" +&#010;&gt;         \"\\t'connector.startup-mode' = 'latest-offset',\\n\" +&#010;&gt;         \"\\t'connector.topic' = 'test_flink_1.11',\\n\" +&#010;&gt;         \"\\t'connector.properties.group.id' = 'domain_testGroup',\\n\" +&#010;&gt;         \"\\t'connector.properties.zookeeper.connect' = '127.0.0.1:2181',\\n\"&#010;&gt; +&#010;&gt;         \"\\t'connector.properties.bootstrap.servers' = '127.0.0.1:9092',\\n\"&#010;&gt; +&#010;&gt;         \"\\t'update-mode' = 'append',\\n\" +&#010;&gt;         \"\\t'format.type' = 'json',\\n\" +&#010;&gt;         \"\\t'format.derive-schema' = 'true'\\n\" +&#010;&gt;         \")\");&#010;&gt;&#010;&gt; tableEnv.executeSql(\"CREATE TABLE fs_table (\\n\" +&#010;&gt;         \"  host STRING,\\n\" +&#010;&gt;         \"  url STRING,\\n\" +&#010;&gt;         \"  public_date STRING\\n\" +&#010;&gt;         \") PARTITIONED BY (public_date) WITH (\\n\" +&#010;&gt;         \"  'connector'='filesystem',\\n\" +&#010;&gt;         \"  'path'='path',\\n\" +&#010;&gt;         \"  'format'='json',\\n\" +&#010;&gt;         \"  'sink.partition-commit.delay'='0s',\\n\" +&#010;&gt;         \"  'sink.partition-commit.policy.kind'='success-file'\\n\" +&#010;&gt;         \")\");&#010;&gt;&#010;&gt; tableEnv.executeSql(\"INSERT INTO  fs_table SELECT host, url,&#010;&gt; DATE_FORMAT(public_date, 'yyyy-MM-dd') FROM source_table\");&#010;&gt; TableResult result = tableEnv.executeSql(\"SELECT * FROM fs_table \");&#010;&gt; result.print();&#010;&gt; 报错如下&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot&#010;&gt; instantiate user function.&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291)&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&amp;gt;(OperatorChain.java:126)&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:453)&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:745)&#010;&gt; Caused by: java.lang.ClassCastException: cannot assign instance of&#010;&gt; org.apache.commons.collections.map.LinkedMap to field&#010;&gt;&#010;&gt;&#010;&gt; 第二个bug&#010;&gt; sink到hdfs时候，采用parquet时候，lib下面有parquet包，pom里面是provided，但是会提示这个error，也试过pom里面不是provided，还是不OK&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_806F808F82910E078458CC820180887CBC07@qq.com>"
    },
    {
        "id": "<tencent_C2BC1F3123620ED8B1DF0544009CF9CCDF08@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 08:05:02 GMT",
        "subject": "回复： flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function",
        "content": "这是使用了parquet的error：&#013;&#010;java.lang.NoClassDefFoundError: org/apache/parquet/hadoop/ParquetWriter$Builder&#013;&#010;&#009;at java.lang.ClassLoader.defineClass1(Native Method)&#013;&#010;&#009;at java.lang.ClassLoader.defineClass(ClassLoader.java:760)&#013;&#010;&#009;at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)&#013;&#010;&#009;at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)&#013;&#010;&#009;at java.net.URLClassLoader.access$100(URLClassLoader.java:73)&#013;&#010;&#009;at java.net.URLClassLoader$1.run(URLClassLoader.java:368)&#013;&#010;&#009;at java.net.URLClassLoader$1.run(URLClassLoader.java:362)&#013;&#010;&#009;at java.security.AccessController.doPrivileged(Native Method)&#013;&#010;&#009;at java.net.URLClassLoader.findClass(URLClassLoader.java:361)&#013;&#010;&#009;at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#013;&#010;&#009;at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)&#013;&#010;&#009;at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#013;&#010;&#009;at org.apache.flink.formats.parquet.ParquetFileSystemFormatFactory.createBulkWriterFactory(ParquetFileSystemFormatFactory.java:110)&#013;&#010;&#009;at org.apache.flink.table.filesystem.FileSystemTableSink.createWriter(FileSystemTableSink.java:274)&#013;&#010;&#009;at org.apache.flink.table.filesystem.FileSystemTableSink.consumeDataStream(FileSystemTableSink.java:154)&#013;&#010;&#009;at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:114)&#013;&#010;&#009;at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48)&#013;&#010;&#009;at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#013;&#010;&#009;at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48)&#013;&#010;&#009;at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:67)&#013;&#010;&#009;at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66)&#013;&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&#009;at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;&#009;at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#013;&#010;&#009;at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#013;&#010;&#009;at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#013;&#010;&#009;at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#013;&#010;&#009;at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#013;&#010;&#009;at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#013;&#010;&#009;at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66)&#013;&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)&#013;&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#013;&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:694)&#013;&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:781)&#013;&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:684)&#013;&#010;&#009;at com.HdfsDDL.main(HdfsDDL.java:71)&#013;&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:497)&#013;&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#013;&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#013;&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#013;&#010;&#009;at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#013;&#010;&#009;at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#013;&#010;&#009;at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#013;&#010;&#009;at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#013;&#010;&#009;at org.apache.flink.client.cli.CliFrontend$$Lambda$67/388104475.call(Unknown Source)&#013;&#010;&#009;at org.apache.flink.runtime.security.contexts.HadoopSecurityContext$$Lambda$68/1470966439.run(Unknown&#010;Source)&#013;&#010;&#009;at java.security.AccessController.doPrivileged(Native Method)&#013;&#010;&#009;at javax.security.auth.Subject.doAs(Subject.java:422)&#013;&#010;&#009;at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1659)&#013;&#010;&#009;at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#013;&#010;&#009;at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#013;&#010;Caused by: java.lang.ClassNotFoundException: org.apache.parquet.hadoop.ParquetWriter$Builder&#013;&#010;&#009;at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#013;&#010;&#009;at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#013;&#010;&#009;at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)&#013;&#010;&#009;at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;godfreyhe@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月17日(星期五) 下午3:29&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;第二个问题的异常栈是啥？&#013;&#010;&#013;&#010;kcz &lt;573693104@qq.com&amp;gt; 于2020年7月17日周五 下午2:17写道：&#013;&#010;&#013;&#010;&amp;gt; 第一个bug提示只需要&#013;&#010;&amp;gt; classloader.resolve-order: parent-first&#013;&#010;&amp;gt; 第二个bug采用了parquet还没解决&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"kcz\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; 573693104@qq.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月17日(星期五) 中午1:32&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;flink-1.11 DDL 写入hdfs问题 Cannot instantiate user function&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; standalone&#013;&#010;&amp;gt; lib&amp;amp;nbsp; jar包如下&#013;&#010;&amp;gt; flink-connector-hive_2.11-1.11.0.jar&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; flink-json-1.11.0.jar&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; flink-sql-connector-kafka_2.12-1.11.0.jar&amp;amp;nbsp; log4j-api-2.12.1.jar&#013;&#010;&amp;gt; flink-csv-1.11.0.jar&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; flink-parquet_2.11-1.11.0.jar&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; flink-table_2.11-1.11.0.jar&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; log4j-core-2.12.1.jar&#013;&#010;&amp;gt; flink-dist_2.11-1.11.0.jar&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; flink-shaded-hadoop-2-uber-2.7.2.11-9.0.jar&amp;amp;nbsp;&#013;&#010;&amp;gt; flink-table-blink_2.11-1.11.0.jar&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; log4j-slf4j-impl-2.12.1.jar&#013;&#010;&amp;gt; flink-hadoop-compatibility_2.11-1.11.0.jar&amp;amp;nbsp;&#013;&#010;&amp;gt; flink-shaded-zookeeper-3.4.14.jar&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; log4j-1.2-api-2.12.1.jar&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 代码如下：idea下不报错&#013;&#010;&amp;gt; StreamExecutionEnvironment env =&#013;&#010;&amp;gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&amp;gt; StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#013;&#010;&amp;gt; env.setParallelism(1);&#013;&#010;&amp;gt; env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);&#013;&#010;&amp;gt; // 同一时间只允许进行一个检查点&#013;&#010;&amp;gt; env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);;&#013;&#010;&amp;gt; env.setStateBackend(new FsStateBackend(path));&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; tableEnv.executeSql(\"CREATE TABLE source_table (\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\thost&#010;STRING,\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\turl&#010;STRING,\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\tpublic_date&#010;STRING\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \")&#010;WITH (\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'connector.type'&#010;= 'kafka',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'connector.version'&#010;= 'universal',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'connector.startup-mode'&#010;= 'latest-offset',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'connector.topic'&#010;= 'test_flink_1.11',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'connector.properties.group.id'&#010;= 'domain_testGroup',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'connector.properties.zookeeper.connect'&#010;= '127.0.0.1:2181',\\n\"&#013;&#010;&amp;gt; +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'connector.properties.bootstrap.servers'&#010;= '127.0.0.1:9092',\\n\"&#013;&#010;&amp;gt; +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'update-mode'&#010;= 'append',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'format.type'&#010;= 'json',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\\t'format.derive-schema'&#010;= 'true'\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \")\");&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; tableEnv.executeSql(\"CREATE TABLE fs_table (\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;host STRING,\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;url STRING,\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;public_date STRING\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \")&#010;PARTITIONED BY (public_date) WITH (\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;'connector'='filesystem',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;'path'='path',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;'format'='json',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;'sink.partition-commit.delay'='0s',\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#010;'sink.partition-commit.policy.kind'='success-file'\\n\" +&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \")\");&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; tableEnv.executeSql(\"INSERT INTO&amp;nbsp; fs_table SELECT host, url,&#013;&#010;&amp;gt; DATE_FORMAT(public_date, 'yyyy-MM-dd') FROM source_table\");&#013;&#010;&amp;gt; TableResult result = tableEnv.executeSql(\"SELECT * FROM fs_table \");&#013;&#010;&amp;gt; result.print();&#013;&#010;&amp;gt; 报错如下&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskException: Cannot&#013;&#010;&amp;gt; instantiate user function.&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.graph.StreamConfig.getStreamOperatorFactory(StreamConfig.java:291)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.OperatorChain.&lt;init&amp;amp;gt;(OperatorChain.java:126)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:453)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#013;&#010;&amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at java.lang.Thread.run(Thread.java:745)&#013;&#010;&amp;gt; Caused by: java.lang.ClassCastException: cannot assign instance of&#013;&#010;&amp;gt; org.apache.commons.collections.map.LinkedMap to field&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 第二个bug&#013;&#010;&amp;gt; sink到hdfs时候，采用parquet时候，lib下面有parquet包，pom里面是provided，但是会提示这个error，也试过pom里面不是provided，还是不OK",
        "depth": "3",
        "reply": "<tencent_806F808F82910E078458CC820180887CBC07@qq.com>"
    },
    {
        "id": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>",
        "from": "Luan Cooper &lt;gc.su...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 06:40:11 GMT",
        "subject": "SQL 报错只有 flink runtime 的 NPE",
        "content": "Hi&#010;&#010;我有这么一个 SQL&#010;INSERT INTO es&#010;SELECT&#010;a,&#010;udf_xxx(b)&#010;FROM mongo_oplog -- 自定义 TableFactory&#010;&#010;Job 提交后 fail 了，从 Job 提交到 Fail 只有一处来自非业务代码的 NPE 如下，没有任何业务代码&#010;Exception，可以稳定重现&#010;&#010;LUE _UTF-16LE'v2'))) AS return_received_time]) (1/1)&#010;(bdf9b131f82a8ebc440165b82b89e570) switched from RUNNING to FAILED.&#010;&#010;java.lang.NullPointerException&#010;&#010;at StreamExecCalc$8016.split$7938$(Unknown Source)&#010;&#010;at StreamExecCalc$8016.processElement(Unknown Source)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&#010;at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&#010;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&#010;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&#010;at java.lang.Thread.run(Thread.java:748)&#010;&#010;请问这种怎样情况排查问题？&#010;有任何线索都可以&#010;&#010;感谢&#010;&#010;",
        "depth": "0",
        "reply": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGtTYzi4q1B3kiYRv+kifnf8gMeskvUtyp9ZZgpXaFe8Sw@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 06:53:30 GMT",
        "subject": "Re: SQL 报错只有 flink runtime 的 NPE",
        "content": "udf_xxx的逻辑是啥？&#010;&#010;&#010;Luan Cooper &lt;gc.suprs@gmail.com&gt; 于2020年7月17日周五 下午2:40写道：&#010;&#010;&gt; Hi&#010;&gt;&#010;&gt; 我有这么一个 SQL&#010;&gt; INSERT INTO es&#010;&gt; SELECT&#010;&gt; a,&#010;&gt; udf_xxx(b)&#010;&gt; FROM mongo_oplog -- 自定义 TableFactory&#010;&gt;&#010;&gt; Job 提交后 fail 了，从 Job 提交到 Fail 只有一处来自非业务代码的&#010;NPE 如下，没有任何业务代码 Exception，可以稳定重现&#010;&gt;&#010;&gt; LUE _UTF-16LE'v2'))) AS return_received_time]) (1/1)&#010;&gt; (bdf9b131f82a8ebc440165b82b89e570) switched from RUNNING to FAILED.&#010;&gt;&#010;&gt; java.lang.NullPointerException&#010;&gt;&#010;&gt; at StreamExecCalc$8016.split$7938$(Unknown Source)&#010;&gt;&#010;&gt; at StreamExecCalc$8016.processElement(Unknown Source)&#010;&gt;&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)&#010;&gt;&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.io&#010;&gt; .StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)&#010;&gt;&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.io&#010;&gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)&#010;&gt;&#010;&gt; at&#010;&gt; org.apache.flink.streaming.runtime.io&#010;&gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt;&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)&#010;&gt;&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt;&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&gt;&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&gt;&#010;&gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;&#010;&gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;&#010;&gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&#010;&gt; 请问这种怎样情况排查问题？&#010;&gt; 有任何线索都可以&#010;&gt;&#010;&gt; 感谢&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>"
    },
    {
        "id": "<CABD9WQH7Zcr3d+7grv2VB3EKzuXnoPt_OcuYoD_iE07ERYoisA@mail.gmail.com>",
        "from": "Luan Cooper &lt;gc.su...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 08:01:42 GMT",
        "subject": "Re: SQL 报错只有 flink runtime 的 NPE",
        "content": "实际有 20 左右个字段，用到的 UDF 有 COALESCE / CAST / JSON_PATH / TIMESTAMP 类&#010;*是指 UDF 返回了 NULL 导致的吗？*&#010;&#010;&#010;On Fri, Jul 17, 2020 at 2:54 PM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#010;&#010;&gt; udf_xxx的逻辑是啥？&#010;&gt;&#010;&gt;&#010;&gt; Luan Cooper &lt;gc.suprs@gmail.com&gt; 于2020年7月17日周五 下午2:40写道：&#010;&gt;&#010;&gt; &gt; Hi&#010;&gt; &gt;&#010;&gt; &gt; 我有这么一个 SQL&#010;&gt; &gt; INSERT INTO es&#010;&gt; &gt; SELECT&#010;&gt; &gt; a,&#010;&gt; &gt; udf_xxx(b)&#010;&gt; &gt; FROM mongo_oplog -- 自定义 TableFactory&#010;&gt; &gt;&#010;&gt; &gt; Job 提交后 fail 了，从 Job 提交到 Fail 只有一处来自非业务代码的&#010;NPE 如下，没有任何业务代码&#010;&gt; Exception，可以稳定重现&#010;&gt; &gt;&#010;&gt; &gt; LUE _UTF-16LE'v2'))) AS return_received_time]) (1/1)&#010;&gt; &gt; (bdf9b131f82a8ebc440165b82b89e570) switched from RUNNING to FAILED.&#010;&gt; &gt;&#010;&gt; &gt; java.lang.NullPointerException&#010;&gt; &gt;&#010;&gt; &gt; at StreamExecCalc$8016.split$7938$(Unknown Source)&#010;&gt; &gt;&#010;&gt; &gt; at StreamExecCalc$8016.processElement(Unknown Source)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt; &gt; .StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&gt; &gt;&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&gt; &gt;&#010;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &gt;&#010;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &gt;&#010;&gt; &gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt; &gt;&#010;&gt; &gt; 请问这种怎样情况排查问题？&#010;&gt; &gt; 有任何线索都可以&#010;&gt; &gt;&#010;&gt; &gt; 感谢&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>"
    },
    {
        "id": "<CABD9WQG5=m9TqGtJV=NAHQCzBGLWP5dMzGzQxSeWa+yJgP=RqQ@mail.gmail.com>",
        "from": "Luan Cooper &lt;gc.su...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 08:11:16 GMT",
        "subject": "Re: SQL 报错只有 flink runtime 的 NPE",
        "content": "附一个 Job Graph 信息，在 Cal 处挂了&#010;[image: image.png]&#010;&#010;On Fri, Jul 17, 2020 at 4:01 PM Luan Cooper &lt;gc.suprs@gmail.com&gt; wrote:&#010;&#010;&gt; 实际有 20 左右个字段，用到的 UDF 有 COALESCE / CAST / JSON_PATH / TIMESTAMP&#010;类&#010;&gt; *是指 UDF 返回了 NULL 导致的吗？*&#010;&gt;&#010;&gt;&#010;&gt; On Fri, Jul 17, 2020 at 2:54 PM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; udf_xxx的逻辑是啥？&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Luan Cooper &lt;gc.suprs@gmail.com&gt; 于2020年7月17日周五 下午2:40写道：&#010;&gt;&gt;&#010;&gt;&gt; &gt; Hi&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 我有这么一个 SQL&#010;&gt;&gt; &gt; INSERT INTO es&#010;&gt;&gt; &gt; SELECT&#010;&gt;&gt; &gt; a,&#010;&gt;&gt; &gt; udf_xxx(b)&#010;&gt;&gt; &gt; FROM mongo_oplog -- 自定义 TableFactory&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Job 提交后 fail 了，从 Job 提交到 Fail 只有一处来自非业务代码的&#010;NPE 如下，没有任何业务代码&#010;&gt;&gt; Exception，可以稳定重现&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; LUE _UTF-16LE'v2'))) AS return_received_time]) (1/1)&#010;&gt;&gt; &gt; (bdf9b131f82a8ebc440165b82b89e570) switched from RUNNING to FAILED.&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; java.lang.NullPointerException&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at StreamExecCalc$8016.split$7938$(Unknown Source)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at StreamExecCalc$8016.processElement(Unknown Source)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; .StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 请问这种怎样情况排查问题？&#010;&gt;&gt; &gt; 有任何线索都可以&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; 感谢&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGt+JB3G9D9OiZq3m2RFG1NqjjuoBTKwVPvBjGVNL7Wfaw@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 07:27:58 GMT",
        "subject": "Re: SQL 报错只有 flink runtime 的 NPE",
        "content": "看不到图片信息，换一个图床工具上传图片吧&#010;&#010;Luan Cooper &lt;gc.suprs@gmail.com&gt; 于2020年7月17日周五 下午4:11写道：&#010;&#010;&gt; 附一个 Job Graph 信息，在 Cal 处挂了&#010;&gt; [image: image.png]&#010;&gt;&#010;&gt; On Fri, Jul 17, 2020 at 4:01 PM Luan Cooper &lt;gc.suprs@gmail.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; 实际有 20 左右个字段，用到的 UDF 有 COALESCE / CAST / JSON_PATH / TIMESTAMP&#010;类&#010;&gt;&gt; *是指 UDF 返回了 NULL 导致的吗？*&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; On Fri, Jul 17, 2020 at 2:54 PM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt;&gt; udf_xxx的逻辑是啥？&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; Luan Cooper &lt;gc.suprs@gmail.com&gt; 于2020年7月17日周五 下午2:40写道：&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt; &gt; Hi&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 我有这么一个 SQL&#010;&gt;&gt;&gt; &gt; INSERT INTO es&#010;&gt;&gt;&gt; &gt; SELECT&#010;&gt;&gt;&gt; &gt; a,&#010;&gt;&gt;&gt; &gt; udf_xxx(b)&#010;&gt;&gt;&gt; &gt; FROM mongo_oplog -- 自定义 TableFactory&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; Job 提交后 fail 了，从 Job 提交到 Fail 只有一处来自非业务代码的&#010;NPE 如下，没有任何业务代码&#010;&gt;&gt;&gt; Exception，可以稳定重现&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; LUE _UTF-16LE'v2'))) AS return_received_time]) (1/1)&#010;&gt;&gt;&gt; &gt; (bdf9b131f82a8ebc440165b82b89e570) switched from RUNNING to FAILED.&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; java.lang.NullPointerException&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at StreamExecCalc$8016.split$7938$(Unknown Source)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at StreamExecCalc$8016.processElement(Unknown Source)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt;&gt;&gt; &gt; .StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt;&gt;&gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#010;&gt;&gt;&gt; &gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; at java.lang.Thread.run(Thread.java:748)&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 请问这种怎样情况排查问题？&#010;&gt;&gt;&gt; &gt; 有任何线索都可以&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 感谢&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>"
    },
    {
        "id": "<CAELO931+JZycL+trnwZVYgUd9bkS=64Rkntd-nwXv+nBA5jfeA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:15:18 GMT",
        "subject": "Re: SQL 报错只有 flink runtime 的 NPE",
        "content": "这个异常一般是由于 UDF 的实现用了主类型（int），但是实际的字段值有&#010;null 值。&#013;&#010;你可以试试先做个 where 条件过滤，将 null 值过滤掉？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;&#013;&#010;On Mon, 20 Jul 2020 at 15:28, godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 看不到图片信息，换一个图床工具上传图片吧&#013;&#010;&gt;&#013;&#010;&gt; Luan Cooper &lt;gc.suprs@gmail.com&gt; 于2020年7月17日周五 下午4:11写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 附一个 Job Graph 信息，在 Cal 处挂了&#013;&#010;&gt; &gt; [image: image.png]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Fri, Jul 17, 2020 at 4:01 PM Luan Cooper &lt;gc.suprs@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; 实际有 20 左右个字段，用到的 UDF 有 COALESCE / CAST / JSON_PATH&#010;/ TIMESTAMP 类&#013;&#010;&gt; &gt;&gt; *是指 UDF 返回了 NULL 导致的吗？*&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; On Fri, Jul 17, 2020 at 2:54 PM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; udf_xxx的逻辑是啥？&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Luan Cooper &lt;gc.suprs@gmail.com&gt; 于2020年7月17日周五 下午2:40写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Hi&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 我有这么一个 SQL&#013;&#010;&gt; &gt;&gt;&gt; &gt; INSERT INTO es&#013;&#010;&gt; &gt;&gt;&gt; &gt; SELECT&#013;&#010;&gt; &gt;&gt;&gt; &gt; a,&#013;&#010;&gt; &gt;&gt;&gt; &gt; udf_xxx(b)&#013;&#010;&gt; &gt;&gt;&gt; &gt; FROM mongo_oplog -- 自定义 TableFactory&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Job 提交后 fail 了，从 Job 提交到 Fail 只有一处来自非业务代码的&#010;NPE 如下，没有任何业务代码&#013;&#010;&gt; &gt;&gt;&gt; Exception，可以稳定重现&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; LUE _UTF-16LE'v2'))) AS return_received_time]) (1/1)&#013;&#010;&gt; &gt;&gt;&gt; &gt; (bdf9b131f82a8ebc440165b82b89e570) switched from RUNNING to FAILED.&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; java.lang.NullPointerException&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at StreamExecCalc$8016.split$7938$(Unknown Source)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at StreamExecCalc$8016.processElement(Unknown Source)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:173)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; .StreamTaskNetworkInput.processElement(StreamTaskNetworkInput.java:151)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt;&gt; &gt; .StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:128)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt; org.apache.flink.streaming.runtime.io&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; .StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 请问这种怎样情况排查问题？&#013;&#010;&gt; &gt;&gt;&gt; &gt; 有任何线索都可以&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 感谢&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>"
    },
    {
        "id": "<202007201926375043500@163.com>",
        "from": "&quot;sjlsumaitong@163.com&quot; &lt;sjlsumait...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 11:26:37 GMT",
        "subject": "测试一下社区邮件",
        "content": "忽略&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;sjlsumaitong@163.com&#013;&#010; &#013;&#010;",
        "depth": "1",
        "reply": "<CABD9WQF6DA4LPDiV6ArMz32E=Fsr-ZqaSt6c8znXYYXBFtx7cw@mail.gmail.com>"
    },
    {
        "id": "<tencent_A0FBE40366E4D3613256AB4009202BFFFA05@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 08:24:48 GMT",
        "subject": "flink-1.11 ddl 写入json 格式数据到hdfs问题",
        "content": "代码引用&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#full-example&#013;&#010;将parquet换成了json之后，chk成功，但是文件状态一直处于in-progress状态，我应该如何让它成功呢？&#013;&#010;parquet目前是已经success了。",
        "depth": "0",
        "reply": "<tencent_A0FBE40366E4D3613256AB4009202BFFFA05@qq.com>"
    },
    {
        "id": "<CABi+2jT6yRVMxX4Us-q3cPhEu16wbTKFUR-UA6tys8ZEJ7M5ww@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 08:57:40 GMT",
        "subject": "Re: flink-1.11 ddl 写入json 格式数据到hdfs问题",
        "content": "Hi,&#013;&#010;&#013;&#010;如同[1]里面说的，对于csv和json，你还需要配置rolling相关参数，因为它们是可以不在checkpoint强行rolling的。&#013;&#010;&#013;&#010;NOTE: For row formats (csv, json), you can set the parameter&#013;&#010;sink.rolling-policy.file-size or sink.rolling-policy.rollover-interval in&#013;&#010;the connector properties and parameter execution.checkpointing.interval in&#013;&#010;flink-conf.yaml together if you don’t want to wait a long period before&#013;&#010;observe the data exists in file system. For other formats (avro, orc), you&#013;&#010;can just set parameter execution.checkpointing.interval in flink-conf.yaml.&#013;&#010;&#013;&#010;所以如果你想通过时间来rolling，你还需要配sink.rolling-policy.rollover-interval和sink.rolling-policy.check-interval&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/filesystem.html#rolling-policy&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 17, 2020 at 4:25 PM kcz &lt;573693104@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 代码引用&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#full-example&#013;&#010;&gt; 将parquet换成了json之后，chk成功，但是文件状态一直处于in-progress状态，我应该如何让它成功呢？&#013;&#010;&gt; parquet目前是已经success了。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_A0FBE40366E4D3613256AB4009202BFFFA05@qq.com>"
    },
    {
        "id": "<tencent_F751EC5D813F404C7ED6F7C347F5C8B18306@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:26:13 GMT",
        "subject": "回复： flink-1.11 ddl 写入json 格式数据到hdfs问题",
        "content": "tks解决了，有一个小问题，文档写了30m,但是代码实际不支持m来代表分钟&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;jingsonglee0@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月17日(星期五) 下午4:57&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink-1.11 ddl 写入json 格式数据到hdfs问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi,&#013;&#010;&#013;&#010;如同[1]里面说的，对于csv和json，你还需要配置rolling相关参数，因为它们是可以不在checkpoint强行rolling的。&#013;&#010;&#013;&#010;NOTE: For row formats (csv, json), you can set the parameter&#013;&#010;sink.rolling-policy.file-size or sink.rolling-policy.rollover-interval in&#013;&#010;the connector properties and parameter execution.checkpointing.interval in&#013;&#010;flink-conf.yaml together if you don’t want to wait a long period before&#013;&#010;observe the data exists in file system. For other formats (avro, orc), you&#013;&#010;can just set parameter execution.checkpointing.interval in flink-conf.yaml.&#013;&#010;&#013;&#010;所以如果你想通过时间来rolling，你还需要配sink.rolling-policy.rollover-interval和sink.rolling-policy.check-interval&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/filesystem.html#rolling-policy&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 17, 2020 at 4:25 PM kcz &lt;573693104@qq.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; 代码引用&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#full-example&#013;&#010;&amp;gt; 将parquet换成了json之后，chk成功，但是文件状态一直处于in-progress状态，我应该如何让它成功呢？&#013;&#010;&amp;gt; parquet目前是已经success了。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee",
        "depth": "2",
        "reply": "<tencent_A0FBE40366E4D3613256AB4009202BFFFA05@qq.com>"
    },
    {
        "id": "<D88CE995-B16B-4275-BDA0-398B22493305@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 11:48:25 GMT",
        "subject": "Re: flink-1.11 ddl 写入json 格式数据到hdfs问题",
        "content": "是的&#010;&#010;感谢反馈，文档里单位问题，分钟对应的是 min&#010;&#010;&#010;&gt; 在 2020年7月17日，17:26，kcz &lt;573693104@qq.com&gt; 写道：&#010;&gt; &#010;&gt; tks解决了，有一个小问题，文档写了30m,但是代码实际不支持m来代表分钟&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:                                                                          &#010;                                             \"user-zh\"                                   &#010;                                                &lt;jingsonglee0@gmail.com &lt;mailto:jingsonglee0@gmail.com&gt;&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月17日(星期五) 下午4:57&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt;&amp;gt;;&#010;&gt; &#010;&gt; 主题:&amp;nbsp;Re: flink-1.11 ddl 写入json 格式数据到hdfs问题&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; Hi,&#010;&gt; &#010;&gt; 如同[1]里面说的，对于csv和json，你还需要配置rolling相关参数，因为它们是可以不在checkpoint强行rolling的。&#010;&gt; &#010;&gt; NOTE: For row formats (csv, json), you can set the parameter&#010;&gt; sink.rolling-policy.file-size or sink.rolling-policy.rollover-interval in&#010;&gt; the connector properties and parameter execution.checkpointing.interval in&#010;&gt; flink-conf.yaml together if you don’t want to wait a long period before&#010;&gt; observe the data exists in file system. For other formats (avro, orc), you&#010;&gt; can just set parameter execution.checkpointing.interval in flink-conf.yaml.&#010;&gt; &#010;&gt; 所以如果你想通过时间来rolling，你还需要配sink.rolling-policy.rollover-interval和sink.rolling-policy.check-interval&#010;&gt; &#010;&gt; [1]&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/filesystem.html#rolling-policy&#010;&gt; &#010;&gt; Best,&#010;&gt; Jingsong&#010;&gt; &#010;&gt; On Fri, Jul 17, 2020 at 4:25 PM kcz &lt;573693104@qq.com&amp;gt; wrote:&#010;&gt; &#010;&gt; &amp;gt; 代码引用&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#full-example&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#full-example&gt;&#010;&gt; &amp;gt; 将parquet换成了json之后，chk成功，但是文件状态一直处于in-progress状态，我应该如何让它成功呢？&#010;&gt; &amp;gt; parquet目前是已经success了。&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; -- &#010;&gt; Best, Jingsong Lee&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_A0FBE40366E4D3613256AB4009202BFFFA05@qq.com>"
    },
    {
        "id": "<CAEZk043gMHjUuk0=Mj4=io+9oGp=9A8wqD0ONVR_PgbH+AK4Wg@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:05:12 GMT",
        "subject": "flink1.11写hive",
        "content": "hi：&#013;&#010;我这面在flink中注册hivecatalog，想将kafka数据流式写入到hive表中，但是现在建立kafka表的时候默认会保存元数据到hive表，请问有办法不保存这个kafka元数据表吗？如果不注册hivecatalog的话没办法写数据到hive吧。。。。&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk043gMHjUuk0=Mj4=io+9oGp=9A8wqD0ONVR_PgbH+AK4Wg@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jQ4=uvytLxXEkAFi6=0SUk2NUnzbMKGSE9VWnUFLDTHXw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:12:50 GMT",
        "subject": "Re: flink1.11写hive",
        "content": "CREATE TEMPORARY TABLE kafka_table...&#013;&#010;好像没文档，我建个JIRA跟踪下&#013;&#010;https://issues.apache.org/jira/browse/FLINK-18624&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 17, 2020 at 5:05 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi：&#013;&#010;&gt;&#013;&#010;&gt; 我这面在flink中注册hivecatalog，想将kafka数据流式写入到hive表中，但是现在建立kafka表的时候默认会保存元数据到hive表，请问有办法不保存这个kafka元数据表吗？如果不注册hivecatalog的话没办法写数据到hive吧。。。。&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk043gMHjUuk0=Mj4=io+9oGp=9A8wqD0ONVR_PgbH+AK4Wg@mail.gmail.com>"
    },
    {
        "id": "<CADH6UNSU+BU+RL80MQuE5HU_evdR=1ND+pAxgYszsuWodqsc4A@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:42:15 GMT",
        "subject": "Re: flink1.11写hive",
        "content": "建表的时候也可以指定Catalog的，创建kafka表指定default_catalog就不会创建到hive&#010;Catalog里去了&#013;&#010;&#013;&#010;On Fri, Jul 17, 2020 at 5:13 PM Jingsong Li &lt;jingsonglee0@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; CREATE TEMPORARY TABLE kafka_table...&#013;&#010;&gt; 好像没文档，我建个JIRA跟踪下&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18624&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Fri, Jul 17, 2020 at 5:05 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我这面在flink中注册hivecatalog，想将kafka数据流式写入到hive表中，但是现在建立kafka表的时候默认会保存元数据到hive表，请问有办法不保存这个kafka元数据表吗？如果不注册hivecatalog的话没办法写数据到hive吧。。。。&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk043gMHjUuk0=Mj4=io+9oGp=9A8wqD0ONVR_PgbH+AK4Wg@mail.gmail.com>"
    },
    {
        "id": "<CAEZk043Lqp9PdDG+Nt_tRBtR4npqbHp14Az1GrVOCOK+P-Wb2A@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:14:19 GMT",
        "subject": "connector hive依赖冲突",
        "content": "hi：&#013;&#010;大佬们，下面连接hive的依赖包的哪个传递依赖导致的jar包冲突，我从1.9到1.11每次在maven按照官方文档引包都会出现依赖冲突。。。。1.9刚发布的时候对下面的引包有做依赖排除，后来文档改了&#013;&#010;&#013;&#010;// Flink's Hive connector.Contains flink-hadoop-compatibility and flink-orc jars&#013;&#010;       flink-connector-hive_2.11-1.11.0.jar&#013;&#010;       // Hive dependencies&#013;&#010;       hive-exec-2.3.4.jar&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk043Lqp9PdDG+Nt_tRBtR4npqbHp14Az1GrVOCOK+P-Wb2A@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jSBPTG8w+v3jk_k0xCC0KU2DSvOLr6fdeao9z3-S40VzA@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:16:37 GMT",
        "subject": "Re: connector hive依赖冲突",
        "content": "用bundle jar可以搞定吗？&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Fri, Jul 17, 2020 at 5:14 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi：&#013;&#010;&gt;&#013;&#010;&gt; 大佬们，下面连接hive的依赖包的哪个传递依赖导致的jar包冲突，我从1.9到1.11每次在maven按照官方文档引包都会出现依赖冲突。。。。1.9刚发布的时候对下面的引包有做依赖排除，后来文档改了&#013;&#010;&gt;&#013;&#010;&gt; // Flink's Hive connector.Contains flink-hadoop-compatibility and&#013;&#010;&gt; flink-orc jars&#013;&#010;&gt;        flink-connector-hive_2.11-1.11.0.jar&#013;&#010;&gt;        // Hive dependencies&#013;&#010;&gt;        hive-exec-2.3.4.jar&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk043Lqp9PdDG+Nt_tRBtR4npqbHp14Az1GrVOCOK+P-Wb2A@mail.gmail.com>"
    },
    {
        "id": "<CAEZk0409dePRp+qwUkKJj4H2Zt-ghWiHWqEmMsBTD-SQr5zuBQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:24:00 GMT",
        "subject": "Re: connector hive依赖冲突",
        "content": "1.9和1.10时候排除一些传递依赖后在idea和打uber jar在集群环境都可以运行，不排除传递依赖的话在idea运行不了；&#013;&#010;1.11现在只在本地测哪，不排除传递依赖idea运行不了，集群环境还没弄，但是我感觉在idea直接run这个功能好多人都需要，文档是不是可以改进一下&#013;&#010;&#013;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月17日周五 下午5:16写道：&#013;&#010;&#013;&#010;&gt; 用bundle jar可以搞定吗？&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Fri, Jul 17, 2020 at 5:14 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 大佬们，下面连接hive的依赖包的哪个传递依赖导致的jar包冲突，我从1.9到1.11每次在maven按照官方文档引包都会出现依赖冲突。。。。1.9刚发布的时候对下面的引包有做依赖排除，后来文档改了&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; // Flink's Hive connector.Contains flink-hadoop-compatibility and&#013;&#010;&gt; &gt; flink-orc jars&#013;&#010;&gt; &gt;        flink-connector-hive_2.11-1.11.0.jar&#013;&#010;&gt; &gt;        // Hive dependencies&#013;&#010;&gt; &gt;        hive-exec-2.3.4.jar&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk043Lqp9PdDG+Nt_tRBtR4npqbHp14Az1GrVOCOK+P-Wb2A@mail.gmail.com>"
    },
    {
        "id": "<CAEZk040nWi8jF4Cb6pptxf-QUeiB9GKEpWWh_1kzZUz0C5GgbQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:32:11 GMT",
        "subject": "Re: connector hive依赖冲突",
        "content": "hi&#013;&#010;我用的是用户定义依赖，没有用捆绑依赖包，捆绑依赖包还要自己下载一次。。。。。&#013;&#010;&#013;&#010;Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月17日周五 下午5:24写道：&#013;&#010;&#013;&#010;&gt; 1.9和1.10时候排除一些传递依赖后在idea和打uber jar在集群环境都可以运行，不排除传递依赖的话在idea运行不了；&#013;&#010;&gt; 1.11现在只在本地测哪，不排除传递依赖idea运行不了，集群环境还没弄，但是我感觉在idea直接run这个功能好多人都需要，文档是不是可以改进一下&#013;&#010;&gt;&#013;&#010;&gt; Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月17日周五 下午5:16写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 用bundle jar可以搞定吗？&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; [1]&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best,&#013;&#010;&gt;&gt; Jingsong&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; On Fri, Jul 17, 2020 at 5:14 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt; hi：&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; 大佬们，下面连接hive的依赖包的哪个传递依赖导致的jar包冲突，我从1.9到1.11每次在maven按照官方文档引包都会出现依赖冲突。。。。1.9刚发布的时候对下面的引包有做依赖排除，后来文档改了&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; // Flink's Hive connector.Contains flink-hadoop-compatibility and&#013;&#010;&gt;&gt; &gt; flink-orc jars&#013;&#010;&gt;&gt; &gt;        flink-connector-hive_2.11-1.11.0.jar&#013;&#010;&gt;&gt; &gt;        // Hive dependencies&#013;&#010;&gt;&gt; &gt;        hive-exec-2.3.4.jar&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; --&#013;&#010;&gt;&gt; Best, Jingsong Lee&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CAEZk043Lqp9PdDG+Nt_tRBtR4npqbHp14Az1GrVOCOK+P-Wb2A@mail.gmail.com>"
    },
    {
        "id": "<CADH6UNSkFZJqAyP=KeZDpOaeKb=nrY_JXkyyjD3q0y5FtViLHQ@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:47:56 GMT",
        "subject": "Re: connector hive依赖冲突",
        "content": "现在具体是遇到了什么冲突呀？hive&#013;&#010;connector本身在依赖hive的时候确实也排除了很多传递依赖，才能正常运行UT和IT。也可以参考我们的pom来看排除了哪些依赖：&#013;&#010;https://github.com/apache/flink/blob/release-1.11.0/flink-connectors/flink-connector-hive/pom.xml&#013;&#010;&#013;&#010;On Fri, Jul 17, 2020 at 5:32 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt; 我用的是用户定义依赖，没有用捆绑依赖包，捆绑依赖包还要自己下载一次。。。。。&#013;&#010;&gt;&#013;&#010;&gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月17日周五 下午5:24写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 1.9和1.10时候排除一些传递依赖后在idea和打uber jar在集群环境都可以运行，不排除传递依赖的话在idea运行不了；&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.11现在只在本地测哪，不排除传递依赖idea运行不了，集群环境还没弄，但是我感觉在idea直接run这个功能好多人都需要，文档是不是可以改进一下&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月17日周五 下午5:16写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; 用bundle jar可以搞定吗？&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best,&#013;&#010;&gt; &gt;&gt; Jingsong&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; On Fri, Jul 17, 2020 at 5:14 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt; hi：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; 大佬们，下面连接hive的依赖包的哪个传递依赖导致的jar包冲突，我从1.9到1.11每次在maven按照官方文档引包都会出现依赖冲突。。。。1.9刚发布的时候对下面的引包有做依赖排除，后来文档改了&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; // Flink's Hive connector.Contains flink-hadoop-compatibility and&#013;&#010;&gt; &gt;&gt; &gt; flink-orc jars&#013;&#010;&gt; &gt;&gt; &gt;        flink-connector-hive_2.11-1.11.0.jar&#013;&#010;&gt; &gt;&gt; &gt;        // Hive dependencies&#013;&#010;&gt; &gt;&gt; &gt;        hive-exec-2.3.4.jar&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; --&#013;&#010;&gt; &gt;&gt; Best, Jingsong Lee&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "4",
        "reply": "<CAEZk043Lqp9PdDG+Nt_tRBtR4npqbHp14Az1GrVOCOK+P-Wb2A@mail.gmail.com>"
    },
    {
        "id": "<CAEZk0426M+Y8k-KARfpQbEiFPXB4PDgm=4GFx6jN3cOPD6=E+A@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 09:20:06 GMT",
        "subject": "Re: connector hive依赖冲突",
        "content": "hi,&#013;&#010;不排除依赖的话环境都起不来的哈，&#013;&#010;java.lang.IncompatibleClassChangeError: Implementing class&#013;&#010;&#013;&#010;at java.lang.ClassLoader.defineClass1(Native Method)&#013;&#010;at java.lang.ClassLoader.defineClass(ClassLoader.java:756)&#013;&#010;at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)&#013;&#010;at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)&#013;&#010;at java.net.URLClassLoader.access$100(URLClassLoader.java:74)&#013;&#010;at java.net.URLClassLoader$1.run(URLClassLoader.java:369)&#013;&#010;at java.net.URLClassLoader$1.run(URLClassLoader.java:363)&#013;&#010;at java.security.AccessController.doPrivileged(Native Method)&#013;&#010;at java.net.URLClassLoader.findClass(URLClassLoader.java:362)&#013;&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:418)&#013;&#010;at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)&#013;&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:351)&#013;&#010;at&#013;&#010;org.apache.flink.table.planner.delegation.PlannerBase.&lt;init&gt;(PlannerBase.scala:112)&#013;&#010;at&#013;&#010;org.apache.flink.table.planner.delegation.StreamPlanner.&lt;init&gt;(StreamPlanner.scala:48)&#013;&#010;at&#013;&#010;org.apache.flink.table.planner.delegation.BlinkPlannerFactory.create(BlinkPlannerFactory.java:50)&#013;&#010;at&#013;&#010;org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:130)&#013;&#010;at&#013;&#010;org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:111)&#013;&#010;at&#013;&#010;com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:24)&#013;&#010;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;at&#013;&#010;sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;at&#013;&#010;sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;at java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;at&#013;&#010;org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#013;&#010;at&#013;&#010;org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#013;&#010;at&#013;&#010;org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#013;&#010;at&#013;&#010;org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#013;&#010;at&#013;&#010;org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#013;&#010;at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#013;&#010;at&#013;&#010;org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#013;&#010;at&#013;&#010;org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#013;&#010;at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#013;&#010;at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#013;&#010;at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#013;&#010;at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#013;&#010;at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#013;&#010;at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#013;&#010;at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#013;&#010;at&#013;&#010;com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#013;&#010;at&#013;&#010;com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#013;&#010;at&#013;&#010;com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#013;&#010;at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#013;&#010;&#013;&#010;Rui Li &lt;lirui.fudan@gmail.com&gt; 于2020年7月20日周一 上午11:48写道：&#013;&#010;&#013;&#010;&gt; 现在具体是遇到了什么冲突呀？hive&#013;&#010;&gt; connector本身在依赖hive的时候确实也排除了很多传递依赖，才能正常运行UT和IT。也可以参考我们的pom来看排除了哪些依赖：&#013;&#010;&gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/release-1.11.0/flink-connectors/flink-connector-hive/pom.xml&#013;&#010;&gt;&#013;&#010;&gt; On Fri, Jul 17, 2020 at 5:32 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi&#013;&#010;&gt; &gt; 我用的是用户定义依赖，没有用捆绑依赖包，捆绑依赖包还要自己下载一次。。。。。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月17日周五 下午5:24写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 1.9和1.10时候排除一些传递依赖后在idea和打uber jar在集群环境都可以运行，不排除传递依赖的话在idea运行不了；&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.11现在只在本地测哪，不排除传递依赖idea运行不了，集群环境还没弄，但是我感觉在idea直接run这个功能好多人都需要，文档是不是可以改进一下&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月17日周五 下午5:16写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; 用bundle jar可以搞定吗？&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; [1]&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; Best,&#013;&#010;&gt; &gt; &gt;&gt; Jingsong&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; On Fri, Jul 17, 2020 at 5:14 PM Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;wrote:&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; &gt; hi：&#013;&#010;&gt; &gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 大佬们，下面连接hive的依赖包的哪个传递依赖导致的jar包冲突，我从1.9到1.11每次在maven按照官方文档引包都会出现依赖冲突。。。。1.9刚发布的时候对下面的引包有做依赖排除，后来文档改了&#013;&#010;&gt; &gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; &gt; // Flink's Hive connector.Contains flink-hadoop-compatibility and&#013;&#010;&gt; &gt; &gt;&gt; &gt; flink-orc jars&#013;&#010;&gt; &gt; &gt;&gt; &gt;        flink-connector-hive_2.11-1.11.0.jar&#013;&#010;&gt; &gt; &gt;&gt; &gt;        // Hive dependencies&#013;&#010;&gt; &gt; &gt;&gt; &gt;        hive-exec-2.3.4.jar&#013;&#010;&gt; &gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt; --&#013;&#010;&gt; &gt; &gt;&gt; Best, Jingsong Lee&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best regards!&#013;&#010;&gt; Rui Li&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAEZk043Lqp9PdDG+Nt_tRBtR4npqbHp14Az1GrVOCOK+P-Wb2A@mail.gmail.com>"
    },
    {
        "id": "<tencent_5E2B0E14A76295A54920A59B9E0204506908@qq.com>",
        "from": "&quot;867127831&quot; &lt;867127...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 09:26:58 GMT",
        "subject": "flink不带参数的udf始终返回第一次调用的结果",
        "content": "hi, 我有一个不带参数的udf，用于返回系统当前时间的字符串格式，但是调用时每次都返回这个udf第一次调用的结果，所以拿到的时间全部都是一样的&#013;&#010;&#013;&#010;&#013;&#010;udf的实时如下：&#013;&#010;&#013;&#010;&#013;&#010;public class GetTimeFunc extends ScalarFunction {&#013;&#010;&#009;public String eval() {&#013;&#010;&#009;&#009;return new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").format(new Date());&#013;&#010;&#009;}&#013;&#010;}&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;请问，针对这种没有入参的udf，flink内部是有做什么优化吗，导致每次调用返回的结果都一样？",
        "depth": "0",
        "reply": "<tencent_5E2B0E14A76295A54920A59B9E0204506908@qq.com>"
    },
    {
        "id": "<CABKuJ_S4k1b5OTTYbc8d7=Qy3AKy9_wSnEq4cQgkhHWnd1D0Cg@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 11:50:50 GMT",
        "subject": "Re: flink不带参数的udf始终返回第一次调用的结果",
        "content": "是的，这种就被当做常量被优化掉了。&#013;&#010;你可以覆盖一下ScalarFunction#isDeterministic方法，说明你这个函数时非确定性的，就不会被优化掉了。&#013;&#010;&#013;&#010;867127831 &lt;867127831@qq.com&gt; 于2020年7月17日周五 下午5:27写道：&#013;&#010;&#013;&#010;&gt; hi, 我有一个不带参数的udf，用于返回系统当前时间的字符串格式，但是调用时每次都返回这个udf第一次调用的结果，所以拿到的时间全部都是一样的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; udf的实时如下：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; public class GetTimeFunc extends ScalarFunction {&#013;&#010;&gt;         public String eval() {&#013;&#010;&gt;                 return new SimpleDateFormat(\"yyyy-MM-dd&#013;&#010;&gt; HH:mm:ss\").format(new Date());&#013;&#010;&gt;         }&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 请问，针对这种没有入参的udf，flink内部是有做什么优化吗，导致每次调用返回的结果都一样？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_5E2B0E14A76295A54920A59B9E0204506908@qq.com>"
    },
    {
        "id": "<1594988110813-0.post@n8.nabble.com>",
        "from": "Jun He &lt;867127...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 12:15:10 GMT",
        "subject": "Re: flink不带参数的udf始终返回第一次调用的结果",
        "content": "感谢，是这个原因。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_5E2B0E14A76295A54920A59B9E0204506908@qq.com>"
    },
    {
        "id": "<tencent_8CBD88635E10004B1F4327727F83355EF506@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 10:22:07 GMT",
        "subject": "Flink Cli 部署问题",
        "content": "大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#010;---&amp;gt; /jobs/{jobid}/savepoints ---&amp;gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#013;&#010;2020-07-17 09:51:48,925 INFO&amp;nbsp; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp;&#010;- Request slot with profile ResourceProfile{UNKNOWN} for job 7639673873b707aa86c4387aa7b4aac3&#010;with allocation id e8865cdbfe4c3c33099c7112bc2e3231.&#013;&#010;2020-07-17 09:51:48,952 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Source: Custom Source -&amp;gt; Filter (1/1) (1177659bff014e8dbc3f0508055d4307)&#010;switched from SCHEDULED to DEPLOYING.&#013;&#010;2020-07-17 09:51:48,952 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Deploying Source: Custom Source -&amp;gt; Filter (1/1)&#010;(attempt #0) to e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;2020-07-17 09:51:48,953 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Source: Custom Source (1/1) (141f0dc22b624b39e21127f637ba63c2)&#010;switched from SCHEDULED to DEPLOYING.&#013;&#010;2020-07-17 09:51:48,953 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to e63d829deafc144cd82efd73979dd056&#010;@ 083f69d029de (dataPort=35758)&#013;&#010;2020-07-17 09:51:48,954 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Source: Custom Source (1/1) (274b3df03e1fab627059c1a78e4a26da)&#010;switched from SCHEDULED to DEPLOYING.&#013;&#010;2020-07-17 09:51:48,954 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to e63d829deafc144cd82efd73979dd056&#010;@ 083f69d029de (dataPort=35758)&#013;&#010;2020-07-17 09:51:48,954 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched&#010;from SCHEDULED to DEPLOYING.&#013;&#010;2020-07-17 09:51:48,954 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to e63d829deafc144cd82efd73979dd056&#010;@ 083f69d029de (dataPort=35758)&#013;&#010;2020-07-17 09:51:48,955 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Co-Process -&amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#010;(618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to DEPLOYING.&#013;&#010;2020-07-17 09:51:48,955 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Deploying Co-Process -&amp;gt; (Sink: Unnamed, Sink: Unnamed)&#010;(1/1) (attempt #0) to e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;2020-07-17 09:51:49,346 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Co-Process -&amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#010;(618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING to RUNNING.&#013;&#010;2020-07-17 09:51:49,370 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Source: Custom Source (1/1) (274b3df03e1fab627059c1a78e4a26da)&#010;switched from DEPLOYING to RUNNING.&#013;&#010;2020-07-17 09:51:49,370 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Source: Custom Source (1/1) (141f0dc22b624b39e21127f637ba63c2)&#010;switched from DEPLOYING to RUNNING.&#013;&#010;2020-07-17 09:51:49,377 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched&#010;from DEPLOYING to RUNNING.&#013;&#010;2020-07-17 09:51:49,377 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Source: Custom Source -&amp;gt; Filter (1/1) (1177659bff014e8dbc3f0508055d4307)&#010;switched from DEPLOYING to RUNNING.&#013;&#010;2020-07-17 09:51:49,493 INFO&amp;nbsp; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched&#010;from RUNNING to FAILED.&#013;&#010;java.lang.Exception: Exception while creating StreamOperatorStateContext.&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&#009;at java.lang.Thread.run(Thread.java:748)&#013;&#010;Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for&#010;LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1) from any of the 1 provided&#010;restore options.&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&#009;... 9 more&#013;&#010;Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&#009;... 11 more&#013;&#010;Caused by: java.io.EOFException&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&#009;at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&#009;... 15 more",
        "depth": "0",
        "reply": "<tencent_8CBD88635E10004B1F4327727F83355EF506@qq.com>"
    },
    {
        "id": "<CAA8tFvuODQompr38MjN6nuD0_5VPAZMCZcKS4-onkpQUmOEjCA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 14:52:10 GMT",
        "subject": "Re: Flink Cli 部署问题",
        "content": "Hi&#010;&#010;请问你使用哪个版本的 Flink 呢？能否分享一下  Co-Process (1/1)&#010;(d0309f26a545e74643382ed3f758269b) 这个 tm 的 log 呢？从上面给的日志看，应该是在&#010;083f69d029de&#010;这台机器上。&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Z-Z &lt;zz9876543210@qq.com&gt; 于2020年7月17日周五 下午6:22写道：&#010;&#010;&gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#010;&gt; ---&amp;gt; /jobs/{jobid}/savepoints ---&amp;gt;&#010;&gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#010;&gt; 2020-07-17 09:51:48,925 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;nbsp; -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 7639673873b707aa86c4387aa7b4aac3 with allocation id&#010;&gt; e8865cdbfe4c3c33099c7112bc2e3231.&#010;&gt; 2020-07-17 09:51:48,952 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Source: Custom Source -&amp;gt; Filter (1/1)&#010;&gt; (1177659bff014e8dbc3f0508055d4307) switched from SCHEDULED to DEPLOYING.&#010;&gt; 2020-07-17 09:51:48,952 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Deploying Source: Custom Source -&amp;gt; Filter (1/1) (attempt #0) to&#010;&gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; 2020-07-17 09:51:48,953 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Source: Custom Source (1/1) (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; 2020-07-17 09:51:48,953 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#010;&gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; 2020-07-17 09:51:48,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Source: Custom Source (1/1) (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; 2020-07-17 09:51:48,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#010;&gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; 2020-07-17 09:51:48,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched from&#010;&gt; SCHEDULED to DEPLOYING.&#010;&gt; 2020-07-17 09:51:48,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to&#010;&gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; 2020-07-17 09:51:48,955 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Co-Process -&amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#010;&gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to DEPLOYING.&#010;&gt; 2020-07-17 09:51:48,955 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Deploying Co-Process -&amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#010;&gt; (attempt #0) to e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; (dataPort=35758)&#010;&gt; 2020-07-17 09:51:49,346 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Co-Process -&amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#010;&gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING to RUNNING.&#010;&gt; 2020-07-17 09:51:49,370 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Source: Custom Source (1/1) (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; switched from DEPLOYING to RUNNING.&#010;&gt; 2020-07-17 09:51:49,370 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Source: Custom Source (1/1) (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; switched from DEPLOYING to RUNNING.&#010;&gt; 2020-07-17 09:51:49,377 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched from&#010;&gt; DEPLOYING to RUNNING.&#010;&gt; 2020-07-17 09:51:49,377 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Source: Custom Source -&amp;gt; Filter (1/1)&#010;&gt; (1177659bff014e8dbc3f0508055d4307) switched from DEPLOYING to RUNNING.&#010;&gt; 2020-07-17 09:51:49,493 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched from&#010;&gt; RUNNING to FAILED.&#010;&gt; java.lang.Exception: Exception while creating StreamOperatorStateContext.&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: org.apache.flink.util.FlinkException: Could not restore keyed&#010;&gt; state backend for&#010;&gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1) from&#010;&gt; any of the 1 provided restore options.&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt;         ... 9 more&#010;&gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught&#010;&gt; unexpected exception.&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt;         ... 11 more&#010;&gt; Caused by: java.io.EOFException&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt;         at&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt;         ... 15 more&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_8CBD88635E10004B1F4327727F83355EF506@qq.com>"
    },
    {
        "id": "<tencent_7C2215791E004C7D699D9FFD25D687D50D08@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 15:10:12 GMT",
        "subject": "回复： Flink Cli 部署问题",
        "content": "Flink 1.10.0 ,taskmanager报错日志如下：&#013;&#010;&#013;&#010;&#013;&#010;2020-07-17 15:06:43,913 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;nbsp;&#010;- Caught unexpected exception.&#013;&#010;java.io.EOFException&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&#009;at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&#009;at java.lang.Thread.run(Thread.java:748)&#013;&#010;2020-07-17 15:06:43,914 WARN&amp;nbsp; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;nbsp;&#010;- Exception while restoring keyed state backend for KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1)&#010;from alternative (1/1), will retry while more alternatives are available.&#013;&#010;org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&#009;at java.lang.Thread.run(Thread.java:748)&#013;&#010;Caused by: java.io.EOFException&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&#009;at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&#009;... 15 more&#013;&#010;2020-07-17 15:06:43,915 INFO&amp;nbsp; org.apache.kafka.clients.producer.KafkaProducer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Closing the&#010;Kafka producer with timeoutMillis = 9223372036854775807 ms.&#013;&#010;2020-07-17 15:06:43,918 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Co-Keyed-Process -&amp;gt; Flat Map -&amp;gt; Sink: Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab)&#010;switched from RUNNING to FAILED.&#013;&#010;java.lang.Exception: Exception while creating StreamOperatorStateContext.&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&#009;at java.lang.Thread.run(Thread.java:748)&#013;&#010;Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for&#010;KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from any of the 1 provided restore&#010;options.&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&#009;... 9 more&#013;&#010;Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&#009;... 11 more&#013;&#010;Caused by: java.io.EOFException&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&#009;at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&#009;at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&#009;... 15 more&#013;&#010;2020-07-17 15:06:43,919 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Freeing task resources for Co-Keyed-Process -&amp;gt; Flat Map -&amp;gt; Sink:&#010;Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab).&#013;&#010;2020-07-17 15:06:43,919 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Ensuring all FileSystem streams are closed for task Co-Keyed-Process -&amp;gt;&#010;Flat Map -&amp;gt; Sink: Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) [FAILED]&#013;&#010;2020-07-17 15:06:43,931 INFO&amp;nbsp; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Un-registering task and sending final&#010;execution state FAILED to JobManager for task Co-Keyed-Process -&amp;gt; Flat Map -&amp;gt;&#010;Sink: Unnamed (1/1) bb8f0a84e07ef90b1e11ca2825e0efab.&#013;&#010;2020-07-17 15:06:43,947 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Attempting to cancel task Source: Custom Source -&amp;gt; Flat Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;2020-07-17 15:06:43,947 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Source: Custom Source -&amp;gt; Flat Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce)&#010;switched from RUNNING to CANCELING.&#013;&#010;2020-07-17 15:06:43,947 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Triggering cancellation of task code Source: Custom Source -&amp;gt; Flat Map&#010;(1/1) (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;2020-07-17 15:06:43,949 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Attempting to cancel task Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;2020-07-17 15:06:43,949 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600) switched from RUNNING&#010;to CANCELING.&#013;&#010;2020-07-17 15:06:43,949 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Triggering cancellation of task code Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;2020-07-17 15:06:43,954 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Source: Custom Source -&amp;gt; Flat Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce)&#010;switched from CANCELING to CANCELED.&#013;&#010;2020-07-17 15:06:43,954 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Freeing task resources for Source: Custom Source -&amp;gt; Flat Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;2020-07-17 15:06:43,954 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Ensuring all FileSystem streams are closed for task Source: Custom Source -&amp;gt;&#010;Flat Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) [CANCELED]&#013;&#010;2020-07-17 15:06:43,954 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600) switched from CANCELING&#010;to CANCELED.&#013;&#010;2020-07-17 15:06:43,955 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Freeing task resources for Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;2020-07-17 15:06:43,954 INFO&amp;nbsp; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Un-registering task and sending final&#010;execution state CANCELED to JobManager for task Source: Custom Source -&amp;gt; Flat Map (1/1)&#010;9cb8dcd4982223adcb6f007f1ffccdce.&#013;&#010;2020-07-17 15:06:43,962 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;- Ensuring all FileSystem streams are closed for task Source: Custom Source (1/1)&#010;(00621ff5d788d00c73ccaaea04717600) [CANCELED]&#013;&#010;2020-07-17 15:06:43,962 INFO&amp;nbsp; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Un-registering task and sending final&#010;execution state CANCELED to JobManager for task Source: Custom Source (1/1) 00621ff5d788d00c73ccaaea04717600.&#013;&#010;2020-07-17 15:06:44,077 WARN&amp;nbsp; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'transaction.timeout.ms'&#010;was supplied but isn't a known config.&#013;&#010;2020-07-17 15:06:44,077 WARN&amp;nbsp; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'key.serializer'&#010;was supplied but isn't a known config.&#013;&#010;2020-07-17 15:06:44,077 WARN&amp;nbsp; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'value.serializer'&#010;was supplied but isn't a known config.&#013;&#010;2020-07-17 15:06:44,077 INFO&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;-&#010;Kafka version : 0.11.0.2&#013;&#010;2020-07-17 15:06:44,077 INFO&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;-&#010;Kafka commitId : 73be1e1168f91ee2&#013;&#010;2020-07-17 15:06:44,077 WARN&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;-&#010;Error registering AppInfo mbean&#013;&#010;javax.management.InstanceAlreadyExistsException: kafka.consumer:type=app-info,id=consumer-3&#013;&#010;&#009;at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&#009;at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&#009;at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&#009;at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:757)&#013;&#010;&#009;at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:633)&#013;&#010;&#009;at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:615)&#013;&#010;&#009;at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&#009;at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;2020-07-17 15:06:44,079 INFO&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;-&#010;Kafka version : 0.11.0.2&#013;&#010;2020-07-17 15:06:44,079 INFO&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;-&#010;Kafka commitId : 73be1e1168f91ee2&#013;&#010;2020-07-17 15:06:44,079 WARN&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;-&#010;Error registering AppInfo mbean&#013;&#010;javax.management.InstanceAlreadyExistsException: kafka.consumer:type=app-info,id=consumer-4&#013;&#010;&#009;at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&#009;at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&#009;at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&#009;at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&#009;at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:757)&#013;&#010;&#009;at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:633)&#013;&#010;&#009;at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:615)&#013;&#010;&#009;at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&#009;at org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月17日(星期五) 晚上10:52&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;请问你使用哪个版本的 Flink 呢？能否分享一下&amp;nbsp; Co-Process (1/1)&#013;&#010;(d0309f26a545e74643382ed3f758269b) 这个 tm 的 log 呢？从上面给的日志看，应该是在&#010;083f69d029de&#013;&#010;这台机器上。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Z-Z &lt;zz9876543210@qq.com&amp;gt; 于2020年7月17日周五 下午6:22写道：&#013;&#010;&#013;&#010;&amp;gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#013;&#010;&amp;gt; ---&amp;amp;gt; /jobs/{jobid}/savepoints ---&amp;amp;gt;&#013;&#010;&amp;gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#013;&#010;&amp;gt; 2020-07-17 09:51:48,925 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;amp;nbsp;&#010;-&#013;&#010;&amp;gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#013;&#010;&amp;gt; 7639673873b707aa86c4387aa7b4aac3 with allocation id&#013;&#010;&amp;gt; e8865cdbfe4c3c33099c7112bc2e3231.&#013;&#010;&amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Source: Custom Source -&amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Deploying Source: Custom Source -&amp;amp;gt; Filter (1/1) (attempt&#010;#0) to&#013;&#010;&amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1) (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#013;&#010;&amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1) (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#013;&#010;&amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched from&#013;&#010;&amp;gt; SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to&#013;&#010;&amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Co-Process -&amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Deploying Co-Process -&amp;amp;gt; (Sink: Unnamed, Sink: Unnamed)&#010;(1/1)&#013;&#010;&amp;gt; (attempt #0) to e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#013;&#010;&amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; 2020-07-17 09:51:49,346 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Co-Process -&amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1) (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1) (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched from&#013;&#010;&amp;gt; DEPLOYING to RUNNING.&#013;&#010;&amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Source: Custom Source -&amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; 2020-07-17 09:51:49,493 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b) switched from&#013;&#010;&amp;gt; RUNNING to FAILED.&#013;&#010;&amp;gt; java.lang.Exception: Exception while creating StreamOperatorStateContext.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore keyed&#013;&#010;&amp;gt; state backend for&#013;&#010;&amp;gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1) from&#013;&#010;&amp;gt; any of the 1 provided restore options.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ...&#010;9 more&#013;&#010;&amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught&#013;&#010;&amp;gt; unexpected exception.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ...&#010;11 more&#013;&#010;&amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ...&#010;15 more",
        "depth": "2",
        "reply": "<tencent_8CBD88635E10004B1F4327727F83355EF506@qq.com>"
    },
    {
        "id": "<CAA8tFvtjXpW3ir1EoTAGeTyfL8J9pLEOLXo5KGw5dy_s9-5vMg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 19 Jul 2020 12:22:00 GMT",
        "subject": "Re: Flink Cli 部署问题",
        "content": "Hi&#010;&#010;从你给的这部分日志看，是恢复的时候遇到 EOF 了，这个比较奇怪&#010;1 你之前的 savepoint 是使用 RocksDBStateBackend 生成的吗&#010;2 你还有之前在 DFS 上的 savepoint 文件吗？可能需要结合 DFS 上的文件一起看一下这个问题怎么来的&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Z-Z &lt;zz9876543210@qq.com&gt; 于2020年7月17日周五 下午11:10写道：&#010;&#010;&gt; Flink 1.10.0 ,taskmanager报错日志如下：&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-17 15:06:43,913 ERROR&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;nbsp;&#010;&gt; - Caught unexpected exception.&#010;&gt; java.io.EOFException&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt;         at&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt; 2020-07-17 15:06:43,914 WARN&amp;nbsp;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;nbsp; -&#010;&gt; Exception while restoring keyed state backend for&#010;&gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#010;&gt; alternative (1/1), will retry while more alternatives are available.&#010;&gt; org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected&#010;&gt; exception.&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: java.io.EOFException&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt;         at&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt;         ... 15 more&#010;&gt; 2020-07-17 15:06:43,915 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.clients.producer.KafkaProducer&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Closing the Kafka producer with timeoutMillis&#010;&gt; = 9223372036854775807 ms.&#010;&gt; 2020-07-17 15:06:43,918 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Co-Keyed-Process -&amp;gt;&#010;Flat Map&#010;&gt; -&amp;gt; Sink: Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) switched from&#010;&gt; RUNNING to FAILED.&#010;&gt; java.lang.Exception: Exception while creating StreamOperatorStateContext.&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: org.apache.flink.util.FlinkException: Could not restore keyed&#010;&gt; state backend for&#010;&gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from any of&#010;&gt; the 1 provided restore options.&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt;         ... 9 more&#010;&gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught&#010;&gt; unexpected exception.&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt;         ... 11 more&#010;&gt; Caused by: java.io.EOFException&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt;         at java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt;         at&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt;         ... 15 more&#010;&gt; 2020-07-17 15:06:43,919 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Freeing task resources&#010;for&#010;&gt; Co-Keyed-Process -&amp;gt; Flat Map -&amp;gt; Sink: Unnamed (1/1)&#010;&gt; (bb8f0a84e07ef90b1e11ca2825e0efab).&#010;&gt; 2020-07-17 15:06:43,919 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Ensuring all FileSystem&#010;streams&#010;&gt; are closed for task Co-Keyed-Process -&amp;gt; Flat Map -&amp;gt; Sink: Unnamed&#010;&gt; (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) [FAILED]&#010;&gt; 2020-07-17 15:06:43,931 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Un-registering task and sending final execution&#010;&gt; state FAILED to JobManager for task Co-Keyed-Process -&amp;gt; Flat Map -&amp;gt;&#010;&gt; Sink: Unnamed (1/1) bb8f0a84e07ef90b1e11ca2825e0efab.&#010;&gt; 2020-07-17 15:06:43,947 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Attempting to cancel&#010;task&#010;&gt; Source: Custom Source -&amp;gt; Flat Map (1/1)&#010;&gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; 2020-07-17 15:06:43,947 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Source: Custom Source&#010;-&amp;gt; Flat&#010;&gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from RUNNING to&#010;&gt; CANCELING.&#010;&gt; 2020-07-17 15:06:43,947 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Triggering cancellation&#010;of task&#010;&gt; code Source: Custom Source -&amp;gt; Flat Map (1/1)&#010;&gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; 2020-07-17 15:06:43,949 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Attempting to cancel&#010;task&#010;&gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#010;&gt; 2020-07-17 15:06:43,949 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Source: Custom Source&#010;(1/1)&#010;&gt; (00621ff5d788d00c73ccaaea04717600) switched from RUNNING to CANCELING.&#010;&gt; 2020-07-17 15:06:43,949 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Triggering cancellation&#010;of task&#010;&gt; code Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#010;&gt; 2020-07-17 15:06:43,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Source: Custom Source&#010;-&amp;gt; Flat&#010;&gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from CANCELING to&#010;&gt; CANCELED.&#010;&gt; 2020-07-17 15:06:43,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Freeing task resources&#010;for&#010;&gt; Source: Custom Source -&amp;gt; Flat Map (1/1)&#010;&gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; 2020-07-17 15:06:43,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Ensuring all FileSystem&#010;streams&#010;&gt; are closed for task Source: Custom Source -&amp;gt; Flat Map (1/1)&#010;&gt; (9cb8dcd4982223adcb6f007f1ffccdce) [CANCELED]&#010;&gt; 2020-07-17 15:06:43,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Source: Custom Source&#010;(1/1)&#010;&gt; (00621ff5d788d00c73ccaaea04717600) switched from CANCELING to CANCELED.&#010;&gt; 2020-07-17 15:06:43,955 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Freeing task resources&#010;for&#010;&gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#010;&gt; 2020-07-17 15:06:43,954 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Un-registering task and sending final execution&#010;&gt; state CANCELED to JobManager for task Source: Custom Source -&amp;gt; Flat Map&#010;&gt; (1/1) 9cb8dcd4982223adcb6f007f1ffccdce.&#010;&gt; 2020-07-17 15:06:43,962 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Ensuring all FileSystem&#010;streams&#010;&gt; are closed for task Source: Custom Source (1/1)&#010;&gt; (00621ff5d788d00c73ccaaea04717600) [CANCELED]&#010;&gt; 2020-07-17 15:06:43,962 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Un-registering task and sending final execution&#010;&gt; state CANCELED to JobManager for task Source: Custom Source (1/1)&#010;&gt; 00621ff5d788d00c73ccaaea04717600.&#010;&gt; 2020-07-17 15:06:44,077 WARN&amp;nbsp;&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'transaction.timeout.ms' was&#010;&gt; supplied but isn't a known config.&#010;&gt; 2020-07-17 15:06:44,077 WARN&amp;nbsp;&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'key.serializer' was supplied but&#010;&gt; isn't a known config.&#010;&gt; 2020-07-17 15:06:44,077 WARN&amp;nbsp;&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'value.serializer' was supplied&#010;&gt; but isn't a known config.&#010;&gt; 2020-07-17 15:06:44,077 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka version : 0.11.0.2&#010;&gt; 2020-07-17 15:06:44,077 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#010;&gt; 2020-07-17 15:06:44,077 WARN&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Error registering&#010;AppInfo mbean&#010;&gt; javax.management.InstanceAlreadyExistsException:&#010;&gt; kafka.consumer:type=app-info,id=consumer-3&#010;&gt;         at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#010;&gt;         at&#010;&gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#010;&gt;         at&#010;&gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#010;&gt;         at&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:757)&#010;&gt;         at&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:633)&#010;&gt;         at&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:615)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#010;&gt; 2020-07-17 15:06:44,079 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka version : 0.11.0.2&#010;&gt; 2020-07-17 15:06:44,079 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#010;&gt; 2020-07-17 15:06:44,079 WARN&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Error registering&#010;AppInfo mbean&#010;&gt; javax.management.InstanceAlreadyExistsException:&#010;&gt; kafka.consumer:type=app-info,id=consumer-4&#010;&gt;         at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#010;&gt;         at&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#010;&gt;         at&#010;&gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#010;&gt;         at&#010;&gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#010;&gt;         at&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:757)&#010;&gt;         at&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:633)&#010;&gt;         at&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;gt;(KafkaConsumer.java:615)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; qcx978132955@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月17日(星期五) 晚上10:52&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: Flink Cli 部署问题&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Hi&#010;&gt;&#010;&gt; 请问你使用哪个版本的 Flink 呢？能否分享一下&amp;nbsp; Co-Process (1/1)&#010;&gt; (d0309f26a545e74643382ed3f758269b) 这个 tm 的 log 呢？从上面给的日志看，应该是在&#010;083f69d029de&#010;&gt; 这台机器上。&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; Z-Z &lt;zz9876543210@qq.com&amp;gt; 于2020年7月17日周五 下午6:22写道：&#010;&gt;&#010;&gt; &amp;gt;&#010;&gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#010;&gt; &amp;gt; ---&amp;amp;gt; /jobs/{jobid}/savepoints ---&amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#010;&gt; &amp;gt; 2020-07-17 09:51:48,925 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;amp;nbsp;&#010;&gt; -&#010;&gt; &amp;gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; &amp;gt; 7639673873b707aa86c4387aa7b4aac3 with allocation id&#010;&gt; &amp;gt; e8865cdbfe4c3c33099c7112bc2e3231.&#010;&gt; &amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Source: Custom Source -&amp;amp;gt; Filter (1/1)&#010;&gt; &amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from SCHEDULED to&#010;&gt; DEPLOYING.&#010;&gt; &amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Deploying Source: Custom Source -&amp;amp;gt; Filter (1/1)&#010;&gt; (attempt #0) to&#010;&gt; &amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; &amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; &amp;gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#010;&gt; &amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; &amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; &amp;gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#010;&gt; &amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; &amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#010;&gt; switched from&#010;&gt; &amp;gt; SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to&#010;&gt; &amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#010;&gt; &amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Co-Process -&amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#010;&gt; &amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to&#010;&gt; DEPLOYING.&#010;&gt; &amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Deploying Co-Process -&amp;amp;gt; (Sink: Unnamed, Sink:&#010;&gt; Unnamed) (1/1)&#010;&gt; &amp;gt; (attempt #0) to e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; &amp;gt; (dataPort=35758)&#010;&gt; &amp;gt; 2020-07-17 09:51:49,346 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Co-Process -&amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#010;&gt; &amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; &amp;gt; switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; &amp;gt; switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#010;&gt; switched from&#010;&gt; &amp;gt; DEPLOYING to RUNNING.&#010;&gt; &amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Source: Custom Source -&amp;amp;gt; Filter (1/1)&#010;&gt; &amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; 2020-07-17 09:51:49,493 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#010;&gt; switched from&#010;&gt; &amp;gt; RUNNING to FAILED.&#010;&gt; &amp;gt; java.lang.Exception: Exception while creating&#010;&gt; StreamOperatorStateContext.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore&#010;&gt; keyed&#010;&gt; &amp;gt; state backend for&#010;&gt; &amp;gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1)&#010;&gt; from&#010;&gt; &amp;gt; any of the 1 provided restore options.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;... 9 more&#010;&gt; &amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException:&#010;&gt; Caught&#010;&gt; &amp;gt; unexpected exception.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;... 11 more&#010;&gt; &amp;gt; Caused by: java.io.EOFException&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;... 15 more&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_8CBD88635E10004B1F4327727F83355EF506@qq.com>"
    },
    {
        "id": "<tencent_2F3C4800883EA9CED47E64EB93EA7F11B307@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 01:42:02 GMT",
        "subject": "回复： Flink Cli 部署问题",
        "content": "谢谢回复：&#013;&#010;之前的savepoint都是通过RocksDBStateBackend生成的；&#013;&#010;这个savepoint我通过webui 提交任务就没问题，你是说在IDE上调试savepoint吗&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                                                                        \"user-zh\"                                                                                    &lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月19日(星期天) 晚上8:22&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;从你给的这部分日志看，是恢复的时候遇到 EOF 了，这个比较奇怪&#013;&#010;1 你之前的 savepoint 是使用 RocksDBStateBackend 生成的吗&#013;&#010;2 你还有之前在 DFS 上的 savepoint 文件吗？可能需要结合 DFS 上的文件一起看一下这个问题怎么来的&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Z-Z &lt;zz9876543210@qq.com&amp;gt; 于2020年7月17日周五 下午11:10写道：&#013;&#010;&#013;&#010;&amp;gt; Flink 1.10.0 ,taskmanager报错日志如下：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 2020-07-17 15:06:43,913 ERROR&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;amp;nbsp;&#013;&#010;&amp;gt; - Caught unexpected exception.&#013;&#010;&amp;gt; java.io.EOFException&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; 2020-07-17 15:06:43,914 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;amp;nbsp; -&#013;&#010;&amp;gt; Exception while restoring keyed state backend for&#013;&#010;&amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#013;&#010;&amp;gt; alternative (1/1), will retry while more alternatives are available.&#013;&#010;&amp;gt; org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected&#013;&#010;&amp;gt; exception.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 15 more&#013;&#010;&amp;gt; 2020-07-17 15:06:43,915 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.producer.KafkaProducer&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Closing the Kafka producer with timeoutMillis&#013;&#010;&amp;gt; = 9223372036854775807 ms.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,918 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Co-Keyed-Process -&amp;amp;gt; Flat Map&#013;&#010;&amp;gt; -&amp;amp;gt; Sink: Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) switched from&#013;&#010;&amp;gt; RUNNING to FAILED.&#013;&#010;&amp;gt; java.lang.Exception: Exception while creating StreamOperatorStateContext.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore keyed&#013;&#010;&amp;gt; state backend for&#013;&#010;&amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from any of&#013;&#010;&amp;gt; the 1 provided restore options.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 9 more&#013;&#010;&amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught&#013;&#010;&amp;gt; unexpected exception.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 11 more&#013;&#010;&amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 15 more&#013;&#010;&amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Freeing task resources for&#013;&#010;&amp;gt; Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt; Sink: Unnamed (1/1)&#013;&#010;&amp;gt; (bb8f0a84e07ef90b1e11ca2825e0efab).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Ensuring all FileSystem streams&#013;&#010;&amp;gt; are closed for task Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt; Sink: Unnamed&#013;&#010;&amp;gt; (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) [FAILED]&#013;&#010;&amp;gt; 2020-07-17 15:06:43,931 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending final execution&#013;&#010;&amp;gt; state FAILED to JobManager for task Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt;&#013;&#010;&amp;gt; Sink: Unnamed (1/1) bb8f0a84e07ef90b1e11ca2825e0efab.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Attempting to cancel task&#013;&#010;&amp;gt; Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source -&amp;amp;gt; Flat&#013;&#010;&amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from RUNNING to&#013;&#010;&amp;gt; CANCELING.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Triggering cancellation of task&#013;&#010;&amp;gt; code Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Attempting to cancel task&#013;&#010;&amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source (1/1)&#013;&#010;&amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from RUNNING to CANCELING.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Triggering cancellation of task&#013;&#010;&amp;gt; code Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source -&amp;amp;gt; Flat&#013;&#010;&amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from CANCELING to&#013;&#010;&amp;gt; CANCELED.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Freeing task resources for&#013;&#010;&amp;gt; Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Ensuring all FileSystem streams&#013;&#010;&amp;gt; are closed for task Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce) [CANCELED]&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source (1/1)&#013;&#010;&amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from CANCELING to CANCELED.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,955 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Freeing task resources for&#013;&#010;&amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending final execution&#013;&#010;&amp;gt; state CANCELED to JobManager for task Source: Custom Source -&amp;amp;gt; Flat Map&#013;&#010;&amp;gt; (1/1) 9cb8dcd4982223adcb6f007f1ffccdce.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Ensuring all FileSystem streams&#013;&#010;&amp;gt; are closed for task Source: Custom Source (1/1)&#013;&#010;&amp;gt; (00621ff5d788d00c73ccaaea04717600) [CANCELED]&#013;&#010;&amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending final execution&#013;&#010;&amp;gt; state CANCELED to JobManager for task Source: Custom Source (1/1)&#013;&#010;&amp;gt; 00621ff5d788d00c73ccaaea04717600.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'transaction.timeout.ms' was&#013;&#010;&amp;gt; supplied but isn't a known config.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'key.serializer' was supplied but&#013;&#010;&amp;gt; isn't a known config.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'value.serializer' was supplied&#013;&#010;&amp;gt; but isn't a known config.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka version : 0.11.0.2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Error registering AppInfo mbean&#013;&#010;&amp;gt; javax.management.InstanceAlreadyExistsException:&#013;&#010;&amp;gt; kafka.consumer:type=app-info,id=consumer-3&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:757)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:633)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:615)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;&amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka version : 0.11.0.2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,079 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Error registering AppInfo mbean&#013;&#010;&amp;gt; javax.management.InstanceAlreadyExistsException:&#013;&#010;&amp;gt; kafka.consumer:type=app-info,id=consumer-4&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:757)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:633)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:615)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;&#013;&#010;&amp;gt; qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月17日(星期五) 晚上10:52&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Hi&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 请问你使用哪个版本的 Flink 呢？能否分享一下&amp;amp;nbsp; Co-Process (1/1)&#013;&#010;&amp;gt; (d0309f26a545e74643382ed3f758269b) 这个 tm 的 log 呢？从上面给的日志看，应该是在 083f69d029de&#013;&#010;&amp;gt; 这台机器上。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Congxian&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Z-Z &lt;zz9876543210@qq.com&amp;amp;gt; 于2020年7月17日周五 下午6:22写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#013;&#010;&amp;gt; &amp;amp;gt; ---&amp;amp;amp;gt; /jobs/{jobid}/savepoints ---&amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,925 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; -&#013;&#010;&amp;gt; &amp;amp;gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#013;&#010;&amp;gt; &amp;amp;gt; 7639673873b707aa86c4387aa7b4aac3 with allocation id&#013;&#010;&amp;gt; &amp;amp;gt; e8865cdbfe4c3c33099c7112bc2e3231.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from SCHEDULED to&#013;&#010;&amp;gt; DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source -&amp;amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; &amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; &amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to&#013;&#010;&amp;gt; DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Co-Process -&amp;amp;amp;gt; (Sink: Unnamed, Sink:&#013;&#010;&amp;gt; Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (attempt #0) to e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#013;&#010;&amp;gt; &amp;amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,346 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; &amp;amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; &amp;amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,493 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; RUNNING to FAILED.&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.Exception: Exception while creating&#013;&#010;&amp;gt; StreamOperatorStateContext.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore&#013;&#010;&amp;gt; keyed&#013;&#010;&amp;gt; &amp;amp;gt; state backend for&#013;&#010;&amp;gt; &amp;amp;gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1)&#013;&#010;&amp;gt; from&#013;&#010;&amp;gt; &amp;amp;gt; any of the 1 provided restore options.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; ... 9 more&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException:&#013;&#010;&amp;gt; Caught&#013;&#010;&amp;gt; &amp;amp;gt; unexpected exception.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; ... 11 more&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; ... 15 more",
        "depth": "4",
        "reply": "<tencent_8CBD88635E10004B1F4327727F83355EF506@qq.com>"
    },
    {
        "id": "<tencent_670E96B137058A0044F977DFD91F56237F08@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 11:29:39 GMT",
        "subject": "flink-1.11 集成hive-1.2.1 DDL问题",
        "content": "idea 本地测试&#013;&#010;跟hive有关pom依赖&#013;&#010;hive-exec flink-connector-hive_2.11&#013;&#010;代码如下:&#013;&#010; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;        env.setParallelism(1);&#013;&#010;        env.enableCheckpointing(60*1000, CheckpointingMode.EXACTLY_ONCE);&#013;&#010;        // 同一时间只允许进行一个检查点&#013;&#010;        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);&#013;&#010;&#013;&#010;        env.setStateBackend(new FsStateBackend(path));&#013;&#010;        &#013;&#010;        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);&#013;&#010;&#013;&#010;        String name            = \"myhive\";&#013;&#010;        String defaultDatabase = \"situation\";&#013;&#010;        String hiveConfDir     = \"/load/data/hive/hive-conf\"; // a local path&#013;&#010;        String version         = \"1.2.1\";&#013;&#010;&#013;&#010;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase, hiveConfDir, version);&#013;&#010;        tableEnv.registerCatalog(\"myhive\", hive);&#013;&#010;&#013;&#010;// set the HiveCatalog as the current catalog of the session&#013;&#010;        tableEnv.useCatalog(\"myhive\");&#013;&#010;        tableEnv.executeSql(\"CREATE DATABASE IF NOT EXISTS stream_tmp\");&#013;&#010;        tableEnv.executeSql(\"DROP TABLE IF EXISTS stream_tmp.source_table\");&#013;&#010;&#013;&#010;&#013;&#010;报错如下：&#013;&#010;&amp;nbsp;&#013;&#010;Exception in thread \"main\" java.lang.IncompatibleClassChangeError: Implementing class&#013;&#010;&#009;at java.lang.ClassLoader.defineClass1(Native Method)&#013;&#010;&#009;at java.lang.ClassLoader.defineClass(ClassLoader.java:763)&#013;&#010;&#009;at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)&#013;&#010;&#009;at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)&#013;&#010;&#009;at java.net.URLClassLoader.access$100(URLClassLoader.java:73)&#013;&#010;&#009;at java.net.URLClassLoader$1.run(URLClassLoader.java:368)&#013;&#010;&#009;at java.net.URLClassLoader$1.run(URLClassLoader.java:362)&#013;&#010;&#009;at java.security.AccessController.doPrivileged(Native Method)&#013;&#010;&#009;at java.net.URLClassLoader.findClass(URLClassLoader.java:361)&#013;&#010;&#009;at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#013;&#010;&#009;at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#013;&#010;&#009;at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#013;&#010;&#009;at org.apache.flink.table.planner.delegation.PlannerBase.&lt;init&amp;gt;(PlannerBase.scala:112)&#013;&#010;&#009;at org.apache.flink.table.planner.delegation.StreamPlanner.&lt;init&amp;gt;(StreamPlanner.scala:48)&#013;&#010;&#009;at org.apache.flink.table.planner.delegation.BlinkPlannerFactory.create(BlinkPlannerFactory.java:50)&#013;&#010;&#009;at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:130)&#013;&#010;&#009;at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:111)&#013;&#010;&#009;at com.hive.HiveTest.main(HiveTest.java:33)",
        "depth": "0",
        "reply": "<tencent_670E96B137058A0044F977DFD91F56237F08@qq.com>"
    },
    {
        "id": "<CADH6UNQYgtoWROKBmdNLOdka0E=Ab5FjfsK5uoWNuLdbju5KYw@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:38:46 GMT",
        "subject": "Re: flink-1.11 集成hive-1.2.1 DDL问题",
        "content": "stacktrace上看起来是创建blink planner的时候出错的。检查下依赖的blink planner版本是不是正确？&#010;&#010;On Fri, Jul 17, 2020 at 7:29 PM kcz &lt;573693104@qq.com&gt; wrote:&#010;&#010;&gt; idea 本地测试&#010;&gt; 跟hive有关pom依赖&#010;&gt; hive-exec flink-connector-hive_2.11&#010;&gt; 代码如下:&#010;&gt;  StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;         env.setParallelism(1);&#010;&gt;         env.enableCheckpointing(60*1000, CheckpointingMode.EXACTLY_ONCE);&#010;&gt;         // 同一时间只允许进行一个检查点&#010;&gt;         env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);&#010;&gt;&#010;&gt;         env.setStateBackend(new FsStateBackend(path));&#010;&gt;&#010;&gt;         StreamTableEnvironment tableEnv =&#010;&gt; StreamTableEnvironment.create(env);&#010;&gt;&#010;&gt;         String name            = \"myhive\";&#010;&gt;         String defaultDatabase = \"situation\";&#010;&gt;         String hiveConfDir     = \"/load/data/hive/hive-conf\"; // a local&#010;&gt; path&#010;&gt;         String version         = \"1.2.1\";&#010;&gt;&#010;&gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; hiveConfDir, version);&#010;&gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt;&#010;&gt; // set the HiveCatalog as the current catalog of the session&#010;&gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt;         tableEnv.executeSql(\"CREATE DATABASE IF NOT EXISTS stream_tmp\");&#010;&gt;         tableEnv.executeSql(\"DROP TABLE IF EXISTS&#010;&gt; stream_tmp.source_table\");&#010;&gt;&#010;&gt;&#010;&gt; 报错如下：&#010;&gt; &amp;nbsp;&#010;&gt; Exception in thread \"main\" java.lang.IncompatibleClassChangeError:&#010;&gt; Implementing class&#010;&gt;         at java.lang.ClassLoader.defineClass1(Native Method)&#010;&gt;         at java.lang.ClassLoader.defineClass(ClassLoader.java:763)&#010;&gt;         at&#010;&gt; java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)&#010;&gt;         at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)&#010;&gt;         at java.net.URLClassLoader.access$100(URLClassLoader.java:73)&#010;&gt;         at java.net.URLClassLoader$1.run(URLClassLoader.java:368)&#010;&gt;         at java.net.URLClassLoader$1.run(URLClassLoader.java:362)&#010;&gt;         at java.security.AccessController.doPrivileged(Native Method)&#010;&gt;         at java.net.URLClassLoader.findClass(URLClassLoader.java:361)&#010;&gt;         at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;&gt;         at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#010;&gt;         at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.&lt;init&amp;gt;(PlannerBase.scala:112)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.&lt;init&amp;gt;(StreamPlanner.scala:48)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.BlinkPlannerFactory.create(BlinkPlannerFactory.java:50)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:130)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:111)&#010;&gt;         at com.hive.HiveTest.main(HiveTest.java:33)&#010;&#010;&#010;&#010;-- &#010;Best regards!&#010;Rui Li&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_670E96B137058A0044F977DFD91F56237F08@qq.com>"
    },
    {
        "id": "<CACt=kDscXcBptpboFXf3yv_vrLAxEKpvY0AjpkjvEXfDFtgM_A@mail.gmail.com>",
        "from": "Kurt Young &lt;ykt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 04:02:53 GMT",
        "subject": "Re: flink-1.11 集成hive-1.2.1 DDL问题",
        "content": "1.11 把默认planner换成blink了，需要添加下blink planner的依赖&#010;&#010;Best,&#010;Kurt&#010;&#010;&#010;On Mon, Jul 20, 2020 at 11:39 AM Rui Li &lt;lirui.fudan@gmail.com&gt; wrote:&#010;&#010;&gt; stacktrace上看起来是创建blink planner的时候出错的。检查下依赖的blink&#010;planner版本是不是正确？&#010;&gt;&#010;&gt; On Fri, Jul 17, 2020 at 7:29 PM kcz &lt;573693104@qq.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; idea 本地测试&#010;&gt; &gt; 跟hive有关pom依赖&#010;&gt; &gt; hive-exec flink-connector-hive_2.11&#010;&gt; &gt; 代码如下:&#010;&gt; &gt;  StreamExecutionEnvironment env =&#010;&gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt;         env.setParallelism(1);&#010;&gt; &gt;         env.enableCheckpointing(60*1000, CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &gt;         // 同一时间只允许进行一个检查点&#010;&gt; &gt;         env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);&#010;&gt; &gt;&#010;&gt; &gt;         env.setStateBackend(new FsStateBackend(path));&#010;&gt; &gt;&#010;&gt; &gt;         StreamTableEnvironment tableEnv =&#010;&gt; &gt; StreamTableEnvironment.create(env);&#010;&gt; &gt;&#010;&gt; &gt;         String name            = \"myhive\";&#010;&gt; &gt;         String defaultDatabase = \"situation\";&#010;&gt; &gt;         String hiveConfDir     = \"/load/data/hive/hive-conf\"; // a local&#010;&gt; &gt; path&#010;&gt; &gt;         String version         = \"1.2.1\";&#010;&gt; &gt;&#010;&gt; &gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; hiveConfDir, version);&#010;&gt; &gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt;&#010;&gt; &gt; // set the HiveCatalog as the current catalog of the session&#010;&gt; &gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt; &gt;         tableEnv.executeSql(\"CREATE DATABASE IF NOT EXISTS stream_tmp\");&#010;&gt; &gt;         tableEnv.executeSql(\"DROP TABLE IF EXISTS&#010;&gt; &gt; stream_tmp.source_table\");&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 报错如下：&#010;&gt; &gt; &amp;nbsp;&#010;&gt; &gt; Exception in thread \"main\" java.lang.IncompatibleClassChangeError:&#010;&gt; &gt; Implementing class&#010;&gt; &gt;         at java.lang.ClassLoader.defineClass1(Native Method)&#010;&gt; &gt;         at java.lang.ClassLoader.defineClass(ClassLoader.java:763)&#010;&gt; &gt;         at&#010;&gt; &gt; java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)&#010;&gt; &gt;         at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)&#010;&gt; &gt;         at java.net.URLClassLoader.access$100(URLClassLoader.java:73)&#010;&gt; &gt;         at java.net.URLClassLoader$1.run(URLClassLoader.java:368)&#010;&gt; &gt;         at java.net.URLClassLoader$1.run(URLClassLoader.java:362)&#010;&gt; &gt;         at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; &gt;         at java.net.URLClassLoader.findClass(URLClassLoader.java:361)&#010;&gt; &gt;         at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;&gt; &gt;         at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#010;&gt; &gt;         at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.&lt;init&amp;gt;(PlannerBase.scala:112)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.&lt;init&amp;gt;(StreamPlanner.scala:48)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.BlinkPlannerFactory.create(BlinkPlannerFactory.java:50)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:130)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:111)&#010;&gt; &gt;         at com.hive.HiveTest.main(HiveTest.java:33)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Best regards!&#010;&gt; Rui Li&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_670E96B137058A0044F977DFD91F56237F08@qq.com>"
    },
    {
        "id": "<tencent_A24E3CA568F93F848264AE155E0FCE7CEB09@qq.com>",
        "from": "&quot;claylin&quot; &lt;1012539...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 12:28:27 GMT",
        "subject": "sql 内嵌josn数组解析报 类型转换报错",
        "content": "hi all我这边有个嵌套的json数组，报类型转换错误(ts AS CAST(FROM_UNIXTIME(hiido_time)&#010;AS TIMESTAMP(3)),这里报错)，是不是不能这么写&#013;&#010;create table hiido_push_sdk_mq (&#013;&#010;datas&amp;nbsp; &amp;nbsp;ARRAY&lt;ROW&lt;`from` string,hdid string,event string,hiido_time&#010;bigint,ts AS CAST(FROM_UNIXTIME(hiido_time) AS TIMESTAMP(3)),WATERMARK FOR ts AS ts - INTERVAL&#010;'5' MINUTE&amp;gt;&amp;gt;&#013;&#010;) with (&#013;&#010;'connector' = 'kafka',&#013;&#010;'topic' = 'hiido_pushsdk_event',&#013;&#010;'properties.bootstrap.servers' = 'kafkafs002-core001.yy.com:8103,kafkafs002-core002.yy.com:8103,kafkafs002-core003.yy.com:8103',&#013;&#010;'properties.group.id' = 'push_click_sql_version_consumer',&#013;&#010;'scan.startup.mode' = 'latest-offset',&#013;&#010;'format.type' = 'json');&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;错误如下：&#013;&#010;[ERROR] 2020-07-17 20:17:50,640(562284338) --&amp;gt; [http-nio-8080-exec-10] com.yy.push.flink.sql.gateway.sql.parse.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:77):&#010;parseBySqlParser, parse: com.yy.push.flink.sql.gateway.context.JobContext$1@5d5f32d1, stmt:&#010;create table hiido_push_sdk_mq (&amp;nbsp; &amp;nbsp; datas&amp;nbsp; &amp;nbsp;ARRAY&lt;ROW&lt;`from`&#010;string,hdid string,event string,hiido_time bigint,ts AS CAST(FROM_UNIXTIME(hiido_time) AS&#010;TIMESTAMP(3)),WATERMARK FOR ts AS ts - INTERVAL '5' MINUTE&amp;gt;&amp;gt;) with ('connector'&#010;= 'kafka','topic' = 'hiido_pushsdk_event','properties.bootstrap.servers' = 'kafkafs002-core001.yy.com:8103,kafkafs002-core002.yy.com:8103,kafkafs002-core003.yy.com:8103','properties.group.id'&#010;= 'push_click_sql_version_consumer','scan.startup.mode' = 'latest-offset','format.type' =&#010;'json'), error info: SQL parse failed. Encountered \"AS\" at line 1, column 115.&#013;&#010;Was expecting one of:&#013;&#010;&amp;nbsp; &amp;nbsp; \"ROW\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; &lt;BRACKET_QUOTED_IDENTIFIER&amp;gt; ...&#013;&#010;&amp;nbsp; &amp;nbsp; &lt;QUOTED_IDENTIFIER&amp;gt; ...&#013;&#010;&amp;nbsp; &amp;nbsp; &lt;BACK_QUOTED_IDENTIFIER&amp;gt; ...&#013;&#010;&amp;nbsp; &amp;nbsp; &lt;IDENTIFIER&amp;gt; ...&#013;&#010;&amp;nbsp; &amp;nbsp; &lt;UNICODE_QUOTED_IDENTIFIER&amp;gt; ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"STRING\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"BYTES\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"ARRAY\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"MULTISET\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"RAW\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"BOOLEAN\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"INTEGER\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"INT\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"TINYINT\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"SMALLINT\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"BIGINT\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"REAL\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"DOUBLE\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"FLOAT\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"BINARY\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"VARBINARY\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"DECIMAL\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"DEC\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"NUMERIC\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"ANY\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"CHARACTER\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"CHAR\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"VARCHAR\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"DATE\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"TIME\" ...&#013;&#010;&amp;nbsp; &amp;nbsp; \"TIMESTAMP\" ...",
        "depth": "0",
        "reply": "<tencent_A24E3CA568F93F848264AE155E0FCE7CEB09@qq.com>"
    },
    {
        "id": "<CABKuJ_ROkO4DHdx9dL7ZO1FsZ54uWB1LR2VecmeFm5EJRQm3GA@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 12:33:33 GMT",
        "subject": "Re: sql 内嵌josn数组解析报 类型转换报错",
        "content": "计算列只能写在最外层，不能在嵌套类型里面有计算列。&#010;&#010;claylin &lt;1012539884@qq.com&gt; 于2020年7月17日周五 下午8:28写道：&#010;&#010;&gt; hi all我这边有个嵌套的json数组，报类型转换错误(ts AS CAST(FROM_UNIXTIME(hiido_time)&#010;AS&#010;&gt; TIMESTAMP(3)),这里报错)，是不是不能这么写&#010;&gt; create table hiido_push_sdk_mq (&#010;&gt; datas&amp;nbsp; &amp;nbsp;ARRAY&lt;ROW&lt;`from` string,hdid string,event&#010;&gt; string,hiido_time bigint,ts AS CAST(FROM_UNIXTIME(hiido_time) AS&#010;&gt; TIMESTAMP(3)),WATERMARK FOR ts AS ts - INTERVAL '5' MINUTE&amp;gt;&amp;gt;&#010;&gt; ) with (&#010;&gt; 'connector' = 'kafka',&#010;&gt; 'topic' = 'hiido_pushsdk_event',&#010;&gt; 'properties.bootstrap.servers' = 'kafkafs002-core001.yy.com:8103,&#010;&gt; kafkafs002-core002.yy.com:8103,kafkafs002-core003.yy.com:8103',&#010;&gt; 'properties.group.id' = 'push_click_sql_version_consumer',&#010;&gt; 'scan.startup.mode' = 'latest-offset',&#010;&gt; 'format.type' = 'json');&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 错误如下：&#010;&gt; [ERROR] 2020-07-17 20:17:50,640(562284338) --&amp;gt; [http-nio-8080-exec-10]&#010;&gt; com.yy.push.flink.sql.gateway.sql.parse.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:77):&#010;&gt; parseBySqlParser, parse:&#010;&gt; com.yy.push.flink.sql.gateway.context.JobContext$1@5d5f32d1, stmt: create&#010;&gt; table hiido_push_sdk_mq (&amp;nbsp; &amp;nbsp; datas&amp;nbsp; &amp;nbsp;ARRAY&lt;ROW&lt;`from`&#010;&gt; string,hdid string,event string,hiido_time bigint,ts AS&#010;&gt; CAST(FROM_UNIXTIME(hiido_time) AS TIMESTAMP(3)),WATERMARK FOR ts AS ts -&#010;&gt; INTERVAL '5' MINUTE&amp;gt;&amp;gt;) with ('connector' = 'kafka','topic' =&#010;&gt; 'hiido_pushsdk_event','properties.bootstrap.servers' = '&#010;&gt; kafkafs002-core001.yy.com:8103,kafkafs002-core002.yy.com:8103,&#010;&gt; kafkafs002-core003.yy.com:8103','properties.group.id' =&#010;&gt; 'push_click_sql_version_consumer','scan.startup.mode' =&#010;&gt; 'latest-offset','format.type' = 'json'), error info: SQL parse failed.&#010;&gt; Encountered \"AS\" at line 1, column 115.&#010;&gt; Was expecting one of:&#010;&gt; &amp;nbsp; &amp;nbsp; \"ROW\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; &lt;BRACKET_QUOTED_IDENTIFIER&amp;gt; ...&#010;&gt; &amp;nbsp; &amp;nbsp; &lt;QUOTED_IDENTIFIER&amp;gt; ...&#010;&gt; &amp;nbsp; &amp;nbsp; &lt;BACK_QUOTED_IDENTIFIER&amp;gt; ...&#010;&gt; &amp;nbsp; &amp;nbsp; &lt;IDENTIFIER&amp;gt; ...&#010;&gt; &amp;nbsp; &amp;nbsp; &lt;UNICODE_QUOTED_IDENTIFIER&amp;gt; ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"STRING\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"BYTES\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"ARRAY\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"MULTISET\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"RAW\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"BOOLEAN\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"INTEGER\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"INT\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"TINYINT\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"SMALLINT\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"BIGINT\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"REAL\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"DOUBLE\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"FLOAT\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"BINARY\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"VARBINARY\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"DECIMAL\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"DEC\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"NUMERIC\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"ANY\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"CHARACTER\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"CHAR\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"VARCHAR\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"DATE\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"TIME\" ...&#010;&gt; &amp;nbsp; &amp;nbsp; \"TIMESTAMP\" ...&#010;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_A24E3CA568F93F848264AE155E0FCE7CEB09@qq.com>"
    },
    {
        "id": "<tencent_071F4541B7D7DCAFD7674975909E6FB40B08@qq.com>",
        "from": "&quot;claylin&quot; &lt;1012539...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 12:37:09 GMT",
        "subject": "回复： sql 内嵌josn数组解析报 类型转换报错",
        "content": "那我这种内嵌式的数据结构是不能在sql里面解析了，数组每行转成表中的一列，还有watermark，只能在外部处理成单条记录然后用flink处理了吗&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月17日(星期五) 晚上8:33&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: sql 内嵌josn数组解析报 类型转换报错&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;计算列只能写在最外层，不能在嵌套类型里面有计算列。&#013;&#010;&#013;&#010;claylin &lt;1012539884@qq.com&amp;gt; 于2020年7月17日周五 下午8:28写道：&#013;&#010;&#013;&#010;&amp;gt; hi all我这边有个嵌套的json数组，报类型转换错误(ts AS CAST(FROM_UNIXTIME(hiido_time)&#010;AS&#013;&#010;&amp;gt; TIMESTAMP(3)),这里报错)，是不是不能这么写&#013;&#010;&amp;gt; create table hiido_push_sdk_mq (&#013;&#010;&amp;gt; datas&amp;amp;nbsp; &amp;amp;nbsp;ARRAY&lt;ROW&lt;`from` string,hdid string,event&#013;&#010;&amp;gt; string,hiido_time bigint,ts AS CAST(FROM_UNIXTIME(hiido_time) AS&#013;&#010;&amp;gt; TIMESTAMP(3)),WATERMARK FOR ts AS ts - INTERVAL '5' MINUTE&amp;amp;gt;&amp;amp;gt;&#013;&#010;&amp;gt; ) with (&#013;&#010;&amp;gt; 'connector' = 'kafka',&#013;&#010;&amp;gt; 'topic' = 'hiido_pushsdk_event',&#013;&#010;&amp;gt; 'properties.bootstrap.servers' = 'kafkafs002-core001.yy.com:8103,&#013;&#010;&amp;gt; kafkafs002-core002.yy.com:8103,kafkafs002-core003.yy.com:8103',&#013;&#010;&amp;gt; 'properties.group.id' = 'push_click_sql_version_consumer',&#013;&#010;&amp;gt; 'scan.startup.mode' = 'latest-offset',&#013;&#010;&amp;gt; 'format.type' = 'json');&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 错误如下：&#013;&#010;&amp;gt; [ERROR] 2020-07-17 20:17:50,640(562284338) --&amp;amp;gt; [http-nio-8080-exec-10]&#013;&#010;&amp;gt; com.yy.push.flink.sql.gateway.sql.parse.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:77):&#013;&#010;&amp;gt; parseBySqlParser, parse:&#013;&#010;&amp;gt; com.yy.push.flink.sql.gateway.context.JobContext$1@5d5f32d1, stmt: create&#013;&#010;&amp;gt; table hiido_push_sdk_mq (&amp;amp;nbsp; &amp;amp;nbsp; datas&amp;amp;nbsp; &amp;amp;nbsp;ARRAY&lt;ROW&lt;`from`&#013;&#010;&amp;gt; string,hdid string,event string,hiido_time bigint,ts AS&#013;&#010;&amp;gt; CAST(FROM_UNIXTIME(hiido_time) AS TIMESTAMP(3)),WATERMARK FOR ts AS ts -&#013;&#010;&amp;gt; INTERVAL '5' MINUTE&amp;amp;gt;&amp;amp;gt;) with ('connector' = 'kafka','topic'&#010;=&#013;&#010;&amp;gt; 'hiido_pushsdk_event','properties.bootstrap.servers' = '&#013;&#010;&amp;gt; kafkafs002-core001.yy.com:8103,kafkafs002-core002.yy.com:8103,&#013;&#010;&amp;gt; kafkafs002-core003.yy.com:8103','properties.group.id' =&#013;&#010;&amp;gt; 'push_click_sql_version_consumer','scan.startup.mode' =&#013;&#010;&amp;gt; 'latest-offset','format.type' = 'json'), error info: SQL parse failed.&#013;&#010;&amp;gt; Encountered \"AS\" at line 1, column 115.&#013;&#010;&amp;gt; Was expecting one of:&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ROW\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;BRACKET_QUOTED_IDENTIFIER&amp;amp;gt; ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;QUOTED_IDENTIFIER&amp;amp;gt; ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;BACK_QUOTED_IDENTIFIER&amp;amp;gt; ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;IDENTIFIER&amp;amp;gt; ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;UNICODE_QUOTED_IDENTIFIER&amp;amp;gt; ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"STRING\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BYTES\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ARRAY\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"MULTISET\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"RAW\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BOOLEAN\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"INTEGER\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"INT\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TINYINT\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"SMALLINT\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BIGINT\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"REAL\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DOUBLE\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"FLOAT\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BINARY\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"VARBINARY\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DECIMAL\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DEC\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"NUMERIC\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ANY\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"CHARACTER\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"CHAR\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"VARCHAR\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DATE\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TIME\" ...&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TIMESTAMP\" ...&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "2",
        "reply": "<tencent_A24E3CA568F93F848264AE155E0FCE7CEB09@qq.com>"
    },
    {
        "id": "<CABKuJ_TXy8P21k5yCY9nD9_m9=PB=GgjWso5YjskHUNF7rJaWw@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 12:41:51 GMT",
        "subject": "Re: sql 内嵌josn数组解析报 类型转换报错",
        "content": "你的意思是想先把json里面的array展开成多行，然后watermark基于这个展开后的数据来生成是么？&#010;&#010;claylin &lt;1012539884@qq.com&gt; 于2020年7月17日周五 下午8:37写道：&#010;&#010;&gt; 那我这种内嵌式的数据结构是不能在sql里面解析了，数组每行转成表中的一列，还有watermark，只能在外部处理成单条记录然后用flink处理了吗&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; libenchao@apache.org&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月17日(星期五) 晚上8:33&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: sql 内嵌josn数组解析报 类型转换报错&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 计算列只能写在最外层，不能在嵌套类型里面有计算列。&#010;&gt;&#010;&gt; claylin &lt;1012539884@qq.com&amp;gt; 于2020年7月17日周五 下午8:28写道：&#010;&gt;&#010;&gt; &amp;gt; hi all我这边有个嵌套的json数组，报类型转换错误(ts AS CAST(FROM_UNIXTIME(hiido_time)&#010;AS&#010;&gt; &amp;gt; TIMESTAMP(3)),这里报错)，是不是不能这么写&#010;&gt; &amp;gt; create table hiido_push_sdk_mq (&#010;&gt; &amp;gt; datas&amp;amp;nbsp; &amp;amp;nbsp;ARRAY&lt;ROW&lt;`from` string,hdid string,event&#010;&gt; &amp;gt; string,hiido_time bigint,ts AS CAST(FROM_UNIXTIME(hiido_time) AS&#010;&gt; &amp;gt; TIMESTAMP(3)),WATERMARK FOR ts AS ts - INTERVAL '5'&#010;&gt; MINUTE&amp;amp;gt;&amp;amp;gt;&#010;&gt; &amp;gt; ) with (&#010;&gt; &amp;gt; 'connector' = 'kafka',&#010;&gt; &amp;gt; 'topic' = 'hiido_pushsdk_event',&#010;&gt; &amp;gt; 'properties.bootstrap.servers' = 'kafkafs002-core001.yy.com:8103,&#010;&gt; &amp;gt; kafkafs002-core002.yy.com:8103,kafkafs002-core003.yy.com:8103',&#010;&gt; &amp;gt; 'properties.group.id' = 'push_click_sql_version_consumer',&#010;&gt; &amp;gt; 'scan.startup.mode' = 'latest-offset',&#010;&gt; &amp;gt; 'format.type' = 'json');&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 错误如下：&#010;&gt; &amp;gt; [ERROR] 2020-07-17 20:17:50,640(562284338) --&amp;amp;gt;&#010;&gt; [http-nio-8080-exec-10]&#010;&gt; &amp;gt;&#010;&gt; com.yy.push.flink.sql.gateway.sql.parse.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:77):&#010;&gt; &amp;gt; parseBySqlParser, parse:&#010;&gt; &amp;gt; com.yy.push.flink.sql.gateway.context.JobContext$1@5d5f32d1, stmt:&#010;&gt; create&#010;&gt; &amp;gt; table hiido_push_sdk_mq (&amp;amp;nbsp; &amp;amp;nbsp; datas&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp;ARRAY&lt;ROW&lt;`from`&#010;&gt; &amp;gt; string,hdid string,event string,hiido_time bigint,ts AS&#010;&gt; &amp;gt; CAST(FROM_UNIXTIME(hiido_time) AS TIMESTAMP(3)),WATERMARK FOR ts AS&#010;&gt; ts -&#010;&gt; &amp;gt; INTERVAL '5' MINUTE&amp;amp;gt;&amp;amp;gt;) with ('connector' =&#010;&gt; 'kafka','topic' =&#010;&gt; &amp;gt; 'hiido_pushsdk_event','properties.bootstrap.servers' = '&#010;&gt; &amp;gt; kafkafs002-core001.yy.com:8103,kafkafs002-core002.yy.com:8103,&#010;&gt; &amp;gt; kafkafs002-core003.yy.com:8103','properties.group.id' =&#010;&gt; &amp;gt; 'push_click_sql_version_consumer','scan.startup.mode' =&#010;&gt; &amp;gt; 'latest-offset','format.type' = 'json'), error info: SQL parse failed.&#010;&gt; &amp;gt; Encountered \"AS\" at line 1, column 115.&#010;&gt; &amp;gt; Was expecting one of:&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ROW\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;BRACKET_QUOTED_IDENTIFIER&amp;amp;gt; ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;QUOTED_IDENTIFIER&amp;amp;gt; ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;BACK_QUOTED_IDENTIFIER&amp;amp;gt; ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;IDENTIFIER&amp;amp;gt; ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;UNICODE_QUOTED_IDENTIFIER&amp;amp;gt; ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"STRING\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BYTES\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ARRAY\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"MULTISET\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"RAW\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BOOLEAN\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"INTEGER\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"INT\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TINYINT\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"SMALLINT\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BIGINT\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"REAL\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DOUBLE\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"FLOAT\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BINARY\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"VARBINARY\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DECIMAL\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DEC\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"NUMERIC\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ANY\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"CHARACTER\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"CHAR\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"VARCHAR\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DATE\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TIME\" ...&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TIMESTAMP\" ...&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_A24E3CA568F93F848264AE155E0FCE7CEB09@qq.com>"
    },
    {
        "id": "<CABKuJ_Q3cX0bp9VDfzcwP_ccEF+-SCBEubxSu+WKp319z=heCQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 12:51:34 GMT",
        "subject": "Re: sql 内嵌josn数组解析报 类型转换报错",
        "content": "如果是的话，现在的确是还做不到，不过有一个issue[1] 正在解决这个问题。&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18590&#010;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月17日周五 下午8:41写道：&#010;&#010;&gt; 你的意思是想先把json里面的array展开成多行，然后watermark基于这个展开后的数据来生成是么？&#010;&gt;&#010;&gt; claylin &lt;1012539884@qq.com&gt; 于2020年7月17日周五 下午8:37写道：&#010;&gt;&#010;&gt;&gt; 那我这种内嵌式的数据结构是不能在sql里面解析了，数组每行转成表中的一列，还有watermark，只能在外部处理成单条记录然后用flink处理了吗&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt;&gt; 发件人:&#010;&gt;&gt;                                                   \"user-zh\"&#010;&gt;&gt;                                                                     &lt;&#010;&gt;&gt; libenchao@apache.org&amp;gt;;&#010;&gt;&gt; 发送时间:&amp;nbsp;2020年7月17日(星期五) 晚上8:33&#010;&gt;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&gt;&#010;&gt;&gt; 主题:&amp;nbsp;Re: sql 内嵌josn数组解析报 类型转换报错&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 计算列只能写在最外层，不能在嵌套类型里面有计算列。&#010;&gt;&gt;&#010;&gt;&gt; claylin &lt;1012539884@qq.com&amp;gt; 于2020年7月17日周五 下午8:28写道：&#010;&gt;&gt;&#010;&gt;&gt; &amp;gt; hi all我这边有个嵌套的json数组，报类型转换错误(ts AS CAST(FROM_UNIXTIME(hiido_time)&#010;AS&#010;&gt;&gt; &amp;gt; TIMESTAMP(3)),这里报错)，是不是不能这么写&#010;&gt;&gt; &amp;gt; create table hiido_push_sdk_mq (&#010;&gt;&gt; &amp;gt; datas&amp;amp;nbsp; &amp;amp;nbsp;ARRAY&lt;ROW&lt;`from` string,hdid string,event&#010;&gt;&gt; &amp;gt; string,hiido_time bigint,ts AS CAST(FROM_UNIXTIME(hiido_time) AS&#010;&gt;&gt; &amp;gt; TIMESTAMP(3)),WATERMARK FOR ts AS ts - INTERVAL '5'&#010;&gt;&gt; MINUTE&amp;amp;gt;&amp;amp;gt;&#010;&gt;&gt; &amp;gt; ) with (&#010;&gt;&gt; &amp;gt; 'connector' = 'kafka',&#010;&gt;&gt; &amp;gt; 'topic' = 'hiido_pushsdk_event',&#010;&gt;&gt; &amp;gt; 'properties.bootstrap.servers' = 'kafkafs002-core001.yy.com:8103,&#010;&gt;&gt; &amp;gt; kafkafs002-core002.yy.com:8103,kafkafs002-core003.yy.com:8103',&#010;&gt;&gt; &amp;gt; 'properties.group.id' = 'push_click_sql_version_consumer',&#010;&gt;&gt; &amp;gt; 'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt; &amp;gt; 'format.type' = 'json');&#010;&gt;&gt; &amp;gt;&#010;&gt;&gt; &amp;gt;&#010;&gt;&gt; &amp;gt;&#010;&gt;&gt; &amp;gt;&#010;&gt;&gt; &amp;gt; 错误如下：&#010;&gt;&gt; &amp;gt; [ERROR] 2020-07-17 20:17:50,640(562284338) --&amp;amp;gt;&#010;&gt;&gt; [http-nio-8080-exec-10]&#010;&gt;&gt; &amp;gt;&#010;&gt;&gt; com.yy.push.flink.sql.gateway.sql.parse.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:77):&#010;&gt;&gt; &amp;gt; parseBySqlParser, parse:&#010;&gt;&gt; &amp;gt; com.yy.push.flink.sql.gateway.context.JobContext$1@5d5f32d1, stmt:&#010;&gt;&gt; create&#010;&gt;&gt; &amp;gt; table hiido_push_sdk_mq (&amp;amp;nbsp; &amp;amp;nbsp; datas&amp;amp;nbsp;&#010;&gt;&gt; &amp;amp;nbsp;ARRAY&lt;ROW&lt;`from`&#010;&gt;&gt; &amp;gt; string,hdid string,event string,hiido_time bigint,ts AS&#010;&gt;&gt; &amp;gt; CAST(FROM_UNIXTIME(hiido_time) AS TIMESTAMP(3)),WATERMARK FOR ts AS&#010;&gt;&gt; ts -&#010;&gt;&gt; &amp;gt; INTERVAL '5' MINUTE&amp;amp;gt;&amp;amp;gt;) with ('connector' =&#010;&gt;&gt; 'kafka','topic' =&#010;&gt;&gt; &amp;gt; 'hiido_pushsdk_event','properties.bootstrap.servers' = '&#010;&gt;&gt; &amp;gt; kafkafs002-core001.yy.com:8103,kafkafs002-core002.yy.com:8103,&#010;&gt;&gt; &amp;gt; kafkafs002-core003.yy.com:8103','properties.group.id' =&#010;&gt;&gt; &amp;gt; 'push_click_sql_version_consumer','scan.startup.mode' =&#010;&gt;&gt; &amp;gt; 'latest-offset','format.type' = 'json'), error info: SQL parse&#010;&gt;&gt; failed.&#010;&gt;&gt; &amp;gt; Encountered \"AS\" at line 1, column 115.&#010;&gt;&gt; &amp;gt; Was expecting one of:&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ROW\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;BRACKET_QUOTED_IDENTIFIER&amp;amp;gt;&#010;...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;QUOTED_IDENTIFIER&amp;amp;gt; ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;BACK_QUOTED_IDENTIFIER&amp;amp;gt; ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;IDENTIFIER&amp;amp;gt; ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &lt;UNICODE_QUOTED_IDENTIFIER&amp;amp;gt;&#010;...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"STRING\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BYTES\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ARRAY\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"MULTISET\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"RAW\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BOOLEAN\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"INTEGER\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"INT\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TINYINT\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"SMALLINT\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BIGINT\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"REAL\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DOUBLE\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"FLOAT\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"BINARY\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"VARBINARY\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DECIMAL\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DEC\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"NUMERIC\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"ANY\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"CHARACTER\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"CHAR\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"VARCHAR\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"DATE\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TIME\" ...&#010;&gt;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; \"TIMESTAMP\" ...&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Benchao Li&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "4",
        "reply": "<tencent_A24E3CA568F93F848264AE155E0FCE7CEB09@qq.com>"
    },
    {
        "id": "<tencent_8908E7FAFF55F00CAD2ED5D38896E2432905@qq.com>",
        "from": "&quot;claylin&quot; &lt;1012539...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 13:26:25 GMT",
        "subject": "回复： sql 内嵌josn数组解析报 类型转换报错",
        "content": "嗯了解了谢谢大佬&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;libenchao@apache.org&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月17日(星期五) 晚上8:51&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: sql 内嵌josn数组解析报 类型转换报错&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;如果是的话，现在的确是还做不到，不过有一个issue[1] 正在解决这个问题。&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18590&#013;&#010;&#013;&#010;Benchao Li &lt;libenchao@apache.org&amp;gt; 于2020年7月17日周五 下午8:41写道：&#013;&#010;&#013;&#010;&amp;gt; 你的意思是想先把json里面的array展开成多行，然后watermark基于这个展开后的数据来生成是么？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; claylin &lt;1012539884@qq.com&amp;gt; 于2020年7月17日周五 下午8:37写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; 那我这种内嵌式的数据结构是不能在sql里面解析了，数组每行转成表中的一列，还有watermark，只能在外部处理成单条记录然后用flink处理了吗&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&#013;&#010;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt;&amp;gt; libenchao@apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月17日(星期五) 晚上8:33&#013;&#010;&amp;gt;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; 主题:&amp;amp;nbsp;Re: sql 内嵌josn数组解析报 类型转换报错&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; 计算列只能写在最外层，不能在嵌套类型里面有计算列。&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; claylin &lt;1012539884@qq.com&amp;amp;gt; 于2020年7月17日周五 下午8:28写道：&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; hi all我这边有个嵌套的json数组，报类型转换错误(ts&#010;AS CAST(FROM_UNIXTIME(hiido_time) AS&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; TIMESTAMP(3)),这里报错)，是不是不能这么写&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; create table hiido_push_sdk_mq (&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; datas&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;ARRAY&lt;ROW&lt;`from`&#010;string,hdid string,event&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; string,hiido_time bigint,ts AS CAST(FROM_UNIXTIME(hiido_time)&#010;AS&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; TIMESTAMP(3)),WATERMARK FOR ts AS ts - INTERVAL '5'&#013;&#010;&amp;gt;&amp;gt; MINUTE&amp;amp;amp;gt;&amp;amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; ) with (&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'connector' = 'kafka',&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'topic' = 'hiido_pushsdk_event',&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'properties.bootstrap.servers' = 'kafkafs002-core001.yy.com:8103,&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; kafkafs002-core002.yy.com:8103,kafkafs002-core003.yy.com:8103',&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'properties.group.id' = 'push_click_sql_version_consumer',&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'scan.startup.mode' = 'latest-offset',&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'format.type' = 'json');&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 错误如下：&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; [ERROR] 2020-07-17 20:17:50,640(562284338) --&amp;amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt; [http-nio-8080-exec-10]&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt; com.yy.push.flink.sql.gateway.sql.parse.SqlCommandParser.parseBySqlParser(SqlCommandParser.java:77):&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; parseBySqlParser, parse:&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; com.yy.push.flink.sql.gateway.context.JobContext$1@5d5f32d1,&#010;stmt:&#013;&#010;&amp;gt;&amp;gt; create&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; table hiido_push_sdk_mq (&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;datas&amp;amp;amp;nbsp;&#013;&#010;&amp;gt;&amp;gt; &amp;amp;amp;nbsp;ARRAY&lt;ROW&lt;`from`&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; string,hdid string,event string,hiido_time bigint,ts AS&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; CAST(FROM_UNIXTIME(hiido_time) AS TIMESTAMP(3)),WATERMARK FOR&#010;ts AS&#013;&#010;&amp;gt;&amp;gt; ts -&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; INTERVAL '5' MINUTE&amp;amp;amp;gt;&amp;amp;amp;gt;) with ('connector'&#010;=&#013;&#010;&amp;gt;&amp;gt; 'kafka','topic' =&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'hiido_pushsdk_event','properties.bootstrap.servers' = '&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; kafkafs002-core001.yy.com:8103,kafkafs002-core002.yy.com:8103,&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; kafkafs002-core003.yy.com:8103','properties.group.id' =&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'push_click_sql_version_consumer','scan.startup.mode' =&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; 'latest-offset','format.type' = 'json'), error info: SQL parse&#013;&#010;&amp;gt;&amp;gt; failed.&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; Encountered \"AS\" at line 1, column 115.&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; Was expecting one of:&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"ROW\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &lt;BRACKET_QUOTED_IDENTIFIER&amp;amp;amp;gt;&#010;...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &lt;QUOTED_IDENTIFIER&amp;amp;amp;gt;&#010;...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &lt;BACK_QUOTED_IDENTIFIER&amp;amp;amp;gt;&#010;...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &lt;IDENTIFIER&amp;amp;amp;gt;&#010;...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &lt;UNICODE_QUOTED_IDENTIFIER&amp;amp;amp;gt;&#010;...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"STRING\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"BYTES\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"ARRAY\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"MULTISET\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"RAW\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"BOOLEAN\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"INTEGER\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"INT\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"TINYINT\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"SMALLINT\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"BIGINT\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"REAL\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"DOUBLE\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"FLOAT\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"BINARY\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"VARBINARY\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"DECIMAL\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"DEC\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"NUMERIC\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"ANY\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"CHARACTER\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"CHAR\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"VARCHAR\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"DATE\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"TIME\" ...&#013;&#010;&amp;gt;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; \"TIMESTAMP\" ...&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; --&#013;&#010;&amp;gt;&amp;gt;&#013;&#010;&amp;gt;&amp;gt; Best,&#013;&#010;&amp;gt;&amp;gt; Benchao Li&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; --&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Benchao Li&#013;&#010;&amp;gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li",
        "depth": "5",
        "reply": "<tencent_A24E3CA568F93F848264AE155E0FCE7CEB09@qq.com>"
    },
    {
        "id": "<tencent_11A2A5049DE257A24CF3D5475B07597A670A@qq.com>",
        "from": "&quot;Evan&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 18 Jul 2020 09:47:24 GMT",
        "subject": "FlinkSQL 任务提交后 任务名称问题",
        "content": "代码大概是这样子的，一张kafka source表，一张es Sink表，最后通过tableEnv.executeSql(\"insert&#010;into esSinkTable select ... from kafkaSourceTable\")执行&#013;&#010;任务提交后任务名称为“inset-into_某某catalog_某某database.某某Table”&#013;&#010;&#013;&#010;&#013;&#010;这样很不友好啊，能不能我自己指定任务名称呢？",
        "depth": "0",
        "reply": "<tencent_11A2A5049DE257A24CF3D5475B07597A670A@qq.com>"
    },
    {
        "id": "<CAADy7x4ghDLVG7OrtskbbvmDjv_12NTeJsuj76i5A_eaYCHRjA@mail.gmail.com>",
        "from": "Jeff Zhang &lt;zjf...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 18 Jul 2020 13:45:33 GMT",
        "subject": "Re: FlinkSQL 任务提交后 任务名称问题",
        "content": "在zeppelin中你可以指定insert 语句的job name，如下图，（对Zeppelin感兴趣的，可以加入钉钉群：32803524）&#013;&#010;&#013;&#010;%flink.ssql(jobName=\"my job\")&#013;&#010;&#013;&#010;insert into sink_kafka select status, direction, cast(event_ts/1000000000&#013;&#010;as timestamp(3)) from source_kafka where status &lt;&gt; 'foo'&#013;&#010;&#013;&#010;[image: image.png]&#013;&#010;&#013;&#010;Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月18日周六 下午5:47写道：&#013;&#010;&#013;&#010;&gt; 代码大概是这样子的，一张kafka source表，一张es Sink表，最后通过tableEnv.executeSql(\"insert&#010;into&#013;&#010;&gt; esSinkTable select ... from kafkaSourceTable\")执行&#013;&#010;&gt; 任务提交后任务名称为“inset-into_某某catalog_某某database.某某Table”&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 这样很不友好啊，能不能我自己指定任务名称呢？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best Regards&#013;&#010;&#013;&#010;Jeff Zhang&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_11A2A5049DE257A24CF3D5475B07597A670A@qq.com>"
    },
    {
        "id": "<CADQYLGv=UQ7=wGFBufNhs_5Wop7bP5g2bwWQNvWWEASkfeeDZQ@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 19 Jul 2020 01:34:49 GMT",
        "subject": "Re: FlinkSQL 任务提交后 任务名称问题",
        "content": "hi Evan,&#013;&#010;感谢反馈，目前已经有一个issue [1]在跟踪该问题，可以关注后续进展&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18545&#013;&#010;&lt;https://issues.apache.org/jira/browse/FLINK-18545?jql=project%20%3D%20FLINK%20AND%20resolution%20%3D%20Unresolved%20AND%20text%20~%20%22job%20name%22%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC&gt;&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;Jeff Zhang &lt;zjffdu@gmail.com&gt; 于2020年7月18日周六 下午9:52写道：&#013;&#010;&#013;&#010;&gt; 在zeppelin中你可以指定insert 语句的job name，如下图，（对Zeppelin感兴趣的，可以加入钉钉群：32803524）&#013;&#010;&gt;&#013;&#010;&gt; %flink.ssql(jobName=\"my job\")&#013;&#010;&gt;&#013;&#010;&gt; insert into sink_kafka select status, direction, cast(event_ts/1000000000&#013;&#010;&gt; as timestamp(3)) from source_kafka where status &lt;&gt; 'foo'&#013;&#010;&gt;&#013;&#010;&gt; [image: image.png]&#013;&#010;&gt;&#013;&#010;&gt; Evan &lt;chengyanan1008@foxmail.com&gt; 于2020年7月18日周六 下午5:47写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 代码大概是这样子的，一张kafka source表，一张es Sink表，最后通过tableEnv.executeSql(\"insert&#010;into&#013;&#010;&gt;&gt; esSinkTable select ... from kafkaSourceTable\")执行&#013;&#010;&gt;&gt; 任务提交后任务名称为“inset-into_某某catalog_某某database.某某Table”&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 这样很不友好啊，能不能我自己指定任务名称呢？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best Regards&#013;&#010;&gt;&#013;&#010;&gt; Jeff Zhang&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_11A2A5049DE257A24CF3D5475B07597A670A@qq.com>"
    },
    {
        "id": "<tencent_C469F1D4530573284C61167C22CCC69B9D08@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 18 Jul 2020 12:37:03 GMT",
        "subject": "如何在ProcessAllWindowFunction中触发清除自定义的ValueState状态",
        "content": "大家好&#013;&#010;&#013;&#010;想问下如何在ProcessAllWindowFunction中触发清除ValueState状态，在KeydProcessFounction中有onTimer方法中清除，但是ProcessAllWindowFunction没有。&#013;&#010;&#013;&#010;&#013;&#010;谢谢！",
        "depth": "0",
        "reply": "<tencent_C469F1D4530573284C61167C22CCC69B9D08@qq.com>"
    },
    {
        "id": "<tencent_46CCEC32A2C07201DA3400F8F211EEE68605@qq.com>",
        "from": "&quot;smq&quot; &lt;374060...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 19 Jul 2020 13:35:23 GMT",
        "subject": "flink sink到kafka",
        "content": "大家好，我想通过avro格式sink到kafka,请问该怎么实现，官网上没找到相关方法。",
        "depth": "0",
        "reply": "<tencent_46CCEC32A2C07201DA3400F8F211EEE68605@qq.com>"
    },
    {
        "id": "<CADQYLGtjkAjmtgJ7YhExa9GP5uex5usuY0pGYGy3FrWMw3hkPQ@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 19 Jul 2020 15:05:18 GMT",
        "subject": "Re: flink sink到kafka",
        "content": "如果你是用flink sql的，可以通过DDL的方式来定义kafka sink，参考 [1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;smq &lt;374060171@qq.com&gt; 于2020年7月19日周日 下午9:36写道：&#013;&#010;&#013;&#010;&gt; 大家好，我想通过avro格式sink到kafka,请问该怎么实现，官网上没找到相关方法。&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_46CCEC32A2C07201DA3400F8F211EEE68605@qq.com>"
    },
    {
        "id": "<tencent_7DF0016C971D70B93787662A11F79672D008@qq.com>",
        "from": "明启 孙 &lt;374060...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 01:15:05 GMT",
        "subject": "回复: flink sink到kafka",
        "content": "谢谢，我试试&#013;&#010;&#013;&#010;发送自 Windows 10 版邮件应用&#013;&#010;&#013;&#010;发件人: godfrey he&#013;&#010;发送时间: 2020年7月19日 23:06&#013;&#010;收件人: user-zh&#013;&#010;主题: Re: flink sink到kafka&#013;&#010;&#013;&#010;如果你是用flink sql的，可以通过DDL的方式来定义kafka sink，参考 [1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;smq &lt;374060171@qq.com&gt; 于2020年7月19日周日 下午9:36写道：&#013;&#010;&#013;&#010;&gt; 大家好，我想通过avro格式sink到kafka,请问该怎么实现，官网上没找到相关方法。&#013;&#010;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_46CCEC32A2C07201DA3400F8F211EEE68605@qq.com>"
    },
    {
        "id": "<tencent_010040FBFE3A4FFBCBAC43AACB4528F26C09@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:32:49 GMT",
        "subject": "回复： Flink Cli 部署问题",
        "content": "这是taskmanager新报的一个错，还是跟之前一样，用cli提交报错，用webui提交就没问题：&#013;&#010;2020-07-20 03:29:25,959 WARN&amp;nbsp; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'value.serializer' was supplied but isn't a known config.&#013;&#010;2020-07-20 03:29:25,959 INFO&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka version : 0.11.0.2&#013;&#010;2020-07-20 03:29:25,959 INFO&amp;nbsp; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#013;&#010;2020-07-20 03:29:25,974 ERROR org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;nbsp; - Caught unexpected exception.&#013;&#010;java.lang.ArrayIndexOutOfBoundsException: 0&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&#009;at java.lang.Thread.run(Thread.java:748)&#013;&#010;2020-07-20 03:29:25,974 WARN&amp;nbsp; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;nbsp; - Exception while restoring keyed state backend for StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1) from alternative (1/1), will retry while more alternatives are available.&#013;&#010;org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&#009;at java.lang.Thread.run(Thread.java:748)&#013;&#010;Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&#009;... 15 more&#013;&#010;2020-07-20 03:29:25,975 INFO&amp;nbsp; org.apache.kafka.clients.producer.KafkaProducer&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.&#013;&#010;2020-07-20 03:29:25,979 INFO&amp;nbsp; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Map -&amp;gt; Filter -&amp;gt; Sink: Unnamed (1/1) (ed554502aa995fe53f1cf0cb8adf633c) switched from RUNNING to FAILED.&#013;&#010;java.lang.Exception: Exception while creating StreamOperatorStateContext.&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&#009;at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&#009;at java.lang.Thread.run(Thread.java:748)&#013;&#010;Caused by: org.apache.flink.util.FlinkException: Could not restore keyed state backend for StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1) from any of the 1 provided restore options.&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&#009;... 9 more&#013;&#010;Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected exception.&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&#009;... 11 more&#013;&#010;Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&#009;at org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;谢谢回复：&#013;&#010;之前的savepoint都是通过RocksDBStateBackend生成的；&#013;&#010;这个savepoint我通过webui 提交任务就没问题，你是说在IDE上调试savepoint吗&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人:                                                                                                                        \"user-zh\"                                                                                    &lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月19日(星期天) 晚上8:22&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;从你给的这部分日志看，是恢复的时候遇到 EOF 了，这个比较奇怪&#013;&#010;1 你之前的 savepoint 是使用 RocksDBStateBackend 生成的吗&#013;&#010;2 你还有之前在 DFS 上的 savepoint 文件吗？可能需要结合 DFS 上的文件一起看一下这个问题怎么来的&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Z-Z &lt;zz9876543210@qq.com&amp;gt; 于2020年7月17日周五 下午11:10写道：&#013;&#010;&#013;&#010;&amp;gt; Flink 1.10.0 ,taskmanager报错日志如下：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 2020-07-17 15:06:43,913 ERROR&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;amp;nbsp;&#013;&#010;&amp;gt; - Caught unexpected exception.&#013;&#010;&amp;gt; java.io.EOFException&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; 2020-07-17 15:06:43,914 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;amp;nbsp; -&#013;&#010;&amp;gt; Exception while restoring keyed state backend for&#013;&#010;&amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#013;&#010;&amp;gt; alternative (1/1), will retry while more alternatives are available.&#013;&#010;&amp;gt; org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected&#013;&#010;&amp;gt; exception.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 15 more&#013;&#010;&amp;gt; 2020-07-17 15:06:43,915 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.producer.KafkaProducer&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Closing the Kafka producer with timeoutMillis&#013;&#010;&amp;gt; = 9223372036854775807 ms.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,918 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Co-Keyed-Process -&amp;amp;gt; Flat Map&#013;&#010;&amp;gt; -&amp;amp;gt; Sink: Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) switched from&#013;&#010;&amp;gt; RUNNING to FAILED.&#013;&#010;&amp;gt; java.lang.Exception: Exception while creating StreamOperatorStateContext.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore keyed&#013;&#010;&amp;gt; state backend for&#013;&#010;&amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from any of&#013;&#010;&amp;gt; the 1 provided restore options.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 9 more&#013;&#010;&amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught&#013;&#010;&amp;gt; unexpected exception.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 11 more&#013;&#010;&amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 15 more&#013;&#010;&amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Freeing task resources for&#013;&#010;&amp;gt; Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt; Sink: Unnamed (1/1)&#013;&#010;&amp;gt; (bb8f0a84e07ef90b1e11ca2825e0efab).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Ensuring all FileSystem streams&#013;&#010;&amp;gt; are closed for task Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt; Sink: Unnamed&#013;&#010;&amp;gt; (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) [FAILED]&#013;&#010;&amp;gt; 2020-07-17 15:06:43,931 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending final execution&#013;&#010;&amp;gt; state FAILED to JobManager for task Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt;&#013;&#010;&amp;gt; Sink: Unnamed (1/1) bb8f0a84e07ef90b1e11ca2825e0efab.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Attempting to cancel task&#013;&#010;&amp;gt; Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source -&amp;amp;gt; Flat&#013;&#010;&amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from RUNNING to&#013;&#010;&amp;gt; CANCELING.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Triggering cancellation of task&#013;&#010;&amp;gt; code Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Attempting to cancel task&#013;&#010;&amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source (1/1)&#013;&#010;&amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from RUNNING to CANCELING.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Triggering cancellation of task&#013;&#010;&amp;gt; code Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source -&amp;amp;gt; Flat&#013;&#010;&amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from CANCELING to&#013;&#010;&amp;gt; CANCELED.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Freeing task resources for&#013;&#010;&amp;gt; Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Ensuring all FileSystem streams&#013;&#010;&amp;gt; are closed for task Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce) [CANCELED]&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Source: Custom Source (1/1)&#013;&#010;&amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from CANCELING to CANCELED.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,955 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Freeing task resources for&#013;&#010;&amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending final execution&#013;&#010;&amp;gt; state CANCELED to JobManager for task Source: Custom Source -&amp;amp;gt; Flat Map&#013;&#010;&amp;gt; (1/1) 9cb8dcd4982223adcb6f007f1ffccdce.&#013;&#010;&amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Ensuring all FileSystem streams&#013;&#010;&amp;gt; are closed for task Source: Custom Source (1/1)&#013;&#010;&amp;gt; (00621ff5d788d00c73ccaaea04717600) [CANCELED]&#013;&#010;&amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending final execution&#013;&#010;&amp;gt; state CANCELED to JobManager for task Source: Custom Source (1/1)&#013;&#010;&amp;gt; 00621ff5d788d00c73ccaaea04717600.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'transaction.timeout.ms' was&#013;&#010;&amp;gt; supplied but isn't a known config.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'key.serializer' was supplied but&#013;&#010;&amp;gt; isn't a known config.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'value.serializer' was supplied&#013;&#010;&amp;gt; but isn't a known config.&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka version : 0.11.0.2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Error registering AppInfo mbean&#013;&#010;&amp;gt; javax.management.InstanceAlreadyExistsException:&#013;&#010;&amp;gt; kafka.consumer:type=app-info,id=consumer-3&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:757)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:633)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:615)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;&amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka version : 0.11.0.2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#013;&#010;&amp;gt; 2020-07-17 15:06:44,079 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Error registering AppInfo mbean&#013;&#010;&amp;gt; javax.management.InstanceAlreadyExistsException:&#013;&#010;&amp;gt; kafka.consumer:type=app-info,id=consumer-4&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:757)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:633)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:615)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; &lt;&#013;&#010;&amp;gt; qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月17日(星期五) 晚上10:52&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Hi&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 请问你使用哪个版本的 Flink 呢？能否分享一下&amp;amp;nbsp; Co-Process (1/1)&#013;&#010;&amp;gt; (d0309f26a545e74643382ed3f758269b) 这个 tm 的 log 呢？从上面给的日志看，应该是在 083f69d029de&#013;&#010;&amp;gt; 这台机器上。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Congxian&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Z-Z &lt;zz9876543210@qq.com&amp;amp;gt; 于2020年7月17日周五 下午6:22写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#013;&#010;&amp;gt; &amp;amp;gt; ---&amp;amp;amp;gt; /jobs/{jobid}/savepoints ---&amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,925 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; -&#013;&#010;&amp;gt; &amp;amp;gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#013;&#010;&amp;gt; &amp;amp;gt; 7639673873b707aa86c4387aa7b4aac3 with allocation id&#013;&#010;&amp;gt; &amp;amp;gt; e8865cdbfe4c3c33099c7112bc2e3231.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from SCHEDULED to&#013;&#010;&amp;gt; DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source -&amp;amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; &amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; &amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1) (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to&#013;&#010;&amp;gt; DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Co-Process -&amp;amp;amp;gt; (Sink: Unnamed, Sink:&#013;&#010;&amp;gt; Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (attempt #0) to e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#013;&#010;&amp;gt; &amp;amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,346 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;gt; (Sink: Unnamed, Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; &amp;amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; &amp;amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,493 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1) (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; RUNNING to FAILED.&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.Exception: Exception while creating&#013;&#010;&amp;gt; StreamOperatorStateContext.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore&#013;&#010;&amp;gt; keyed&#013;&#010;&amp;gt; &amp;amp;gt; state backend for&#013;&#010;&amp;gt; &amp;amp;gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1)&#013;&#010;&amp;gt; from&#013;&#010;&amp;gt; &amp;amp;gt; any of the 1 provided restore options.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; ... 9 more&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException:&#013;&#010;&amp;gt; Caught&#013;&#010;&amp;gt; &amp;amp;gt; unexpected exception.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; ... 11 more&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; ... 15 more",
        "depth": "0",
        "reply": "<tencent_010040FBFE3A4FFBCBAC43AACB4528F26C09@qq.com>"
    },
    {
        "id": "<CAA8tFvvk1EcvBAOiBx6vu+539qKde+rg5tjf8uKNxxua_+-kXg@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 06:30:33 GMT",
        "subject": "Re: Flink Cli 部署问题",
        "content": "Hi&#010;&#010;这个调试可以在 IDEA 进行的。&#010;&#010;另外你说的通过 web ui 提交没有问题。请问下，是同一个 savepoint 通过 flink run 提交有问题，通过 web ui&#010;提交没有问题吗？如果是的，能否分享下你的操作过程和命令呢？&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Z-Z &lt;zz9876543210@qq.com&gt; 于2020年7月20日周一 上午11:33写道：&#010;&#010;&gt; 这是taskmanager新报的一个错，还是跟之前一样，用cli提交报错，用webui提交就没问题：&#010;&gt; 2020-07-20 03:29:25,959 WARN&amp;nbsp;&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; - The configuration 'value.serializer' was supplied&#010;&gt; but isn't a known config.&#010;&gt; 2020-07-20 03:29:25,959 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka version : 0.11.0.2&#010;&gt; 2020-07-20 03:29:25,959 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#010;&gt; 2020-07-20 03:29:25,974 ERROR&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;nbsp;&#010;&gt; - Caught unexpected exception.&#010;&gt; java.lang.ArrayIndexOutOfBoundsException: 0&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt; 2020-07-20 03:29:25,974 WARN&amp;nbsp;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;nbsp; -&#010;&gt; Exception while restoring keyed state backend for&#010;&gt; StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1) from alternative (1/1),&#010;&gt; will retry while more alternatives are available.&#010;&gt; org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected&#010;&gt; exception.&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt;         ... 15 more&#010;&gt; 2020-07-20 03:29:25,975 INFO&amp;nbsp;&#010;&gt; org.apache.kafka.clients.producer.KafkaProducer&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Closing the Kafka producer with timeoutMillis&#010;&gt; = 9223372036854775807 ms.&#010;&gt; 2020-07-20 03:29:25,979 INFO&amp;nbsp;&#010;&gt; org.apache.flink.runtime.taskmanager.Task&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;- Map -&amp;gt; Filter -&amp;gt; Sink:&#010;&gt; Unnamed (1/1) (ed554502aa995fe53f1cf0cb8adf633c) switched from RUNNING to&#010;&gt; FAILED.&#010;&gt; java.lang.Exception: Exception while creating StreamOperatorStateContext.&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt; Caused by: org.apache.flink.util.FlinkException: Could not restore keyed&#010;&gt; state backend for StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1) from any&#010;&gt; of the 1 provided restore options.&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt;         ... 9 more&#010;&gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught&#010;&gt; unexpected exception.&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt;         at&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt;         ... 11 more&#010;&gt; Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt;         at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 谢谢回复：&#010;&gt; 之前的savepoint都是通过RocksDBStateBackend生成的；&#010;&gt; 这个savepoint我通过webui 提交任务就没问题，你是说在IDE上调试savepoint吗&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------ 原始邮件 ------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; qcx978132955@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月19日(星期天) 晚上8:22&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: Flink Cli 部署问题&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Hi&#010;&gt;&#010;&gt; 从你给的这部分日志看，是恢复的时候遇到 EOF 了，这个比较奇怪&#010;&gt; 1 你之前的 savepoint 是使用 RocksDBStateBackend 生成的吗&#010;&gt; 2 你还有之前在 DFS 上的 savepoint 文件吗？可能需要结合 DFS 上的文件一起看一下这个问题怎么来的&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; Z-Z &lt;zz9876543210@qq.com&amp;gt; 于2020年7月17日周五 下午11:10写道：&#010;&gt;&#010;&gt; &amp;gt; Flink 1.10.0 ,taskmanager报错日志如下：&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 2020-07-17 15:06:43,913 ERROR&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;amp;nbsp;&#010;&gt; &amp;gt; - Caught unexpected exception.&#010;&gt; &amp;gt; java.io.EOFException&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; 2020-07-17 15:06:43,914 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;amp;nbsp;&#010;&gt; -&#010;&gt; &amp;gt; Exception while restoring keyed state backend for&#010;&gt; &amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#010;&gt; &amp;gt; alternative (1/1), will retry while more alternatives are available.&#010;&gt; &amp;gt; org.apache.flink.runtime.state.BackendBuildingException: Caught&#010;&gt; unexpected&#010;&gt; &amp;gt; exception.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; Caused by: java.io.EOFException&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 15 more&#010;&gt; &amp;gt; 2020-07-17 15:06:43,915 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.clients.producer.KafkaProducer&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Closing the Kafka&#010;&gt; producer with timeoutMillis&#010;&gt; &amp;gt; = 9223372036854775807 ms.&#010;&gt; &amp;gt; 2020-07-17 15:06:43,918 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Co-Keyed-Process -&amp;amp;gt; Flat Map&#010;&gt; &amp;gt; -&amp;amp;gt; Sink: Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab)&#010;&gt; switched from&#010;&gt; &amp;gt; RUNNING to FAILED.&#010;&gt; &amp;gt; java.lang.Exception: Exception while creating&#010;&gt; StreamOperatorStateContext.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore&#010;&gt; keyed&#010;&gt; &amp;gt; state backend for&#010;&gt; &amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#010;&gt; any of&#010;&gt; &amp;gt; the 1 provided restore options.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 9 more&#010;&gt; &amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException:&#010;&gt; Caught&#010;&gt; &amp;gt; unexpected exception.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 11 more&#010;&gt; &amp;gt; Caused by: java.io.EOFException&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ... 15 more&#010;&gt; &amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Freeing task resources for&#010;&gt; &amp;gt; Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt; Sink: Unnamed (1/1)&#010;&gt; &amp;gt; (bb8f0a84e07ef90b1e11ca2825e0efab).&#010;&gt; &amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Ensuring all FileSystem streams&#010;&gt; &amp;gt; are closed for task Co-Keyed-Process -&amp;amp;gt; Flat Map -&amp;amp;gt;&#010;&gt; Sink: Unnamed&#010;&gt; &amp;gt; (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) [FAILED]&#010;&gt; &amp;gt; 2020-07-17 15:06:43,931 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending&#010;&gt; final execution&#010;&gt; &amp;gt; state FAILED to JobManager for task Co-Keyed-Process -&amp;amp;gt; Flat&#010;&gt; Map -&amp;amp;gt;&#010;&gt; &amp;gt; Sink: Unnamed (1/1) bb8f0a84e07ef90b1e11ca2825e0efab.&#010;&gt; &amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Attempting to cancel task&#010;&gt; &amp;gt; Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#010;&gt; &amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; &amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Source: Custom Source -&amp;amp;gt; Flat&#010;&gt; &amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from RUNNING to&#010;&gt; &amp;gt; CANCELING.&#010;&gt; &amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Triggering cancellation of task&#010;&gt; &amp;gt; code Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#010;&gt; &amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; &amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Attempting to cancel task&#010;&gt; &amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#010;&gt; &amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Source: Custom Source (1/1)&#010;&gt; &amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from RUNNING to CANCELING.&#010;&gt; &amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Triggering cancellation of task&#010;&gt; &amp;gt; code Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#010;&gt; &amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Source: Custom Source -&amp;amp;gt; Flat&#010;&gt; &amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from CANCELING&#010;&gt; to&#010;&gt; &amp;gt; CANCELED.&#010;&gt; &amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Freeing task resources for&#010;&gt; &amp;gt; Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#010;&gt; &amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; &amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Ensuring all FileSystem streams&#010;&gt; &amp;gt; are closed for task Source: Custom Source -&amp;amp;gt; Flat Map (1/1)&#010;&gt; &amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce) [CANCELED]&#010;&gt; &amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Source: Custom Source (1/1)&#010;&gt; &amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from CANCELING to&#010;&gt; CANCELED.&#010;&gt; &amp;gt; 2020-07-17 15:06:43,955 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Freeing task resources for&#010;&gt; &amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#010;&gt; &amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending&#010;&gt; final execution&#010;&gt; &amp;gt; state CANCELED to JobManager for task Source: Custom Source -&amp;amp;gt;&#010;&gt; Flat Map&#010;&gt; &amp;gt; (1/1) 9cb8dcd4982223adcb6f007f1ffccdce.&#010;&gt; &amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Ensuring all FileSystem streams&#010;&gt; &amp;gt; are closed for task Source: Custom Source (1/1)&#010;&gt; &amp;gt; (00621ff5d788d00c73ccaaea04717600) [CANCELED]&#010;&gt; &amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - Un-registering task and sending&#010;&gt; final execution&#010;&gt; &amp;gt; state CANCELED to JobManager for task Source: Custom Source (1/1)&#010;&gt; &amp;gt; 00621ff5d788d00c73ccaaea04717600.&#010;&gt; &amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration '&#010;&gt; transaction.timeout.ms' was&#010;&gt; &amp;gt; supplied but isn't a known config.&#010;&gt; &amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'key.serializer'&#010;&gt; was supplied but&#010;&gt; &amp;gt; isn't a known config.&#010;&gt; &amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration&#010;&gt; 'value.serializer' was supplied&#010;&gt; &amp;gt; but isn't a known config.&#010;&gt; &amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Kafka version : 0.11.0.2&#010;&gt; &amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Kafka commitId : 73be1e1168f91ee2&#010;&gt; &amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Error registering AppInfo mbean&#010;&gt; &amp;gt; javax.management.InstanceAlreadyExistsException:&#010;&gt; &amp;gt; kafka.consumer:type=app-info,id=consumer-3&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:757)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:633)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:615)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#010;&gt; &amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Kafka version : 0.11.0.2&#010;&gt; &amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Kafka commitId : 73be1e1168f91ee2&#010;&gt; &amp;gt; 2020-07-17 15:06:44,079 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Error registering AppInfo mbean&#010;&gt; &amp;gt; javax.management.InstanceAlreadyExistsException:&#010;&gt; &amp;gt; kafka.consumer:type=app-info,id=consumer-4&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:757)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:633)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;gt;(KafkaConsumer.java:615)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#010;&gt; &amp;gt; 发件人:&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; \"user-zh\"&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &amp;nbsp; &lt;&#010;&gt; &amp;gt; qcx978132955@gmail.com&amp;amp;gt;;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月17日(星期五) 晚上10:52&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: Flink Cli 部署问题&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; Hi&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 请问你使用哪个版本的 Flink 呢？能否分享一下&amp;amp;nbsp; Co-Process (1/1)&#010;&gt; &amp;gt; (d0309f26a545e74643382ed3f758269b) 这个 tm 的 log 呢？从上面给的日志看，应该是在&#010;&gt; 083f69d029de&#010;&gt; &amp;gt; 这台机器上。&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; Best,&#010;&gt; &amp;gt; Congxian&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; Z-Z &lt;zz9876543210@qq.com&amp;amp;gt; 于2020年7月17日周五 下午6:22写道：&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#010;&gt; &amp;gt; &amp;amp;gt; ---&amp;amp;amp;gt; /jobs/{jobid}/savepoints ---&amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,925 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; -&#010;&gt; &amp;gt; &amp;amp;gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; &amp;gt; &amp;amp;gt; 7639673873b707aa86c4387aa7b4aac3 with allocation id&#010;&gt; &amp;gt; &amp;amp;gt; e8865cdbfe4c3c33099c7112bc2e3231.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;gt; Filter&#010;&gt; (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from SCHEDULED to&#010;&gt; &amp;gt; DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source&#010;&gt; -&amp;amp;amp;gt; Filter (1/1)&#010;&gt; &amp;gt; (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; &amp;gt; &amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1)&#010;&gt; (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; &amp;gt; &amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1)&#010;&gt; (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1)&#010;&gt; (d0309f26a545e74643382ed3f758269b)&#010;&gt; &amp;gt; switched from&#010;&gt; &amp;gt; &amp;amp;gt; SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;gt; (Sink: Unnamed,&#010;&gt; Sink: Unnamed) (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to&#010;&gt; &amp;gt; DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Deploying Co-Process -&amp;amp;amp;gt; (Sink:&#010;&gt; Unnamed, Sink:&#010;&gt; &amp;gt; Unnamed) (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (attempt #0) to e63d829deafc144cd82efd73979dd056 @&#010;&gt; 083f69d029de&#010;&gt; &amp;gt; &amp;amp;gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,346 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;gt; (Sink: Unnamed,&#010;&gt; Sink: Unnamed) (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING&#010;&gt; to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; &amp;gt; &amp;amp;gt; switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; &amp;gt; &amp;amp;gt; switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1)&#010;&gt; (d0309f26a545e74643382ed3f758269b)&#010;&gt; &amp;gt; switched from&#010;&gt; &amp;gt; &amp;amp;gt; DEPLOYING to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;gt; Filter&#010;&gt; (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from DEPLOYING&#010;&gt; to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 09:51:49,493 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; - Co-Process (1/1)&#010;&gt; (d0309f26a545e74643382ed3f758269b)&#010;&gt; &amp;gt; switched from&#010;&gt; &amp;gt; &amp;amp;gt; RUNNING to FAILED.&#010;&gt; &amp;gt; &amp;amp;gt; java.lang.Exception: Exception while creating&#010;&gt; &amp;gt; StreamOperatorStateContext.&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; &amp;amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not&#010;&gt; restore&#010;&gt; &amp;gt; keyed&#010;&gt; &amp;gt; &amp;amp;gt; state backend for&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1)&#010;&gt; &amp;gt; from&#010;&gt; &amp;gt; &amp;amp;gt; any of the 1 provided restore options.&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; ... 9 more&#010;&gt; &amp;gt; &amp;amp;gt; Caused by:&#010;&gt; org.apache.flink.runtime.state.BackendBuildingException:&#010;&gt; &amp;gt; Caught&#010;&gt; &amp;gt; &amp;amp;gt; unexpected exception.&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; ... 11 more&#010;&gt; &amp;gt; &amp;amp;gt; Caused by: java.io.EOFException&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; ... 15 more&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_010040FBFE3A4FFBCBAC43AACB4528F26C09@qq.com>"
    },
    {
        "id": "<tencent_34754EF6AB58B7571B8990E4B90004F48D0A@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 08:15:07 GMT",
        "subject": "回复： Flink Cli 部署问题",
        "content": "通过 cli 命令是 在jobmanager目录 执行 bin/flink run -d -p 1 -s {savepointuri} /data/test.jar&amp;nbsp; ----&amp;gt;这种会报莫名其妙的错误，如之前的邮件&#013;&#010;通过webui就是在http://jobmanager:8081&amp;nbsp; submit new&amp;nbsp; job里添加jar包，指定相同的savepoint path和并行度提交任务 ----&amp;gt; 这样操作就没问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                                                                        \"user-zh\"                                                                                    &lt;qcx978132955@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月20日(星期一) 下午2:30&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi&#013;&#010;&#013;&#010;这个调试可以在 IDEA 进行的。&#013;&#010;&#013;&#010;另外你说的通过 web ui 提交没有问题。请问下，是同一个 savepoint 通过 flink run 提交有问题，通过 web ui&#013;&#010;提交没有问题吗？如果是的，能否分享下你的操作过程和命令呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Z-Z &lt;zz9876543210@qq.com&amp;gt; 于2020年7月20日周一 上午11:33写道：&#013;&#010;&#013;&#010;&amp;gt; 这是taskmanager新报的一个错，还是跟之前一样，用cli提交报错，用webui提交就没问题：&#013;&#010;&amp;gt; 2020-07-20 03:29:25,959 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration 'value.serializer' was supplied&#013;&#010;&amp;gt; but isn't a known config.&#013;&#010;&amp;gt; 2020-07-20 03:29:25,959 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka version : 0.11.0.2&#013;&#010;&amp;gt; 2020-07-20 03:29:25,959 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Kafka commitId : 73be1e1168f91ee2&#013;&#010;&amp;gt; 2020-07-20 03:29:25,974 ERROR&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;amp;nbsp;&#013;&#010;&amp;gt; - Caught unexpected exception.&#013;&#010;&amp;gt; java.lang.ArrayIndexOutOfBoundsException: 0&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; 2020-07-20 03:29:25,974 WARN&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;amp;nbsp; -&#013;&#010;&amp;gt; Exception while restoring keyed state backend for&#013;&#010;&amp;gt; StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1) from alternative (1/1),&#013;&#010;&amp;gt; will retry while more alternatives are available.&#013;&#010;&amp;gt; org.apache.flink.runtime.state.BackendBuildingException: Caught unexpected&#013;&#010;&amp;gt; exception.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 15 more&#013;&#010;&amp;gt; 2020-07-20 03:29:25,975 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.kafka.clients.producer.KafkaProducer&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Closing the Kafka producer with timeoutMillis&#013;&#010;&amp;gt; = 9223372036854775807 ms.&#013;&#010;&amp;gt; 2020-07-20 03:29:25,979 INFO&amp;amp;nbsp;&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Map -&amp;amp;gt; Filter -&amp;amp;gt; Sink:&#013;&#010;&amp;gt; Unnamed (1/1) (ed554502aa995fe53f1cf0cb8adf633c) switched from RUNNING to&#013;&#010;&amp;gt; FAILED.&#013;&#010;&amp;gt; java.lang.Exception: Exception while creating StreamOperatorStateContext.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore keyed&#013;&#010;&amp;gt; state backend for StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1) from any&#013;&#010;&amp;gt; of the 1 provided restore options.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 9 more&#013;&#010;&amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException: Caught&#013;&#010;&amp;gt; unexpected exception.&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 11 more&#013;&#010;&amp;gt; Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 谢谢回复：&#013;&#010;&amp;gt; 之前的savepoint都是通过RocksDBStateBackend生成的；&#013;&#010;&amp;gt; 这个savepoint我通过webui 提交任务就没问题，你是说在IDE上调试savepoint吗&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;&#013;&#010;&amp;gt; qcx978132955@gmail.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月19日(星期天) 晚上8:22&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Hi&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 从你给的这部分日志看，是恢复的时候遇到 EOF 了，这个比较奇怪&#013;&#010;&amp;gt; 1 你之前的 savepoint 是使用 RocksDBStateBackend 生成的吗&#013;&#010;&amp;gt; 2 你还有之前在 DFS 上的 savepoint 文件吗？可能需要结合 DFS 上的文件一起看一下这个问题怎么来的&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Congxian&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Z-Z &lt;zz9876543210@qq.com&amp;amp;gt; 于2020年7月17日周五 下午11:10写道：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Flink 1.10.0 ,taskmanager报错日志如下：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,913 ERROR&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; - Caught unexpected exception.&#013;&#010;&amp;gt; &amp;amp;gt; java.io.EOFException&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,914 WARN&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; -&#013;&#010;&amp;gt; &amp;amp;gt; Exception while restoring keyed state backend for&#013;&#010;&amp;gt; &amp;amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#013;&#010;&amp;gt; &amp;amp;gt; alternative (1/1), will retry while more alternatives are available.&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.state.BackendBuildingException: Caught&#013;&#010;&amp;gt; unexpected&#013;&#010;&amp;gt; &amp;amp;gt; exception.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ... 15 more&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,915 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.clients.producer.KafkaProducer&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;- Closing the Kafka&#013;&#010;&amp;gt; producer with timeoutMillis&#013;&#010;&amp;gt; &amp;amp;gt; = 9223372036854775807 ms.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,918 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Co-Keyed-Process -&amp;amp;amp;gt; Flat Map&#013;&#010;&amp;gt; &amp;amp;gt; -&amp;amp;amp;gt; Sink: Unnamed (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab)&#013;&#010;&amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; RUNNING to FAILED.&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.Exception: Exception while creating&#013;&#010;&amp;gt; StreamOperatorStateContext.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore&#013;&#010;&amp;gt; keyed&#013;&#010;&amp;gt; &amp;amp;gt; state backend for&#013;&#010;&amp;gt; &amp;amp;gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#013;&#010;&amp;gt; any of&#013;&#010;&amp;gt; &amp;amp;gt; the 1 provided restore options.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ... 9 more&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException:&#013;&#010;&amp;gt; Caught&#013;&#010;&amp;gt; &amp;amp;gt; unexpected exception.&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ... 11 more&#013;&#010;&amp;gt; &amp;amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; ... 15 more&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Freeing task resources for&#013;&#010;&amp;gt; &amp;amp;gt; Co-Keyed-Process -&amp;amp;amp;gt; Flat Map -&amp;amp;amp;gt; Sink: Unnamed (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (bb8f0a84e07ef90b1e11ca2825e0efab).&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Ensuring all FileSystem streams&#013;&#010;&amp;gt; &amp;amp;gt; are closed for task Co-Keyed-Process -&amp;amp;amp;gt; Flat Map -&amp;amp;amp;gt;&#013;&#010;&amp;gt; Sink: Unnamed&#013;&#010;&amp;gt; &amp;amp;gt; (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) [FAILED]&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,931 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - Un-registering task and sending&#013;&#010;&amp;gt; final execution&#013;&#010;&amp;gt; &amp;amp;gt; state FAILED to JobManager for task Co-Keyed-Process -&amp;amp;amp;gt; Flat&#013;&#010;&amp;gt; Map -&amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Sink: Unnamed (1/1) bb8f0a84e07ef90b1e11ca2825e0efab.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Attempting to cancel task&#013;&#010;&amp;gt; &amp;amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat&#013;&#010;&amp;gt; &amp;amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from RUNNING to&#013;&#010;&amp;gt; &amp;amp;gt; CANCELING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Triggering cancellation of task&#013;&#010;&amp;gt; &amp;amp;gt; code Source: Custom Source -&amp;amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Attempting to cancel task&#013;&#010;&amp;gt; &amp;amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from RUNNING to CANCELING.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Triggering cancellation of task&#013;&#010;&amp;gt; &amp;amp;gt; code Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat&#013;&#010;&amp;gt; &amp;amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from CANCELING&#013;&#010;&amp;gt; to&#013;&#010;&amp;gt; &amp;amp;gt; CANCELED.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Freeing task resources for&#013;&#010;&amp;gt; &amp;amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Ensuring all FileSystem streams&#013;&#010;&amp;gt; &amp;amp;gt; are closed for task Source: Custom Source -&amp;amp;amp;gt; Flat Map (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce) [CANCELED]&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from CANCELING to&#013;&#010;&amp;gt; CANCELED.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,955 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Freeing task resources for&#013;&#010;&amp;gt; &amp;amp;gt; Source: Custom Source (1/1) (00621ff5d788d00c73ccaaea04717600).&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - Un-registering task and sending&#013;&#010;&amp;gt; final execution&#013;&#010;&amp;gt; &amp;amp;gt; state CANCELED to JobManager for task Source: Custom Source -&amp;amp;amp;gt;&#013;&#010;&amp;gt; Flat Map&#013;&#010;&amp;gt; &amp;amp;gt; (1/1) 9cb8dcd4982223adcb6f007f1ffccdce.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Ensuring all FileSystem streams&#013;&#010;&amp;gt; &amp;amp;gt; are closed for task Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (00621ff5d788d00c73ccaaea04717600) [CANCELED]&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - Un-registering task and sending&#013;&#010;&amp;gt; final execution&#013;&#010;&amp;gt; &amp;amp;gt; state CANCELED to JobManager for task Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; 00621ff5d788d00c73ccaaea04717600.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - The configuration '&#013;&#010;&amp;gt; transaction.timeout.ms' was&#013;&#010;&amp;gt; &amp;amp;gt; supplied but isn't a known config.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - The configuration 'key.serializer'&#013;&#010;&amp;gt; was supplied but&#013;&#010;&amp;gt; &amp;amp;gt; isn't a known config.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - The configuration&#013;&#010;&amp;gt; 'value.serializer' was supplied&#013;&#010;&amp;gt; &amp;amp;gt; but isn't a known config.&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Kafka version : 0.11.0.2&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Kafka commitId : 73be1e1168f91ee2&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Error registering AppInfo mbean&#013;&#010;&amp;gt; &amp;amp;gt; javax.management.InstanceAlreadyExistsException:&#013;&#010;&amp;gt; &amp;amp;gt; kafka.consumer:type=app-info,id=consumer-3&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:757)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:633)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:615)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Kafka version : 0.11.0.2&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Kafka commitId : 73be1e1168f91ee2&#013;&#010;&amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,079 WARN&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#013;&#010;&amp;gt; Error registering AppInfo mbean&#013;&#010;&amp;gt; &amp;amp;gt; javax.management.InstanceAlreadyExistsException:&#013;&#010;&amp;gt; &amp;amp;gt; kafka.consumer:type=app-info,id=consumer-4&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:757)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:633)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:615)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp; &amp;amp;nbsp; at&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#013;&#010;&amp;gt; &amp;amp;gt; 发件人:&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; \"user-zh\"&#013;&#010;&amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;nbsp; &lt;&#013;&#010;&amp;gt; &amp;amp;gt; qcx978132955@gmail.com&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月17日(星期五) 晚上10:52&#013;&#010;&amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;amp;gt;;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: Flink Cli 部署问题&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Hi&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; 请问你使用哪个版本的 Flink 呢？能否分享一下&amp;amp;amp;nbsp; Co-Process (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (d0309f26a545e74643382ed3f758269b) 这个 tm 的 log 呢？从上面给的日志看，应该是在&#013;&#010;&amp;gt; 083f69d029de&#013;&#010;&amp;gt; &amp;amp;gt; 这台机器上。&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Best,&#013;&#010;&amp;gt; &amp;amp;gt; Congxian&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; Z-Z &lt;zz9876543210@qq.com&amp;amp;amp;gt; 于2020年7月17日周五 下午6:22写道：&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ---&amp;amp;amp;amp;gt; /jobs/{jobid}/savepoints ---&amp;amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,925 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; -&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 7639673873b707aa86c4387aa7b4aac3 with allocation id&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e8865cdbfe4c3c33099c7112bc2e3231.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;amp;gt; Filter&#013;&#010;&amp;gt; (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from SCHEDULED to&#013;&#010;&amp;gt; &amp;amp;gt; DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Source: Custom Source&#013;&#010;&amp;gt; -&amp;amp;amp;amp;gt; Filter (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#013;&#010;&amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1)&#013;&#010;&amp;gt; (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#013;&#010;&amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Source: Custom Source (1/1)&#013;&#010;&amp;gt; (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#013;&#010;&amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process (1/1)&#013;&#010;&amp;gt; (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; &amp;amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; SCHEDULED to DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Co-Process (1/1) (attempt #0) to&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#013;&#010;&amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;amp;gt; (Sink: Unnamed,&#013;&#010;&amp;gt; Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from SCHEDULED to&#013;&#010;&amp;gt; &amp;amp;gt; DEPLOYING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Co-Process -&amp;amp;amp;amp;gt; (Sink:&#013;&#010;&amp;gt; Unnamed, Sink:&#013;&#010;&amp;gt; &amp;amp;gt; Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (attempt #0) to e63d829deafc144cd82efd73979dd056 @&#013;&#010;&amp;gt; 083f69d029de&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (dataPort=35758)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,346 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process -&amp;amp;amp;amp;gt; (Sink: Unnamed,&#013;&#010;&amp;gt; Sink: Unnamed) (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched from DEPLOYING&#013;&#010;&amp;gt; to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process (1/1)&#013;&#010;&amp;gt; (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; &amp;amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; DEPLOYING to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source -&amp;amp;amp;amp;gt; Filter&#013;&#010;&amp;gt; (1/1)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched from DEPLOYING&#013;&#010;&amp;gt; to RUNNING.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,493 INFO&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process (1/1)&#013;&#010;&amp;gt; (d0309f26a545e74643382ed3f758269b)&#013;&#010;&amp;gt; &amp;amp;gt; switched from&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; RUNNING to FAILED.&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.Exception: Exception while creating&#013;&#010;&amp;gt; &amp;amp;gt; StreamOperatorStateContext.&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not&#013;&#010;&amp;gt; restore&#013;&#010;&amp;gt; &amp;amp;gt; keyed&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; state backend for&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1)&#013;&#010;&amp;gt; &amp;amp;gt; from&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; any of the 1 provided restore options.&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; ... 9 more&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Caused by:&#013;&#010;&amp;gt; org.apache.flink.runtime.state.BackendBuildingException:&#013;&#010;&amp;gt; &amp;amp;gt; Caught&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; unexpected exception.&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; ... 11 more&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Caused by: java.io.EOFException&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; at&#013;&#010;&amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#013;&#010;&amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#013;&#010;&amp;gt; ... 15 more",
        "depth": "2",
        "reply": "<tencent_010040FBFE3A4FFBCBAC43AACB4528F26C09@qq.com>"
    },
    {
        "id": "<CAA8tFvtrBEMvkOE2nTTES29h09K=fntvY-Y8-s_2eZ3E1SBAuQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:09:18 GMT",
        "subject": "Re: Flink Cli 部署问题",
        "content": "Hi  Z-Z&#010;    这种情况比较奇怪的。你这个是稳定复现的吗？能否分享一个稳定复现的作业代码，以及相关步骤呢？我尝试本地复现一下&#010;&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Z-Z &lt;zz9876543210@qq.com&gt; 于2020年7月20日周一 下午4:17写道：&#010;&#010;&gt; 通过 cli 命令是 在jobmanager目录 执行 bin/flink run -d -p 1 -s {savepointuri}&#010;&gt; /data/test.jar&amp;nbsp; ----&amp;gt;这种会报莫名其妙的错误，如之前的邮件&#010;&gt; 通过webui就是在http://jobmanager:8081&amp;nbsp; submit new&amp;nbsp;&#010;&gt; job里添加jar包，指定相同的savepoint path和并行度提交任务 ----&amp;gt; 这样操作就没问题&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:&#010;&gt;                                                   \"user-zh\"&#010;&gt;                                                                     &lt;&#010;&gt; qcx978132955@gmail.com&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月20日(星期一) 下午2:30&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#010;&gt;&#010;&gt; 主题:&amp;nbsp;Re: Flink Cli 部署问题&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Hi&#010;&gt;&#010;&gt; 这个调试可以在 IDEA 进行的。&#010;&gt;&#010;&gt; 另外你说的通过 web ui 提交没有问题。请问下，是同一个 savepoint 通过 flink run 提交有问题，通过 web ui&#010;&gt; 提交没有问题吗？如果是的，能否分享下你的操作过程和命令呢？&#010;&gt;&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; Z-Z &lt;zz9876543210@qq.com&amp;gt; 于2020年7月20日周一 上午11:33写道：&#010;&gt;&#010;&gt; &amp;gt; 这是taskmanager新报的一个错，还是跟之前一样，用cli提交报错，用webui提交就没问题：&#010;&gt; &amp;gt; 2020-07-20 03:29:25,959 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; - The configuration&#010;&gt; 'value.serializer' was supplied&#010;&gt; &amp;gt; but isn't a known config.&#010;&gt; &amp;gt; 2020-07-20 03:29:25,959 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Kafka version : 0.11.0.2&#010;&gt; &amp;gt; 2020-07-20 03:29:25,959 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Kafka commitId : 73be1e1168f91ee2&#010;&gt; &amp;gt; 2020-07-20 03:29:25,974 ERROR&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;amp;nbsp;&#010;&gt; &amp;gt; - Caught unexpected exception.&#010;&gt; &amp;gt; java.lang.ArrayIndexOutOfBoundsException: 0&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; 2020-07-20 03:29:25,974 WARN&amp;amp;nbsp;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;amp;nbsp;&#010;&gt; -&#010;&gt; &amp;gt; Exception while restoring keyed state backend for&#010;&gt; &amp;gt; StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1) from alternative&#010;&gt; (1/1),&#010;&gt; &amp;gt; will retry while more alternatives are available.&#010;&gt; &amp;gt; org.apache.flink.runtime.state.BackendBuildingException: Caught&#010;&gt; unexpected&#010;&gt; &amp;gt; exception.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 15 more&#010;&gt; &amp;gt; 2020-07-20 03:29:25,975 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.kafka.clients.producer.KafkaProducer&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;- Closing the Kafka&#010;&gt; producer with timeoutMillis&#010;&gt; &amp;gt; = 9223372036854775807 ms.&#010;&gt; &amp;gt; 2020-07-20 03:29:25,979 INFO&amp;amp;nbsp;&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;-&#010;&gt; Map -&amp;amp;gt; Filter -&amp;amp;gt; Sink:&#010;&gt; &amp;gt; Unnamed (1/1) (ed554502aa995fe53f1cf0cb8adf633c) switched from&#010;&gt; RUNNING to&#010;&gt; &amp;gt; FAILED.&#010;&gt; &amp;gt; java.lang.Exception: Exception while creating&#010;&gt; StreamOperatorStateContext.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not restore&#010;&gt; keyed&#010;&gt; &amp;gt; state backend for StreamMap_caf773fe289bfdb867e0b4bd0c431c5f_(1/1)&#010;&gt; from any&#010;&gt; &amp;gt; of the 1 provided restore options.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 9 more&#010;&gt; &amp;gt; Caused by: org.apache.flink.runtime.state.BackendBuildingException:&#010;&gt; Caught&#010;&gt; &amp;gt; unexpected exception.&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ... 11 more&#010;&gt; &amp;gt; Caused by: java.lang.ArrayIndexOutOfBoundsException: 0&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.snapshot.RocksSnapshotUtil.hasMetaDataFollowsFlag(RocksSnapshotUtil.java:45)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:223)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; at&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 谢谢回复：&#010;&gt; &amp;gt; 之前的savepoint都是通过RocksDBStateBackend生成的；&#010;&gt; &amp;gt; 这个savepoint我通过webui 提交任务就没问题，你是说在IDE上调试savepoint吗&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; ------------------ 原始邮件 ------------------&#010;&gt; &amp;gt; 发件人:&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; \"user-zh\"&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; &lt;&#010;&gt; &amp;gt; qcx978132955@gmail.com&amp;amp;gt;;&#010;&gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月19日(星期天) 晚上8:22&#010;&gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 主题:&amp;amp;nbsp;Re: Flink Cli 部署问题&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; Hi&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 从你给的这部分日志看，是恢复的时候遇到 EOF 了，这个比较奇怪&#010;&gt; &amp;gt; 1 你之前的 savepoint 是使用 RocksDBStateBackend 生成的吗&#010;&gt; &amp;gt; 2 你还有之前在 DFS 上的 savepoint 文件吗？可能需要结合 DFS 上的文件一起看一下这个问题怎么来的&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; Best,&#010;&gt; &amp;gt; Congxian&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; Z-Z &lt;zz9876543210@qq.com&amp;amp;gt; 于2020年7月17日周五 下午11:10写道：&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; Flink 1.10.0 ,taskmanager报错日志如下：&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,913 ERROR&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; - Caught unexpected exception.&#010;&gt; &amp;gt; &amp;amp;gt; java.io.EOFException&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,914 WARN&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; -&#010;&gt; &amp;gt; &amp;amp;gt; Exception while restoring keyed state backend for&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#010;&gt; &amp;gt; &amp;amp;gt; alternative (1/1), will retry while more alternatives are&#010;&gt; available.&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.state.BackendBuildingException:&#010;&gt; Caught&#010;&gt; &amp;gt; unexpected&#010;&gt; &amp;gt; &amp;amp;gt; exception.&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; &amp;amp;gt; Caused by: java.io.EOFException&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; ... 15 more&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,915 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.kafka.clients.producer.KafkaProducer&amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; Closing the Kafka&#010;&gt; &amp;gt; producer with timeoutMillis&#010;&gt; &amp;gt; &amp;amp;gt; = 9223372036854775807 ms.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,918 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Co-Keyed-Process -&amp;amp;amp;gt; Flat Map&#010;&gt; &amp;gt; &amp;amp;gt; -&amp;amp;amp;gt; Sink: Unnamed (1/1)&#010;&gt; (bb8f0a84e07ef90b1e11ca2825e0efab)&#010;&gt; &amp;gt; switched from&#010;&gt; &amp;gt; &amp;amp;gt; RUNNING to FAILED.&#010;&gt; &amp;gt; &amp;amp;gt; java.lang.Exception: Exception while creating&#010;&gt; &amp;gt; StreamOperatorStateContext.&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; &amp;amp;gt; Caused by: org.apache.flink.util.FlinkException: Could not&#010;&gt; restore&#010;&gt; &amp;gt; keyed&#010;&gt; &amp;gt; &amp;amp;gt; state backend for&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; KeyedCoProcessOperator_00360b8021b192d84949201d4fea80f2_(1/1) from&#010;&gt; &amp;gt; any of&#010;&gt; &amp;gt; &amp;amp;gt; the 1 provided restore options.&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; ... 9 more&#010;&gt; &amp;gt; &amp;amp;gt; Caused by:&#010;&gt; org.apache.flink.runtime.state.BackendBuildingException:&#010;&gt; &amp;gt; Caught&#010;&gt; &amp;gt; &amp;amp;gt; unexpected exception.&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; ... 11 more&#010;&gt; &amp;gt; &amp;amp;gt; Caused by: java.io.EOFException&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; ... 15 more&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Freeing task resources for&#010;&gt; &amp;gt; &amp;amp;gt; Co-Keyed-Process -&amp;amp;amp;gt; Flat Map -&amp;amp;amp;gt; Sink:&#010;&gt; Unnamed (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (bb8f0a84e07ef90b1e11ca2825e0efab).&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,919 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Ensuring all FileSystem streams&#010;&gt; &amp;gt; &amp;amp;gt; are closed for task Co-Keyed-Process -&amp;amp;amp;gt; Flat Map&#010;&gt; -&amp;amp;amp;gt;&#010;&gt; &amp;gt; Sink: Unnamed&#010;&gt; &amp;gt; &amp;amp;gt; (1/1) (bb8f0a84e07ef90b1e11ca2825e0efab) [FAILED]&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,931 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; -&#010;&gt; Un-registering task and sending&#010;&gt; &amp;gt; final execution&#010;&gt; &amp;gt; &amp;amp;gt; state FAILED to JobManager for task Co-Keyed-Process&#010;&gt; -&amp;amp;amp;gt; Flat&#010;&gt; &amp;gt; Map -&amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; Sink: Unnamed (1/1) bb8f0a84e07ef90b1e11ca2825e0efab.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Attempting to cancel task&#010;&gt; &amp;gt; &amp;amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat Map (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat&#010;&gt; &amp;gt; &amp;amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from&#010;&gt; RUNNING to&#010;&gt; &amp;gt; &amp;amp;gt; CANCELING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,947 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Triggering cancellation of task&#010;&gt; &amp;gt; &amp;amp;gt; code Source: Custom Source -&amp;amp;amp;gt; Flat Map (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Attempting to cancel task&#010;&gt; &amp;gt; &amp;amp;gt; Source: Custom Source (1/1)&#010;&gt; (00621ff5d788d00c73ccaaea04717600).&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Source: Custom Source (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from RUNNING to&#010;&gt; CANCELING.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,949 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Triggering cancellation of task&#010;&gt; &amp;gt; &amp;amp;gt; code Source: Custom Source (1/1)&#010;&gt; (00621ff5d788d00c73ccaaea04717600).&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat&#010;&gt; &amp;gt; &amp;amp;gt; Map (1/1) (9cb8dcd4982223adcb6f007f1ffccdce) switched from&#010;&gt; CANCELING&#010;&gt; &amp;gt; to&#010;&gt; &amp;gt; &amp;amp;gt; CANCELED.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Freeing task resources for&#010;&gt; &amp;gt; &amp;amp;gt; Source: Custom Source -&amp;amp;amp;gt; Flat Map (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce).&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Ensuring all FileSystem streams&#010;&gt; &amp;gt; &amp;amp;gt; are closed for task Source: Custom Source -&amp;amp;amp;gt; Flat&#010;&gt; Map (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (9cb8dcd4982223adcb6f007f1ffccdce) [CANCELED]&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Source: Custom Source (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (00621ff5d788d00c73ccaaea04717600) switched from CANCELING to&#010;&gt; &amp;gt; CANCELED.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,955 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Freeing task resources for&#010;&gt; &amp;gt; &amp;amp;gt; Source: Custom Source (1/1)&#010;&gt; (00621ff5d788d00c73ccaaea04717600).&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,954 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; -&#010;&gt; Un-registering task and sending&#010;&gt; &amp;gt; final execution&#010;&gt; &amp;gt; &amp;amp;gt; state CANCELED to JobManager for task Source: Custom Source&#010;&gt; -&amp;amp;amp;gt;&#010;&gt; &amp;gt; Flat Map&#010;&gt; &amp;gt; &amp;amp;gt; (1/1) 9cb8dcd4982223adcb6f007f1ffccdce.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Ensuring all FileSystem streams&#010;&gt; &amp;gt; &amp;amp;gt; are closed for task Source: Custom Source (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (00621ff5d788d00c73ccaaea04717600) [CANCELED]&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:43,962 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.taskexecutor.TaskExecutor&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; -&#010;&gt; Un-registering task and sending&#010;&gt; &amp;gt; final execution&#010;&gt; &amp;gt; &amp;amp;gt; state CANCELED to JobManager for task Source: Custom Source&#010;&gt; (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; 00621ff5d788d00c73ccaaea04717600.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - The&#010;&gt; configuration '&#010;&gt; &amp;gt; transaction.timeout.ms' was&#010;&gt; &amp;gt; &amp;amp;gt; supplied but isn't a known config.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - The&#010;&gt; configuration 'key.serializer'&#010;&gt; &amp;gt; was supplied but&#010;&gt; &amp;gt; &amp;amp;gt; isn't a known config.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.ConsumerConfig&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; - The&#010;&gt; configuration&#010;&gt; &amp;gt; 'value.serializer' was supplied&#010;&gt; &amp;gt; &amp;amp;gt; but isn't a known config.&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Kafka version : 0.11.0.2&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Kafka commitId : 73be1e1168f91ee2&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,077 WARN&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Error registering AppInfo mbean&#010;&gt; &amp;gt; &amp;amp;gt; javax.management.InstanceAlreadyExistsException:&#010;&gt; &amp;gt; &amp;amp;gt; kafka.consumer:type=app-info,id=consumer-3&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:757)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:633)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:615)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Kafka version : 0.11.0.2&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,079 INFO&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Kafka commitId : 73be1e1168f91ee2&#010;&gt; &amp;gt; &amp;amp;gt; 2020-07-17 15:06:44,079 WARN&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.kafka.common.utils.AppInfoParser&amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;&#010;&gt; &amp;amp;amp;nbsp; &amp;amp;amp;nbsp;-&#010;&gt; &amp;gt; Error registering AppInfo mbean&#010;&gt; &amp;gt; &amp;amp;gt; javax.management.InstanceAlreadyExistsException:&#010;&gt; &amp;gt; &amp;amp;gt; kafka.consumer:type=app-info,id=consumer-4&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:58)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:757)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:633)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&amp;amp;amp;gt;(KafkaConsumer.java:615)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.getConsumer(KafkaConsumerThread.java:502)&#010;&gt; &amp;gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.run(KafkaConsumerThread.java:181)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; ------------------&amp;amp;amp;nbsp;原始邮件&amp;amp;amp;nbsp;------------------&#010;&gt; &amp;gt; &amp;amp;gt; 发件人:&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; \"user-zh\"&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;gt;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;nbsp; &lt;&#010;&gt; &amp;gt; &amp;amp;gt; qcx978132955@gmail.com&amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt; 发送时间:&amp;amp;amp;nbsp;2020年7月17日(星期五) 晚上10:52&#010;&gt; &amp;gt; &amp;amp;gt; 收件人:&amp;amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#010;&gt; &amp;amp;amp;gt;;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 主题:&amp;amp;amp;nbsp;Re: Flink Cli 部署问题&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; Hi&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; 请问你使用哪个版本的 Flink 呢？能否分享一下&amp;amp;amp;nbsp; Co-Process (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (d0309f26a545e74643382ed3f758269b) 这个 tm 的 log&#010;&gt; 呢？从上面给的日志看，应该是在&#010;&gt; &amp;gt; 083f69d029de&#010;&gt; &amp;gt; &amp;amp;gt; 这台机器上。&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; Best,&#010;&gt; &amp;gt; &amp;amp;gt; Congxian&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; Z-Z &lt;zz9876543210@qq.com&amp;amp;amp;gt; 于2020年7月17日周五 下午6:22写道：&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; 大家好，我在部署的时候发现了一个问题，我通过restAPI接口停掉了一个任务并保存了它的savepoint(步骤：/jobs/overview&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; ---&amp;amp;amp;amp;gt; /jobs/{jobid}/savepoints&#010;&gt; ---&amp;amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; /jobs/{jobid}/savepoints/{triggerid})，但我通过flink命令带上savepoint部署任务时会报错，但通过webui上传jar并带上savepoint就不会报错，报错堆栈如下：&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,925 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; -&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Request slot with profile&#010;&gt; ResourceProfile{UNKNOWN} for job&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 7639673873b707aa86c4387aa7b4aac3 with&#010;&gt; allocation id&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e8865cdbfe4c3c33099c7112bc2e3231.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source&#010;&gt; -&amp;amp;amp;amp;gt; Filter&#010;&gt; &amp;gt; (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched&#010;&gt; from SCHEDULED to&#010;&gt; &amp;gt; &amp;amp;gt; DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,952 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Source: Custom&#010;&gt; Source&#010;&gt; &amp;gt; -&amp;amp;amp;amp;gt; Filter (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; &amp;gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,953 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Source: Custom&#010;&gt; Source (1/1)&#010;&gt; &amp;gt; (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; &amp;gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Source: Custom&#010;&gt; Source (1/1)&#010;&gt; &amp;gt; (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; &amp;gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process (1/1)&#010;&gt; &amp;gt; (d0309f26a545e74643382ed3f758269b)&#010;&gt; &amp;gt; &amp;amp;gt; switched from&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; SCHEDULED to DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,954 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Co-Process (1/1)&#010;&gt; (attempt #0) to&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; e63d829deafc144cd82efd73979dd056 @ 083f69d029de&#010;&gt; &amp;gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process&#010;&gt; -&amp;amp;amp;amp;gt; (Sink: Unnamed,&#010;&gt; &amp;gt; Sink: Unnamed) (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched&#010;&gt; from SCHEDULED to&#010;&gt; &amp;gt; &amp;amp;gt; DEPLOYING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:48,955 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Deploying Co-Process&#010;&gt; -&amp;amp;amp;amp;gt; (Sink:&#010;&gt; &amp;gt; Unnamed, Sink:&#010;&gt; &amp;gt; &amp;amp;gt; Unnamed) (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (attempt #0) to&#010;&gt; e63d829deafc144cd82efd73979dd056 @&#010;&gt; &amp;gt; 083f69d029de&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (dataPort=35758)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,346 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process&#010;&gt; -&amp;amp;amp;amp;gt; (Sink: Unnamed,&#010;&gt; &amp;gt; Sink: Unnamed) (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (618b75fcf5ea05fb5c6487bec6426e31) switched&#010;&gt; from DEPLOYING&#010;&gt; &amp;gt; to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (274b3df03e1fab627059c1a78e4a26da)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,370 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; (141f0dc22b624b39e21127f637ba63c2)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; switched from DEPLOYING to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process (1/1)&#010;&gt; &amp;gt; (d0309f26a545e74643382ed3f758269b)&#010;&gt; &amp;gt; &amp;amp;gt; switched from&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; DEPLOYING to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,377 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Source: Custom Source&#010;&gt; -&amp;amp;amp;amp;gt; Filter&#010;&gt; &amp;gt; (1/1)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; (1177659bff014e8dbc3f0508055d4307) switched&#010;&gt; from DEPLOYING&#010;&gt; &amp;gt; to RUNNING.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; 2020-07-17 09:51:49,493 INFO&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph&amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;amp;nbsp; &amp;amp;amp;amp;nbsp;&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; &amp;amp;amp;amp;nbsp; - Co-Process (1/1)&#010;&gt; &amp;gt; (d0309f26a545e74643382ed3f758269b)&#010;&gt; &amp;gt; &amp;amp;gt; switched from&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; RUNNING to FAILED.&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; java.lang.Exception: Exception while creating&#010;&gt; &amp;gt; &amp;amp;gt; StreamOperatorStateContext.&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:191)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:255)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:1006)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:454)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:449)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:461)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; java.lang.Thread.run(Thread.java:748)&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Caused by:&#010;&gt; org.apache.flink.util.FlinkException: Could not&#010;&gt; &amp;gt; restore&#010;&gt; &amp;gt; &amp;amp;gt; keyed&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; state backend for&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; LegacyKeyedCoProcessOperator_65e7116c7aa972ad18a796ae22bd6327_(1/1)&#010;&gt; &amp;gt; &amp;amp;gt; from&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; any of the 1 provided restore options.&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:304)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:131)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; ... 9 more&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Caused by:&#010;&gt; &amp;gt; org.apache.flink.runtime.state.BackendBuildingException:&#010;&gt; &amp;gt; &amp;amp;gt; Caught&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; unexpected exception.&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:336)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackend.createKeyedStateBackend(RocksDBStateBackend.java:548)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:288)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:142)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:121)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; ... 11 more&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt; Caused by: java.io.EOFException&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:197)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; java.io.DataInputStream.readFully(DataInputStream.java:169)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.deserialize(BytePrimitiveArraySerializer.java:85)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKVStateData(RocksDBFullRestoreOperation.java:221)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restoreKeyGroupsInStateHandle(RocksDBFullRestoreOperation.java:168)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.restore.RocksDBFullRestoreOperation.restore(RocksDBFullRestoreOperation.java:151)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;amp;gt; &amp;amp;amp;gt;&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackendBuilder.build(RocksDBKeyedStateBackendBuilder.java:279)&#010;&gt; &amp;gt; &amp;amp;gt;&#010;&gt; &amp;gt;&#010;&gt; &amp;amp;amp;gt;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&amp;amp;amp;nbsp;&#010;&gt; &amp;gt; ... 15 more&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_010040FBFE3A4FFBCBAC43AACB4528F26C09@qq.com>"
    },
    {
        "id": "<AD8A1463-9AD3-4862-9F04-FBBAEEC91F30@gmail.com>",
        "from": "snack white &lt;amazingu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 07:14:19 GMT",
        "subject": "flink job 跑一段时间 watermark 不推进的问题",
        "content": "HI: &#013;&#010;      flink job 跑一段时间 watermark 不推进，任务没挂，source 是 kafka ，kafka&#010;各个partition 均有数据， flink job statue backend 为 memory 。有debug 的姿势推荐吗？&#010; 看过 CPU GC 等指标，看不出来有异常。 &#013;&#010;&#013;&#010;Best regards!&#013;&#010;white&#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<AD8A1463-9AD3-4862-9F04-FBBAEEC91F30@gmail.com>"
    },
    {
        "id": "<tencent_407D08B93E72B481CB223AE62D6C50DA3508@qq.com>",
        "from": "&quot;Cayden chen&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 08:00:02 GMT",
        "subject": "回复：flink job 跑一段时间 watermark 不推进的问题",
        "content": "可以在本地idea起local模式打断点调试&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;amazinguizz@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月20日(星期一) 下午3:14&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;flink job 跑一段时间 watermark 不推进的问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;HI: &#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; flink job 跑一段时间 watermark 不推进，任务没挂，source&#010;是 kafka ，kafka 各个partition 均有数据， flink job statue backend 为 memory 。有debug&#010;的姿势推荐吗？&amp;nbsp; 看过 CPU GC 等指标，看不出来有异常。 &#013;&#010;&#013;&#010;Best regards!&#013;&#010;white",
        "depth": "1",
        "reply": "<AD8A1463-9AD3-4862-9F04-FBBAEEC91F30@gmail.com>"
    },
    {
        "id": "<CAOMLN=aKakLNua6V1xBxiScxvi127qKRiuOcBVjhPkf8zJVrjQ@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 08:23:47 GMT",
        "subject": "Re: flink job 跑一段时间 watermark 不推进的问题",
        "content": "Hi,&#013;&#010;&#013;&#010;Flink&#013;&#010;metrics里有一项是task相关的指标currentWatermark，从中可以知道subtask_index,task_name,watermark三项信息，应该能帮助排查watermark的推进情况。&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;snack white &lt;amazinguizz@gmail.com&gt; 于2020年7月20日周一 下午3:51写道：&#013;&#010;&#013;&#010;&gt; HI:&#013;&#010;&gt;       flink job 跑一段时间 watermark 不推进，任务没挂，source 是 kafka&#010;，kafka 各个partition&#013;&#010;&gt; 均有数据， flink job statue backend 为 memory 。有debug 的姿势推荐吗？&#010; 看过 CPU GC&#013;&#010;&gt; 等指标，看不出来有异常。&#013;&#010;&gt;&#013;&#010;&gt; Best regards!&#013;&#010;&gt; white&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<AD8A1463-9AD3-4862-9F04-FBBAEEC91F30@gmail.com>"
    },
    {
        "id": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 07:36:11 GMT",
        "subject": "flink1.11启动问题",
        "content": "&#010;&#010;Flink1.11启动时报错：&#010;java.lang.LinkageError: ClassCastException: attempting to castjar:file:/data/rt/jar_version/sql/6.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;to jar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;        at javax.ws.rs.ext.RuntimeDelegate.findDelegate(RuntimeDelegate.java:125)&#010;        at javax.ws.rs.ext.RuntimeDelegate.getInstance(RuntimeDelegate.java:97)&#010;        at javax.ws.rs.core.MediaType.valueOf(MediaType.java:172)&#010;        at com.sun.jersey.core.header.MediaTypes.&lt;clinit&gt;(MediaTypes.java:65)&#010;        at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:182)&#010;        at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:175)&#010;        at com.sun.jersey.core.spi.factory.MessageBodyFactory.init(MessageBodyFactory.java:162)&#010;        at com.sun.jersey.api.client.Client.init(Client.java:342)&#010;        at com.sun.jersey.api.client.Client.access$000(Client.java:118)&#010;        at com.sun.jersey.api.client.Client$1.f(Client.java:191)&#010;        at com.sun.jersey.api.client.Client$1.f(Client.java:187)&#010;        at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)&#010;        at com.sun.jersey.api.client.Client.&lt;init&gt;(Client.java:187)&#010;        at com.sun.jersey.api.client.Client.&lt;init&gt;(Client.java:170)&#010;        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:280)&#010;        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#010;        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:169)&#010;        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#010;        at org.apache.flink.yarn.YarnClusterClientFactory.getClusterDescriptor(YarnClusterClientFactory.java:76)&#010;        at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:61)&#010;        at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:43)&#010;        at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:64)&#010;        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1812)&#010;        at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128)&#010;        at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76)&#010;        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1699)&#010;        at com.missfresh.Main.main(Main.java:142)&#010;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;        at java.lang.reflect.Method.invoke(Method.java:498)&#010;        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;        at java.security.AccessController.doPrivileged(Native Method)&#010;        at javax.security.auth.Subject.doAs(Subject.java:422)&#010;        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)&#010;        at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&#010;&#010;我已经在lib下添加了javax.ws.rs-api-2.1.1.jar&#010;&#010;",
        "depth": "1",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<E993FE2D-F2C7-4C0C-B768-67F4C8876565@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 12:06:40 GMT",
        "subject": "Re: flink1.11启动问题",
        "content": "Hi,&#010;看起来像是依赖冲突问题，报错信息看起来是classpath加载到了两个相同的jar,&#010;javax.ws.rs-api-2.1.1.jar 这个jar包是你集群需要的吗？&#010;可以把你场景说细点，比如这个问题如何复现，这样大家可以帮忙一起排查&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月20日，15:36，酷酷的浑蛋 &lt;apache22@163.com&gt; 写道：&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; Flink1.11启动时报错：&#010;&gt; java.lang.LinkageError: ClassCastException: attempting to castjar:file:/data/rt/jar_version/sql/6.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;to jar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;&gt;        at javax.ws.rs.ext.RuntimeDelegate.findDelegate(RuntimeDelegate.java:125)&#010;&gt;        at javax.ws.rs.ext.RuntimeDelegate.getInstance(RuntimeDelegate.java:97)&#010;&gt;        at javax.ws.rs.core.MediaType.valueOf(MediaType.java:172)&#010;&gt;        at com.sun.jersey.core.header.MediaTypes.&lt;clinit&gt;(MediaTypes.java:65)&#010;&gt;        at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:182)&#010;&gt;        at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:175)&#010;&gt;        at com.sun.jersey.core.spi.factory.MessageBodyFactory.init(MessageBodyFactory.java:162)&#010;&gt;        at com.sun.jersey.api.client.Client.init(Client.java:342)&#010;&gt;        at com.sun.jersey.api.client.Client.access$000(Client.java:118)&#010;&gt;        at com.sun.jersey.api.client.Client$1.f(Client.java:191)&#010;&gt;        at com.sun.jersey.api.client.Client$1.f(Client.java:187)&#010;&gt;        at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)&#010;&gt;        at com.sun.jersey.api.client.Client.&lt;init&gt;(Client.java:187)&#010;&gt;        at com.sun.jersey.api.client.Client.&lt;init&gt;(Client.java:170)&#010;&gt;        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:280)&#010;&gt;        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#010;&gt;        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:169)&#010;&gt;        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#010;&gt;        at org.apache.flink.yarn.YarnClusterClientFactory.getClusterDescriptor(YarnClusterClientFactory.java:76)&#010;&gt;        at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:61)&#010;&gt;        at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:43)&#010;&gt;        at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:64)&#010;&gt;        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1812)&#010;&gt;        at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128)&#010;&gt;        at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76)&#010;&gt;        at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1699)&#010;&gt;        at com.missfresh.Main.main(Main.java:142)&#010;&gt;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;        at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt;        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt;        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt;        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt;        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt;        at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt;        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt;        at java.security.AccessController.doPrivileged(Native Method)&#010;&gt;        at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt;        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)&#010;&gt;        at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt;        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; &#010;&gt; &#010;&gt; 我已经在lib下添加了javax.ws.rs-api-2.1.1.jar&#010;&gt; &#010;&#010;&#010;",
        "depth": "2",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<7ad26bcf.6383.1736c268cc4.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 12:15:32 GMT",
        "subject": "回复： flink1.11启动问题",
        "content": "1. flink1.11解压后，启动会报：&#010;java.lang.NoClassDefFoundError: com/sun/jersey/core/util/FeaturesAndProperties&#010;然后将jersey-client-1.9.jar、jersey-core-1.9.jar复制到lib下&#010;2. 再次启动，报错：&#010;Caused by: java.lang.ClassNotFoundException: javax.ws.rs.ext.MessageBodyReader&#010;然后将javax.ws.rs-api-2.1.1.jar复制到lib下&#010;3. 再次启动，报错：&#010;Caused by: java.lang.ClassNotFoundException: org.glassfish.jersey.internal.RuntimeDelegateImpl&#010;将jersey-common-3.0.0-M6.jar复制到lib下&#010;4. 再次启动，报错：&#010;Caused by: java.lang.ClassNotFoundException: jakarta.ws.rs.ext.RuntimeDelegate&#010;将jakarta.ws.rs-api-3.0.0-M1.jar复制到lib下&#010;5. 再次启动，报：&#010;java.lang.LinkageError: ClassCastException: attempting to castjar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;to jar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;6. 我崩溃了&#010;&#010;&#010;&#010;&#010;在2020年07月20日 20:06，Leonard Xu&lt;xbjtdcq@gmail.com&gt; 写道：&#010;Hi,&#010;看起来像是依赖冲突问题，报错信息看起来是classpath加载到了两个相同的jar,&#010;javax.ws.rs-api-2.1.1.jar 这个jar包是你集群需要的吗？&#010;可以把你场景说细点，比如这个问题如何复现，这样大家可以帮忙一起排查&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;在 2020年7月20日，15:36，酷酷的浑蛋 &lt;apache22@163.com&gt; 写道：&#010;&#010;&#010;&#010;Flink1.11启动时报错：&#010;java.lang.LinkageError: ClassCastException: attempting to castjar:file:/data/rt/jar_version/sql/6.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;to jar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;at javax.ws.rs.ext.RuntimeDelegate.findDelegate(RuntimeDelegate.java:125)&#010;at javax.ws.rs.ext.RuntimeDelegate.getInstance(RuntimeDelegate.java:97)&#010;at javax.ws.rs.core.MediaType.valueOf(MediaType.java:172)&#010;at com.sun.jersey.core.header.MediaTypes.&lt;clinit&gt;(MediaTypes.java:65)&#010;at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:182)&#010;at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:175)&#010;at com.sun.jersey.core.spi.factory.MessageBodyFactory.init(MessageBodyFactory.java:162)&#010;at com.sun.jersey.api.client.Client.init(Client.java:342)&#010;at com.sun.jersey.api.client.Client.access$000(Client.java:118)&#010;at com.sun.jersey.api.client.Client$1.f(Client.java:191)&#010;at com.sun.jersey.api.client.Client$1.f(Client.java:187)&#010;at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)&#010;at com.sun.jersey.api.client.Client.&lt;init&gt;(Client.java:187)&#010;at com.sun.jersey.api.client.Client.&lt;init&gt;(Client.java:170)&#010;at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:280)&#010;at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#010;at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:169)&#010;at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#010;at org.apache.flink.yarn.YarnClusterClientFactory.getClusterDescriptor(YarnClusterClientFactory.java:76)&#010;at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:61)&#010;at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:43)&#010;at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:64)&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1812)&#010;at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128)&#010;at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76)&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1699)&#010;at com.missfresh.Main.main(Main.java:142)&#010;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;at java.lang.reflect.Method.invoke(Method.java:498)&#010;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;at java.security.AccessController.doPrivileged(Native Method)&#010;at javax.security.auth.Subject.doAs(Subject.java:422)&#010;at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)&#010;at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&#010;&#010;我已经在lib下添加了javax.ws.rs-api-2.1.1.jar&#010;&#010;",
        "depth": "3",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<C007AAC6-665A-487B-9BF3-D6726E1E699E@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 13:11:01 GMT",
        "subject": "Re: flink1.11启动问题",
        "content": "Hi，&#010;&#010;&gt; 在 2020年7月20日，20:15，酷酷的浑蛋 &lt;apache22@163.com&gt; 写道：&#010;&gt; &#010;&gt; 1. flink1.11解压后，启动会报：&#010;&gt; java.lang.NoClassDefFoundError: com/sun/jersey/core/util/FeaturesAndProperties&#010;&#010;第一步就报错，应该是你本地环境问题，后面加的包应该都是不需要的，你本地用的JDK版本是多少呀？&#010;&#010;祝好&#010;Leonard Xu&#010;",
        "depth": "4",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<tencent_EDAC4F2C5D32E5521E4EFD2A7BA71FEB5D09@qq.com>",
        "from": "&quot;Evan&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 01:43:16 GMT",
        "subject": "回复： flink1.11启动问题",
        "content": "Hi,你这个应该就是依赖的问题，你额外添加了jar包，第1步到第4步，看起来是缺少了什么依赖，到第5步看起来是依赖冲突了，这边建议你从额外添加的这个jar包着手查起。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;apache22@163.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月20日(星期一) 晚上8:15&#013;&#010;收件人:&amp;nbsp;\"user-zh@flink.apache.org\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;回复： flink1.11启动问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;1. flink1.11解压后，启动会报：&#013;&#010;java.lang.NoClassDefFoundError: com/sun/jersey/core/util/FeaturesAndProperties&#013;&#010;然后将jersey-client-1.9.jar、jersey-core-1.9.jar复制到lib下&#013;&#010;2. 再次启动，报错：&#013;&#010;Caused by: java.lang.ClassNotFoundException: javax.ws.rs.ext.MessageBodyReader&#013;&#010;然后将javax.ws.rs-api-2.1.1.jar复制到lib下&#013;&#010;3. 再次启动，报错：&#013;&#010;Caused by: java.lang.ClassNotFoundException: org.glassfish.jersey.internal.RuntimeDelegateImpl&#013;&#010;将jersey-common-3.0.0-M6.jar复制到lib下&#013;&#010;4. 再次启动，报错：&#013;&#010;Caused by: java.lang.ClassNotFoundException: jakarta.ws.rs.ext.RuntimeDelegate&#013;&#010;将jakarta.ws.rs-api-3.0.0-M1.jar复制到lib下&#013;&#010;5. 再次启动，报：&#013;&#010;java.lang.LinkageError: ClassCastException: attempting to castjar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;to jar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#013;&#010;6. 我崩溃了&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;在2020年07月20日 20:06，Leonard Xu&lt;xbjtdcq@gmail.com&amp;gt; 写道：&#013;&#010;Hi,&#013;&#010;看起来像是依赖冲突问题，报错信息看起来是classpath加载到了两个相同的jar,&#010;javax.ws.rs-api-2.1.1.jar 这个jar包是你集群需要的吗？&#013;&#010;可以把你场景说细点，比如这个问题如何复现，这样大家可以帮忙一起排查&#013;&#010;&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;在 2020年7月20日，15:36，酷酷的浑蛋 &lt;apache22@163.com&amp;gt; 写道：&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Flink1.11启动时报错：&#013;&#010;java.lang.LinkageError: ClassCastException: attempting to castjar:file:/data/rt/jar_version/sql/6.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;to jar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#013;&#010;at javax.ws.rs.ext.RuntimeDelegate.findDelegate(RuntimeDelegate.java:125)&#013;&#010;at javax.ws.rs.ext.RuntimeDelegate.getInstance(RuntimeDelegate.java:97)&#013;&#010;at javax.ws.rs.core.MediaType.valueOf(MediaType.java:172)&#013;&#010;at com.sun.jersey.core.header.MediaTypes.&lt;clinit&amp;gt;(MediaTypes.java:65)&#013;&#010;at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:182)&#013;&#010;at com.sun.jersey.core.spi.factory.MessageBodyFactory.initReaders(MessageBodyFactory.java:175)&#013;&#010;at com.sun.jersey.core.spi.factory.MessageBodyFactory.init(MessageBodyFactory.java:162)&#013;&#010;at com.sun.jersey.api.client.Client.init(Client.java:342)&#013;&#010;at com.sun.jersey.api.client.Client.access$000(Client.java:118)&#013;&#010;at com.sun.jersey.api.client.Client$1.f(Client.java:191)&#013;&#010;at com.sun.jersey.api.client.Client$1.f(Client.java:187)&#013;&#010;at com.sun.jersey.spi.inject.Errors.processWithErrors(Errors.java:193)&#013;&#010;at com.sun.jersey.api.client.Client.&lt;init&amp;gt;(Client.java:187)&#013;&#010;at com.sun.jersey.api.client.Client.&lt;init&amp;gt;(Client.java:170)&#013;&#010;at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:280)&#013;&#010;at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#013;&#010;at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:169)&#013;&#010;at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&#013;&#010;at org.apache.flink.yarn.YarnClusterClientFactory.getClusterDescriptor(YarnClusterClientFactory.java:76)&#013;&#010;at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:61)&#013;&#010;at org.apache.flink.yarn.YarnClusterClientFactory.createClusterDescriptor(YarnClusterClientFactory.java:43)&#013;&#010;at org.apache.flink.client.deployment.executors.AbstractJobClusterExecutor.execute(AbstractJobClusterExecutor.java:64)&#013;&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:1812)&#013;&#010;at org.apache.flink.client.program.StreamContextEnvironment.executeAsync(StreamContextEnvironment.java:128)&#013;&#010;at org.apache.flink.client.program.StreamContextEnvironment.execute(StreamContextEnvironment.java:76)&#013;&#010;at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.execute(StreamExecutionEnvironment.java:1699)&#013;&#010;at com.missfresh.Main.main(Main.java:142)&#013;&#010;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;at java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#013;&#010;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#013;&#010;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#013;&#010;at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#013;&#010;at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#013;&#010;at org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#013;&#010;at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#013;&#010;at java.security.AccessController.doPrivileged(Native Method)&#013;&#010;at javax.security.auth.Subject.doAs(Subject.java:422)&#013;&#010;at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1754)&#013;&#010;at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#013;&#010;at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#013;&#010;&#013;&#010;&#013;&#010;我已经在lib下添加了javax.ws.rs-api-2.1.1.jar",
        "depth": "4",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<56fc0d26.5f64.1736be01fbb.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 10:58:37 GMT",
        "subject": "flink1.11启动问题",
        "content": "&#010;&#010;这flink1.11啥情况啊，一启动就报&#010;java.lang.LinkageError: ClassCastException: attempting to castjar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;to jar:file:/data/server/flink-1.11.0/lib/javax.ws.rs-api-2.1.1.jar!/javax/ws/rs/ext/RuntimeDelegate.class&#010;&#010;&#010;jersey.xxxx.jar   javax.ws.xxx.jar我都放到lib了，怎么还是不行啊？这报的什么鬼东西？&#010;&#010;",
        "depth": "1",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<5889fdc7.9fb9.1737084c3fc.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 08:36:55 GMT",
        "subject": "flink1.11启动问题",
        "content": "&#010;&#010;服了啊，这个flink1.11启动怎么净是问题啊&#010;&#010;&#010;我1.7，1.8，1.9 都没有问题，到11就不行&#010;./bin/flink run -m yarn-cluster -yqu root.rt_constant -ys 2 -yjm 1024 -yjm 1024 -ynm sql_test&#010;./examples/batch/WordCount.jar --input hdfs://xxx/data/wangty/LICENSE-2.0.txt --output hdfs://xxx/data/wangty/a&#010;&#010;&#010;报错：&#010;Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could&#010;not allocate the required slot within slot request timeout. Please make sure that the cluster&#010;has enough resources. at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;... 45 more Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;... 25 more&#010;&#010;&#010;我资源是足的啊，就flink1.11起不来，一直卡在那里，卡好久然后报这个错，大神们帮看看吧，昨天的jar包冲突问题已经解决（只有flink1.11存在的问题），&#010;&#010;",
        "depth": "1",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<CABvJ6uWppow75f3D__3G__mKWqoN-U3_46wZd1MVPnE3zMnqtw@mail.gmail.com>",
        "from": "Shuiqiang Chen &lt;acqua....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 09:22:48 GMT",
        "subject": "Re: flink1.11启动问题",
        "content": "Hi，&#013;&#010;&#013;&#010;可以尝试在jm的log里看看是在申请哪个资源的时候超时了, 对比下所申请的资源规格和集群可用资源&#013;&#010;&#013;&#010;Best，&#013;&#010;Shuiqiang&#013;&#010;&#013;&#010;酷酷的浑蛋 &lt;apache22@163.com&gt; 于2020年7月21日周二 下午4:37写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 服了啊，这个flink1.11启动怎么净是问题啊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我1.7，1.8，1.9 都没有问题，到11就不行&#013;&#010;&gt; ./bin/flink run -m yarn-cluster -yqu root.rt_constant -ys 2 -yjm 1024 -yjm&#013;&#010;&gt; 1024 -ynm sql_test ./examples/batch/WordCount.jar --input&#013;&#010;&gt; hdfs://xxx/data/wangty/LICENSE-2.0.txt --output hdfs://xxx/data/wangty/a&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 报错：&#013;&#010;&gt; Caused by:&#013;&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#013;&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#013;&#010;&gt; make sure that the cluster has enough resources. at&#013;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#013;&#010;&gt; ... 45 more Caused by: java.util.concurrent.CompletionException:&#013;&#010;&gt; java.util.concurrent.TimeoutException at&#013;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#013;&#010;&gt; at&#013;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#013;&#010;&gt; at&#013;&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#013;&#010;&gt; at&#013;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#013;&#010;&gt; ... 25 more&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我资源是足的啊，就flink1.11起不来，一直卡在那里，卡好久然后报这个错，大神们帮看看吧，昨天的jar包冲突问题已经解决（只有flink1.11存在的问题），&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<27000760.148f.17374763bfb.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:59:32 GMT",
        "subject": "回复： flink1.11启动问题",
        "content": "jm里面没有日志啊，关键是配置都是一样的，我在1.9里运行就没问题，在flink1.11就一直卡在那里，不分配资源，到底启动方式改变了啥呢？&#010;集群资源是有的，可是任务一直卡在那说没资源，这怎么办&#010;&#010;&#010;&#010;&#010;在2020年07月21日 17:22，Shuiqiang Chen&lt;acqua.csq@gmail.com&gt; 写道：&#010;Hi，&#010;&#010;可以尝试在jm的log里看看是在申请哪个资源的时候超时了, 对比下所申请的资源规格和集群可用资源&#010;&#010;Best，&#010;Shuiqiang&#010;&#010;酷酷的浑蛋 &lt;apache22@163.com&gt; 于2020年7月21日周二 下午4:37写道：&#010;&#010;&#010;&#010;服了啊，这个flink1.11启动怎么净是问题啊&#010;&#010;&#010;我1.7，1.8，1.9 都没有问题，到11就不行&#010;./bin/flink run -m yarn-cluster -yqu root.rt_constant -ys 2 -yjm 1024 -yjm&#010;1024 -ynm sql_test ./examples/batch/WordCount.jar --input&#010;hdfs://xxx/data/wangty/LICENSE-2.0.txt --output hdfs://xxx/data/wangty/a&#010;&#010;&#010;报错：&#010;Caused by:&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources. at&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;... 45 more Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException at&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;at&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;at&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#010;at&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;... 25 more&#010;&#010;&#010;&#010;我资源是足的啊，就flink1.11起不来，一直卡在那里，卡好久然后报这个错，大神们帮看看吧，昨天的jar包冲突问题已经解决（只有flink1.11存在的问题），&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<CAP+gf36OHu1-0qFsh1RLotf_QgtzadWLxR_GA+ONJnAco1B5YA@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:49:24 GMT",
        "subject": "Re: flink1.11启动问题",
        "content": "可以的话，发一下client端和JM端的log&#013;&#010;&#013;&#010;1.11是对提交方式有一些变化，但应该都是和之前兼容的，你的提交命令看着也是没有问题的&#013;&#010;我自己试了一下也是可以正常运行的&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;酷酷的浑蛋 &lt;apache22@163.com&gt; 于2020年7月22日周三 上午11:06写道：&#013;&#010;&#013;&#010;&gt; jm里面没有日志啊，关键是配置都是一样的，我在1.9里运行就没问题，在flink1.11就一直卡在那里，不分配资源，到底启动方式改变了啥呢？&#013;&#010;&gt; 集群资源是有的，可是任务一直卡在那说没资源，这怎么办&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月21日 17:22，Shuiqiang Chen&lt;acqua.csq@gmail.com&gt; 写道：&#013;&#010;&gt; Hi，&#013;&#010;&gt;&#013;&#010;&gt; 可以尝试在jm的log里看看是在申请哪个资源的时候超时了, 对比下所申请的资源规格和集群可用资源&#013;&#010;&gt;&#013;&#010;&gt; Best，&#013;&#010;&gt; Shuiqiang&#013;&#010;&gt;&#013;&#010;&gt; 酷酷的浑蛋 &lt;apache22@163.com&gt; 于2020年7月21日周二 下午4:37写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 服了啊，这个flink1.11启动怎么净是问题啊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我1.7，1.8，1.9 都没有问题，到11就不行&#013;&#010;&gt; ./bin/flink run -m yarn-cluster -yqu root.rt_constant -ys 2 -yjm 1024 -yjm&#013;&#010;&gt; 1024 -ynm sql_test ./examples/batch/WordCount.jar --input&#013;&#010;&gt; hdfs://xxx/data/wangty/LICENSE-2.0.txt --output hdfs://xxx/data/wangty/a&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 报错：&#013;&#010;&gt; Caused by:&#013;&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#013;&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#013;&#010;&gt; make sure that the cluster has enough resources. at&#013;&#010;&gt;&#013;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#013;&#010;&gt; ... 45 more Caused by: java.util.concurrent.CompletionException:&#013;&#010;&gt; java.util.concurrent.TimeoutException at&#013;&#010;&gt;&#013;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#013;&#010;&gt; at&#013;&#010;&gt;&#013;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#013;&#010;&gt; at&#013;&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593)&#013;&#010;&gt; at&#013;&#010;&gt;&#013;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#013;&#010;&gt; ... 25 more&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我资源是足的啊，就flink1.11起不来，一直卡在那里，卡好久然后报这个错，大神们帮看看吧，昨天的jar包冲突问题已经解决（只有flink1.11存在的问题），&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<1b86e89e.1f9e.17374e3235a.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:58:29 GMT",
        "subject": "回复：flink1.11启动问题",
        "content": "Hi&#010;报错显示的是资源不足了 你确定yarn上的资源是够的吗 看下是不是节点挂了&#010;1.11我这边提交任务都是正常的&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月21日 16:36，酷酷的浑蛋 写道：&#010;&#010;&#010;服了啊，这个flink1.11启动怎么净是问题啊&#010;&#010;&#010;我1.7，1.8，1.9 都没有问题，到11就不行&#010;./bin/flink run -m yarn-cluster -yqu root.rt_constant -ys 2 -yjm 1024 -yjm 1024 -ynm sql_test&#010;./examples/batch/WordCount.jar --input hdfs://xxx/data/wangty/LICENSE-2.0.txt --output hdfs://xxx/data/wangty/a&#010;&#010;&#010;报错：&#010;Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could&#010;not allocate the required slot within slot request timeout. Please make sure that the cluster&#010;has enough resources. at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;... 45 more Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;... 25 more&#010;&#010;&#010;我资源是足的啊，就flink1.11起不来，一直卡在那里，卡好久然后报这个错，大神们帮看看吧，昨天的jar包冲突问题已经解决（只有flink1.11存在的问题），&#010;&#010;",
        "depth": "2",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<tencent_DFEE4CCC94D35392CAD7E9CBFA466B00CB07@qq.com>",
        "from": "&quot;chengyanan1008@foxmail.com&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:35:14 GMT",
        "subject": "Re: 回复：flink1.11启动问题",
        "content": "看一下yarn-containers-vcores这个参数：&#013;&#010;&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/ops/config.html#yarn-containers-vcores&#013;&#010;&#013;&#010;结合自己的集群，适当调低这个参数&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;chengyanan1008@foxmail.com&#013;&#010; &#013;&#010;发件人： JasonLee&#013;&#010;发送时间： 2020-07-22 12:58&#013;&#010;收件人： user-zh&#013;&#010;主题： 回复：flink1.11启动问题&#013;&#010;Hi&#013;&#010;报错显示的是资源不足了 你确定yarn上的资源是够的吗 看下是不是节点挂了&#010;1.11我这边提交任务都是正常的&#013;&#010; &#013;&#010; &#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010; &#013;&#010;Signature is customized by Netease Mail Master&#013;&#010; &#013;&#010;在2020年07月21日 16:36，酷酷的浑蛋 写道：&#013;&#010; &#013;&#010; &#013;&#010;服了啊，这个flink1.11启动怎么净是问题啊&#013;&#010; &#013;&#010; &#013;&#010;我1.7，1.8，1.9 都没有问题，到11就不行&#013;&#010;./bin/flink run -m yarn-cluster -yqu root.rt_constant -ys 2 -yjm 1024 -yjm 1024 -ynm sql_test&#010;./examples/batch/WordCount.jar --input hdfs://xxx/data/wangty/LICENSE-2.0.txt --output hdfs://xxx/data/wangty/a&#013;&#010; &#013;&#010; &#013;&#010;报错：&#013;&#010;Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could&#010;not allocate the required slot within slot request timeout. Please make sure that the cluster&#010;has enough resources. at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;... 45 more Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:593) at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:577)&#010;... 25 more&#013;&#010; &#013;&#010; &#013;&#010;我资源是足的啊，就flink1.11起不来，一直卡在那里，卡好久然后报这个错，大神们帮看看吧，昨天的jar包冲突问题已经解决（只有flink1.11存在的问题），&#013;&#010; &#013;&#010;",
        "depth": "2",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<3efb12de.1d09.17374d6babe.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:44:56 GMT",
        "subject": "flink1.11启动问题",
        "content": "现在是为什么啊？启动任务自动占用所有core？core数量一直在增加，直到达到队列最大值，然后就卡住了，这flink1.11啥情况啊？&#010; &#010;&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<2a983e9e.1f1b.17374d9cfe5.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:48:18 GMT",
        "subject": "回复：flink1.11启动问题",
        "content": "HI&#010;你使用的什么模式？启动任务的命令发出来看一下吧&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月22日 12:44，酷酷的浑蛋 写道：&#010;现在是为什么啊？启动任务自动占用所有core？core数量一直在增加，直到达到队列最大值，然后就卡住了，这flink1.11啥情况啊？&#010; &#010;&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<2867b315.228f.173752303f1.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 06:08:15 GMT",
        "subject": "回复：flink1.11启动问题",
        "content": "这是我的启动命令：./bin/flink run -m yarn-cluster -p 2 -ys 2 -yqu rt_constant -c com.xx.Main&#010;-yjm 1024 -ynm RTC_TEST xx.jar&#010;任务到yarn上后就一直在占用core，core数量和内存数量一直在增加&#010;&#010;&#010;&#010;&#010;在2020年07月22日 12:48，JasonLee&lt;17610775726@163.com&gt; 写道：&#010;HI&#010;你使用的什么模式？启动任务的命令发出来看一下吧&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月22日 12:44，酷酷的浑蛋 写道：&#010;现在是为什么啊？启动任务自动占用所有core？core数量一直在增加，直到达到队列最大值，然后就卡住了，这flink1.11啥情况啊？&#010; &#010;&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<2d04edb6.431a.17376900633.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 12:46:57 GMT",
        "subject": "回复：flink1.11启动问题",
        "content": "我找到问题了，我觉得我发现了一个bug，很严重，会导致flink持续占资源，一直增加&#010;&#010;&#010;&#010;&#010;在2020年07月22日 14:08，酷酷的浑蛋&lt;apache22@163.com&gt; 写道：&#010;这是我的启动命令：./bin/flink run -m yarn-cluster -p 2 -ys 2 -yqu rt_constant -c&#010;com.xx.Main -yjm 1024 -ynm RTC_TEST xx.jar&#010;任务到yarn上后就一直在占用core，core数量和内存数量一直在增加&#010;&#010;&#010;&#010;&#010;在2020年07月22日 12:48，JasonLee&lt;17610775726@163.com&gt; 写道：&#010;HI&#010;你使用的什么模式？启动任务的命令发出来看一下吧&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月22日 12:44，酷酷的浑蛋 写道：&#010;现在是为什么啊？启动任务自动占用所有core？core数量一直在增加，直到达到队列最大值，然后就卡住了，这flink1.11啥情况啊？&#010; &#010;&#010;&#010;&#010;",
        "depth": "4",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<A301FD04-AC5D-43C2-A022-0E22ED95AAB6@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 12:49:25 GMT",
        "subject": "Re: flink1.11启动问题",
        "content": "Hello，&#010;可以描述下这个问题吗？ 如果确认是bug的话可以去jira上开个issue的。&#010;&#010;祝好&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月22日，20:46，酷酷的浑蛋 &lt;apache22@163.com&gt; 写道：&#010;&gt; &#010;&gt; 我找到问题了，我觉得我发现了一个bug，很严重，会导致flink持续占资源，一直增加&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在2020年07月22日 14:08，酷酷的浑蛋&lt;apache22@163.com&gt; 写道：&#010;&gt; 这是我的启动命令：./bin/flink run -m yarn-cluster -p 2 -ys 2 -yqu rt_constant&#010;-c com.xx.Main -yjm 1024 -ynm RTC_TEST xx.jar&#010;&gt; 任务到yarn上后就一直在占用core，core数量和内存数量一直在增加&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在2020年07月22日 12:48，JasonLee&lt;17610775726@163.com&gt; 写道：&#010;&gt; HI&#010;&gt; 你使用的什么模式？启动任务的命令发出来看一下吧&#010;&gt; &#010;&gt; &#010;&gt; | |&#010;&gt; JasonLee&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：17610775726@163.com&#010;&gt; |&#010;&gt; &#010;&gt; Signature is customized by Netease Mail Master&#010;&gt; &#010;&gt; 在2020年07月22日 12:44，酷酷的浑蛋 写道：&#010;&gt; 现在是为什么啊？启动任务自动占用所有core？core数量一直在增加，直到达到队列最大值，然后就卡住了，这flink1.11啥情况啊？&#010; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&#010;&#010;",
        "depth": "5",
        "reply": "<48c9332a.4b49.1736b26ca8e.Coremail.apache22@163.com>"
    },
    {
        "id": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 08:29:06 GMT",
        "subject": "flink1.11 run",
        "content": "hi，我这面请一个一个kafka到hive的程序，但程序无法运行，请问什么原因：&#010;&#010;异常：&#010;The program finished with the following exception:&#010;&#010;org.apache.flink.client.program.ProgramInvocationException: The main method&#010;caused an error: No operators defined in streaming topology. Cannot&#010;generate StreamGraph.&#010;at&#010;org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;at&#010;org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;at&#010;org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;at&#010;org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;at&#010;org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;at java.security.AccessController.doPrivileged(Native Method)&#010;at javax.security.auth.Subject.doAs(Subject.java:422)&#010;at&#010;org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)&#010;at&#010;org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;Caused by: java.lang.IllegalStateException: No operators defined in&#010;streaming topology. Cannot generate StreamGraph.&#010;at&#010;org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;at&#010;org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;at&#010;org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;at&#010;com.akulaku.data.flink.StreamingWriteToHive.main(StreamingWriteToHive.java:80)&#010;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;at&#010;sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;at&#010;sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;at java.lang.reflect.Method.invoke(Method.java:498)&#010;at&#010;org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;... 11 more&#010;代码：&#010;&#010; StreamExecutionEnvironment environment =&#010;StreamExecutionEnvironment.getExecutionEnvironment();&#010;        EnvironmentSettings settings =&#010;EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#010;        StreamTableEnvironment tableEnv =&#010;StreamTableEnvironment.create(environment, settings);&#010;&#010;        environment.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;        environment.setStateBackend(new MemoryStateBackend());&#010;        environment.getCheckpointConfig().setCheckpointInterval(5000);&#010;&#010;        String name = \"myhive\";&#010;        String defaultDatabase = \"tmp\";&#010;        String hiveConfDir = \"/etc/alternatives/hive-conf/\";&#010;        String version = \"1.1.0\";&#010;&#010;        HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;hiveConfDir, version);&#010;        tableEnv.registerCatalog(\"myhive\", hive);&#010;        tableEnv.useCatalog(\"myhive\");&#010;&#010;        tableEnv.executeSql(\"CREATE TABLE tmp.user_behavior (\\n\" +&#010;                \"  user_id BIGINT,\\n\" +&#010;                \"  item_id STRING,\\n\" +&#010;                \"  behavior STRING,\\n\" +&#010;                \"  ts AS PROCTIME()\\n\" +&#010;                \") WITH (\\n\" +&#010;                \" 'connector' = 'kafka-0.11',\\n\" +&#010;                \" 'topic' = 'user_behavior',\\n\" +&#010;                \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;                \" 'properties.group.id' = 'testGroup',\\n\" +&#010;                \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;                \" 'format' = 'json',\\n\" +&#010;                \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;                \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;                \")\");&#010;&#010;//        tableEnv.executeSql(\"CREATE TABLE print_table (\\n\" +&#010;//                \" user_id BIGINT,\\n\" +&#010;//                \" item_id STRING,\\n\" +&#010;//                \" behavior STRING,\\n\" +&#010;//                \" tsdata STRING\\n\" +&#010;//                \") WITH (\\n\" +&#010;//                \" 'connector' = 'print'\\n\" +&#010;//                \")\");&#010;        tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;        tableEnv.executeSql(\"CREATE TABLE tmp.streamhivetest (\\n\" +&#010;                \" user_id BIGINT,\\n\" +&#010;                \" item_id STRING,\\n\" +&#010;                \" behavior STRING,\\n\" +&#010;                \" tsdata STRING\\n\" +&#010;                \") STORED AS parquet TBLPROPERTIES (\\n\" +&#010;                \" 'sink.rolling-policy.file-size' = '12MB',\\n\" +&#010;                \" 'sink.rolling-policy.rollover-interval' = '1 min',\\n\" +&#010;                \" 'sink.rolling-policy.check-interval' = '1 min',\\n\" +&#010;                \" 'execution.checkpointing.interval' = 'true'\\n\" +&#010;                \")\");&#010;&#010;        tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;        tableEnv.executeSql(\"insert into streamhivetest select&#010;user_id,item_id,behavior,DATE_FORMAT(ts, 'yyyy-MM-dd') as tsdata from&#010;user_behavior\");&#010;&#010;        tableEnv.execute(\"stream-write-hive\");&#010;&#010;",
        "depth": "0",
        "reply": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>"
    },
    {
        "id": "<CADH6UNSAfERA-3mb=n2DvYV0_-kfmz-O=eB94WaCwjrQww=iGQ@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 08:43:37 GMT",
        "subject": "Re: flink1.11 run",
        "content": "tableEnv.executeSql就已经提交作业了，不需要再执行execute了哈&#010;&#010;On Mon, Jul 20, 2020 at 4:29 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&#010;&gt; hi，我这面请一个一个kafka到hive的程序，但程序无法运行，请问什么原因：&#010;&gt;&#010;&gt; 异常：&#010;&gt; The program finished with the following exception:&#010;&gt;&#010;&gt; org.apache.flink.client.program.ProgramInvocationException: The main method&#010;&gt; caused an error: No operators defined in streaming topology. Cannot&#010;&gt; generate StreamGraph.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; Caused by: java.lang.IllegalStateException: No operators defined in&#010;&gt; streaming topology. Cannot generate StreamGraph.&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; at&#010;&gt;&#010;&gt; com.akulaku.data.flink.StreamingWriteToHive.main(StreamingWriteToHive.java:80)&#010;&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; at&#010;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; at&#010;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; ... 11 more&#010;&gt; 代码：&#010;&gt;&#010;&gt;  StreamExecutionEnvironment environment =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt;         EnvironmentSettings settings =&#010;&gt;&#010;&gt; EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#010;&gt;         StreamTableEnvironment tableEnv =&#010;&gt; StreamTableEnvironment.create(environment, settings);&#010;&gt;&#010;&gt;&#010;&gt; environment.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&gt;         environment.setStateBackend(new MemoryStateBackend());&#010;&gt;         environment.getCheckpointConfig().setCheckpointInterval(5000);&#010;&gt;&#010;&gt;         String name = \"myhive\";&#010;&gt;         String defaultDatabase = \"tmp\";&#010;&gt;         String hiveConfDir = \"/etc/alternatives/hive-conf/\";&#010;&gt;         String version = \"1.1.0\";&#010;&gt;&#010;&gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; hiveConfDir, version);&#010;&gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt;&#010;&gt;         tableEnv.executeSql(\"CREATE TABLE tmp.user_behavior (\\n\" +&#010;&gt;                 \"  user_id BIGINT,\\n\" +&#010;&gt;                 \"  item_id STRING,\\n\" +&#010;&gt;                 \"  behavior STRING,\\n\" +&#010;&gt;                 \"  ts AS PROCTIME()\\n\" +&#010;&gt;                 \") WITH (\\n\" +&#010;&gt;                 \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt;                 \" 'topic' = 'user_behavior',\\n\" +&#010;&gt;                 \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;&gt;                 \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt;                 \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt;                 \" 'format' = 'json',\\n\" +&#010;&gt;                 \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt;                 \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt;                 \")\");&#010;&gt;&#010;&gt; //        tableEnv.executeSql(\"CREATE TABLE print_table (\\n\" +&#010;&gt; //                \" user_id BIGINT,\\n\" +&#010;&gt; //                \" item_id STRING,\\n\" +&#010;&gt; //                \" behavior STRING,\\n\" +&#010;&gt; //                \" tsdata STRING\\n\" +&#010;&gt; //                \") WITH (\\n\" +&#010;&gt; //                \" 'connector' = 'print'\\n\" +&#010;&gt; //                \")\");&#010;&gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt;         tableEnv.executeSql(\"CREATE TABLE tmp.streamhivetest (\\n\" +&#010;&gt;                 \" user_id BIGINT,\\n\" +&#010;&gt;                 \" item_id STRING,\\n\" +&#010;&gt;                 \" behavior STRING,\\n\" +&#010;&gt;                 \" tsdata STRING\\n\" +&#010;&gt;                 \") STORED AS parquet TBLPROPERTIES (\\n\" +&#010;&gt;                 \" 'sink.rolling-policy.file-size' = '12MB',\\n\" +&#010;&gt;                 \" 'sink.rolling-policy.rollover-interval' = '1 min',\\n\" +&#010;&gt;                 \" 'sink.rolling-policy.check-interval' = '1 min',\\n\" +&#010;&gt;                 \" 'execution.checkpointing.interval' = 'true'\\n\" +&#010;&gt;                 \")\");&#010;&gt;&#010;&gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt;         tableEnv.executeSql(\"insert into streamhivetest select&#010;&gt; user_id,item_id,behavior,DATE_FORMAT(ts, 'yyyy-MM-dd') as tsdata from&#010;&gt; user_behavior\");&#010;&gt;&#010;&gt;         tableEnv.execute(\"stream-write-hive\");&#010;&gt;&#010;&#010;&#010;-- &#010;Best regards!&#010;Rui Li&#010;&#010;",
        "depth": "1",
        "reply": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>"
    },
    {
        "id": "<CAEZk0432vhYxxLsC5LMVsVoVYgT-ovHByYTE+jdeVNb7E3-txg@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 09:13:23 GMT",
        "subject": "Re: flink1.11 run",
        "content": " hi&#010;好的，想问一下stream写hive表的时候：&#010;1、一定要在flink内部先建立hive表吗？&#010;2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&#010;Rui Li &lt;lirui.fudan@gmail.com&gt; 于2020年7月20日周一 下午4:44写道：&#010;&#010;&gt; tableEnv.executeSql就已经提交作业了，不需要再执行execute了哈&#010;&gt;&#010;&gt; On Mon, Jul 20, 2020 at 4:29 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; hi，我这面请一个一个kafka到hive的程序，但程序无法运行，请问什么原因：&#010;&gt; &gt;&#010;&gt; &gt; 异常：&#010;&gt; &gt; The program finished with the following exception:&#010;&gt; &gt;&#010;&gt; &gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt; method&#010;&gt; &gt; caused an error: No operators defined in streaming topology. Cannot&#010;&gt; &gt; generate StreamGraph.&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt; at&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; &gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; &gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; &gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; &gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; &gt; Caused by: java.lang.IllegalStateException: No operators defined in&#010;&gt; &gt; streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.StreamingWriteToHive.main(StreamingWriteToHive.java:80)&#010;&gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt; ... 11 more&#010;&gt; &gt; 代码：&#010;&gt; &gt;&#010;&gt; &gt;  StreamExecutionEnvironment environment =&#010;&gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt;         EnvironmentSettings settings =&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#010;&gt; &gt;         StreamTableEnvironment tableEnv =&#010;&gt; &gt; StreamTableEnvironment.create(environment, settings);&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; environment.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &gt;         environment.setStateBackend(new MemoryStateBackend());&#010;&gt; &gt;         environment.getCheckpointConfig().setCheckpointInterval(5000);&#010;&gt; &gt;&#010;&gt; &gt;         String name = \"myhive\";&#010;&gt; &gt;         String defaultDatabase = \"tmp\";&#010;&gt; &gt;         String hiveConfDir = \"/etc/alternatives/hive-conf/\";&#010;&gt; &gt;         String version = \"1.1.0\";&#010;&gt; &gt;&#010;&gt; &gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; hiveConfDir, version);&#010;&gt; &gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt; &gt;&#010;&gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.user_behavior (\\n\" +&#010;&gt; &gt;                 \"  user_id BIGINT,\\n\" +&#010;&gt; &gt;                 \"  item_id STRING,\\n\" +&#010;&gt; &gt;                 \"  behavior STRING,\\n\" +&#010;&gt; &gt;                 \"  ts AS PROCTIME()\\n\" +&#010;&gt; &gt;                 \") WITH (\\n\" +&#010;&gt; &gt;                 \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt; &gt;                 \" 'topic' = 'user_behavior',\\n\" +&#010;&gt; &gt;                 \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;&gt; &gt;                 \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt; &gt;                 \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt; &gt;                 \" 'format' = 'json',\\n\" +&#010;&gt; &gt;                 \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt; &gt;                 \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt; &gt;                 \")\");&#010;&gt; &gt;&#010;&gt; &gt; //        tableEnv.executeSql(\"CREATE TABLE print_table (\\n\" +&#010;&gt; &gt; //                \" user_id BIGINT,\\n\" +&#010;&gt; &gt; //                \" item_id STRING,\\n\" +&#010;&gt; &gt; //                \" behavior STRING,\\n\" +&#010;&gt; &gt; //                \" tsdata STRING\\n\" +&#010;&gt; &gt; //                \") WITH (\\n\" +&#010;&gt; &gt; //                \" 'connector' = 'print'\\n\" +&#010;&gt; &gt; //                \")\");&#010;&gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.streamhivetest (\\n\" +&#010;&gt; &gt;                 \" user_id BIGINT,\\n\" +&#010;&gt; &gt;                 \" item_id STRING,\\n\" +&#010;&gt; &gt;                 \" behavior STRING,\\n\" +&#010;&gt; &gt;                 \" tsdata STRING\\n\" +&#010;&gt; &gt;                 \") STORED AS parquet TBLPROPERTIES (\\n\" +&#010;&gt; &gt;                 \" 'sink.rolling-policy.file-size' = '12MB',\\n\" +&#010;&gt; &gt;                 \" 'sink.rolling-policy.rollover-interval' = '1 min',\\n\" +&#010;&gt; &gt;                 \" 'sink.rolling-policy.check-interval' = '1 min',\\n\" +&#010;&gt; &gt;                 \" 'execution.checkpointing.interval' = 'true'\\n\" +&#010;&gt; &gt;                 \")\");&#010;&gt; &gt;&#010;&gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt;         tableEnv.executeSql(\"insert into streamhivetest select&#010;&gt; &gt; user_id,item_id,behavior,DATE_FORMAT(ts, 'yyyy-MM-dd') as tsdata from&#010;&gt; &gt; user_behavior\");&#010;&gt; &gt;&#010;&gt; &gt;         tableEnv.execute(\"stream-write-hive\");&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Best regards!&#010;&gt; Rui Li&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jS=njg62_tF3KD4G=BQbrMMQfKmSrhQRX_YHk8gmgimSw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 10:11:13 GMT",
        "subject": "Re: flink1.11 run",
        "content": "Hi Dream,&#010;&#010;&gt; 1.一定要在flink内部先建立hive表吗？&#010;&#010;不用，哪边建无所谓&#010;&#010;&gt; 2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&#010;可以，默认下 128MB 滚动，Checkpoint 滚动。&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Mon, Jul 20, 2020 at 5:15 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&#010;&gt;  hi&#010;&gt; 好的，想问一下stream写hive表的时候：&#010;&gt; 1、一定要在flink内部先建立hive表吗？&#010;&gt; 2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&gt;&#010;&gt; Rui Li &lt;lirui.fudan@gmail.com&gt; 于2020年7月20日周一 下午4:44写道：&#010;&gt;&#010;&gt; &gt; tableEnv.executeSql就已经提交作业了，不需要再执行execute了哈&#010;&gt; &gt;&#010;&gt; &gt; On Mon, Jul 20, 2020 at 4:29 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi，我这面请一个一个kafka到hive的程序，但程序无法运行，请问什么原因：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 异常：&#010;&gt; &gt; &gt; The program finished with the following exception:&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt; &gt; method&#010;&gt; &gt; &gt; caused an error: No operators defined in streaming topology. Cannot&#010;&gt; &gt; &gt; generate StreamGraph.&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; &gt; &gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; &gt; &gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; &gt; &gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; &gt; &gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; &gt; &gt; Caused by: java.lang.IllegalStateException: No operators defined in&#010;&gt; &gt; &gt; streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.StreamingWriteToHive.main(StreamingWriteToHive.java:80)&#010;&gt; &gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; &gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt; &gt; ... 11 more&#010;&gt; &gt; &gt; 代码：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;  StreamExecutionEnvironment environment =&#010;&gt; &gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt; &gt;         EnvironmentSettings settings =&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#010;&gt; &gt; &gt;         StreamTableEnvironment tableEnv =&#010;&gt; &gt; &gt; StreamTableEnvironment.create(environment, settings);&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; environment.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &gt; &gt;         environment.setStateBackend(new MemoryStateBackend());&#010;&gt; &gt; &gt;         environment.getCheckpointConfig().setCheckpointInterval(5000);&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         String name = \"myhive\";&#010;&gt; &gt; &gt;         String defaultDatabase = \"tmp\";&#010;&gt; &gt; &gt;         String hiveConfDir = \"/etc/alternatives/hive-conf/\";&#010;&gt; &gt; &gt;         String version = \"1.1.0\";&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; &gt; hiveConfDir, version);&#010;&gt; &gt; &gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt; &gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.user_behavior (\\n\" +&#010;&gt; &gt; &gt;                 \"  user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt;                 \"  item_id STRING,\\n\" +&#010;&gt; &gt; &gt;                 \"  behavior STRING,\\n\" +&#010;&gt; &gt; &gt;                 \"  ts AS PROCTIME()\\n\" +&#010;&gt; &gt; &gt;                 \") WITH (\\n\" +&#010;&gt; &gt; &gt;                 \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt; &gt; &gt;                 \" 'topic' = 'user_behavior',\\n\" +&#010;&gt; &gt; &gt;                 \" 'properties.bootstrap.servers' =&#010;&gt; 'localhost:9092',\\n\" +&#010;&gt; &gt; &gt;                 \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt; &gt; &gt;                 \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt; &gt; &gt;                 \" 'format' = 'json',\\n\" +&#010;&gt; &gt; &gt;                 \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt; &gt; &gt;                 \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; //        tableEnv.executeSql(\"CREATE TABLE print_table (\\n\" +&#010;&gt; &gt; &gt; //                \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; //                \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt; //                \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt; //                \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt; //                \") WITH (\\n\" +&#010;&gt; &gt; &gt; //                \" 'connector' = 'print'\\n\" +&#010;&gt; &gt; &gt; //                \")\");&#010;&gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.streamhivetest (\\n\" +&#010;&gt; &gt; &gt;                 \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt;                 \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt;                 \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt;                 \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt;                 \") STORED AS parquet TBLPROPERTIES (\\n\" +&#010;&gt; &gt; &gt;                 \" 'sink.rolling-policy.file-size' = '12MB',\\n\" +&#010;&gt; &gt; &gt;                 \" 'sink.rolling-policy.rollover-interval' = '1&#010;&gt; min',\\n\" +&#010;&gt; &gt; &gt;                 \" 'sink.rolling-policy.check-interval' = '1 min',\\n\" +&#010;&gt; &gt; &gt;                 \" 'execution.checkpointing.interval' = 'true'\\n\" +&#010;&gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt; &gt;         tableEnv.executeSql(\"insert into streamhivetest select&#010;&gt; &gt; &gt; user_id,item_id,behavior,DATE_FORMAT(ts, 'yyyy-MM-dd') as tsdata from&#010;&gt; &gt; &gt; user_behavior\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         tableEnv.execute(\"stream-write-hive\");&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt; Best regards!&#010;&gt; &gt; Rui Li&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "3",
        "reply": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>"
    },
    {
        "id": "<CAEZk042x2L55HC-2e_v_nZ333gBFvpvZcOB4TajnxVCO1-wa=Q@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 11:09:35 GMT",
        "subject": "Re: flink1.11 run",
        "content": "hi、&#010;对于下面这两个的滚动方式，是选优先到达的吗，就是1min的checkpoint和128mb的file&#010;size，不管哪个先到都会滚动生成新的文件&#010;&#010;》可以，默认下 128MB 滚动，Checkpoint 滚动&#010;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月20日周一 下午6:12写道：&#010;&#010;&gt; Hi Dream,&#010;&gt;&#010;&gt; &gt; 1.一定要在flink内部先建立hive表吗？&#010;&gt;&#010;&gt; 不用，哪边建无所谓&#010;&gt;&#010;&gt; &gt; 2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&gt;&#010;&gt; 可以，默认下 128MB 滚动，Checkpoint 滚动。&#010;&gt;&#010;&gt; Best,&#010;&gt; Jingsong&#010;&gt;&#010;&gt; On Mon, Jul 20, 2020 at 5:15 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt;&#010;&gt; &gt;  hi&#010;&gt; &gt; 好的，想问一下stream写hive表的时候：&#010;&gt; &gt; 1、一定要在flink内部先建立hive表吗？&#010;&gt; &gt; 2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&gt; &gt;&#010;&gt; &gt; Rui Li &lt;lirui.fudan@gmail.com&gt; 于2020年7月20日周一 下午4:44写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; tableEnv.executeSql就已经提交作业了，不需要再执行execute了哈&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On Mon, Jul 20, 2020 at 4:29 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; hi，我这面请一个一个kafka到hive的程序，但程序无法运行，请问什么原因：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 异常：&#010;&gt; &gt; &gt; &gt; The program finished with the following exception:&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt; &gt; &gt; method&#010;&gt; &gt; &gt; &gt; caused an error: No operators defined in streaming topology. Cannot&#010;&gt; &gt; &gt; &gt; generate StreamGraph.&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; &gt; &gt; &gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; &gt; &gt; &gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; &gt; &gt; &gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; &gt; &gt; &gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; &gt; &gt; &gt; Caused by: java.lang.IllegalStateException: No operators defined in&#010;&gt; &gt; &gt; &gt; streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.StreamingWriteToHive.main(StreamingWriteToHive.java:80)&#010;&gt; &gt; &gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; &gt; &gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt; &gt; &gt; ... 11 more&#010;&gt; &gt; &gt; &gt; 代码：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;  StreamExecutionEnvironment environment =&#010;&gt; &gt; &gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt; &gt; &gt;         EnvironmentSettings settings =&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#010;&gt; &gt; &gt; &gt;         StreamTableEnvironment tableEnv =&#010;&gt; &gt; &gt; &gt; StreamTableEnvironment.create(environment, settings);&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; environment.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &gt; &gt; &gt;         environment.setStateBackend(new MemoryStateBackend());&#010;&gt; &gt; &gt; &gt;&#010;&gt;  environment.getCheckpointConfig().setCheckpointInterval(5000);&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;         String name = \"myhive\";&#010;&gt; &gt; &gt; &gt;         String defaultDatabase = \"tmp\";&#010;&gt; &gt; &gt; &gt;         String hiveConfDir = \"/etc/alternatives/hive-conf/\";&#010;&gt; &gt; &gt; &gt;         String version = \"1.1.0\";&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; &gt; &gt; hiveConfDir, version);&#010;&gt; &gt; &gt; &gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt; &gt; &gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.user_behavior (\\n\" +&#010;&gt; &gt; &gt; &gt;                 \"  user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; &gt;                 \"  item_id STRING,\\n\" +&#010;&gt; &gt; &gt; &gt;                 \"  behavior STRING,\\n\" +&#010;&gt; &gt; &gt; &gt;                 \"  ts AS PROCTIME()\\n\" +&#010;&gt; &gt; &gt; &gt;                 \") WITH (\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'topic' = 'user_behavior',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'properties.bootstrap.servers' =&#010;&gt; &gt; 'localhost:9092',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'format' = 'json',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt; &gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; //        tableEnv.executeSql(\"CREATE TABLE print_table (\\n\" +&#010;&gt; &gt; &gt; &gt; //                \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; &gt; //                \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; //                \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; //                \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt; &gt; //                \") WITH (\\n\" +&#010;&gt; &gt; &gt; &gt; //                \" 'connector' = 'print'\\n\" +&#010;&gt; &gt; &gt; &gt; //                \")\");&#010;&gt; &gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.streamhivetest (\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt; &gt;                 \") STORED AS parquet TBLPROPERTIES (\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'sink.rolling-policy.file-size' = '12MB',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'sink.rolling-policy.rollover-interval' = '1&#010;&gt; &gt; min',\\n\" +&#010;&gt; &gt; &gt; &gt;                 \" 'sink.rolling-policy.check-interval' = '1 min',\\n\"&#010;&gt; +&#010;&gt; &gt; &gt; &gt;                 \" 'execution.checkpointing.interval' = 'true'\\n\" +&#010;&gt; &gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt; &gt; &gt;         tableEnv.executeSql(\"insert into streamhivetest select&#010;&gt; &gt; &gt; &gt; user_id,item_id,behavior,DATE_FORMAT(ts, 'yyyy-MM-dd') as tsdata from&#010;&gt; &gt; &gt; &gt; user_behavior\");&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;         tableEnv.execute(\"stream-write-hive\");&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; --&#010;&gt; &gt; &gt; Best regards!&#010;&gt; &gt; &gt; Rui Li&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Best, Jingsong Lee&#010;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jRq9mzxMeT6oO-bowYazu0qBCGxfawXwLUKzZMvqw6zPw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 11:23:03 GMT",
        "subject": "Re: flink1.11 run",
        "content": "是的。&#010;&#010;但是不管怎么滚动，最终都是checkpoint完成后文件才可见&#010;&#010;On Mon, Jul 20, 2020 at 7:10 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&#010;&gt; hi、&#010;&gt; 对于下面这两个的滚动方式，是选优先到达的吗，就是1min的checkpoint和128mb的file&#010;size，不管哪个先到都会滚动生成新的文件&#010;&gt;&#010;&gt; 》可以，默认下 128MB 滚动，Checkpoint 滚动&#010;&gt;&#010;&gt; Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月20日周一 下午6:12写道：&#010;&gt;&#010;&gt; &gt; Hi Dream,&#010;&gt; &gt;&#010;&gt; &gt; &gt; 1.一定要在flink内部先建立hive表吗？&#010;&gt; &gt;&#010;&gt; &gt; 不用，哪边建无所谓&#010;&gt; &gt;&#010;&gt; &gt; &gt; 2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&gt; &gt;&#010;&gt; &gt; 可以，默认下 128MB 滚动，Checkpoint 滚动。&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Jingsong&#010;&gt; &gt;&#010;&gt; &gt; On Mon, Jul 20, 2020 at 5:15 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; &gt;  hi&#010;&gt; &gt; &gt; 好的，想问一下stream写hive表的时候：&#010;&gt; &gt; &gt; 1、一定要在flink内部先建立hive表吗？&#010;&gt; &gt; &gt; 2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Rui Li &lt;lirui.fudan@gmail.com&gt; 于2020年7月20日周一 下午4:44写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; tableEnv.executeSql就已经提交作业了，不需要再执行execute了哈&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; On Mon, Jul 20, 2020 at 4:29 PM Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;&gt; wrote:&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; hi，我这面请一个一个kafka到hive的程序，但程序无法运行，请问什么原因：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 异常：&#010;&gt; &gt; &gt; &gt; &gt; The program finished with the following exception:&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.client.program.ProgramInvocationException: The&#010;&gt; main&#010;&gt; &gt; &gt; &gt; method&#010;&gt; &gt; &gt; &gt; &gt; caused an error: No operators defined in streaming topology. Cannot&#010;&gt; &gt; &gt; &gt; &gt; generate StreamGraph.&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; &gt; &gt; &gt; &gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; &gt; &gt; &gt; &gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; &gt; &gt; &gt; &gt; Caused by: java.lang.IllegalStateException: No operators defined&#010;in&#010;&gt; &gt; &gt; &gt; &gt; streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.StreamingWriteToHive.main(StreamingWriteToHive.java:80)&#010;&gt; &gt; &gt; &gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; &gt; &gt; &gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; &gt; &gt; &gt; at&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt; &gt; &gt; &gt; ... 11 more&#010;&gt; &gt; &gt; &gt; &gt; 代码：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;  StreamExecutionEnvironment environment =&#010;&gt; &gt; &gt; &gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt; &gt; &gt; &gt;         EnvironmentSettings settings =&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#010;&gt; &gt; &gt; &gt; &gt;         StreamTableEnvironment tableEnv =&#010;&gt; &gt; &gt; &gt; &gt; StreamTableEnvironment.create(environment, settings);&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; environment.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &gt; &gt; &gt; &gt;         environment.setStateBackend(new MemoryStateBackend());&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;  environment.getCheckpointConfig().setCheckpointInterval(5000);&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;         String name = \"myhive\";&#010;&gt; &gt; &gt; &gt; &gt;         String defaultDatabase = \"tmp\";&#010;&gt; &gt; &gt; &gt; &gt;         String hiveConfDir = \"/etc/alternatives/hive-conf/\";&#010;&gt; &gt; &gt; &gt; &gt;         String version = \"1.1.0\";&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; &gt; &gt; &gt; hiveConfDir, version);&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.user_behavior (\\n\"&#010;+&#010;&gt; &gt; &gt; &gt; &gt;                 \"  user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \"  item_id STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \"  behavior STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \"  ts AS PROCTIME()\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \") WITH (\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'topic' = 'user_behavior',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'properties.bootstrap.servers' =&#010;&gt; &gt; &gt; 'localhost:9092',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'format' = 'json',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; //        tableEnv.executeSql(\"CREATE TABLE print_table (\\n\" +&#010;&gt; &gt; &gt; &gt; &gt; //                \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt; //                \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt; //                \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt; //                \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt; &gt; &gt; //                \") WITH (\\n\" +&#010;&gt; &gt; &gt; &gt; &gt; //                \" 'connector' = 'print'\\n\" +&#010;&gt; &gt; &gt; &gt; &gt; //                \")\");&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.streamhivetest (\\n\"&#010;+&#010;&gt; &gt; &gt; &gt; &gt;                 \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \") STORED AS parquet TBLPROPERTIES (\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'sink.rolling-policy.file-size' = '12MB',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'sink.rolling-policy.rollover-interval' = '1&#010;&gt; &gt; &gt; min',\\n\" +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'sink.rolling-policy.check-interval' = '1&#010;&gt; min',\\n\"&#010;&gt; &gt; +&#010;&gt; &gt; &gt; &gt; &gt;                 \" 'execution.checkpointing.interval' = 'true'\\n\"&#010;+&#010;&gt; &gt; &gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.executeSql(\"insert into streamhivetest select&#010;&gt; &gt; &gt; &gt; &gt; user_id,item_id,behavior,DATE_FORMAT(ts, 'yyyy-MM-dd') as tsdata&#010;&gt; from&#010;&gt; &gt; &gt; &gt; &gt; user_behavior\");&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;         tableEnv.execute(\"stream-write-hive\");&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; --&#010;&gt; &gt; &gt; &gt; Best regards!&#010;&gt; &gt; &gt; &gt; Rui Li&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt; Best, Jingsong Lee&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "5",
        "reply": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>"
    },
    {
        "id": "<CADH6UNSST2j3GOB6SCCqbqx7uuMvq_gteO3zzOC=i2XVR9QoVA@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 02:37:03 GMT",
        "subject": "Re: flink1.11 run",
        "content": "可以写已有的表，相关的配置 [1] 需要添加到表的property当中。&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html#streaming-writing&#010;&#010;On Mon, Jul 20, 2020 at 5:14 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&#010;&gt;  hi&#010;&gt; 好的，想问一下stream写hive表的时候：&#010;&gt; 1、一定要在flink内部先建立hive表吗？&#010;&gt; 2、如果直接写hive内（hue建表）已经建好的hive表可以吗，文件会有滚动策略吗&#010;&gt;&#010;&gt; Rui Li &lt;lirui.fudan@gmail.com&gt; 于2020年7月20日周一 下午4:44写道：&#010;&gt;&#010;&gt; &gt; tableEnv.executeSql就已经提交作业了，不需要再执行execute了哈&#010;&gt; &gt;&#010;&gt; &gt; On Mon, Jul 20, 2020 at 4:29 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi，我这面请一个一个kafka到hive的程序，但程序无法运行，请问什么原因：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 异常：&#010;&gt; &gt; &gt; The program finished with the following exception:&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; org.apache.flink.client.program.ProgramInvocationException: The main&#010;&gt; &gt; method&#010;&gt; &gt; &gt; caused an error: No operators defined in streaming topology. Cannot&#010;&gt; &gt; &gt; generate StreamGraph.&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:699)&#010;&gt; &gt; &gt; at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:232)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.parseParameters(CliFrontend.java:916)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:992)&#010;&gt; &gt; &gt; at java.security.AccessController.doPrivileged(Native Method)&#010;&gt; &gt; &gt; at javax.security.auth.Subject.doAs(Subject.java:422)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)&#010;&gt; &gt; &gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:992)&#010;&gt; &gt; &gt; Caused by: java.lang.IllegalStateException: No operators defined in&#010;&gt; &gt; &gt; streaming topology. Cannot generate StreamGraph.&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.utils.ExecutorUtils.generateStreamGraph(ExecutorUtils.java:47)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.StreamExecutor.createPipeline(StreamExecutor.java:47)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.StreamingWriteToHive.main(StreamingWriteToHive.java:80)&#010;&gt; &gt; &gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; &gt; at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;&gt; &gt; &gt; ... 11 more&#010;&gt; &gt; &gt; 代码：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;  StreamExecutionEnvironment environment =&#010;&gt; &gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt; &gt;         EnvironmentSettings settings =&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; EnvironmentSettings.newInstance().inStreamingMode().useBlinkPlanner().build();&#010;&gt; &gt; &gt;         StreamTableEnvironment tableEnv =&#010;&gt; &gt; &gt; StreamTableEnvironment.create(environment, settings);&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; environment.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);&#010;&gt; &gt; &gt;         environment.setStateBackend(new MemoryStateBackend());&#010;&gt; &gt; &gt;         environment.getCheckpointConfig().setCheckpointInterval(5000);&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         String name = \"myhive\";&#010;&gt; &gt; &gt;         String defaultDatabase = \"tmp\";&#010;&gt; &gt; &gt;         String hiveConfDir = \"/etc/alternatives/hive-conf/\";&#010;&gt; &gt; &gt;         String version = \"1.1.0\";&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         HiveCatalog hive = new HiveCatalog(name, defaultDatabase,&#010;&gt; &gt; &gt; hiveConfDir, version);&#010;&gt; &gt; &gt;         tableEnv.registerCatalog(\"myhive\", hive);&#010;&gt; &gt; &gt;         tableEnv.useCatalog(\"myhive\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.user_behavior (\\n\" +&#010;&gt; &gt; &gt;                 \"  user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt;                 \"  item_id STRING,\\n\" +&#010;&gt; &gt; &gt;                 \"  behavior STRING,\\n\" +&#010;&gt; &gt; &gt;                 \"  ts AS PROCTIME()\\n\" +&#010;&gt; &gt; &gt;                 \") WITH (\\n\" +&#010;&gt; &gt; &gt;                 \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt; &gt; &gt;                 \" 'topic' = 'user_behavior',\\n\" +&#010;&gt; &gt; &gt;                 \" 'properties.bootstrap.servers' =&#010;&gt; 'localhost:9092',\\n\" +&#010;&gt; &gt; &gt;                 \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt; &gt; &gt;                 \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt; &gt; &gt;                 \" 'format' = 'json',\\n\" +&#010;&gt; &gt; &gt;                 \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt; &gt; &gt;                 \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; //        tableEnv.executeSql(\"CREATE TABLE print_table (\\n\" +&#010;&gt; &gt; &gt; //                \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt; //                \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt; //                \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt; //                \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt; //                \") WITH (\\n\" +&#010;&gt; &gt; &gt; //                \" 'connector' = 'print'\\n\" +&#010;&gt; &gt; &gt; //                \")\");&#010;&gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.HIVE);&#010;&gt; &gt; &gt;         tableEnv.executeSql(\"CREATE TABLE tmp.streamhivetest (\\n\" +&#010;&gt; &gt; &gt;                 \" user_id BIGINT,\\n\" +&#010;&gt; &gt; &gt;                 \" item_id STRING,\\n\" +&#010;&gt; &gt; &gt;                 \" behavior STRING,\\n\" +&#010;&gt; &gt; &gt;                 \" tsdata STRING\\n\" +&#010;&gt; &gt; &gt;                 \") STORED AS parquet TBLPROPERTIES (\\n\" +&#010;&gt; &gt; &gt;                 \" 'sink.rolling-policy.file-size' = '12MB',\\n\" +&#010;&gt; &gt; &gt;                 \" 'sink.rolling-policy.rollover-interval' = '1&#010;&gt; min',\\n\" +&#010;&gt; &gt; &gt;                 \" 'sink.rolling-policy.check-interval' = '1 min',\\n\" +&#010;&gt; &gt; &gt;                 \" 'execution.checkpointing.interval' = 'true'\\n\" +&#010;&gt; &gt; &gt;                 \")\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         tableEnv.getConfig().setSqlDialect(SqlDialect.DEFAULT);&#010;&gt; &gt; &gt;         tableEnv.executeSql(\"insert into streamhivetest select&#010;&gt; &gt; &gt; user_id,item_id,behavior,DATE_FORMAT(ts, 'yyyy-MM-dd') as tsdata from&#010;&gt; &gt; &gt; user_behavior\");&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         tableEnv.execute(\"stream-write-hive\");&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt; Best regards!&#010;&gt; &gt; Rui Li&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;Best regards!&#010;Rui Li&#010;&#010;",
        "depth": "3",
        "reply": "<CAEZk043sO+0RtHCNFLg7_rDv0QyR-L0jLwY1JjYW2LHrwJLHgA@mail.gmail.com>"
    },
    {
        "id": "<tencent_A24CB98A6B82517D3BC96928154713285B0A@qq.com>",
        "from": "&quot;jiafu&quot; &lt;530496...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 11:31:28 GMT",
        "subject": "长时间运行的flink任务会有以下报错",
        "content": "flink任务运行时会有以下的报错，用的是flink-1.8.1&#013;&#010;org.apache.flink.runtime.executiongraph.ExecutionGraphException: Trying to eagerly schedule&#010;a task whose inputs are not ready (result type: PIPELINED_BOUNDED, partition consumable: false,&#010;producer state: SCHEDULED, producer slot: null). &#009;at org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.fromEdges(InputChannelDeploymentDescriptor.java:145)&#010;&#009;at org.apache.flink.runtime.executiongraph.ExecutionVertex.createDeploymentDescriptor(ExecutionVertex.java:840)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.deploy(Execution.java:621) &#009;at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:50)&#010;&#009;at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) &#009;at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717)&#010;&#009;at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) &#009;at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:436)&#010;&#009;at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:637)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.restart(FailoverRegion.java:229)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.reset(FailoverRegion.java:186)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.allVerticesInTerminalState(FailoverRegion.java:96)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.lambda$cancel$0(FailoverRegion.java:146)&#010;&#009;at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656) &#009;at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632)&#010;&#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) &#009;at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)&#010;&#009;at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:633)&#010;&#009;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) &#009;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) &#009;at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.lambda$releaseAssignedResource$11(Execution.java:1350)&#010;&#009;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) &#009;at&#010;java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:778) &#009;at&#010;java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2140) &#009;at org.apache.flink.runtime.executiongraph.Execution.releaseAssignedResource(Execution.java:1345)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.finishCancellation(Execution.java:1115)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1094)&#010;&#009;at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:1628)&#010;&#009;at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:517)&#010;&#009;at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source) &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) &#009;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:274)&#010;&#009;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:189)&#010;&#009;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&#009;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:147) &#009;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)&#010;&#009;at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165) &#009;at akka.actor.Actor$class.aroundReceive(Actor.scala:502)&#010;&#009;at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95) &#009;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)&#010;&#009;at akka.actor.ActorCell.invoke(ActorCell.scala:495) &#009;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)&#010;&#009;at akka.dispatch.Mailbox.run(Mailbox.scala:224) &#009;at akka.dispatch.Mailbox.exec(Mailbox.scala:234)&#010;&#009;at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) &#009;at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&#009;at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) &#009;at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",
        "depth": "0",
        "reply": "<tencent_A24CB98A6B82517D3BC96928154713285B0A@qq.com>"
    },
    {
        "id": "<202007201950300801251@163.com>",
        "from": "&quot;sjlsumaitong@163.com&quot; &lt;sjlsumait...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 11:50:30 GMT",
        "subject": "回复: 长时间运行的flink任务会有以下报错",
        "content": "同问，大佬看到帮下忙！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;sjlsumaitong@163.com&#013;&#010; &#013;&#010;发件人： jiafu&#013;&#010;发送时间： 2020-07-20 19:31&#013;&#010;收件人： user-zh&#013;&#010;主题： 长时间运行的flink任务会有以下报错&#013;&#010;flink任务运行时会有以下的报错，用的是flink-1.8.1&#013;&#010;org.apache.flink.runtime.executiongraph.ExecutionGraphException: Trying to eagerly schedule&#010;a task whose inputs are not ready (result type: PIPELINED_BOUNDED, partition consumable: false,&#010;producer state: SCHEDULED, producer slot: null). at org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.fromEdges(InputChannelDeploymentDescriptor.java:145)&#010;at org.apache.flink.runtime.executiongraph.ExecutionVertex.createDeploymentDescriptor(ExecutionVertex.java:840)&#010;at org.apache.flink.runtime.executiongraph.Execution.deploy(Execution.java:621) at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:50)&#010;at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717)&#010;at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:436)&#010;at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:637)&#010;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.restart(FailoverRegion.java:229)&#010;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.reset(FailoverRegion.java:186)&#010;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.allVerticesInTerminalState(FailoverRegion.java:96)&#010;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.lambda$cancel$0(FailoverRegion.java:146)&#010;at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656) at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632)&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)&#010;at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:633)&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)&#010;at org.apache.flink.runtime.executiongraph.Execution.lambda$releaseAssignedResource$11(Execution.java:1350)&#010;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) at java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:778)&#010;at java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2140) at org.apache.flink.runtime.executiongraph.Execution.releaseAssignedResource(Execution.java:1345)&#010;at org.apache.flink.runtime.executiongraph.Execution.finishCancellation(Execution.java:1115)&#010;at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1094)&#010;at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:1628)&#010;at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:517)&#010;at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:274)&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:189)&#010;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:147) at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)&#010;at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165) at akka.actor.Actor$class.aroundReceive(Actor.scala:502)&#010;at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:495) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:224) at akka.dispatch.Mailbox.exec(Mailbox.scala:234)&#010;at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_A24CB98A6B82517D3BC96928154713285B0A@qq.com>"
    },
    {
        "id": "<tencent_2FC2E25192A00A190FB7BC787AE9FC8D800A@qq.com>",
        "from": "&quot;蒋佳成(Jiacheng Jiang)&quot; &lt;920334...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 01:27:56 GMT",
        "subject": "liststate",
        "content": "大家好：&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;我发现RocksDBListState#get方法是一次性把数据全加载到内存中，如果list太大这样会不会造成内存溢出。可不可以在next方法中才加载数据？",
        "depth": "0",
        "reply": "<tencent_2FC2E25192A00A190FB7BC787AE9FC8D800A@qq.com>"
    },
    {
        "id": "<CAA8tFvs9QzAHqhVmVY0_vhdMU=3X6G6beboxjaJ6wYnrUz_AvA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:05:39 GMT",
        "subject": "Re: liststate",
        "content": "Hi&#013;&#010;&#013;&#010;ListState 中的 value 是一个整体，所以一次性会取回来，现在 RocksDBMapStat&#010;中有一些操作&#013;&#010;(iterator/entries/values 等）是使用 next 方法加载的。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;蒋佳成(Jiacheng Jiang) &lt;920334586@qq.com&gt; 于2020年7月21日周二 上午9:28写道：&#013;&#010;&#013;&#010;&gt; 大家好：&#013;&#010;&gt; &amp;nbsp; &amp;nbsp;&#013;&#010;&gt; &amp;nbsp;我发现RocksDBListState#get方法是一次性把数据全加载到内存中，如果list太大这样会不会造成内存溢出。可不可以在next方法中才加载数据？&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_2FC2E25192A00A190FB7BC787AE9FC8D800A@qq.com>"
    },
    {
        "id": "<32a8e5d8.23a8.1736f6d1a0c.Coremail.felixzh2020@126.com>",
        "from": "felixzh &lt;felixzh2...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:31:27 GMT",
        "subject": "Flink整合hive之后，通过flink创建的表，hive beeline可见表，不可见字段？",
        "content": "参照文档https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive&#010;通过flink创建表：CREATE TABLE Orders (product STRING, amount INT)&#010;在beeline端可见表，但是desc看不到字段，select * from orders也不可用&#010;&#010;",
        "depth": "0",
        "reply": "<32a8e5d8.23a8.1736f6d1a0c.Coremail.felixzh2020@126.com>"
    },
    {
        "id": "<CABi+2jTC0pcz4K2QmhRPRQwPy79CRspGz7qr-sQp7fuYkY99iQ@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:34:18 GMT",
        "subject": "Re: Flink整合hive之后，通过flink创建的表，hive beeline可见表，不可见字段？",
        "content": "默认创建的是Flink表，Hive端不可见。&#013;&#010;你想创建Hive表的话，用Hive dialect。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Tue, Jul 21, 2020 at 11:31 AM felixzh &lt;felixzh2020@126.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; 参照文档&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive&#013;&#010;&gt; 通过flink创建表：CREATE TABLE Orders (product STRING, amount INT)&#013;&#010;&gt; 在beeline端可见表，但是desc看不到字段，select * from orders也不可用&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<32a8e5d8.23a8.1736f6d1a0c.Coremail.felixzh2020@126.com>"
    },
    {
        "id": "<6d867f4c.24dc.1736f70e358.Coremail.cxydevelop@163.com>",
        "from": "chenxuying &lt;cxydeve...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:35:36 GMT",
        "subject": "官方pyflink 例子的执行问题",
        "content": "官方例子:&#010;https://flink.apache.org/2020/04/09/pyflink-udf-support-flink.html&#010;按照例子写了程序,也安装了pyflink&#010;|&#010;python -m pip install apache-flink&#010;|&#010;代码:&#010;|&#010;from pyflink.datastream import StreamExecutionEnvironment&#010;from pyflink.table import StreamTableEnvironment, DataTypes&#010;from pyflink.table.descriptors import Schema, OldCsv, FileSystem&#010;from pyflink.table.udf import udf&#010;&#010;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#010;env.set_parallelism(1)&#010;t_env = StreamTableEnvironment.create(env)&#010;&#010;&#010;add = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()], DataTypes.BIGINT())&#010;&#010;&#010;t_env.register_function(\"add\", add)&#010;&#010;&#010;t_env.connect(FileSystem().path('C:/Users/xuyin/Desktop/docker_compose_test/src.txt')) \\&#010;.with_format(OldCsv()&#010;.field('a', DataTypes.BIGINT())&#010;.field('b', DataTypes.BIGINT())) \\&#010;.with_schema(Schema()&#010;.field('a', DataTypes.BIGINT())&#010;.field('b', DataTypes.BIGINT())) \\&#010;.create_temporary_table('mySource')&#010;&#010;&#010;t_env.connect(FileSystem().path('C:/Users/xuyin/Desktop/docker_compose_test/tar.txt')) \\&#010;.with_format(OldCsv()&#010;.field('sum', DataTypes.BIGINT())) \\&#010;.with_schema(Schema()&#010;.field('sum', DataTypes.BIGINT())) \\&#010;.create_temporary_table('mySink')&#010;&#010;&#010;t_env.from_path('mySource')\\&#010;.select(\"add(a, b)\") \\&#010;.insert_into('mySink')&#010;&#010;&#010;t_env.execute(\"tutorial_job\")&#010;|&#010;&#010;执行:&#010;&#010;|&#010;python test_pyflink.py&#010;|&#010;&#010;报错:&#010;&#010;&#010;|&#010;Traceback (most recent call last):&#010;  File \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\util\\exceptions.py\",&#010;line 147, in deco&#010;    return f(*a, **kw)&#010;  File \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\protocol.py\",&#010;line 328, in get_return_value&#010;    format(target_id, \".\", name), value)&#010;py4j.protocol.Py4JJavaError: An error occurred while calling o2.execute.&#010;: org.apache.flink.table.api.TableException: The configured Task Off-Heap Memory 0 bytes is&#010;less than the least required Python worker Memory 79 mb. The Task Off-Heap Memory can be configured&#010;using the configuration key 'taskmanager.memory.task.off-heap.size'.&#010;        at org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.checkPythonWorkerMemory(CommonPythonBase.scala:158)&#010;        at org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.getMergedConfiguration(CommonPythonBase.scala:119)&#010;        at org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.getConfig(CommonPythonBase.scala:102)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.getConfig(StreamExecPythonCalc.scala:35)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.translateToPlanInternal(StreamExecPythonCalc.scala:61)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.translateToPlanInternal(StreamExecPythonCalc.scala:35)&#010;        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:106)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48)&#010;        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48)&#010;        at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:67)&#010;        at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66)&#010;        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;        at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;        at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66)&#010;        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)&#010;        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1240)&#010;        at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;        at java.lang.reflect.Method.invoke(Method.java:497)&#010;        at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;        at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;        at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;        at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;        at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;        at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;        at java.lang.Thread.run(Thread.java:745)&#010;&#010;&#010;&#010;&#010;During handling of the above exception, another exception occurred:&#010;&#010;&#010;Traceback (most recent call last):&#010;  File \"test_pyflink.py\", line 34, in &lt;module&gt;&#010;    t_env.execute(\"tutorial_job\")&#010;  File \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\table\\table_environment.py\",&#010;line 1057, in execute&#010;    return JobExecutionResult(self._j_tenv.execute(job_name))&#010;  File \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\java_gateway.py\",&#010;line 1286, in __call__&#010;    answer, self.gateway_client, self.target_id, self.name)&#010;  File \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\util\\exceptions.py\",&#010;line 154, in deco&#010;    raise exception_mapping[exception](s.split(': ', 1)[1], stack_trace)&#010;pyflink.util.exceptions.TableException: \"The configured Task Off-Heap Memory 0 bytes is less&#010;than the least required Python worker Memory 79 mb. The Task Off-Heap Memory can be configured&#010;using the configuration key 'taskmanager.memory.task.off-heap.size'.\"&#010;|&#010;&#010;&#010;&#010;&#010;里面提到还要配置taskmanager.memory.task.off-heap.size这个属性吗 , &#010;&#010;我找到..\\Python\\Python37\\Lib\\site-packages\\pyflink\\conf下面的flink-conf.yaml&#010;&#010;增加了taskmanager.memory.task.off-heap.size: 100m&#010;&#010;但是还是报一样的错误&#010;&#010;请问用python安装的flink去哪里配置属性&#010;&#010;&#010;",
        "depth": "0",
        "reply": "<6d867f4c.24dc.1736f70e358.Coremail.cxydevelop@163.com>"
    },
    {
        "id": "<CAPxmL=EFH_P+D=tfn+UGiAmhcXhCtkWLqEhmAcbr=cBS6dt-uQ@mail.gmail.com>",
        "from": "Xingbo Huang &lt;hxbks...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:44:23 GMT",
        "subject": "Re: 官方pyflink 例子的执行问题",
        "content": "Hi,&#010;你需要添加配置，如果你没有使用RocksDB作为statebackend的话，你直接配置t_env.get_config().get_configuration().set_boolean(\"python.fn-execution.memory.managed\",&#010;True)就行，如果你用了的话，就需要配置off-heap&#010;memory了，table_env.get_config().get_configuration().set_string(\"taskmanager.memory.task.off-heap.size\",&#010;'80m')。你可以参考文档上的例子，以及对应的note说明[1]&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/python/python_udfs.html#scalar-functions&#010;&#010;Best,&#010;Xingbo&#010;&#010;&#010;chenxuying &lt;cxydevelop@163.com&gt; 于2020年7月21日周二 上午11:36写道：&#010;&#010;&gt; 官方例子:&#010;&gt; https://flink.apache.org/2020/04/09/pyflink-udf-support-flink.html&#010;&gt; 按照例子写了程序,也安装了pyflink&#010;&gt; |&#010;&gt; python -m pip install apache-flink&#010;&gt; |&#010;&gt; 代码:&#010;&gt; |&#010;&gt; from pyflink.datastream import StreamExecutionEnvironment&#010;&gt; from pyflink.table import StreamTableEnvironment, DataTypes&#010;&gt; from pyflink.table.descriptors import Schema, OldCsv, FileSystem&#010;&gt; from pyflink.table.udf import udf&#010;&gt;&#010;&gt;&#010;&gt; env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt; env.set_parallelism(1)&#010;&gt; t_env = StreamTableEnvironment.create(env)&#010;&gt;&#010;&gt;&#010;&gt; add = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()],&#010;&gt; DataTypes.BIGINT())&#010;&gt;&#010;&gt;&#010;&gt; t_env.register_function(\"add\", add)&#010;&gt;&#010;&gt;&#010;&gt; t_env.connect(FileSystem().path('C:/Users/xuyin/Desktop/docker_compose_test/src.txt'))&#010;&gt; \\&#010;&gt; .with_format(OldCsv()&#010;&gt; .field('a', DataTypes.BIGINT())&#010;&gt; .field('b', DataTypes.BIGINT())) \\&#010;&gt; .with_schema(Schema()&#010;&gt; .field('a', DataTypes.BIGINT())&#010;&gt; .field('b', DataTypes.BIGINT())) \\&#010;&gt; .create_temporary_table('mySource')&#010;&gt;&#010;&gt;&#010;&gt; t_env.connect(FileSystem().path('C:/Users/xuyin/Desktop/docker_compose_test/tar.txt'))&#010;&gt; \\&#010;&gt; .with_format(OldCsv()&#010;&gt; .field('sum', DataTypes.BIGINT())) \\&#010;&gt; .with_schema(Schema()&#010;&gt; .field('sum', DataTypes.BIGINT())) \\&#010;&gt; .create_temporary_table('mySink')&#010;&gt;&#010;&gt;&#010;&gt; t_env.from_path('mySource')\\&#010;&gt; .select(\"add(a, b)\") \\&#010;&gt; .insert_into('mySink')&#010;&gt;&#010;&gt;&#010;&gt; t_env.execute(\"tutorial_job\")&#010;&gt; |&#010;&gt;&#010;&gt; 执行:&#010;&gt;&#010;&gt; |&#010;&gt; python test_pyflink.py&#010;&gt; |&#010;&gt;&#010;&gt; 报错:&#010;&gt;&#010;&gt;&#010;&gt; |&#010;&gt; Traceback (most recent call last):&#010;&gt;   File&#010;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\util\\exceptions.py\",&#010;&gt; line 147, in deco&#010;&gt;     return f(*a, **kw)&#010;&gt;   File&#010;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\protocol.py\",&#010;&gt; line 328, in get_return_value&#010;&gt;     format(target_id, \".\", name), value)&#010;&gt; py4j.protocol.Py4JJavaError: An error occurred while calling o2.execute.&#010;&gt; : org.apache.flink.table.api.TableException: The configured Task Off-Heap&#010;&gt; Memory 0 bytes is less than the least required Python worker Memory 79 mb.&#010;&gt; The Task Off-Heap Memory can be configured using the configuration key&#010;&gt; 'taskmanager.memory.task.off-heap.size'.&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.checkPythonWorkerMemory(CommonPythonBase.scala:158)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.getMergedConfiguration(CommonPythonBase.scala:119)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.getConfig(CommonPythonBase.scala:102)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.getConfig(StreamExecPythonCalc.scala:35)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.translateToPlanInternal(StreamExecPythonCalc.scala:61)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.translateToPlanInternal(StreamExecPythonCalc.scala:35)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:106)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:67)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66)&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;         at&#010;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;         at&#010;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1240)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;         at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;         at java.lang.reflect.Method.invoke(Method.java:497)&#010;&gt;         at&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;&gt;         at&#010;&gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;&gt;         at&#010;&gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;&gt;         at&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;&gt;         at&#010;&gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;&gt;         at&#010;&gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;&gt;         at java.lang.Thread.run(Thread.java:745)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; During handling of the above exception, another exception occurred:&#010;&gt;&#010;&gt;&#010;&gt; Traceback (most recent call last):&#010;&gt;   File \"test_pyflink.py\", line 34, in &lt;module&gt;&#010;&gt;     t_env.execute(\"tutorial_job\")&#010;&gt;   File&#010;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\table\\table_environment.py\",&#010;&gt; line 1057, in execute&#010;&gt;     return JobExecutionResult(self._j_tenv.execute(job_name))&#010;&gt;   File&#010;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\java_gateway.py\",&#010;&gt; line 1286, in __call__&#010;&gt;     answer, self.gateway_client, self.target_id, self.name)&#010;&gt;   File&#010;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\util\\exceptions.py\",&#010;&gt; line 154, in deco&#010;&gt;     raise exception_mapping[exception](s.split(': ', 1)[1], stack_trace)&#010;&gt; pyflink.util.exceptions.TableException: \"The configured Task Off-Heap&#010;&gt; Memory 0 bytes is less than the least required Python worker Memory 79 mb.&#010;&gt; The Task Off-Heap Memory can be configured using the configuration key&#010;&gt; 'taskmanager.memory.task.off-heap.size'.\"&#010;&gt; |&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 里面提到还要配置taskmanager.memory.task.off-heap.size这个属性吗 ,&#010;&gt;&#010;&gt; 我找到..\\Python\\Python37\\Lib\\site-packages\\pyflink\\conf下面的flink-conf.yaml&#010;&gt;&#010;&gt; 增加了taskmanager.memory.task.off-heap.size: 100m&#010;&gt;&#010;&gt; 但是还是报一样的错误&#010;&gt;&#010;&gt; 请问用python安装的flink去哪里配置属性&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<6d867f4c.24dc.1736f70e358.Coremail.cxydevelop@163.com>"
    },
    {
        "id": "<5cd34596.2afe.1736faf5d2b.Coremail.cxydevelop@163.com>",
        "from": "chenxuying  &lt;cxydeve...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 04:43:50 GMT",
        "subject": "Re:Re: 官方pyflink 例子的执行问题",
        "content": "你好&#010;明白了,感谢 , 我文档没看清楚哈&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-21 11:44:23，\"Xingbo Huang\" &lt;hxbks2ks@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;你需要添加配置，如果你没有使用RocksDB作为statebackend的话，你直接配置t_env.get_config().get_configuration().set_boolean(\"python.fn-execution.memory.managed\",&#010;&gt;True)就行，如果你用了的话，就需要配置off-heap&#010;&gt;memory了，table_env.get_config().get_configuration().set_string(\"taskmanager.memory.task.off-heap.size\",&#010;&gt;'80m')。你可以参考文档上的例子，以及对应的note说明[1]&#010;&gt;&#010;&gt;[1]&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/python/python_udfs.html#scalar-functions&#010;&gt;&#010;&gt;Best,&#010;&gt;Xingbo&#010;&gt;&#010;&gt;&#010;&gt;chenxuying &lt;cxydevelop@163.com&gt; 于2020年7月21日周二 上午11:36写道：&#010;&gt;&#010;&gt;&gt; 官方例子:&#010;&gt;&gt; https://flink.apache.org/2020/04/09/pyflink-udf-support-flink.html&#010;&gt;&gt; 按照例子写了程序,也安装了pyflink&#010;&gt;&gt; |&#010;&gt;&gt; python -m pip install apache-flink&#010;&gt;&gt; |&#010;&gt;&gt; 代码:&#010;&gt;&gt; |&#010;&gt;&gt; from pyflink.datastream import StreamExecutionEnvironment&#010;&gt;&gt; from pyflink.table import StreamTableEnvironment, DataTypes&#010;&gt;&gt; from pyflink.table.descriptors import Schema, OldCsv, FileSystem&#010;&gt;&gt; from pyflink.table.udf import udf&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt;&gt; env.set_parallelism(1)&#010;&gt;&gt; t_env = StreamTableEnvironment.create(env)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; add = udf(lambda i, j: i + j, [DataTypes.BIGINT(), DataTypes.BIGINT()],&#010;&gt;&gt; DataTypes.BIGINT())&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; t_env.register_function(\"add\", add)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; t_env.connect(FileSystem().path('C:/Users/xuyin/Desktop/docker_compose_test/src.txt'))&#010;&gt;&gt; \\&#010;&gt;&gt; .with_format(OldCsv()&#010;&gt;&gt; .field('a', DataTypes.BIGINT())&#010;&gt;&gt; .field('b', DataTypes.BIGINT())) \\&#010;&gt;&gt; .with_schema(Schema()&#010;&gt;&gt; .field('a', DataTypes.BIGINT())&#010;&gt;&gt; .field('b', DataTypes.BIGINT())) \\&#010;&gt;&gt; .create_temporary_table('mySource')&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; t_env.connect(FileSystem().path('C:/Users/xuyin/Desktop/docker_compose_test/tar.txt'))&#010;&gt;&gt; \\&#010;&gt;&gt; .with_format(OldCsv()&#010;&gt;&gt; .field('sum', DataTypes.BIGINT())) \\&#010;&gt;&gt; .with_schema(Schema()&#010;&gt;&gt; .field('sum', DataTypes.BIGINT())) \\&#010;&gt;&gt; .create_temporary_table('mySink')&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; t_env.from_path('mySource')\\&#010;&gt;&gt; .select(\"add(a, b)\") \\&#010;&gt;&gt; .insert_into('mySink')&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; t_env.execute(\"tutorial_job\")&#010;&gt;&gt; |&#010;&gt;&gt;&#010;&gt;&gt; 执行:&#010;&gt;&gt;&#010;&gt;&gt; |&#010;&gt;&gt; python test_pyflink.py&#010;&gt;&gt; |&#010;&gt;&gt;&#010;&gt;&gt; 报错:&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; |&#010;&gt;&gt; Traceback (most recent call last):&#010;&gt;&gt;   File&#010;&gt;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\util\\exceptions.py\",&#010;&gt;&gt; line 147, in deco&#010;&gt;&gt;     return f(*a, **kw)&#010;&gt;&gt;   File&#010;&gt;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\protocol.py\",&#010;&gt;&gt; line 328, in get_return_value&#010;&gt;&gt;     format(target_id, \".\", name), value)&#010;&gt;&gt; py4j.protocol.Py4JJavaError: An error occurred while calling o2.execute.&#010;&gt;&gt; : org.apache.flink.table.api.TableException: The configured Task Off-Heap&#010;&gt;&gt; Memory 0 bytes is less than the least required Python worker Memory 79 mb.&#010;&gt;&gt; The Task Off-Heap Memory can be configured using the configuration key&#010;&gt;&gt; 'taskmanager.memory.task.off-heap.size'.&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.checkPythonWorkerMemory(CommonPythonBase.scala:158)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.getMergedConfiguration(CommonPythonBase.scala:119)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.common.CommonPythonBase$class.getConfig(CommonPythonBase.scala:102)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.getConfig(StreamExecPythonCalc.scala:35)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.translateToPlanInternal(StreamExecPythonCalc.scala:61)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecPythonCalc.translateToPlanInternal(StreamExecPythonCalc.scala:35)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecCalcBase.translateToPlan(StreamExecCalcBase.scala:38)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToTransformation(StreamExecLegacySink.scala:158)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:106)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:67)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;&gt;&gt;         at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#010;&gt;&gt;         at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#010;&gt;&gt;         at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#010;&gt;&gt;         at&#010;&gt;&gt; scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#010;&gt;&gt;         at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1240)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#010;&gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:497)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#010;&gt;&gt;         at java.lang.Thread.run(Thread.java:745)&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; During handling of the above exception, another exception occurred:&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Traceback (most recent call last):&#010;&gt;&gt;   File \"test_pyflink.py\", line 34, in &lt;module&gt;&#010;&gt;&gt;     t_env.execute(\"tutorial_job\")&#010;&gt;&gt;   File&#010;&gt;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\table\\table_environment.py\",&#010;&gt;&gt; line 1057, in execute&#010;&gt;&gt;     return JobExecutionResult(self._j_tenv.execute(job_name))&#010;&gt;&gt;   File&#010;&gt;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\py4j\\java_gateway.py\",&#010;&gt;&gt; line 1286, in __call__&#010;&gt;&gt;     answer, self.gateway_client, self.target_id, self.name)&#010;&gt;&gt;   File&#010;&gt;&gt; \"C:\\Users\\xuyin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pyflink\\util\\exceptions.py\",&#010;&gt;&gt; line 154, in deco&#010;&gt;&gt;     raise exception_mapping[exception](s.split(': ', 1)[1], stack_trace)&#010;&gt;&gt; pyflink.util.exceptions.TableException: \"The configured Task Off-Heap&#010;&gt;&gt; Memory 0 bytes is less than the least required Python worker Memory 79 mb.&#010;&gt;&gt; The Task Off-Heap Memory can be configured using the configuration key&#010;&gt;&gt; 'taskmanager.memory.task.off-heap.size'.\"&#010;&gt;&gt; |&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 里面提到还要配置taskmanager.memory.task.off-heap.size这个属性吗 ,&#010;&gt;&gt;&#010;&gt;&gt; 我找到..\\Python\\Python37\\Lib\\site-packages\\pyflink\\conf下面的flink-conf.yaml&#010;&gt;&gt;&#010;&gt;&gt; 增加了taskmanager.memory.task.off-heap.size: 100m&#010;&gt;&gt;&#010;&gt;&gt; 但是还是报一样的错误&#010;&gt;&gt;&#010;&gt;&gt; 请问用python安装的flink去哪里配置属性&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;",
        "depth": "2",
        "reply": "<6d867f4c.24dc.1736f70e358.Coremail.cxydevelop@163.com>"
    },
    {
        "id": "<tencent_55DB7025A98478D5F51947291466EE26F106@qq.com>",
        "from": "&quot;胡云川&quot; &lt;huyunchuan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:40:32 GMT",
        "subject": "Flink 往kafka生产数据，频繁出现checkpoint失败的错误",
        "content": "&amp;gt;&amp;gt;&amp;gt;&amp;gt;大佬们好，&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;gt;在Flink 往kafka生产数据的时候，频繁出席那checkpoint失败的错误，开启EXACTLY_ONCE&#013;&#010;&amp;gt;&amp;gt;报错代码如下：&#013;&#010;Producer attempted an operation with an old epoch.Either there is a newer producer with the&#010;same transactionalId, or the producer's transaction has been expired by the broker&#013;&#010;&#013;&#010;&#013;&#010;&amp;gt;&amp;gt;大佬们有遇到过吗？&#013;&#010;&amp;gt;&amp;gt;不胜感激！",
        "depth": "0",
        "reply": "<tencent_55DB7025A98478D5F51947291466EE26F106@qq.com>"
    },
    {
        "id": "<tencent_D1BDD20BD21C32235A267556B799B23A0309@qq.com>",
        "from": "&quot;sun&quot; &lt;1392427...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:52:54 GMT",
        "subject": "想知道state写到checkpoint文件没有",
        "content": "&amp;nbsp; 请问怎么反编译checkpoint文件啊，我想知道state写到checkpoint文件没有&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp;&#005; &#002; &#009;_default_&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; OPERATOR_STATE_DISTRIBUTION_MODE&#010;&#016;SPLIT_DISTRIBUTE&amp;nbsp; &amp;nbsp;&#001; &#016;VALUE_SERIALIZER&amp;nbsp; &amp;nbsp;&#002; Gorg.apache.flink.api.common.typeutils.ParameterlessTypeSerializerConfigzS酿&amp;nbsp;&#010;&amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;脂?&#005;sr -org.apache.flink.runtime.state.JavaSerializerFSX韦4&#013;&#010;?&amp;nbsp; xr Borg.apache.flink.api.common.typeutils.base.TypeSerializerSingletony﹪.wE&#002;&amp;nbsp;&#010;xr 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp;&#010;xp&amp;nbsp; &amp;nbsp;&#001; -org.apache.flink.runtime.state.JavaSerializer &#029;topic-partition-offset-states&amp;nbsp;&#010;&amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; OPERATOR_STATE_DISTRIBUTION_MODE &#005;UNION&amp;nbsp;&#010;&amp;nbsp;&#001; &#016;VALUE_SERIALIZER&amp;nbsp; &amp;nbsp;&#002; Iorg.apache.flink.api.java.typeutils.runtime.TupleSerializerConfigSnapshotzS酿&amp;nbsp;&#010;&amp;nbsp;&#001;&amp;nbsp; &#008;矛?&#005;sr ;org.apache.flink.api.java.typeutils.runtime.TupleSerializer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp; xr ?org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002; &#004;I &#005;arityI &#006;length[ &#016;fieldSerializerst 7[Lorg/apache/flink/api/common/typeutils/TypeSerializer;L&amp;nbsp;&#013;&#010;tupleClasst &#017;Ljava/lang/Class;xr 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp; xp&amp;nbsp; &amp;nbsp;&#002;r 7[Lorg.apache.flink.api.common.typeutils.TypeSerializer;9?Ч&#022;麡&#002;&amp;nbsp;&#010;xp&amp;nbsp; &amp;nbsp;&#002;sr ?org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#003;&#002; &#007;L &#024;defaultSerializerClassest &#025;Ljava/util/LinkedHashMap;L&#010;&#018;defaultSerializersq ~ &#009;L &#017;kryoRegistrationsq ~ &#009;L &#015;registeredTypest &#025;Ljava/util/LinkedHashSet;L&#010;$registeredTypesWithSerializerClassesq ~ &#009;L &#030;registeredTypesWithSerializersq ~ &#009;L &#004;typeq ~&#010;&#003;xq ~ &#004;sr &#023;java.util.LinkedHashMap4繬\\&#016;l利&#002; &#001;Z &#011;accessOrderxr &#017;java.util.HashMap&#005;&#007;诹?`?&#010;&#002;F&amp;nbsp;&#013;&#010;loadFactorI &#009;thresholdxp?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w&#008;&amp;nbsp; &amp;nbsp;&#016;&amp;nbsp;&#010;&amp;nbsp; x sq ~ &#012;?@&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;w&#008;&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;&#010;x sq ~ &#012;?@&amp;nbsp; &amp;nbsp; &amp;nbsp;&#003;w&#008;&amp;nbsp; &amp;nbsp;&#004;&amp;nbsp; &amp;nbsp;&#002;t&#010;Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionsr &lt;org.apache.flink.api.java.typeutils.runtime.KryoRegistrationJ?坣厏o&#002;&#010;&#004;L &#015;registeredClassq ~ &#003;L &#030;serializableSerializerInstancet DLorg/apache/flink/api/common/ExecutionConfig$SerializableSerializer;L&#010;&#015;serializerClassq ~ &#003;L &#024;serializerDefinitionTypet WLorg/apache/flink/api/java/typeutils/runtime/KryoRegistration$SerializerDefinitionType;xpvr&#010;Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&#013;&#010;&#005;[/剀X5&#002; &#003;I&amp;nbsp;&#013;&#010;cachedHashI &#009;partitionL &#005;topict &#018;Ljava/lang/String;xppp~r Uorg.apache.flink.api.java.typeutils.runtime.KryoRegistration$SerializerDefinitionType&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &#018;&amp;nbsp; xr &#014;java.lang.Enum&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &#018;&amp;nbsp; xpt &#011;UNSPECIFIEDt )org.apache.avro.generic.GenericData$Arraysq ~ &#018;vr&#010;Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xppvr Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xp~q ~ &#025;t &#005;CLASSx pppq ~ &#024;sr 9org.apache.flink.api.common.typeutils.base.LongSerializer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp; xr Borg.apache.flink.api.common.typeutils.base.TypeSerializerSingletony﹪.wE&#002;&amp;nbsp;&#010;xq ~ &#004;vr &amp;amp;org.apache.flink.api.java.tuple.Tuple2&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&#010;&#002;L &#002;f0t &#018;Ljava/lang/Object;L &#002;f1q ~ )xr %org.apache.flink.api.java.tuple.Tuple&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp; xp&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;&#002;&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &#006;;&amp;nbsp; &#014;? &#015;? &#016;)&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; &#006;3 &#005;sr ?org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#003;&#002; &#007;L &#024;defaultSerializerClassest &#025;Ljava/util/LinkedHashMap;L&#010;&#018;defaultSerializersq ~ &#001;L &#017;kryoRegistrationsq ~ &#001;L &#015;registeredTypest &#025;Ljava/util/LinkedHashSet;L&#010;$registeredTypesWithSerializerClassesq ~ &#001;L &#030;registeredTypesWithSerializersq ~ &#001;L &#004;typet &#017;Ljava/lang/Class;xr&#010;4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp;&#010;xpsr &#023;java.util.LinkedHashMap4繬\\&#016;l利&#002; &#001;Z &#011;accessOrderxr &#017;java.util.HashMap&#005;&#007;诹?`? &#002;F&amp;nbsp;&#013;&#010;loadFactorI &#009;thresholdxp?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w&#008;&amp;nbsp; &amp;nbsp;&#016;&amp;nbsp;&#010;&amp;nbsp; x sq ~ &#006;?@&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;w&#008;&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;&#010;x sq ~ &#006;?@&amp;nbsp; &amp;nbsp; &amp;nbsp;&#003;w&#008;&amp;nbsp; &amp;nbsp;&#004;&amp;nbsp; &amp;nbsp;&#002;t&#010;Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionsr &lt;org.apache.flink.api.java.typeutils.runtime.KryoRegistrationJ?坣厏o&#002;&#010;&#004;L &#015;registeredClassq ~ &#003;L &#030;serializableSerializerInstancet DLorg/apache/flink/api/common/ExecutionConfig$SerializableSerializer;L&#010;&#015;serializerClassq ~ &#003;L &#024;serializerDefinitionTypet WLorg/apache/flink/api/java/typeutils/runtime/KryoRegistration$SerializerDefinitionType;xpvr&#010;Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&#013;&#010;&#005;[/剀X5&#002; &#003;I&amp;nbsp;&#013;&#010;cachedHashI &#009;partitionL &#005;topict &#018;Ljava/lang/String;xppp~r Uorg.apache.flink.api.java.typeutils.runtime.KryoRegistration$SerializerDefinitionType&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &#018;&amp;nbsp; xr &#014;java.lang.Enum&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &#018;&amp;nbsp; xpt &#011;UNSPECIFIEDt )org.apache.avro.generic.GenericData$Arraysq ~ &#012;vr&#010;Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xppvr Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xp~q ~ &#019;t &#005;CLASSx pppq ~ &#018;&amp;nbsp;&#010;&amp;nbsp;&#002; \\org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer$KryoSerializerConfigSnapshotzS酿&amp;nbsp;&#010;&amp;nbsp;&#001;&amp;nbsp; &#006;3 &#005;sr ?org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#003;&#002; &#007;L &#024;defaultSerializerClassest &#025;Ljava/util/LinkedHashMap;L&#010;&#018;defaultSerializersq ~ &#001;L &#017;kryoRegistrationsq ~ &#001;L &#015;registeredTypest &#025;Ljava/util/LinkedHashSet;L&#010;$registeredTypesWithSerializerClassesq ~ &#001;L &#030;registeredTypesWithSerializersq ~ &#001;L &#004;typet &#017;Ljava/lang/Class;xr&#010;4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp;&#010;xpsr &#023;java.util.LinkedHashMap4繬\\&#016;l利&#002; &#001;Z &#011;accessOrderxr &#017;java.util.HashMap&#005;&#007;诹?`? &#002;F&amp;nbsp;&#013;&#010;loadFactorI &#009;thresholdxp?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w&#008;&amp;nbsp; &amp;nbsp;&#016;&amp;nbsp;&#010;&amp;nbsp; x sq ~ &#006;?@&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;w&#008;&amp;nbsp; &amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;&#010;x sq ~ &#006;?@&amp;nbsp; &amp;nbsp; &amp;nbsp;&#003;w&#008;&amp;nbsp; &amp;nbsp;&#004;&amp;nbsp; &amp;nbsp;&#002;t&#010;Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionsr &lt;org.apache.flink.api.java.typeutils.runtime.KryoRegistrationJ?坣厏o&#002;&#010;&#004;L &#015;registeredClassq ~ &#003;L &#030;serializableSerializerInstancet DLorg/apache/flink/api/common/ExecutionConfig$SerializableSerializer;L&#010;&#015;serializerClassq ~ &#003;L &#024;serializerDefinitionTypet WLorg/apache/flink/api/java/typeutils/runtime/KryoRegistration$SerializerDefinitionType;xpvr&#010;Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&#013;&#010;&#005;[/剀X5&#002; &#003;I&amp;nbsp;&#013;&#010;cach发送edHashI &#009;partitionL &#005;topict &#018;Ljava/lang/String;xppp~r Uorg.apache.flink.api.java.typeutils.runtime.KryoRegistration$SerializerDefinitionType&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &#018;&amp;nbsp; xr &#014;java.lang.Enum&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &#018;&amp;nbsp; xpt &#011;UNSPECIFIEDt )org.apache.avro.generic.GenericData$Arraysq ~ &#012;vr&#010;Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xppvr Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xp~q ~ &#019;t &#005;CLASSx pppq ~ &#018;&amp;nbsp;&#010;&amp;nbsp;&#001; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;nbsp;&#010;&amp;nbsp;&#002; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;)org.apache.avro.generic.GenericData$Array Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&amp;nbsp;&#001; Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&amp;nbsp;&#001;&amp;nbsp; &amp;nbsp;猬?&#005;sr 9org.apache.flink.api.common.typeutils.base.LongSerializer&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp; xr Borg.apache.flink.api.common.typeutils.base.TypeSerializerSingletony﹪.wE&#002;&amp;nbsp;&#010;xr 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp;&#010;xp&amp;nbsp; &amp;nbsp;&#002; Porg.apache.flink.api.common.typeutils.base.LongSerializer$LongSerializerSnapshot&amp;nbsp;&#010;&amp;nbsp;&#002; 9org.apache.flink.api.common.typeutils.base.LongSerializer &#005;vr &amp;amp;org.apache.flink.api.java.tuple.Tuple2&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002; &#002;L &#002;f0t &#018;Ljava/lang/Object;L &#002;f1q ~ &#001;xr %org.apache.flink.api.java.tuple.Tuple&amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;&#001;&#002;&amp;nbsp; xp&amp;nbsp;&amp;nbsp;",
        "depth": "0",
        "reply": "<tencent_D1BDD20BD21C32235A267556B799B23A0309@qq.com>"
    },
    {
        "id": "<CAA8tFvunHVRyXOVUOCnrVUXovULr8LUrxpNcqp+-+4ms8qW73Q@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:17:58 GMT",
        "subject": "Re: 想知道state写到checkpoint文件没有",
        "content": "Hi&#010;    Checkpoint 包括两部分：1）meta 文件；2）具体的数据。如果是 Meta 部分可以参考&#010;CheckpointMetadataLoadingTest[1] 自己写一个测试，如果你知道具体的内容，或许也可以看一下&#010;StatePorcessAPI[2]&#010;&#010;[1]&#010;https://github.com/apache/flink/blob/master/flink-runtime/src/test/java/org/apache/flink/runtime/checkpoint/CheckpointMetadataLoadingTest.java&#010;[2]&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/libs/state_processor_api.html&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;sun &lt;1392427699@qq.com&gt; 于2020年7月21日周二 下午12:02写道：&#010;&#010;&gt; &amp;nbsp; 请问怎么反编译checkpoint文件啊，我想知道state写到checkpoint文件没有&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; &amp;nbsp;          _default_&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; OPERATOR_STATE_DISTRIBUTION_MODE  SPLIT_DISTRIBUTE&amp;nbsp; &amp;nbsp;&#010;&gt;  VALUE_SERIALIZER&amp;nbsp; &amp;nbsp;&#010;&gt; Gorg.apache.flink.api.common.typeutils.ParameterlessTypeSerializerConfigzS酿&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;脂? sr&#010;&gt; -org.apache.flink.runtime.state.JavaSerializerFSX韦4&#010;&gt; ?&amp;nbsp; xr&#010;&gt; Borg.apache.flink.api.common.typeutils.base.TypeSerializerSingletony﹪.wE&#010;&gt; &amp;nbsp; xr 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xp&amp;nbsp; &amp;nbsp;&#010;&gt; -org.apache.flink.runtime.state.JavaSerializer&#010;&gt; topic-partition-offset-states&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; OPERATOR_STATE_DISTRIBUTION_MODE  UNION&amp;nbsp; &amp;nbsp;&#010;&gt;  VALUE_SERIALIZER&amp;nbsp; &amp;nbsp;&#010;&gt; Iorg.apache.flink.api.java.typeutils.runtime.TupleSerializerConfigSnapshotzS酿&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;  矛? sr&#010;&gt; ;org.apache.flink.api.java.typeutils.runtime.TupleSerializer&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xr&#010;&gt; ?org.apache.flink.api.java.typeutils.runtime.TupleSerializerBase&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;    I  arityI  length[  fieldSerializerst&#010;&gt; 7[Lorg/apache/flink/api/common/typeutils/TypeSerializer;L&amp;nbsp;&#010;&gt; tupleClasst  Ljava/lang/Class;xr&#010;&gt; 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;  &amp;nbsp; xp&amp;nbsp; &amp;nbsp; r&#010;&gt; 7[Lorg.apache.flink.api.common.typeutils.TypeSerializer;9?Ч 麡 &amp;nbsp;&#010;&gt; xp&amp;nbsp; &amp;nbsp; sr&#010;&gt; ?org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;    L  defaultSerializerClassest&#010;&gt; Ljava/util/LinkedHashMap;L  defaultSerializersq ~  L  kryoRegistrationsq ~&#010;&gt;        L  registeredTypest  Ljava/util/LinkedHashSet;L&#010;&gt; $registeredTypesWithSerializerClassesq ~        L&#010;&gt; registeredTypesWithSerializersq ~    L  typeq ~  xq ~  sr&#010;&gt; java.util.LinkedHashMap4繬\\ l利   Z  accessOrderxr  java.util.HashMap  诹?`?&#010;&gt; F&amp;nbsp;&#010;&gt; loadFactorI     thresholdxp?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; x sq ~ ?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; x sq ~&#010;&gt; ?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; t&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionsr&#010;&gt; &lt;org.apache.flink.api.java.typeutils.runtime.KryoRegistrationJ?坣厏o   L&#010;&gt; registeredClassq ~  L  serializableSerializerInstancet&#010;&gt; DLorg/apache/flink/api/common/ExecutionConfig$SerializableSerializer;L&#010;&gt; serializerClassq ~  L  serializerDefinitionTypet&#010;&gt; WLorg/apache/flink/api/java/typeutils/runtime/KryoRegistration$SerializerDefinitionType;xpvr&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&#010;&gt;  [/剀X5   I&amp;nbsp;&#010;&gt; cachedHashI     partitionL  topict  Ljava/lang/String;xppp~r&#010;&gt; Uorg.apache.flink.api.java.typeutils.runtime.KryoRegistration$SerializerDefinitionType&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xr  java.lang.Enum&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; xpt  UNSPECIFIEDt )org.apache.avro.generic.GenericData$Arraysq ~  vr&#010;&gt; Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xppvr&#010;&gt; Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xp~q ~  t  CLASSx pppq ~  sr&#010;&gt; 9org.apache.flink.api.common.typeutils.base.LongSerializer&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xr&#010;&gt; Borg.apache.flink.api.common.typeutils.base.TypeSerializerSingletony﹪.wE&#010;&gt; &amp;nbsp; xq ~  vr &amp;amp;org.apache.flink.api.java.tuple.Tuple2&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;    L  f0t  Ljava/lang/Object;L  f1q ~ )xr&#010;&gt; %org.apache.flink.api.java.tuple.Tuple&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;  &amp;nbsp;&#010;&gt; xp&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;  ;&amp;nbsp;&#010; ?  ?  )&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;  3  sr&#010;&gt; ?org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;    L  defaultSerializerClassest&#010;&gt; Ljava/util/LinkedHashMap;L  defaultSerializersq ~  L  kryoRegistrationsq ~&#010;&gt; L  registeredTypest  Ljava/util/LinkedHashSet;L&#010;&gt; $registeredTypesWithSerializerClassesq ~  L&#010;&gt; registeredTypesWithSerializersq ~  L  typet  Ljava/lang/Class;xr&#010;&gt; 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;  &amp;nbsp; xpsr  java.util.LinkedHashMap4繬\\ l利   Z  accessOrderxr&#010;&gt; java.util.HashMap  诹?`?  F&amp;nbsp;&#010;&gt; loadFactorI     thresholdxp?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; x sq ~  ?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; x sq ~&#010;&gt; ?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; t&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionsr&#010;&gt; &lt;org.apache.flink.api.java.typeutils.runtime.KryoRegistrationJ?坣厏o   L&#010;&gt; registeredClassq ~  L  serializableSerializerInstancet&#010;&gt; DLorg/apache/flink/api/common/ExecutionConfig$SerializableSerializer;L&#010;&gt; serializerClassq ~  L  serializerDefinitionTypet&#010;&gt; WLorg/apache/flink/api/java/typeutils/runtime/KryoRegistration$SerializerDefinitionType;xpvr&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&#010;&gt;  [/剀X5   I&amp;nbsp;&#010;&gt; cachedHashI     partitionL  topict  Ljava/lang/String;xppp~r&#010;&gt; Uorg.apache.flink.api.java.typeutils.runtime.KryoRegistration$SerializerDefinitionType&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xr  java.lang.Enum&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; xpt  UNSPECIFIEDt )org.apache.avro.generic.GenericData$Arraysq ~ vr&#010;&gt; Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xppvr&#010;&gt; Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xp~q ~  t  CLASSx pppq ~  &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; \\org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer$KryoSerializerConfigSnapshotzS酿&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;  3  sr&#010;&gt; ?org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;    L  defaultSerializerClassest&#010;&gt; Ljava/util/LinkedHashMap;L  defaultSerializersq ~  L  kryoRegistrationsq ~&#010;&gt; L  registeredTypest  Ljava/util/LinkedHashSet;L&#010;&gt; $registeredTypesWithSerializerClassesq ~  L&#010;&gt; registeredTypesWithSerializersq ~  L  typet  Ljava/lang/Class;xr&#010;&gt; 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;  &amp;nbsp; xpsr  java.util.LinkedHashMap4繬\\ l利   Z  accessOrderxr&#010;&gt; java.util.HashMap  诹?`?  F&amp;nbsp;&#010;&gt; loadFactorI     thresholdxp?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; &amp;nbsp; x sq ~  ?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; x sq ~&#010;&gt; ?@&amp;nbsp; &amp;nbsp; &amp;nbsp; w &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; t&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionsr&#010;&gt; &lt;org.apache.flink.api.java.typeutils.runtime.KryoRegistrationJ?坣厏o   L&#010;&gt; registeredClassq ~  L  serializableSerializerInstancet&#010;&gt; DLorg/apache/flink/api/common/ExecutionConfig$SerializableSerializer;L&#010;&gt; serializerClassq ~  L  serializerDefinitionTypet&#010;&gt; WLorg/apache/flink/api/java/typeutils/runtime/KryoRegistration$SerializerDefinitionType;xpvr&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&#010;&gt;  [/剀X5   I&amp;nbsp;&#010;&gt; cach发送edHashI   partitionL  topict  Ljava/lang/String;xppp~r&#010;&gt; Uorg.apache.flink.api.java.typeutils.runtime.KryoRegistration$SerializerDefinitionType&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xr  java.lang.Enum&amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; xpt  UNSPECIFIEDt )org.apache.avro.generic.GenericData$Arraysq ~ vr&#010;&gt; Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xppvr&#010;&gt; Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;xp~q ~  t  CLASSx pppq ~  &amp;nbsp;&#010;&amp;nbsp;&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;nbsp;&#010;&gt; &amp;nbsp;&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&#010;&gt; Iorg.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartition&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;)org.apache.avro.generic.GenericData$Array&#010;&gt; Uorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroRegisteredClass&amp;nbsp;&#010;&gt; &amp;nbsp;&#010;&gt; Yorg.apache.flink.api.java.typeutils.runtime.kryo.Serializers$DummyAvroKryoSerializerClass&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;猬? sr&#010;&gt; 9org.apache.flink.api.common.typeutils.base.LongSerializer&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xr&#010;&gt; Borg.apache.flink.api.common.typeutils.base.TypeSerializerSingletony﹪.wE&#010;&gt; &amp;nbsp; xr 4org.apache.flink.api.common.typeutils.TypeSerializer&amp;nbsp;&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp;  &amp;nbsp; xp&amp;nbsp; &amp;nbsp;&#010;&gt; Porg.apache.flink.api.common.typeutils.base.LongSerializer$LongSerializerSnapshot&amp;nbsp;&#010;&gt; &amp;nbsp;  9org.apache.flink.api.common.typeutils.base.LongSerializer  vr&#010;&gt; &amp;amp;org.apache.flink.api.java.tuple.Tuple2&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; L  f0t  Ljava/lang/Object;L  f1q ~  xr&#010;&gt; %org.apache.flink.api.java.tuple.Tuple&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;  &amp;nbsp;&#010;&gt; xp&amp;nbsp;&amp;nbsp;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_D1BDD20BD21C32235A267556B799B23A0309@qq.com>"
    },
    {
        "id": "<1595305859055-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 04:30:59 GMT",
        "subject": "flink 1.11 cdc: kafka中存了canal-json格式的多张表信息，需要按表解析做处理，sink至不同的下游，要怎么支持？",
        "content": "例如：&#010;&#010;mysql表：&#010;CREATE TABLE `test` (&#010;  `id` int(11) NOT NULL,&#010;  `name` varchar(255) NOT NULL,&#010;  `time` datetime NOT NULL,&#010;  `status` int(11) NOT NULL,&#010;  PRIMARY KEY (`id`)&#010;) ENGINE=InnoDB DEFAULT CHARSET=utf8&#010;&#010;CREATE TABLE `status` (&#010;  `id` int(11) NOT NULL,&#010;  `name` varchar(255) NOT NULL,&#010;  PRIMARY KEY (`id`)&#010;) ENGINE=InnoDB DEFAULT CHARSET=utf8&#010;&#010;kafka中数据：&#010;// 表test 中insert事件&#010;{\"data\":[{\"id\":\"1745\",\"name\":\"jindy1745\",\"time\":\"2020-07-03&#010;18:04:22\",\"status\":\"0\"}],\"database\":\"ai_audio_lyric_task\",\"es\":1594968168000,\"id\":42,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"time\":\"datetime\",\"status\":\"int(11)\"},\"old\":null,\"pkNames\":[\"id\"],\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"time\":93,\"status\":4},\"table\":\"test\",\"ts\":1594968168789,\"type\":\"INSERT\"}&#010;&#010;//表status 中的事件&#010;{\"data\":[{\"id\":\"10\",\"name\":\"status\"}],\"database\":\"ai_audio_lyric_task\",\"es\":1595305259000,\"id\":589240,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\"},\"old\":null,\"pkNames\":[\"id\"],\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12},\"table\":\"status\",\"ts\":1595305259386,\"type\":\"INSERT\"}&#010;&#010;如何由于kafka中的json动态的变化的，比如新增一个表，如何能转成应对的RowData，&#010;感觉无法直接用JsonRowDeserializationSchema或CanalJsonDeserializationSchema来做处理。&#010;&#010;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1595305859055-0.post@n8.nabble.com>"
    },
    {
        "id": "<CADQYLGtos9J8kRw1VFSoPSRHaPbFm_rSvJT=f4WwBokBuR9ThA@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 07:18:38 GMT",
        "subject": "Re: flink 1.11 cdc: kafka中存了canal-json格式的多张表信息，需要按表解析做处理，sink至不同的下游，要怎么支持？",
        "content": "http://apache-flink.147419.n8.nabble.com/flink-1-10-sql-kafka-format-json-schema-json-object-tt4665.html&#013;&#010; 这个邮件里提到了类似的问题。&#013;&#010;&#013;&#010;https://issues.apache.org/jira/browse/FLINK-18002 这个issue完成后(1.12)，你可以将&#013;&#010;“data”，“mysqlType”等格式不确定的字段定义为String类型，&#013;&#010;下游通过udf自己再解析对应的json&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;jindy_liu &lt;286729788@qq.com&gt; 于2020年7月21日周二 下午12:37写道：&#013;&#010;&#013;&#010;&gt; 例如：&#013;&#010;&gt;&#013;&#010;&gt; mysql表：&#013;&#010;&gt; CREATE TABLE `test` (&#013;&#010;&gt;   `id` int(11) NOT NULL,&#013;&#010;&gt;   `name` varchar(255) NOT NULL,&#013;&#010;&gt;   `time` datetime NOT NULL,&#013;&#010;&gt;   `status` int(11) NOT NULL,&#013;&#010;&gt;   PRIMARY KEY (`id`)&#013;&#010;&gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE `status` (&#013;&#010;&gt;   `id` int(11) NOT NULL,&#013;&#010;&gt;   `name` varchar(255) NOT NULL,&#013;&#010;&gt;   PRIMARY KEY (`id`)&#013;&#010;&gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8&#013;&#010;&gt;&#013;&#010;&gt; kafka中数据：&#013;&#010;&gt; // 表test 中insert事件&#013;&#010;&gt; {\"data\":[{\"id\":\"1745\",\"name\":\"jindy1745\",\"time\":\"2020-07-03&#013;&#010;&gt;&#013;&#010;&gt; 18:04:22\",\"status\":\"0\"}],\"database\":\"ai_audio_lyric_task\",\"es\":1594968168000,\"id\":42,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"time\":\"datetime\",\"status\":\"int(11)\"},\"old\":null,\"pkNames\":[\"id\"],\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"time\":93,\"status\":4},\"table\":\"test\",\"ts\":1594968168789,\"type\":\"INSERT\"}&#013;&#010;&gt;&#013;&#010;&gt; //表status 中的事件&#013;&#010;&gt;&#013;&#010;&gt; {\"data\":[{\"id\":\"10\",\"name\":\"status\"}],\"database\":\"ai_audio_lyric_task\",\"es\":1595305259000,\"id\":589240,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\"},\"old\":null,\"pkNames\":[\"id\"],\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12},\"table\":\"status\",\"ts\":1595305259386,\"type\":\"INSERT\"}&#013;&#010;&gt;&#013;&#010;&gt; 如何由于kafka中的json动态的变化的，比如新增一个表，如何能转成应对的RowData，&#013;&#010;&gt; 感觉无法直接用JsonRowDeserializationSchema或CanalJsonDeserializationSchema来做处理。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<1595305859055-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO931JwYGTo9Nu772=4LqKDOmOY8ik5TQDgOqrzXfkYpYVQA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:23:40 GMT",
        "subject": "Re: flink 1.11 cdc: kafka中存了canal-json格式的多张表信息，需要按表解析做处理，sink至不同的下游，要怎么支持？",
        "content": "Hi,&#013;&#010;&#013;&#010;目前 Flink SQL CDC 是不支持自动感知新表的，得要提前定义要表的 schema&#010;然后提交同步作业。比如你上面的例子，就需要定义两个&#013;&#010;source 表：&#013;&#010;&#013;&#010;CREATE TABLE `test` (&#013;&#010;  `id` int,&#013;&#010;  `name` string,&#013;&#010;  `time` timestamp(3),&#013;&#010;  `status` int&#013;&#010;) with (&#013;&#010;  'connector' = 'kafka',&#013;&#010;  'format' = 'canal-json',&#013;&#010;  ...&#013;&#010;);&#013;&#010;&#013;&#010;insert into downstream1 select * from `test`;&#013;&#010;&#013;&#010;&#013;&#010;CREATE TABLE `status` (&#013;&#010;  `id` int&#013;&#010;  `name` string&#013;&#010;) with (&#013;&#010;  'connector' = 'kafka',&#013;&#010;  'format' = 'canal-json',&#013;&#010;  ...&#013;&#010;);&#013;&#010;&#013;&#010;insert into downstream2 select * from `status`;&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;On Tue, 21 Jul 2020 at 15:19, godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/flink-1-10-sql-kafka-format-json-schema-json-object-tt4665.html&#013;&#010;&gt;  这个邮件里提到了类似的问题。&#013;&#010;&gt;&#013;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18002 这个issue完成后(1.12)，你可以将&#013;&#010;&gt; “data”，“mysqlType”等格式不确定的字段定义为String类型，&#013;&#010;&gt; 下游通过udf自己再解析对应的json&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Godfrey&#013;&#010;&gt;&#013;&#010;&gt; jindy_liu &lt;286729788@qq.com&gt; 于2020年7月21日周二 下午12:37写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 例如：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; mysql表：&#013;&#010;&gt; &gt; CREATE TABLE `test` (&#013;&#010;&gt; &gt;   `id` int(11) NOT NULL,&#013;&#010;&gt; &gt;   `name` varchar(255) NOT NULL,&#013;&#010;&gt; &gt;   `time` datetime NOT NULL,&#013;&#010;&gt; &gt;   `status` int(11) NOT NULL,&#013;&#010;&gt; &gt;   PRIMARY KEY (`id`)&#013;&#010;&gt; &gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; CREATE TABLE `status` (&#013;&#010;&gt; &gt;   `id` int(11) NOT NULL,&#013;&#010;&gt; &gt;   `name` varchar(255) NOT NULL,&#013;&#010;&gt; &gt;   PRIMARY KEY (`id`)&#013;&#010;&gt; &gt; ) ENGINE=InnoDB DEFAULT CHARSET=utf8&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; kafka中数据：&#013;&#010;&gt; &gt; // 表test 中insert事件&#013;&#010;&gt; &gt; {\"data\":[{\"id\":\"1745\",\"name\":\"jindy1745\",\"time\":\"2020-07-03&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 18:04:22\",\"status\":\"0\"}],\"database\":\"ai_audio_lyric_task\",\"es\":1594968168000,\"id\":42,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\",\"time\":\"datetime\",\"status\":\"int(11)\"},\"old\":null,\"pkNames\":[\"id\"],\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12,\"time\":93,\"status\":4},\"table\":\"test\",\"ts\":1594968168789,\"type\":\"INSERT\"}&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; //表status 中的事件&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; {\"data\":[{\"id\":\"10\",\"name\":\"status\"}],\"database\":\"ai_audio_lyric_task\",\"es\":1595305259000,\"id\":589240,\"isDdl\":false,\"mysqlType\":{\"id\":\"int(11)\",\"name\":\"varchar(255)\"},\"old\":null,\"pkNames\":[\"id\"],\"sql\":\"\",\"sqlType\":{\"id\":4,\"name\":12},\"table\":\"status\",\"ts\":1595305259386,\"type\":\"INSERT\"}&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如何由于kafka中的json动态的变化的，比如新增一个表，如何能转成应对的RowData，&#013;&#010;&gt; &gt; 感觉无法直接用JsonRowDeserializationSchema或CanalJsonDeserializationSchema来做处理。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<1595305859055-0.post@n8.nabble.com>"
    },
    {
        "id": "<1595316869156-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 07:34:29 GMT",
        "subject": "flink1.11 pyflink stream job 退出",
        "content": "python flink_cep_example.py 过几秒就退出了，应该一直运行不退出的啊。&#010;代码如下，使用了MATCH_RECOGNIZE：&#010;&#010;    s_env = StreamExecutionEnvironment.get_execution_environment()&#010;    b_s_settings =&#010;EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#010;    st_env = StreamTableEnvironment.create(s_env,&#010;environment_settings=b_s_settings)&#010;    configuration = st_env.get_config().get_configuration()&#010;    configuration.set_string(\"taskmanager.memory.task.off-heap.size\",&#010;\"500m\")&#010;&#010;    s_env.set_parallelism(1)&#010;&#010;    kafka_source = \"\"\"CREATE TABLE source (&#010;         flow_name STRING,&#010;         flow_id STRING,&#010;         component STRING,&#010;         filename STRING,&#010;         event_time TIMESTAMP(3),&#010;         WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND&#010;        ) WITH (&#010;         'connector' = 'kafka',&#010;         'topic' = 'cep',&#010;         'properties.bootstrap.servers' = 'localhost:9092',&#010;         'format' = 'json',&#010;         'scan.startup.mode' = 'latest-offset'&#010;        )\"\"\"&#010;&#010;&#010;&#010;    postgres_sink = \"\"\"&#010;        CREATE TABLE cep_result (&#010;        `filename`         STRING,&#010;        `start_tstamp`          TIMESTAMP(3),&#010;        `end_tstamp`           TIMESTAMP(3)&#010;        ) WITH (&#010;        'connector.type' = 'jdbc',&#010;        'connector.url' = 'jdbc:postgresql://127.0.0.1:5432/postgres',&#010;        'connector.table' = 'cep_result',&#010;        'connector.driver' = 'org.postgresql.Driver',&#010;        'connector.username' = 'postgres',&#010;        'connector.password' = 'my_password',&#010;        'connector.write.flush.max-rows' = '1'&#010;        )&#010;        \"\"\"&#010;&#010;    st_env.sql_update(kafka_source)&#010;    st_env.sql_update(postgres_sink)&#010;&#010;    postgres_sink_sql = '''&#010;        INSERT INTO cep_result&#010;        SELECT *&#010;        FROM source&#010;            MATCH_RECOGNIZE (&#010;                PARTITION BY filename&#010;                ORDER BY event_time&#010;                MEASURES&#010;                    (A.event_time) AS start_tstamp,&#010;                    (D.event_time) AS end_tstamp&#010;                ONE ROW PER MATCH&#010;                AFTER MATCH SKIP PAST LAST ROW&#010;                PATTERN (A B C D)&#010;                DEFINE&#010;                    A AS component = 'XXX',&#010;                    B AS component = 'YYY',&#010;                    C AS component = 'ZZZ',&#010;                    D AS component = 'WWW'&#010;            ) MR&#010;    '''&#010;&#010;    sql_result = st_env.execute_sql(postgres_sink_sql)&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1595316869156-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAPxmL=ELkWmfK4TihptLtxwEXk4ZfoDGZ0cjcX5fjGN_BUqsjQ@mail.gmail.com>",
        "from": "Xingbo Huang &lt;hxbks...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 07:40:52 GMT",
        "subject": "Re: flink1.11 pyflink stream job 退出",
        "content": "Hi,&#010;execute_sql是一个异步非阻塞的方法，所以你需要在你的代码末尾加上&#010;sql_result.get_job_client().get_job_execution_result().result()&#010;对此我已经创建了JIRA[1]&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18598&#010;&#010;Best,&#010;Xingbo&#010;&#010;lgs &lt;9925174@qq.com&gt; 于2020年7月21日周二 下午3:35写道：&#010;&#010;&gt; python flink_cep_example.py 过几秒就退出了，应该一直运行不退出的啊。&#010;&gt; 代码如下，使用了MATCH_RECOGNIZE：&#010;&gt;&#010;&gt;     s_env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt;     b_s_settings =&#010;&gt;&#010;&gt; EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#010;&gt;     st_env = StreamTableEnvironment.create(s_env,&#010;&gt; environment_settings=b_s_settings)&#010;&gt;     configuration = st_env.get_config().get_configuration()&#010;&gt;     configuration.set_string(\"taskmanager.memory.task.off-heap.size\",&#010;&gt; \"500m\")&#010;&gt;&#010;&gt;     s_env.set_parallelism(1)&#010;&gt;&#010;&gt;     kafka_source = \"\"\"CREATE TABLE source (&#010;&gt;          flow_name STRING,&#010;&gt;          flow_id STRING,&#010;&gt;          component STRING,&#010;&gt;          filename STRING,&#010;&gt;          event_time TIMESTAMP(3),&#010;&gt;          WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND&#010;&gt;         ) WITH (&#010;&gt;          'connector' = 'kafka',&#010;&gt;          'topic' = 'cep',&#010;&gt;          'properties.bootstrap.servers' = 'localhost:9092',&#010;&gt;          'format' = 'json',&#010;&gt;          'scan.startup.mode' = 'latest-offset'&#010;&gt;         )\"\"\"&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;     postgres_sink = \"\"\"&#010;&gt;         CREATE TABLE cep_result (&#010;&gt;         `filename`         STRING,&#010;&gt;         `start_tstamp`          TIMESTAMP(3),&#010;&gt;         `end_tstamp`           TIMESTAMP(3)&#010;&gt;         ) WITH (&#010;&gt;         'connector.type' = 'jdbc',&#010;&gt;         'connector.url' = 'jdbc:postgresql://127.0.0.1:5432/postgres',&#010;&gt;         'connector.table' = 'cep_result',&#010;&gt;         'connector.driver' = 'org.postgresql.Driver',&#010;&gt;         'connector.username' = 'postgres',&#010;&gt;         'connector.password' = 'my_password',&#010;&gt;         'connector.write.flush.max-rows' = '1'&#010;&gt;         )&#010;&gt;         \"\"\"&#010;&gt;&#010;&gt;     st_env.sql_update(kafka_source)&#010;&gt;     st_env.sql_update(postgres_sink)&#010;&gt;&#010;&gt;     postgres_sink_sql = '''&#010;&gt;         INSERT INTO cep_result&#010;&gt;         SELECT *&#010;&gt;         FROM source&#010;&gt;             MATCH_RECOGNIZE (&#010;&gt;                 PARTITION BY filename&#010;&gt;                 ORDER BY event_time&#010;&gt;                 MEASURES&#010;&gt;                     (A.event_time) AS start_tstamp,&#010;&gt;                     (D.event_time) AS end_tstamp&#010;&gt;                 ONE ROW PER MATCH&#010;&gt;                 AFTER MATCH SKIP PAST LAST ROW&#010;&gt;                 PATTERN (A B C D)&#010;&gt;                 DEFINE&#010;&gt;                     A AS component = 'XXX',&#010;&gt;                     B AS component = 'YYY',&#010;&gt;                     C AS component = 'ZZZ',&#010;&gt;                     D AS component = 'WWW'&#010;&gt;             ) MR&#010;&gt;     '''&#010;&gt;&#010;&gt;     sql_result = st_env.execute_sql(postgres_sink_sql)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<1595316869156-0.post@n8.nabble.com>"
    },
    {
        "id": "<1595318179889-0.post@n8.nabble.com>",
        "from": "lgs &lt;9925...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 07:56:19 GMT",
        "subject": "Re: flink1.11 pyflink stream job 退出",
        "content": "谢谢。加上后就可以了。&#010;&#010;改成原来的sql_update然后st_env.execute(\"job\")好像也可以。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "2",
        "reply": "<1595316869156-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAPxmL=Fw1gWvQfNKPc+yRGL5fcvS++-9jW=V8mcKAqZoOnL0mg@mail.gmail.com>",
        "from": "Xingbo Huang &lt;hxbks...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 08:01:26 GMT",
        "subject": "Re: flink1.11 pyflink stream job 退出",
        "content": "是的，execute是1.10及以前使用的，execute_sql是1.11之后推荐使用的&#013;&#010;&#013;&#010;Best,&#013;&#010;Xingbo&#013;&#010;&#013;&#010;lgs &lt;9925174@qq.com&gt; 于2020年7月21日周二 下午3:57写道：&#013;&#010;&#013;&#010;&gt; 谢谢。加上后就可以了。&#013;&#010;&gt;&#013;&#010;&gt; 改成原来的sql_update然后st_env.execute(\"job\")好像也可以。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<1595316869156-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 08:08:05 GMT",
        "subject": "flink解析kafka json数据",
        "content": "hi&#013;&#010;我这面在使用sql api解析kafka&#013;&#010;json数据，在创建表的时候发现json数据解析的时候有下面两项，这两项如果开启那么解析失败的数据是会被丢掉吗，有没有方式可以把解析失败的数据打到外部存储&#013;&#010;&#013;&#010;json.ignore-parse-errors&#013;&#010;son.fail-on-missing-field&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>"
    },
    {
        "id": "<9DB7BC3E-8CF9-4217-9E51-F1E87AE331CF@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 08:18:06 GMT",
        "subject": "Re: flink解析kafka json数据",
        "content": "Hi,&#013;&#010;我理解应该做不到，因为这两个format参数在format里就做的。&#013;&#010;json.ignore-parse-errors 是在 format解析时跳过解析失败的数据继续解析下一行，json.fail-on-missing-field&#010;是标记如果字段少时是否失败还是继续(缺少的字段用null补上）&#013;&#010;这两个不能同时为ture，语义上就是互斥的。&#013;&#010;&#013;&#010;Best&#013;&#010;Leonard Xu&#013;&#010;&gt; 在 2020年7月21日，16:08，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; json数据，在创建表的时候发现json数据解析的时候有下面两项，这两项如果开启那么解析失败的数据是会被丢掉吗，有没有方式可以把解析失败的数据打到外部存储&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>"
    },
    {
        "id": "<CAEZk042wH3w_rPqmjZ3_LxkcZB=qa+AOu7giDMe+ibekOn7Ajg@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 10:52:34 GMT",
        "subject": "Re: flink解析kafka json数据",
        "content": "hi&#013;&#010; json.ignore-parse-errors那只配置这个就好了， 其实我想把解析失败的数据存储到外部系统，而不是直接丢弃&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月21日周二 下午4:18写道：&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt; 我理解应该做不到，因为这两个format参数在format里就做的。&#013;&#010;&gt; json.ignore-parse-errors 是在&#013;&#010;&gt; format解析时跳过解析失败的数据继续解析下一行，json.fail-on-missing-field&#013;&#010;&gt; 是标记如果字段少时是否失败还是继续(缺少的字段用null补上）&#013;&#010;&gt; 这两个不能同时为ture，语义上就是互斥的。&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; &gt; 在 2020年7月21日，16:08，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; json数据，在创建表的时候发现json数据解析的时候有下面两项，这两项如果开启那么解析失败的数据是会被丢掉吗，有没有方式可以把解析失败的数据打到外部存储&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>"
    },
    {
        "id": "<CAELO930GUN6bNuhfq=KGmV1+z3WEvaFumDY4DDr6LJLJD9euZA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:29:55 GMT",
        "subject": "Re: flink解析kafka json数据",
        "content": "目前是不支持的。这个需求有点太业务特定了。flink 不可能为了一个错误日志去抽象、对接各种存储系统。&#013;&#010;一种方案是社区可以考虑支持下打印到日志里，然后用户可以通过自定义插件&#010;log appender 写入外部存储。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Tue, 21 Jul 2020 at 18:53, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt;  json.ignore-parse-errors那只配置这个就好了， 其实我想把解析失败的数据存储到外部系统，而不是直接丢弃&#013;&#010;&gt;&#013;&#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月21日周二 下午4:18写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi,&#013;&#010;&gt; &gt; 我理解应该做不到，因为这两个format参数在format里就做的。&#013;&#010;&gt; &gt; json.ignore-parse-errors 是在&#013;&#010;&gt; &gt; format解析时跳过解析失败的数据继续解析下一行，json.fail-on-missing-field&#013;&#010;&gt; &gt; 是标记如果字段少时是否失败还是继续(缺少的字段用null补上）&#013;&#010;&gt; &gt; 这两个不能同时为ture，语义上就是互斥的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best&#013;&#010;&gt; &gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; 在 2020年7月21日，16:08，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; json数据，在创建表的时候发现json数据解析的时候有下面两项，这两项如果开启那么解析失败的数据是会被丢掉吗，有没有方式可以把解析失败的数据打到外部存储&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>"
    },
    {
        "id": "<CAEZk040+2AdHOpiZb0At5bg8TaDPvyWkM8GnGsBOuc3h_e41qg@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 03:36:02 GMT",
        "subject": "Re: flink解析kafka json数据",
        "content": "hi jark wu、&#013;&#010;将解析错误数据直接打到日志里确实是比较通用的解决方案；&#013;&#010;我现在使用flink sql对接kafka&#013;&#010;json数据的时候，发现对json数据的解析有一些局限性，即比如我有一条数据是jsonobject，但是我在定义flink&#010;sql&#013;&#010;connector数据类型的时候如果直接定义为string，会导致数据解析失败（当然，这个失败是正常的）&#013;&#010;但是这会有一个局限性就是我没办法以一个string方式获取一个jsonobject数据（由于一些比较尴尬的原因就想以string方式获取jsonobject数据），查看代码发现这是jackson导致的获取失败，这个社区考虑兼容一下吗？&#013;&#010;&#013;&#010;if (simpleTypeInfo == Types.STRING) {&#013;&#010;   return Optional.of((mapper, jsonNode) -&gt; jsonNode.asText());//&#013;&#010;这里会返回空，改成下面样子兼容一下&#013;&#010;&#013;&#010;if (simpleTypeInfo == Types.STRING) {&#013;&#010;   return Optional.of((mapper, jsonNode) -&gt;&#013;&#010;jsonNode.isTextual()?jsonNode.asText():jsonNode.toString());&#013;&#010;&#013;&#010;&#013;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月21日周二 下午10:30写道：&#013;&#010;&#013;&#010;&gt; 目前是不支持的。这个需求有点太业务特定了。flink 不可能为了一个错误日志去抽象、对接各种存储系统。&#013;&#010;&gt; 一种方案是社区可以考虑支持下打印到日志里，然后用户可以通过自定义插件&#010;log appender 写入外部存储。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Tue, 21 Jul 2020 at 18:53, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi&#013;&#010;&gt; &gt;  json.ignore-parse-errors那只配置这个就好了， 其实我想把解析失败的数据存储到外部系统，而不是直接丢弃&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月21日周二 下午4:18写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi,&#013;&#010;&gt; &gt; &gt; 我理解应该做不到，因为这两个format参数在format里就做的。&#013;&#010;&gt; &gt; &gt; json.ignore-parse-errors 是在&#013;&#010;&gt; &gt; &gt; format解析时跳过解析失败的数据继续解析下一行，json.fail-on-missing-field&#013;&#010;&gt; &gt; &gt; 是标记如果字段少时是否失败还是继续(缺少的字段用null补上）&#013;&#010;&gt; &gt; &gt; 这两个不能同时为ture，语义上就是互斥的。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best&#013;&#010;&gt; &gt; &gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt; 在 2020年7月21日，16:08，Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; json数据，在创建表的时候发现json数据解析的时候有下面两项，这两项如果开启那么解析失败的数据是会被丢掉吗，有没有方式可以把解析失败的数据打到外部存储&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>"
    },
    {
        "id": "<CAELO931_fU0EJoyWMQ=ReVPdpOLxaWaTNdikdpMQgHLwBZ4thw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 04:16:01 GMT",
        "subject": "Re: flink解析kafka json数据",
        "content": "哈哈，看来是一个很通用的需求啊。 本超同学已经在1.12 中支持了这个功能了，&#010;see&#013;&#010;https://issues.apache.org/jira/browse/FLINK-18002&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Fri, 24 Jul 2020 at 11:36, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi jark wu、&#013;&#010;&gt; 将解析错误数据直接打到日志里确实是比较通用的解决方案；&#013;&#010;&gt; 我现在使用flink sql对接kafka&#013;&#010;&gt; json数据的时候，发现对json数据的解析有一些局限性，即比如我有一条数据是jsonobject，但是我在定义flink&#010;sql&#013;&#010;&gt; connector数据类型的时候如果直接定义为string，会导致数据解析失败（当然，这个失败是正常的）&#013;&#010;&gt;&#013;&#010;&gt; 但是这会有一个局限性就是我没办法以一个string方式获取一个jsonobject数据（由于一些比较尴尬的原因就想以string方式获取jsonobject数据），查看代码发现这是jackson导致的获取失败，这个社区考虑兼容一下吗？&#013;&#010;&gt;&#013;&#010;&gt; if (simpleTypeInfo == Types.STRING) {&#013;&#010;&gt;    return Optional.of((mapper, jsonNode) -&gt; jsonNode.asText());//&#013;&#010;&gt; 这里会返回空，改成下面样子兼容一下&#013;&#010;&gt;&#013;&#010;&gt; if (simpleTypeInfo == Types.STRING) {&#013;&#010;&gt;    return Optional.of((mapper, jsonNode) -&gt;&#013;&#010;&gt; jsonNode.isTextual()?jsonNode.asText():jsonNode.toString());&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月21日周二 下午10:30写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 目前是不支持的。这个需求有点太业务特定了。flink 不可能为了一个错误日志去抽象、对接各种存储系统。&#013;&#010;&gt; &gt; 一种方案是社区可以考虑支持下打印到日志里，然后用户可以通过自定义插件&#010;log appender 写入外部存储。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Tue, 21 Jul 2020 at 18:53, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; hi&#013;&#010;&gt; &gt; &gt;  json.ignore-parse-errors那只配置这个就好了， 其实我想把解析失败的数据存储到外部系统，而不是直接丢弃&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月21日周二 下午4:18写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hi,&#013;&#010;&gt; &gt; &gt; &gt; 我理解应该做不到，因为这两个format参数在format里就做的。&#013;&#010;&gt; &gt; &gt; &gt; json.ignore-parse-errors 是在&#013;&#010;&gt; &gt; &gt; &gt; format解析时跳过解析失败的数据继续解析下一行，json.fail-on-missing-field&#013;&#010;&gt; &gt; &gt; &gt; 是标记如果字段少时是否失败还是继续(缺少的字段用null补上）&#013;&#010;&gt; &gt; &gt; &gt; 这两个不能同时为ture，语义上就是互斥的。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Best&#013;&#010;&gt; &gt; &gt; &gt; Leonard Xu&#013;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020年7月21日，16:08，Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; json数据，在创建表的时候发现json数据解析的时候有下面两项，这两项如果开启那么解析失败的数据是会被丢掉吗，有没有方式可以把解析失败的数据打到外部存储&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>"
    },
    {
        "id": "<DM6PR01MB3625BB7333A3C958994444D6A3750@DM6PR01MB3625.prod.exchangelabs.com>",
        "from": "吴 祥平 &lt;wxp4...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 26 Jul 2020 01:35:05 GMT",
        "subject": "Re: flink解析kafka json数据",
        "content": "改用csv用一个很不常用的分隔符去获取是可以的，比如/u0005&#013;&#010;&#013;&#010;Get Outlook for Android&lt;https://aka.ms/ghei36&gt;&#013;&#010;&#013;&#010;",
        "depth": "5",
        "reply": "<CAEZk040ypJ95iC7C-SikBo9HiCW5+KDEaj5mNL7GjZJwJxkSiw@mail.gmail.com>"
    },
    {
        "id": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 08:32:46 GMT",
        "subject": "flink table同时作为写出及输入时下游无数据",
        "content": "各位大佬好，请教一个问题，就是在flink内部定义一个表g_unit(初始为空)，接受一个kafka源的写入，同时g_unit又要作为下游表g_summary的输入源，测试发现g_line表一直不会写入数据，代码如下，烦请大佬解答。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;kafka_source_ddl = \"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&amp;nbsp;&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'gg',&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'scan.startup.mode' = 'specific-offsets',&amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'scan.startup.specific-offsets'='partition:1,offset:0',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '****',&#013;&#010;&amp;nbsp;'format' = 'json'&amp;nbsp;&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_unit_sink_ddl = \"\"\"&#013;&#010;CREATE TABLE g_sink_unit (&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&amp;nbsp;&#013;&#010;&amp;nbsp;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_unit',&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_summary_ddl = \"\"\"&#013;&#010;CREATE TABLE g_summary_base(&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR&amp;nbsp;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_summary',&amp;nbsp;&amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;&#013;&#010;t_env.execute_sql(kafka_source_ddl)&#013;&#010;t_env.execute_sql(g_unit_sink_ddl)&#013;&#010;t_env.execute_sql(g_summary_ddl)&#013;&#010;&#013;&#010;&#013;&#010;sql1='''Insert into g_unit_sink_ddl select alarm_id,trck_id from kafka_source_tab'''&#013;&#010;sql2='''Insert into g_summary_ddl select alarm_id,trck_id from g_unit_sink_ddl'''&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;stmt_set = t_env.create_statement_set()&#013;&#010;stmt_set.add_insert_sql(sql1)&#013;&#010;stmt_set.add_insert_sql(sql2)&#013;&#010;&#013;&#010;&#013;&#010;stmt_set.execute().get_job_client().get_job_execution_result().result()",
        "depth": "1",
        "reply": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>"
    },
    {
        "id": "<tencent_BCB2BBB235EA17507CF6DC57D2F143AE3A08@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 08:38:20 GMT",
        "subject": "flink table同时作为写出及输入时下游无数据",
        "content": "各位大佬好，请教一个问题，就是在flink内部定义一个表g_unit(初始为空)，接受一个kafka源的写入，同时g_unit又要作为下游表g_summary的输入源，测试发现g_line表一直不会写入数据，代码如下，烦请大佬解答。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;kafka_source_ddl = \"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'gg',&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;'scan.startup.mode' = 'specific-offsets',&amp;nbsp; &#013;&#010;&amp;nbsp;'scan.startup.specific-offsets'='partition:1,offset:0',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '****',&#013;&#010;&amp;nbsp;'format' = 'json' &#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_unit_sink_ddl = \"\"\"&#013;&#010;CREATE TABLE g_sink_unit (&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;&amp;nbsp;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_unit',&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_summary_ddl = \"\"\"&#013;&#010;CREATE TABLE g_summary_base(&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_summary',&amp;nbsp; &#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;&#013;&#010;t_env.execute_sql(kafka_source_ddl)&#013;&#010;t_env.execute_sql(g_unit_sink_ddl)&#013;&#010;t_env.execute_sql(g_summary_ddl)&#013;&#010;&#013;&#010;&#013;&#010;sql1='''Insert into g_unit_sink_ddl select alarm_id,trck_id from kafka_source_tab'''&#013;&#010;sql2='''Insert into g_summary_ddl select alarm_id,trck_id from g_unit_sink_ddl'''&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;stmt_set = t_env.create_statement_set()&#013;&#010;stmt_set.add_insert_sql(sql1)&#013;&#010;stmt_set.add_insert_sql(sql2)&#013;&#010;&#013;&#010;&#013;&#010;stmt_set.execute().get_job_client().get_job_execution_result().result()",
        "depth": "1",
        "reply": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>"
    },
    {
        "id": "<tencent_DF5D99202962A778955FD3ADF597D6169608@qq.com>",
        "from": "&quot;chengyanan1008@foxmail.com&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 09:02:40 GMT",
        "subject": "回复: flink table同时作为写出及输入时下游无数据",
        "content": "你好：&#013;&#010;&#013;&#010;sql1='''Insert into g_unit_sink_ddl select alarm_id,trck_id from kafka_source_tab'''&#013;&#010;sql2='''Insert into g_summary_ddl select alarm_id,trck_id from g_unit_sink_ddl'''&#013;&#010;&#013;&#010;这里写错了吧，你的表名是g_sink_unit 和 g_summary_base&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;chengyanan1008@foxmail.com&#013;&#010; &#013;&#010;发件人： 小学生&#013;&#010;发送时间： 2020-07-21 16:38&#013;&#010;收件人： user-zh&#013;&#010;主题： flink table同时作为写出及输入时下游无数据&#013;&#010;各位大佬好，请教一个问题，就是在flink内部定义一个表g_unit(初始为空)，接受一个kafka源的写入，同时g_unit又要作为下游表g_summary的输入源，测试发现g_line表一直不会写入数据，代码如下，烦请大佬解答。&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;kafka_source_ddl = \"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010; &#013;&#010; &#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'gg',&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;'scan.startup.mode' = 'specific-offsets',&amp;nbsp; &#013;&#010;&amp;nbsp;'scan.startup.specific-offsets'='partition:1,offset:0',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '****',&#013;&#010;&amp;nbsp;'format' = 'json' &#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_unit_sink_ddl = \"\"\"&#013;&#010;CREATE TABLE g_sink_unit (&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;&amp;nbsp;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_unit',&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_summary_ddl = \"\"\"&#013;&#010;CREATE TABLE g_summary_base(&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_summary',&amp;nbsp; &#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010; &#013;&#010;t_env.execute_sql(kafka_source_ddl)&#013;&#010;t_env.execute_sql(g_unit_sink_ddl)&#013;&#010;t_env.execute_sql(g_summary_ddl)&#013;&#010; &#013;&#010; &#013;&#010;sql1='''Insert into g_unit_sink_ddl select alarm_id,trck_id from kafka_source_tab'''&#013;&#010;sql2='''Insert into g_summary_ddl select alarm_id,trck_id from g_unit_sink_ddl'''&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;stmt_set = t_env.create_statement_set()&#013;&#010;stmt_set.add_insert_sql(sql1)&#013;&#010;stmt_set.add_insert_sql(sql2)&#013;&#010; &#013;&#010; &#013;&#010;stmt_set.execute().get_job_client().get_job_execution_result().result()&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>"
    },
    {
        "id": "<tencent_C17525A805FBB82BF2F80DD73D592ED97107@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 09:40:36 GMT",
        "subject": "flink table同时作为写出及输入时下游无数据",
        "content": "各位大佬好，请教一个问题，就是在flink内部定义一个表g_unit(初始为空)，接受一个kafka源的写入，同时g_unit又要作为下游表g_summary的输入源，测试发现g_line表一直不会写入数据，代码如下，烦请大佬解答。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;kafka_source_ddl = \"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&#013;&#010;&amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'gg',&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;'scan.startup.mode' = 'specific-offsets',&amp;nbsp; &#013;&#010;&amp;nbsp;'scan.startup.specific-offsets'='partition:1,offset:0',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '****',&#013;&#010;&amp;nbsp;'format' = 'json' &#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_unit_sink_ddl = \"\"\"&#013;&#010;CREATE TABLE g_sink_unit (&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;&amp;nbsp;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_unit',&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;g_summary_ddl = \"\"\"&#013;&#010;CREATE TABLE g_summary_base(&#013;&#010;&amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;nbsp;trck_id VARCHAR &#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#013;&#010;&amp;nbsp;'table-name' = 'g_summary',&amp;nbsp; &#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = 'root',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;&#013;&#010;t_env.execute_sql(kafka_source_ddl)&#013;&#010;t_env.execute_sql(g_unit_sink_ddl)&#013;&#010;t_env.execute_sql(g_summary_ddl)&#013;&#010;&#013;&#010;&#013;&#010;sql1='''Insert into g_sink_unit&amp;nbsp;select alarm_id,trck_id from kafka_source_tab'''&#013;&#010;sql2='''Insert into g_summary_base&amp;nbsp;select alarm_id,trck_id from g_sink_unit&amp;nbsp;'''&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;stmt_set = t_env.create_statement_set()&#013;&#010;stmt_set.add_insert_sql(sql1)&#013;&#010;stmt_set.add_insert_sql(sql2)&#013;&#010;&#013;&#010;&#013;&#010;stmt_set.execute().get_job_client().get_job_execution_result().result()",
        "depth": "1",
        "reply": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>"
    },
    {
        "id": "<CADQYLGsAyAe5kQuKT29tfzYTuzua85D42=rTxo-8QyevkCzH=w@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 10:09:15 GMT",
        "subject": "Re: flink table同时作为写出及输入时下游无数据",
        "content": "你可以先只跑第一个insert，然后check一下g_sink_unit是否有数据。&#010;&#010;另外，你可以把query 改为都读取 kafka_source_tab 再分别写到两个不同的sink：&#010;&#010;sql1='''Insert into g_sink_unit select alarm_id,trck_id from&#010;kafka_source_tab'''&#010;sql2='''Insert into g_summary_base select alarm_id,trck_id from&#010;kafka_source_tab;'''&#010;&#010;小学生 &lt;201782053@qq.com&gt; 于2020年7月21日周二 下午5:47写道：&#010;&#010;&gt;&#010;&gt; 各位大佬好，请教一个问题，就是在flink内部定义一个表g_unit(初始为空)，接受一个kafka源的写入，同时g_unit又要作为下游表g_summary的输入源，测试发现g_line表一直不会写入数据，代码如下，烦请大佬解答。&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; from pyflink.datastream import StreamExecutionEnvironment,&#010;&gt; TimeCharacteristic, CheckpointingMode&#010;&gt; from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;&gt; env.set_parallelism(1)&#010;&gt; env_settings =&#010;&gt; EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#010;&gt; t_env = StreamTableEnvironment.create(env,&#010;&gt; environment_settings=env_settings)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; kafka_source_ddl = \"\"\"&#010;&gt; CREATE TABLE kafka_source_tab (&#010;&gt; &amp;nbsp;id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;trck_id VARCHAR&#010;&gt;&#010;&gt;&#010;&gt; ) WITH (&#010;&gt; &amp;nbsp;'connector' = 'kafka',&#010;&gt; &amp;nbsp;'topic' = 'gg',&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;'scan.startup.mode' = 'specific-offsets',&amp;nbsp;&#010;&gt; &amp;nbsp;'scan.startup.specific-offsets'='partition:1,offset:0',&#010;&gt; &amp;nbsp;'properties.bootstrap.servers' = '****',&#010;&gt; &amp;nbsp;'format' = 'json'&#010;&gt; )&#010;&gt; \"\"\"&#010;&gt; g_unit_sink_ddl = \"\"\"&#010;&gt; CREATE TABLE g_sink_unit (&#010;&gt; &amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;trck_id VARCHAR&#010;&gt; &amp;nbsp;&#010;&gt; ) WITH (&#010;&gt; &amp;nbsp;'connector' = 'jdbc',&#010;&gt; &amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#010;&gt; &amp;nbsp;'table-name' = 'g_unit',&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;'username' = 'root',&#010;&gt; &amp;nbsp;'password' = 'root',&#010;&gt; &amp;nbsp;'sink.buffer-flush.interval' = '1s'&amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&gt; )&#010;&gt; \"\"\"&#010;&gt; g_summary_ddl = \"\"\"&#010;&gt; CREATE TABLE g_summary_base(&#010;&gt; &amp;nbsp;alarm_id VARCHAR,&amp;nbsp; &amp;nbsp;&#010;&gt; &amp;nbsp;trck_id VARCHAR&#010;&gt; ) WITH (&#010;&gt; &amp;nbsp;'connector' = 'jdbc',&#010;&gt; &amp;nbsp;'url' = 'jdbc:mysql://10.2.2.70:3306/bdoa?useSSL=false',&#010;&gt; &amp;nbsp;'table-name' = 'g_summary',&amp;nbsp;&#010;&gt; &amp;nbsp;'username' = 'root',&#010;&gt; &amp;nbsp;'password' = 'root',&#010;&gt; &amp;nbsp;'sink.buffer-flush.interval' = '1s'&#010;&gt; )&#010;&gt; \"\"\"&#010;&gt;&#010;&gt; t_env.execute_sql(kafka_source_ddl)&#010;&gt; t_env.execute_sql(g_unit_sink_ddl)&#010;&gt; t_env.execute_sql(g_summary_ddl)&#010;&gt;&#010;&gt;&#010;&gt; sql1='''Insert into g_sink_unit&amp;nbsp;select alarm_id,trck_id from&#010;&gt; kafka_source_tab'''&#010;&gt; sql2='''Insert into g_summary_base&amp;nbsp;select alarm_id,trck_id from&#010;&gt; g_sink_unit&amp;nbsp;'''&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; stmt_set = t_env.create_statement_set()&#010;&gt; stmt_set.add_insert_sql(sql1)&#010;&gt; stmt_set.add_insert_sql(sql2)&#010;&gt;&#010;&gt;&#010;&gt; stmt_set.execute().get_job_client().get_job_execution_result().result()&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>"
    },
    {
        "id": "<tencent_CE196495D475A004ECFDAE78E2115D496409@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 11:39:03 GMT",
        "subject": "Re: flink table同时作为写出及输入时下游无数据",
        "content": "1.你可以先只跑第一个insert，然后check一下g_sink_unit是否有数据，这个是有数据的，&#013;&#010;2.这个例子里当然都写完kafka的初始源是完全没问题的，实际中确实是需要g_unit作为桥梁的。",
        "depth": "3",
        "reply": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>"
    },
    {
        "id": "<1595331365591-0.post@n8.nabble.com>",
        "from": "咿咿呀呀 &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 11:36:05 GMT",
        "subject": "Re: flink table同时作为写出及输入时下游无数据",
        "content": "就是没有数据，我这个是简化版本的，都切换为kafka的初始源是没问题的&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "2",
        "reply": "<tencent_21EA535062F66A5754F102AEA1B223B28E08@qq.com>"
    }
]