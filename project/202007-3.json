[
    {
        "id": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>",
        "from": "hua mulan &lt;deadwi...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:34:26 GMT",
        "subject": "Flink SQL处理Array型的JSON",
        "content": "Hi&#013;&#010;&#013;&#010;Kafka中的JSON结构是个Array例子如下。&#013;&#010;[&#013;&#010; { \"id\": 1},&#013;&#010; { \"id\": 2}&#013;&#010;]&#013;&#010;读出来变成表的两行。Flink SQL层面最佳实践是什么？&#013;&#010;如果没有办法是不是只能改JSON结构了。&#013;&#010;&#013;&#010;&#013;&#010;可爱的木兰&#013;&#010;",
        "depth": "0",
        "reply": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<6AC098D8-DF89-4E30-B91D-803A5B447B7A@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:42:26 GMT",
        "subject": "Re: Flink SQL处理Array型的JSON",
        "content": "Hello，可爱的木兰&#010;&#010;可以不用改json的，可以用 UNNEST 把数组拆成多行，也可以写UDTF自己解析对应字段，参考[1]&#010;&#010;SELECT users, tag&#010;FROM Orders CROSS JOIN UNNEST(tags) AS t (tag)&#010;&#010;Best，&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html &lt;https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html&gt;&#010;&#010;&gt; 在 2020年7月14日，10:34，hua mulan &lt;deadwind4@outlook.com&gt; 写道：&#010;&gt; &#010;&gt; 可爱的木兰&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<CABKuJ_RRRcVCqTBuX_qYV9Fkec7fbX-LEeGqVK-ROaE++OixTg@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:00:11 GMT",
        "subject": "Re: Flink SQL处理Array型的JSON",
        "content": "我感觉这是一个合理的需求，因为1.11之后我们支持了format返回多条数据，我们可以支持这种形式的数据了，&#013;&#010;我建了一个issue[1].&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18590&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月14日周二 上午10:42写道：&#013;&#010;&#013;&#010;&gt; Hello，可爱的木兰&#013;&#010;&gt;&#013;&#010;&gt; 可以不用改json的，可以用 UNNEST 把数组拆成多行，也可以写UDTF自己解析对应字段，参考[1]&#013;&#010;&gt;&#013;&#010;&gt; SELECT users, tag&#013;&#010;&gt; FROM Orders CROSS JOIN UNNEST(tags) AS t (tag)&#013;&#010;&gt;&#013;&#010;&gt; Best，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html&#013;&#010;&gt; &lt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月14日，10:34，hua mulan &lt;deadwind4@outlook.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 可爱的木兰&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "2",
        "reply": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<SG2PR02MB322699CA2E68CA19A01DFB91F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>",
        "from": "hua mulan &lt;deadwi...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:25:04 GMT",
        "subject": "回复: Flink SQL处理Array型的JSON",
        "content": "Hi&#013;&#010;&#013;&#010;那我觉得目前最佳实践就是，我用DataStream的API先把数据清洗成 json object&#010;in top level 在导入Kafka，之后再FlinkSQL 处理。&#013;&#010;&#013;&#010;可爱的木兰&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: Benchao Li &lt;libenchao@apache.org&gt;&#013;&#010;发送时间: 2020年7月14日 11:00&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: Flink SQL处理Array型的JSON&#013;&#010;&#013;&#010;我感觉这是一个合理的需求，因为1.11之后我们支持了format返回多条数据，我们可以支持这种形式的数据了，&#013;&#010;我建了一个issue[1].&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18590&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月14日周二 上午10:42写道：&#013;&#010;&#013;&#010;&gt; Hello，可爱的木兰&#013;&#010;&gt;&#013;&#010;&gt; 可以不用改json的，可以用 UNNEST 把数组拆成多行，也可以写UDTF自己解析对应字段，参考[1]&#013;&#010;&gt;&#013;&#010;&gt; SELECT users, tag&#013;&#010;&gt; FROM Orders CROSS JOIN UNNEST(tags) AS t (tag)&#013;&#010;&gt;&#013;&#010;&gt; Best，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html&#013;&#010;&gt; &lt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月14日，10:34，hua mulan &lt;deadwind4@outlook.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 可爱的木兰&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;--&#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "3",
        "reply": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<1595327059608-0.post@n8.nabble.com>",
        "from": "wxpcc &lt;wxp4...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 10:24:19 GMT",
        "subject": "Re: 回复: Flink SQL处理Array型的JSON",
        "content": "如果不等待最新版本的话也可以这样&#010;&#010;将 纯数组的数据作为字符串 从source消费，增加自定义的json解析函数，判断&#010;isArray 之后 遍历进行 collect&#010;&#010;if (Objects.nonNull(str)) {&#010;                if (isArray) {&#010;                    JsonNode node = objectMapper.readTree(str);&#010;                    if (node.isArray()) {&#010;                        Iterator&lt;JsonNode&gt; nodeIterator = node.elements();&#010;                        while (nodeIterator.hasNext()) {&#010;                           &#010;collect(deserializationSchema.deserialize(nodeIterator.next().toString().getBytes()));&#010;                        }&#010;                    }&#010;                } else {&#010;                   &#010;collect(deserializationSchema.deserialize(str.getBytes()));&#010;                }&#010;            }&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "4",
        "reply": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<1595327212295-0.post@n8.nabble.com>",
        "from": "wxpcc &lt;wxp4...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 10:26:52 GMT",
        "subject": "Re: 回复: Flink SQL处理Array型的JSON",
        "content": "补充：&#010;最终查询为&#010;&#010;SELECT&#010; t.*&#010;FROM&#010;  kafka_source,&#010;  LATERAL TABLE( fromJson(data) ) as t&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "5",
        "reply": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<SG2PR02MB32269EB8C95405738F506C42F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>",
        "from": "hua mulan &lt;deadwi...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:55:37 GMT",
        "subject": "回复: Flink SQL处理Array型的JSON",
        "content": "Hello，Leonard Xu&#013;&#010;&#013;&#010;我这边JSON 不是&#013;&#010;&#013;&#010;{&#013;&#010;    \"id\": 2,&#013;&#010;    \"heap\": [&#013;&#010;        {&#013;&#010;            \"foo\": 14,&#013;&#010;            \"bar\": \"foo\"&#013;&#010;&#013;&#010;        },&#013;&#010;        {&#013;&#010;            \"foo\": 16,&#013;&#010;            \"bar\": \"bar\"&#013;&#010;        }&#013;&#010;    ],&#013;&#010;}&#013;&#010;&#013;&#010;而是直接一个Array&#013;&#010;&#013;&#010;[&#013;&#010;        {&#013;&#010;            \"foo\": 14,&#013;&#010;            \"bar\": \"foo\"&#013;&#010;&#013;&#010;        },&#013;&#010;        {&#013;&#010;            \"foo\": 16,&#013;&#010;            \"bar\": \"bar\"&#013;&#010;        }&#013;&#010;    ]&#013;&#010;&#013;&#010;我发现DDL没法声明，SQL层面我不知道怎么做了。&#013;&#010;&#013;&#010;可爱的木兰&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#013;&#010;发送时间: 2020年7月14日 10:42&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: Flink SQL处理Array型的JSON&#013;&#010;&#013;&#010;Hello，可爱的木兰&#013;&#010;&#013;&#010;可以不用改json的，可以用 UNNEST 把数组拆成多行，也可以写UDTF自己解析对应字段，参考[1]&#013;&#010;&#013;&#010;SELECT users, tag&#013;&#010;FROM Orders CROSS JOIN UNNEST(tags) AS t (tag)&#013;&#010;&#013;&#010;Best，&#013;&#010;Leonard Xu&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html &lt;https://ci.apache.org/projects/flink/flink-docs-master/zh/dev/table/sql/queries.html&gt;&#013;&#010;&#013;&#010;&gt; 在 2020年7月14日，10:34，hua mulan &lt;deadwind4@outlook.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt; 可爱的木兰&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<SG2PR02MB3226753A6C7FCFE91D364E95F8610@SG2PR02MB3226.apcprd02.prod.outlook.com>"
    },
    {
        "id": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:42:36 GMT",
        "subject": "flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "hello，&#010;&#010;        当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#010;&#010;&#010;Caused by: java.nio.file.NoSuchFileException:&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#010;-&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#010;&#010;配置和1.9.2 一样：&#010;state.backend: rocksdb&#010;state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#010;state.savepoints.dir: hdfs:///flink/savepoints/wc/&#010;state.backend.incremental: true&#010;&#010;代码上都有&#010;&#010;env.enableCheckpointing(10000);&#010;env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#010;org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#010;&#010;&#010;          是1.10.0 需要做什么特别配置么？&#010;&#010;",
        "depth": "0",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<DM5PR2001MB17850B4F49922B29B787D614DA610@DM5PR2001MB1785.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:57:35 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Peihui&#013;&#010;&#013;&#010;你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#010;cause。&#013;&#010;&#013;&#010;[1] https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#013;&#010;&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;Sent: Tuesday, July 14, 2020 10:42&#013;&#010;To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&#013;&#010;hello，&#013;&#010;&#013;&#010;        当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&#013;&#010;&#013;&#010;Caused by: java.nio.file.NoSuchFileException:&#013;&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;-&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&#013;&#010;配置和1.9.2 一样：&#013;&#010;state.backend: rocksdb&#013;&#010;state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;state.backend.incremental: true&#013;&#010;&#013;&#010;代码上都有&#013;&#010;&#013;&#010;env.enableCheckpointing(10000);&#013;&#010;env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&#013;&#010;&#013;&#010;          是1.10.0 需要做什么特别配置么？&#013;&#010;",
        "depth": "1",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvs+a1gV4GefTRqRZ8p_YRwH7ECLBZ4pFK5WLvf-+_L6UA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 05:54:04 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi&#013;&#010;&#013;&#010;这个出错是从 1.9 升级到 1.10 遇到的问题，还是说 1.10 能正常跑了，然后跑着跑着&#010;failover 了再次恢复的时候出错了呢？&#013;&#010;另外你可以看下 tm log 看看有没有其他异常&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:57写道：&#013;&#010;&#013;&#010;&gt; Hi Peihui&#013;&#010;&gt;&#013;&#010;&gt; 你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#013;&#010;&gt; cause。&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; 唐云&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&#013;&#010;&gt; hello，&#013;&#010;&gt;&#013;&#010;&gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; -&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&#013;&#010;&gt; 配置和1.9.2 一样：&#013;&#010;&gt; state.backend: rocksdb&#013;&#010;&gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; state.backend.incremental: true&#013;&#010;&gt;&#013;&#010;&gt; 代码上都有&#013;&#010;&gt;&#013;&#010;&gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpZkX8Y_vp57DtWNW-tZ+5e6LVqr24YQvdxEpc9KbEztrw@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:46:15 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Congxian，&#013;&#010;&#013;&#010;这个错误是从1.9 升级到1.10 遇到的问题。用简单的wordcount 测试，自己根据特定word&#013;&#010;抛出runtimeException，就能够重现。flink on yarn 和 flink on k8s 都出现这个问题。1.10&#013;&#010;都不能从上次的checkpoint状态中恢复。不知道是不是1.10需要其他配置呢？&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月14日周二 下午1:54写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 这个出错是从 1.9 升级到 1.10 遇到的问题，还是说 1.10 能正常跑了，然后跑着跑着&#010;failover 了再次恢复的时候出错了呢？&#013;&#010;&gt; 另外你可以看下 tm log 看看有没有其他异常&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:57写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Peihui&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#013;&#010;&gt; &gt; cause。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 祝好&#013;&#010;&gt; &gt; 唐云&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hello，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt; -&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 代码上都有&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvue4su1JUNiqGCwfATKJiVUEQhkMzZzjBwm6FkuwPq+-g@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 05:04:12 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi&#013;&#010;&#013;&#010;我尝试理解一下：&#013;&#010;1 你用 1.9 跑 wordcount 作业，然后执行了一些 checkpoint，然后停止作业，然后使用&#010;1.10 从之前 1.9 的作业生成的&#013;&#010;checkpoint 恢复，发现恢复不了？&#013;&#010;2 你用作业 1.10 跑 wordcount，然后遇到特定的 word 会抛异常，然后 failover，发现不能从&#010;checkpoint 恢复？&#013;&#010;&#013;&#010;你这里的问题是第 1 种还是第 2 种呢？&#013;&#010;&#013;&#010;另外能否分享一下你的操作步骤以及出错时候的 taskmanager log 呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月14日周二 下午2:46写道：&#013;&#010;&#013;&#010;&gt; Hi Congxian，&#013;&#010;&gt;&#013;&#010;&gt; 这个错误是从1.9 升级到1.10 遇到的问题。用简单的wordcount 测试，自己根据特定word&#013;&#010;&gt; 抛出runtimeException，就能够重现。flink on yarn 和 flink on k8s 都出现这个问题。1.10&#013;&#010;&gt; 都不能从上次的checkpoint状态中恢复。不知道是不是1.10需要其他配置呢？&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月14日周二 下午1:54写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 这个出错是从 1.9 升级到 1.10 遇到的问题，还是说 1.10 能正常跑了，然后跑着跑着&#010;failover 了再次恢复的时候出错了呢？&#013;&#010;&gt; &gt; 另外你可以看下 tm log 看看有没有其他异常&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:57写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi Peihui&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#013;&#010;&gt; &gt; &gt; cause。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; [1]&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 祝好&#013;&#010;&gt; &gt; &gt; 唐云&#013;&#010;&gt; &gt; &gt; ________________________________&#013;&#010;&gt; &gt; &gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; &gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; &gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; hello，&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt; &gt; -&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt; &gt; &gt; state.backend: rocksdb&#013;&#010;&gt; &gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; &gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; &gt; &gt; state.backend.incremental: true&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 代码上都有&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; &gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; &gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpZ5tyN8J5cST7+96r6vDfuqir-RwziG=tmFB2pL2Loq5g@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:05:06 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi  Congxian,&#013;&#010;&#013;&#010;不好意思，本来想准备下例子再回下邮件的，一直拖了这么久。&#013;&#010;情况是你说的第2种。&#013;&#010;同@chenxyz遇到的情况类似，日志可以参考chenxyz发的&#013;&#010;http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&#013;&#010;&#013;&#010;按照chenxyz 的建议换了1.10.1版本后就没有问题了。&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月15日周三 下午1:04写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt;&#013;&#010;&gt; 我尝试理解一下：&#013;&#010;&gt; 1 你用 1.9 跑 wordcount 作业，然后执行了一些 checkpoint，然后停止作业，然后使用&#010;1.10 从之前 1.9 的作业生成的&#013;&#010;&gt; checkpoint 恢复，发现恢复不了？&#013;&#010;&gt; 2 你用作业 1.10 跑 wordcount，然后遇到特定的 word 会抛异常，然后&#010;failover，发现不能从 checkpoint 恢复？&#013;&#010;&gt;&#013;&#010;&gt; 你这里的问题是第 1 种还是第 2 种呢？&#013;&#010;&gt;&#013;&#010;&gt; 另外能否分享一下你的操作步骤以及出错时候的 taskmanager log 呢？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月14日周二 下午2:46写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Congxian，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 这个错误是从1.9 升级到1.10 遇到的问题。用简单的wordcount 测试，自己根据特定word&#013;&#010;&gt; &gt; 抛出runtimeException，就能够重现。flink on yarn 和 flink on k8s 都出现这个问题。1.10&#013;&#010;&gt; &gt; 都不能从上次的checkpoint状态中恢复。不知道是不是1.10需要其他配置呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best wishes.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月14日周二 下午1:54写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 这个出错是从 1.9 升级到 1.10 遇到的问题，还是说 1.10 能正常跑了，然后跑着跑着&#010;failover 了再次恢复的时候出错了呢？&#013;&#010;&gt; &gt; &gt; 另外你可以看下 tm log 看看有没有其他异常&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:57写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hi Peihui&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#013;&#010;&gt; &gt; &gt; &gt; cause。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; [1]&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 祝好&#013;&#010;&gt; &gt; &gt; &gt; 唐云&#013;&#010;&gt; &gt; &gt; &gt; ________________________________&#013;&#010;&gt; &gt; &gt; &gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; &gt; &gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; &gt; &gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt; &gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; hello，&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt; &gt; &gt; -&gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt; &gt; &gt; &gt; state.backend: rocksdb&#013;&#010;&gt; &gt; &gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; &gt; &gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; &gt; &gt; &gt; state.backend.incremental: true&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 代码上都有&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; &gt; &gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; &gt; &gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpa3+tfw17r6A3MzjcLsNnov=48h_cCn7Lj778-RLnKYnA@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:41:53 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Yun，&#013;&#010;&#013;&#010;我这边用一个word count 例子，socket -&gt; flatmap -&gt; keyBy -&gt; reduce -&gt;&#013;&#010;print. 在flatmap 中当出现特定word的时候就抛出一个runtimeException。在1.9.2&#013;&#010;里面是可以从checkpoint中自动恢复上次做checkpoint的时候的状态，但是用1.10.0&#010;就不能。环境是flink on&#013;&#010;yarn。&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:57写道：&#013;&#010;&#013;&#010;&gt; Hi Peihui&#013;&#010;&gt;&#013;&#010;&gt; 你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#013;&#010;&gt; cause。&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; 唐云&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&#013;&#010;&gt; hello，&#013;&#010;&gt;&#013;&#010;&gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; -&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&#013;&#010;&gt; 配置和1.9.2 一样：&#013;&#010;&gt; state.backend: rocksdb&#013;&#010;&gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; state.backend.incremental: true&#013;&#010;&gt;&#013;&#010;&gt; 代码上都有&#013;&#010;&gt;&#013;&#010;&gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<141378c.9085.17352bfcafc.Coremail.chenxyz@163.com>",
        "from": "chenxyz  &lt;chen...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 13:52:47 GMT",
        "subject": "Re:Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "&#010;&#010;&#010;Hello，&#010;Peihui，可以参考下是不是和这个问题类似？之前我在1.10.0也遇到过。&#010;http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html#a2239&#010;解决方式：&#010;1. 使用hdfs作为状态后端不会报错&#010;2. 升级至1.10.1使用rocksdb也不会出现该问题&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 14:41:53，\"Peihui He\" &lt;peihuihe@gmail.com&gt; 写道：&#010;&gt;Hi Yun，&#010;&gt;&#010;&gt;我这边用一个word count 例子，socket -&gt; flatmap -&gt; keyBy -&gt; reduce -&gt;&#010;&gt;print. 在flatmap 中当出现特定word的时候就抛出一个runtimeException。在1.9.2&#010;&gt;里面是可以从checkpoint中自动恢复上次做checkpoint的时候的状态，但是用1.10.0&#010;就不能。环境是flink on&#010;&gt;yarn。&#010;&gt;&#010;&gt;Best wishes.&#010;&gt;&#010;&gt;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:57写道：&#010;&gt;&#010;&gt;&gt; Hi Peihui&#010;&gt;&gt;&#010;&gt;&gt; 你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#010;&gt;&gt; cause。&#010;&gt;&gt;&#010;&gt;&gt; [1]&#010;&gt;&gt; https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 祝好&#010;&gt;&gt; 唐云&#010;&gt;&gt; ________________________________&#010;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#010;&gt;&gt; Sent: Tuesday, July 14, 2020 10:42&#010;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#010;&gt;&gt;&#010;&gt;&gt; hello，&#010;&gt;&gt;&#010;&gt;&gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#010;&gt;&gt;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#010;&gt;&gt; -&gt;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#010;&gt;&gt;&#010;&gt;&gt; 配置和1.9.2 一样：&#010;&gt;&gt; state.backend: rocksdb&#010;&gt;&gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#010;&gt;&gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#010;&gt;&gt; state.backend.incremental: true&#010;&gt;&gt;&#010;&gt;&gt; 代码上都有&#010;&gt;&gt;&#010;&gt;&gt; env.enableCheckpointing(10000);&#010;&gt;&gt;&#010;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;&gt;&gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#010;&gt;&gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;           是1.10.0 需要做什么特别配置么？&#010;&gt;&gt;&#010;",
        "depth": "3",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpYW_5X+=4hTaAvGaY_ipbnk39FGonRjap4=bnAYUh-YwQ@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:06:56 GMT",
        "subject": "Re: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi chenxyz,&#013;&#010;&#013;&#010;我们遇到的问题应该是一样的，换了1.10.1 后就可以从checkpoint 中恢复了。🤗&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;chenxyz &lt;chenxyz@163.com&gt; 于2020年7月15日周三 下午9:53写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Hello，&#013;&#010;&gt; Peihui，可以参考下是不是和这个问题类似？之前我在1.10.0也遇到过。&#013;&#010;&gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html#a2239&#013;&#010;&gt; 解决方式：&#013;&#010;&gt; 1. 使用hdfs作为状态后端不会报错&#013;&#010;&gt; 2. 升级至1.10.1使用rocksdb也不会出现该问题&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-14 14:41:53，\"Peihui He\" &lt;peihuihe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;Hi Yun，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;我这边用一个word count 例子，socket -&gt; flatmap -&gt; keyBy -&gt; reduce&#010;-&gt;&#013;&#010;&gt; &gt;print. 在flatmap 中当出现特定word的时候就抛出一个runtimeException。在1.9.2&#013;&#010;&gt; &gt;里面是可以从checkpoint中自动恢复上次做checkpoint的时候的状态，但是用1.10.0&#010;就不能。环境是flink on&#013;&#010;&gt; &gt;yarn。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best wishes.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:57写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi Peihui&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; 你的异常应该是从增量Checkpoint恢复时，文件已经下载到本地了，做硬链时[1]，发现源文件不见了，有很大的可能是当时发生了异常，导致restore流程退出了，所以这个问题应该不是root&#013;&#010;&gt; &gt;&gt; cause。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://github.com/apache/flink/blob/2a3b642b1efb957f3d4f20502c40398786ab1469/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/restore/RocksDBIncrementalRestoreOperation.java#L473&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 祝好&#013;&#010;&gt; &gt;&gt; 唐云&#013;&#010;&gt; &gt;&gt; ________________________________&#013;&#010;&gt; &gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; &gt;&gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; &gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; hello，&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt;&gt; -&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 配置和1.9.2 一样：&#013;&#010;&gt; &gt;&gt; state.backend: rocksdb&#013;&#010;&gt; &gt;&gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; &gt;&gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; &gt;&gt; state.backend.incremental: true&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 代码上都有&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; env.enableCheckpointing(10000);&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; &gt;&gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; &gt;&gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<1594801437339-0.post@n8.nabble.com>",
        "from": "Robin Zhang &lt;vincent2015q...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 08:23:57 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#010;&#010;Best&#010;Robin Zhang&#010;________________________________&#010;From: Peihui He &lt;[hidden email]&gt;&#010;Sent: Tuesday, July 14, 2020 10:42&#010;To: [hidden email] &lt;[hidden email]&gt;&#010;Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#010;&#010;hello，&#010;&#010;        当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#010;&#010;&#010;Caused by: java.nio.file.NoSuchFileException:&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#010;-&gt;&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#010;&#010;配置和1.9.2 一样：&#010;state.backend: rocksdb&#010;state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#010;state.savepoints.dir: hdfs:///flink/savepoints/wc/&#010;state.backend.incremental: true&#010;&#010;代码上都有&#010;&#010;env.enableCheckpointing(10000);&#010;env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#010;env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#010;org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#010;&#010;&#010;          是1.10.0 需要做什么特别配置么？&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<DM5PR2001MB17856EEC9D62FD9AC34B6545DA7E0@DM5PR2001MB1785.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 08:35:11 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Robin&#013;&#010;&#013;&#010;其实你的说法不是很准确，社区是明文保证savepoint的兼容性 [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#010;schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&#013;&#010;另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#010;cause，还请在日志中找一下无法恢复的root cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&#013;&#010;&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;[2] https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;&#013;&#010;&#013;&#010;________________________________&#013;&#010;From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;Sent: Wednesday, July 15, 2020 16:23&#013;&#010;To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&#013;&#010;据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&#013;&#010;Best&#013;&#010;Robin Zhang&#013;&#010;________________________________&#013;&#010;From: Peihui He &lt;[hidden email]&gt;&#013;&#010;Sent: Tuesday, July 14, 2020 10:42&#013;&#010;To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&#013;&#010;hello，&#013;&#010;&#013;&#010;        当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&#013;&#010;&#013;&#010;Caused by: java.nio.file.NoSuchFileException:&#013;&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;-&gt;&#013;&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&#013;&#010;配置和1.9.2 一样：&#013;&#010;state.backend: rocksdb&#013;&#010;state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;state.backend.incremental: true&#013;&#010;&#013;&#010;代码上都有&#013;&#010;&#013;&#010;env.enableCheckpointing(10000);&#013;&#010;env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&#013;&#010;&#013;&#010;          是1.10.0 需要做什么特别配置么？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;",
        "depth": "2",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpayr+Yd2TqJ618OYHAGUfn+WPswerBG5iVx2b9x3=2mkQ@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:15:18 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Yun,&#013;&#010;&#013;&#010;不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;输入的特定的word抛出runtimeexception 使task&#013;&#010;失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&#013;&#010;Caused by: java.nio.file.NoSuchFileException:&#013;&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;-&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&#013;&#010;情况和@chenxyz 类似。&#013;&#010;http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&#013;&#010;换成1.10.1 就可以了&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&#013;&#010;&gt; Hi Robin&#013;&#010;&gt;&#013;&#010;&gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt;&#013;&#010;&gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#010;cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt; [2]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; 唐云&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&#013;&#010;&gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Robin Zhang&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&#013;&#010;&gt; hello，&#013;&#010;&gt;&#013;&#010;&gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; -&gt;&#013;&#010;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&#013;&#010;&gt; 配置和1.9.2 一样：&#013;&#010;&gt; state.backend: rocksdb&#013;&#010;&gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; state.backend.incremental: true&#013;&#010;&gt;&#013;&#010;&gt; 代码上都有&#013;&#010;&gt;&#013;&#010;&gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<DM5PR2001MB178571CED937CDB1BCD6C4EFDA7F0@DM5PR2001MB1785.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 09:04:44 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Peihui&#013;&#010;&#013;&#010;Flink-1.10.1 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&#013;&#010;&#013;&#010;[1] https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;Sent: Thursday, July 16, 2020 16:15&#013;&#010;To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&#013;&#010;Hi Yun,&#013;&#010;&#013;&#010;不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;输入的特定的word抛出runtimeexception 使task&#013;&#010;失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&#013;&#010;Caused by: java.nio.file.NoSuchFileException:&#013;&#010;/data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;-&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&#013;&#010;情况和@chenxyz 类似。&#013;&#010;http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&#013;&#010;换成1.10.1 就可以了&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&#013;&#010;&gt; Hi Robin&#013;&#010;&gt;&#013;&#010;&gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt;&#013;&#010;&gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#010;cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt; [2]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; 唐云&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&#013;&#010;&gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Robin Zhang&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&#013;&#010;&gt; hello，&#013;&#010;&gt;&#013;&#010;&gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; -&gt;&#013;&#010;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&#013;&#010;&gt; 配置和1.9.2 一样：&#013;&#010;&gt; state.backend: rocksdb&#013;&#010;&gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; state.backend.incremental: true&#013;&#010;&gt;&#013;&#010;&gt; 代码上都有&#013;&#010;&gt;&#013;&#010;&gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpY8ohmAOw828Py1QSsgLE1Z2D-Jw55ymRb3upA77=uuGQ@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 09:26:01 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Yun，&#013;&#010;&#013;&#010;作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四 下午5:04写道：&#013;&#010;&#013;&#010;&gt; Hi Peihui&#013;&#010;&gt;&#013;&#010;&gt; Flink-1.10.1&#013;&#010;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt; 祝好&#013;&#010;&gt; 唐云&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&#013;&#010;&gt; Hi Yun,&#013;&#010;&gt;&#013;&#010;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt;&#013;&#010;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; -&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&#013;&#010;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt;&#013;&#010;&gt; 换成1.10.1 就可以了&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Robin&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt; &gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#010;cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt; &gt; [2]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 祝好&#013;&#010;&gt; &gt; 唐云&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best&#013;&#010;&gt; &gt; Robin Zhang&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hello，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt; -&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 代码上都有&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpZ6AUDRG1zHrDAf5s5747oFytEpyi_J+JeV+hi+FG-cuw@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 09:53:49 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Yun,&#013;&#010;&#013;&#010;我这边测试需要在集群上跑的，本地idea跑是没有问题的。&#013;&#010;flink 1.10.1 的flink-conf.yaml 是cope flink 1.10.0 的，但是1.10.0 就是报错。&#013;&#010;&#013;&#010;附件就是源码job。如果你要的跑需要改下socket host的。只要socket 中输入hepeihui&#010;就会抛异常的。&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:26写道：&#013;&#010;&#013;&#010;&gt; Hi Yun，&#013;&#010;&gt;&#013;&#010;&gt; 作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四 下午5:04写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi Peihui&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Flink-1.10.1&#013;&#010;&gt;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; [1]&#013;&#010;&gt;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt;&gt; 祝好&#013;&#010;&gt;&gt; 唐云&#013;&#010;&gt;&gt; ________________________________&#013;&#010;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Hi Yun,&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt; -&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 换成1.10.1 就可以了&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best wishes.&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt; Hi Robin&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt;&gt; &gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#010;cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; [1]&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt;&gt; &gt; [2]&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 祝好&#013;&#010;&gt;&gt; &gt; 唐云&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; Best&#013;&#010;&gt;&gt; &gt; Robin Zhang&#013;&#010;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; hello，&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt; &gt; -&gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 代码上都有&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; --&#013;&#010;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvuPCDRO1Z+Fb8Z6ir8X-OkfksEXHJ2CCpyBS3U0bRjiPA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 12:23:48 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Peihui&#013;&#010;&#013;&#010;感谢你的回信。能否帮忙用 1.10.0 复现一次，然后把相关的日志(JM log&#010;和 TM Log，方便的话，也开启一下 debug&#013;&#010;日志）分享一下呢？如果日志太大的话，可以尝试贴待 gist[1] 然后邮件列表回复一个地址即可，&#013;&#010;非常感谢~&#013;&#010;&#013;&#010;[1] https://gist.github.com/&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:54写道：&#013;&#010;&#013;&#010;&gt; Hi Yun,&#013;&#010;&gt;&#013;&#010;&gt; 我这边测试需要在集群上跑的，本地idea跑是没有问题的。&#013;&#010;&gt; flink 1.10.1 的flink-conf.yaml 是cope flink 1.10.0 的，但是1.10.0 就是报错。&#013;&#010;&gt;&#013;&#010;&gt; 附件就是源码job。如果你要的跑需要改下socket host的。只要socket 中输入hepeihui&#010;就会抛异常的。&#013;&#010;&gt;&#013;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:26写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi Yun，&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best wishes.&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四 下午5:04写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Hi Peihui&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Flink-1.10.1&#013;&#010;&gt;&gt;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt;&gt;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; [1]&#013;&#010;&gt;&gt;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt;&gt;&gt; 祝好&#013;&#010;&gt;&gt;&gt; 唐云&#013;&#010;&gt;&gt;&gt; ________________________________&#013;&#010;&gt;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt;&gt;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Hi Yun,&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt;&gt;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt;&gt;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt;&gt; -&gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 换成1.10.1 就可以了&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Best wishes.&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt; Hi Robin&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt;&gt;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#013;&#010;&gt;&gt;&gt; cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt;&gt;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; [1]&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt;&gt;&gt; &gt; [2]&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; 祝好&#013;&#010;&gt;&gt;&gt; &gt; 唐云&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt;&gt;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt;&gt;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; Best&#013;&#010;&gt;&gt;&gt; &gt; Robin Zhang&#013;&#010;&gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt;&gt;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt;&gt;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt;&gt;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; hello，&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt;&gt; &gt; -&gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt;&gt;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt;&gt;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt;&gt;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt;&gt;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; 代码上都有&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt;&gt;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt;&gt;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; --&#013;&#010;&gt;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpZ0XzeyYx5bPcMc5dRwSQzLgvSxhr0tcCdhEoDxdL7rsA@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:12:52 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Congxian&#013;&#010;&#013;&#010;见附件。&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月16日周四 下午8:24写道：&#013;&#010;&#013;&#010;&gt; Hi Peihui&#013;&#010;&gt;&#013;&#010;&gt; 感谢你的回信。能否帮忙用 1.10.0 复现一次，然后把相关的日志(JM&#010;log 和 TM Log，方便的话，也开启一下 debug&#013;&#010;&gt; 日志）分享一下呢？如果日志太大的话，可以尝试贴待 gist[1] 然后邮件列表回复一个地址即可，&#013;&#010;&gt; 非常感谢~&#013;&#010;&gt;&#013;&#010;&gt; [1] https://gist.github.com/&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:54写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Yun,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我这边测试需要在集群上跑的，本地idea跑是没有问题的。&#013;&#010;&gt; &gt; flink 1.10.1 的flink-conf.yaml 是cope flink 1.10.0 的，但是1.10.0 就是报错。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 附件就是源码job。如果你要的跑需要改下socket host的。只要socket&#010;中输入hepeihui 就会抛异常的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:26写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi Yun，&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四 下午5:04写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Hi Peihui&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Flink-1.10.1&#013;&#010;&gt; &gt;&gt;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt; &gt;&gt;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; [1]&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt; &gt;&gt;&gt; 祝好&#013;&#010;&gt; &gt;&gt;&gt; 唐云&#013;&#010;&gt; &gt;&gt;&gt; ________________________________&#013;&#010;&gt; &gt;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; &gt;&gt;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt; &gt;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Hi Yun,&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt; &gt;&gt;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt; &gt;&gt;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt;&gt;&gt; -&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; 换成1.10.1 就可以了&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Hi Robin&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt; &gt;&gt;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#013;&#010;&gt; &gt;&gt;&gt; cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt; &gt;&gt;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; [1]&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt; &gt;&gt;&gt; &gt; [2]&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 祝好&#013;&#010;&gt; &gt;&gt;&gt; &gt; 唐云&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt; &gt;&gt;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt; &gt;&gt;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Best&#013;&#010;&gt; &gt;&gt;&gt; &gt; Robin Zhang&#013;&#010;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt; &gt;&gt;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; &gt;&gt;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; hello，&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt;&gt;&gt; &gt; -&gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt; &gt;&gt;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt; &gt;&gt;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; &gt;&gt;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; &gt;&gt;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; 代码上都有&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; &gt;&gt;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; &gt;&gt;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt; &gt; --&#013;&#010;&gt; &gt;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvtnEKR1K6EDNs8rcLAZvwbq4Y06fFH2k=j_tnguqKw7hw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 05:31:22 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi  Peihui&#013;&#010;&#013;&#010;感谢你的回复，我这边没有看到附件，你那边能否确认下呢？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月17日周五 上午10:13写道：&#013;&#010;&#013;&#010;&gt; Hi Congxian&#013;&#010;&gt;&#013;&#010;&gt; 见附件。&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月16日周四 下午8:24写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi Peihui&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 感谢你的回信。能否帮忙用 1.10.0 复现一次，然后把相关的日志(JM&#010;log 和 TM Log，方便的话，也开启一下 debug&#013;&#010;&gt;&gt; 日志）分享一下呢？如果日志太大的话，可以尝试贴待 gist[1]&#010;然后邮件列表回复一个地址即可，&#013;&#010;&gt;&gt; 非常感谢~&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; [1] https://gist.github.com/&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best,&#013;&#010;&gt;&gt; Congxian&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:54写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt; Hi Yun,&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 我这边测试需要在集群上跑的，本地idea跑是没有问题的。&#013;&#010;&gt;&gt; &gt; flink 1.10.1 的flink-conf.yaml 是cope flink 1.10.0 的，但是1.10.0 就是报错。&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 附件就是源码job。如果你要的跑需要改下socket host的。只要socket&#010;中输入hepeihui 就会抛异常的。&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:26写道：&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; Hi Yun，&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; 作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; Best wishes.&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四 下午5:04写道：&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Hi Peihui&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Flink-1.10.1&#013;&#010;&gt;&gt; &gt;&gt;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt;&gt; &gt;&gt;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; [1]&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt;&gt; &gt;&gt;&gt; 祝好&#013;&#010;&gt;&gt; &gt;&gt;&gt; 唐云&#013;&#010;&gt;&gt; &gt;&gt;&gt; ________________________________&#013;&#010;&gt;&gt; &gt;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt;&gt; &gt;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Hi Yun,&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt;&gt; &gt;&gt;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt;&gt; &gt;&gt;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt; &gt;&gt;&gt; -&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; 换成1.10.1 就可以了&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Best wishes.&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Hi Robin&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#013;&#010;&gt;&gt; &gt;&gt;&gt; cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; [1]&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; [2]&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; 祝好&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; 唐云&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Best&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Robin Zhang&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; hello，&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; -&gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; 代码上都有&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; --&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "9",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpYLXu=SyFCDSRaZOO8krQn+Q_Ggx9FYfJV5QkadKLSXqg@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 08:21:33 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Congxian,&#013;&#010;&#013;&#010;[image: Snipaste_2020-07-17_16-20-06.png]&#013;&#010;&#013;&#010;我这边通过chrome 浏览器看到是上传了的，并且可以下载的。&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月17日周五 下午1:31写道：&#013;&#010;&#013;&#010;&gt; Hi  Peihui&#013;&#010;&gt;&#013;&#010;&gt; 感谢你的回复，我这边没有看到附件，你那边能否确认下呢？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月17日周五 上午10:13写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 见附件。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best wishes.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月16日周四 下午8:24写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi Peihui&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 感谢你的回信。能否帮忙用 1.10.0 复现一次，然后把相关的日志(JM&#010;log 和 TM Log，方便的话，也开启一下 debug&#013;&#010;&gt; &gt;&gt; 日志）分享一下呢？如果日志太大的话，可以尝试贴待 gist[1]&#010;然后邮件列表回复一个地址即可，&#013;&#010;&gt; &gt;&gt; 非常感谢~&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; [1] https://gist.github.com/&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Best,&#013;&#010;&gt; &gt;&gt; Congxian&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:54写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt; Hi Yun,&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 我这边测试需要在集群上跑的，本地idea跑是没有问题的。&#013;&#010;&gt; &gt;&gt; &gt; flink 1.10.1 的flink-conf.yaml 是cope flink 1.10.0 的，但是1.10.0&#010;就是报错。&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; 附件就是源码job。如果你要的跑需要改下socket host的。只要socket&#010;中输入hepeihui 就会抛异常的。&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:26写道：&#013;&#010;&gt; &gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; Hi Yun，&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; 作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四 下午5:04写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Hi Peihui&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Flink-1.10.1&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; [1]&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 祝好&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 唐云&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; ________________________________&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Hi Yun,&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; -&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; 换成1.10.1 就可以了&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Best wishes.&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三 下午4:35写道：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Robin&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; [1]&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; [2]&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 祝好&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 唐云&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Best&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Robin Zhang&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; hello，&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; -&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 代码上都有&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; --&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "10",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpbD1KrDDb5Kh_Mna0RbMLkguV=cEwJxqZ8HMnT+tiJKzg@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 00:56:55 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Congxian,&#013;&#010;&#013;&#010;这个问题有结论没呢?&#013;&#010;&#013;&#010;Best wishes.&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月17日周五 下午4:21写道：&#013;&#010;&#013;&#010;&gt; Hi Congxian,&#013;&#010;&gt;&#013;&#010;&gt; [image: Snipaste_2020-07-17_16-20-06.png]&#013;&#010;&gt;&#013;&#010;&gt; 我这边通过chrome 浏览器看到是上传了的，并且可以下载的。&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月17日周五 下午1:31写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi  Peihui&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 感谢你的回复，我这边没有看到附件，你那边能否确认下呢？&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best,&#013;&#010;&gt;&gt; Congxian&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月17日周五 上午10:13写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; &gt; Hi Congxian&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; 见附件。&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; Best wishes.&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月16日周四 下午8:24写道：&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; Hi Peihui&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; 感谢你的回信。能否帮忙用 1.10.0 复现一次，然后把相关的日志(JM&#010;log 和 TM Log，方便的话，也开启一下 debug&#013;&#010;&gt;&gt; &gt;&gt; 日志）分享一下呢？如果日志太大的话，可以尝试贴待&#010;gist[1] 然后邮件列表回复一个地址即可，&#013;&#010;&gt;&gt; &gt;&gt; 非常感谢~&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; [1] https://gist.github.com/&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; Best,&#013;&#010;&gt;&gt; &gt;&gt; Congxian&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:54写道：&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt; Hi Yun,&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt; 我这边测试需要在集群上跑的，本地idea跑是没有问题的。&#013;&#010;&gt;&gt; &gt;&gt; &gt; flink 1.10.1 的flink-conf.yaml 是cope flink 1.10.0 的，但是1.10.0&#010;就是报错。&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt; 附件就是源码job。如果你要的跑需要改下socket host的。只要socket&#010;中输入hepeihui 就会抛异常的。&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:26写道：&#013;&#010;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Hi Yun，&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt; 作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Best wishes.&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四 下午5:04写道：&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hi Peihui&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Flink-1.10.1&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; [1]&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 祝好&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 唐云&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; ________________________________&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hi Yun,&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; -&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 换成1.10.1 就可以了&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best wishes.&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三&#010;下午4:35写道：&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Robin&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; [1]&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; [2]&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 祝好&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 唐云&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Best&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Robin Zhang&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; hello，&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; -&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 代码上都有&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; org.apache.flink.api.common.time.Time.of(10, TimeUnit.SECONDS)));&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; --&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "11",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvtK2fpgsxr08zZ9EASVh1Mirm+XoDS0VtfHVQJ6Pq_suA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 05:18:31 GMT",
        "subject": "Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复",
        "content": "Hi Peihui&#013;&#010;   不确定是什么原因我这边暂时没看到附件，我再私聊你要一下具体的&#010;log 然后看看&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月23日周四 上午8:57写道：&#013;&#010;&#013;&#010;&gt; Hi Congxian,&#013;&#010;&gt;&#013;&#010;&gt; 这个问题有结论没呢?&#013;&#010;&gt;&#013;&#010;&gt; Best wishes.&#013;&#010;&gt;&#013;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月17日周五 下午4:21写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi Congxian,&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; [image: Snipaste_2020-07-17_16-20-06.png]&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; 我这边通过chrome 浏览器看到是上传了的，并且可以下载的。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Best wishes.&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月17日周五 下午1:31写道：&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Hi  Peihui&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 感谢你的回复，我这边没有看到附件，你那边能否确认下呢？&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Best,&#013;&#010;&gt;&gt;&gt; Congxian&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月17日周五 上午10:13写道：&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt; Hi Congxian&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; 见附件。&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; Best wishes.&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月16日周四 下午8:24写道：&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; Hi Peihui&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; 感谢你的回信。能否帮忙用 1.10.0 复现一次，然后把相关的日志(JM&#010;log 和 TM Log，方便的话，也开启一下 debug&#013;&#010;&gt;&gt;&gt; &gt;&gt; 日志）分享一下呢？如果日志太大的话，可以尝试贴待&#010;gist[1] 然后邮件列表回复一个地址即可，&#013;&#010;&gt;&gt;&gt; &gt;&gt; 非常感谢~&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; [1] https://gist.github.com/&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; Best,&#013;&#010;&gt;&gt;&gt; &gt;&gt; Congxian&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四 下午5:54写道：&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt; Hi Yun,&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt; 我这边测试需要在集群上跑的，本地idea跑是没有问题的。&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt; flink 1.10.1 的flink-conf.yaml 是cope flink 1.10.0 的，但是1.10.0&#010;就是报错。&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt; 附件就是源码job。如果你要的跑需要改下socket host的。只要socket&#010;中输入hepeihui 就会抛异常的。&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月16日周四&#010;下午5:26写道：&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt; Hi Yun，&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt; 作业没有开启local recovery， 我这边测试1.10.0是必现的。&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt; Best wishes.&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月16日周四&#010;下午5:04写道：&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hi Peihui&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Flink-1.10.1&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; 里面涉及到相关代码的改动就是更改了restore时path的类[1]，但是你们的操作系统并不是windows，按道理应该是没有关系的。&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 另外，这个问题在你遇到failover时候是必现的么？从文件路径看，作业也没有开启local&#010;recovery是吧？&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; [1]&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; https://github.com/apache/flink/commit/399329275e5e2baca9ed9494cce97ff732ac077a&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 祝好&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 唐云&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; ________________________________&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; From: Peihui He &lt;peihuihe@gmail.com&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Sent: Thursday, July 16, 2020 16:15&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Hi Yun,&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 不好意思这么久回复，是@Congxian 描述的第2种情况。异常就是我通过socket&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 输入的特定的word抛出runtimeexception 使task&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 失败，然后job会尝试从checkpoint中恢复，但是恢复的过程中就报&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; -&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 情况和@chenxyz 类似。&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; http://apache-flink.147419.n8.nabble.com/rocksdb-Could-not-restore-keyed-state-backend-for-KeyedProcessOperator-td2232.html&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; 换成1.10.1 就可以了&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Best wishes.&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; Yun Tang &lt;myasuka@live.com&gt; 于2020年7月15日周三&#010;下午4:35写道：&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Hi Robin&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 其实你的说法不是很准确，社区是明文保证savepoint的兼容性&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; [1]，但是并不意味着跨大版本时无法从checkpoint恢复，社区不承诺主要还是维护其太耗费精力，但是实际从代码角度来说，在合理使用state&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; schema evolution [2]的前提下，目前跨版本checkpoint恢复基本都是兼容的.&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 另外 @Peihui 也请麻烦对你的异常描述清晰一些，我的第一次回复已经推测该异常不是root&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; cause，还请在日志中找一下无法恢复的root&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; cause，如果不知道怎么从日志里面找，可以把相关日志分享出来。&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; [1]&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/upgrading.html#compatibility-table&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; [2]&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/state/schema_evolution.html&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 祝好&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 唐云&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; From: Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent: Wednesday, July 15, 2020 16:23&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Subject: Re: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 据我所知，跨大版本的不能直接从checkoint恢复，只能放弃状态重新跑&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Best&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Robin Zhang&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; ________________________________&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; From: Peihui He &lt;[hidden email]&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent: Tuesday, July 14, 2020 10:42&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; To: [hidden email] &lt;[hidden email]&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Subject: flink 1.9.2 升级 1.10.0 任务失败不能从checkpoint恢复&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; hello，&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;         当升级到1.10.0 时候，程序出错后会尝试从checkpoint恢复，但是总是失败，提示&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Caused by: java.nio.file.NoSuchFileException:&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/db/000009.sst&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; -&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; /data/hadoop/yarn/local/usercache/hdfs/appcache/application_1589438582606_30760/flink-io-26af2be2-2b14-4eab-90d8-9ebb32ace6e3/job_6b6cacb02824b8521808381113f57eff_op_StreamGroupedReduce_54cc3719665e6629c9000e9308537a5e__1_1__uuid_afda2b8b-0b79-449e-88b5-c34c27c1a079/8f609663-4fbb-483f-83c0-de04654310f7/000009.sst&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 配置和1.9.2 一样：&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.backend: rocksdb&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.checkpoints.dir: hdfs:///flink/checkpoints/wc/&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.savepoints.dir: hdfs:///flink/savepoints/wc/&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; state.backend.incremental: true&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; 代码上都有&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; env.enableCheckpointing(10000);&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; org.apache.flink.api.common.time.Time.of(10,&#013;&#010;&gt;&gt;&gt; TimeUnit.SECONDS)));&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;           是1.10.0 需要做什么特别配置么？&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; --&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&gt;&#013;&#010;&gt;&gt;&gt; &gt;&#013;&#010;&gt;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;",
        "depth": "12",
        "reply": "<CAGR9zpaoLCgOw3h1GudONDzcmhw39AgH1E5BXNxurp4u1er45w@mail.gmail.com>"
    },
    {
        "id": "<87D0B776-D5DF-4535-8768-A779F560F913@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 02:50:39 GMT",
        "subject": "自定义的sql connector在sql-cli中运行问题",
        "content": "hi all，&#010;我自定义了一个sql connector，在本地idea里面是调试通过的，数据能正常写入，但是整个flink编译之后，用编译后的包在本地起了standalone集群，在sql-cli中运行报错如下&#010;2020-07-14 10:36:29,148 WARN  org.apache.flink.table.client.cli.CliClient                &#010; [] - Could not execute SQL statement.&#010;org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL update statement.&#010;   at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:698)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:576)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:527)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.cli.CliClient.callInsert(CliClient.java:551) ~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:299) ~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_251]&#010;   at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:200) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:125) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;Caused by: scala.MatchError: null&#010;   at org.apache.flink.table.planner.sinks.TableSinkUtils$.inferSinkPhysicalSchema(TableSinkUtils.scala:165)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateLogicalPhysicalTypesCompatible(TableSinkUtils.scala:305)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:194)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:190)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.Option.map(Option.scala:146) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:190)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:150)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:767)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:571)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.sqlUpdate(StreamTableEnvironmentImpl.java:341)&#010;~[flink-table-api-java-bridge_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$applyUpdate$17(LocalExecutor.java:691)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:246)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:689)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;   ... 9 more&#010;flink版本1.10.1 blink planner&#010;测试的sql为：&#010;Flink SQL&gt; CREATE TABLE prometheus_table (value DOUBLE )WITH ('connector.type' = 'prometheus','connector.job'&#010;= 'testJob','connector.metrics' = 'testMetrics','connector.address' = 'localhost:9091');&#010;Flink SQL&gt; insert into prometheus_table select cast(100.01 as double) as value;&#010;看报错的地方应该是&#010;def inferSinkPhysicalSchema(&#010;    queryLogicalType: RowType,&#010;    sink: TableSink[_]): TableSchema = {&#010;  val withChangeFlag = sink match {&#010;    case _: RetractStreamTableSink[_] | _: UpsertStreamTableSink[_] =&gt; true&#010;    case _: StreamTableSink[_] =&gt; false&#010;    case dsts: DataStreamTableSink[_] =&gt; dsts.withChangeFlag&#010;  }&#010;  inferSinkPhysicalSchema(sink.getConsumedDataType, queryLogicalType, withChangeFlag)&#010;}&#010;sink没有match到，但是我的tablesink是实现了AppendStreamTableSink的&#010;想远程debug调试一下，按照网上的方法[1]也没成功&#010;&#010;大佬们有没有什么思路指导一下。感谢&#010;&#010;[1]https://blog.csdn.net/xianzhen376/article/details/80117637 &lt;https://blog.csdn.net/xianzhen376/article/details/80117637&gt;&#010;",
        "depth": "0",
        "reply": "<87D0B776-D5DF-4535-8768-A779F560F913@163.com>"
    },
    {
        "id": "<0F525F03-CD72-47D8-AB47-E8229BB9C00C@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 13:26:55 GMT",
        "subject": "Re: 自定义的sql connector在sql-cli中运行问题",
        "content": "解决了，原因是我同时实现了createTableSink和createStreamTableSink导致&#010;删掉createTableSink就可以了&#010;&#010;&#010;&gt; 2020年7月14日 上午10:50，admin &lt;17626017841@163.com&gt; 写道：&#010;&gt; &#010;&gt; hi all，&#010;&gt; 我自定义了一个sql connector，在本地idea里面是调试通过的，数据能正常写入，但是整个flink编译之后，用编译后的包在本地起了standalone集群，在sql-cli中运行报错如下&#010;&gt; 2020-07-14 10:36:29,148 WARN  org.apache.flink.table.client.cli.CliClient           &#010;      [] - Could not execute SQL statement.&#010;&gt; org.apache.flink.table.client.gateway.SqlExecutionException: Invalid SQL update statement.&#010;&gt;    at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:698)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdateInternal(LocalExecutor.java:576)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.gateway.local.LocalExecutor.executeUpdate(LocalExecutor.java:527)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.cli.CliClient.callInsert(CliClient.java:551) ~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.cli.CliClient.callCommand(CliClient.java:299) ~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_251]&#010;&gt;    at org.apache.flink.table.client.cli.CliClient.open(CliClient.java:200) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:125) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.SqlClient.start(SqlClient.java:104) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.SqlClient.main(SqlClient.java:178) [flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt; Caused by: scala.MatchError: null&#010;&gt;    at org.apache.flink.table.planner.sinks.TableSinkUtils$.inferSinkPhysicalSchema(TableSinkUtils.scala:165)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.planner.sinks.TableSinkUtils$.validateLogicalPhysicalTypesCompatible(TableSinkUtils.scala:305)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:194)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$2.apply(PlannerBase.scala:190)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.Option.map(Option.scala:146) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:190)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.planner.delegation.PlannerBase$$anonfun$1.apply(PlannerBase.scala:150)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#010;~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.Iterator$class.foreach(Iterator.scala:891) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-dist_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:150)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:767)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlUpdate(TableEnvironmentImpl.java:571)&#010;~[flink-table-blink_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl.sqlUpdate(StreamTableEnvironmentImpl.java:341)&#010;~[flink-table-api-java-bridge_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.gateway.local.LocalExecutor.lambda$applyUpdate$17(LocalExecutor.java:691)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.gateway.local.ExecutionContext.wrapClassLoader(ExecutionContext.java:246)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    at org.apache.flink.table.client.gateway.local.LocalExecutor.applyUpdate(LocalExecutor.java:689)&#010;~[flink-sql-client_2.11-1.11-SNAPSHOT.jar:1.11-SNAPSHOT]&#010;&gt;    ... 9 more&#010;&gt; flink版本1.10.1 blink planner&#010;&gt; 测试的sql为：&#010;&gt; Flink SQL&gt; CREATE TABLE prometheus_table (value DOUBLE )WITH ('connector.type' = 'prometheus','connector.job'&#010;= 'testJob','connector.metrics' = 'testMetrics','connector.address' = 'localhost:9091');&#010;&gt; Flink SQL&gt; insert into prometheus_table select cast(100.01 as double) as value;&#010;&gt; 看报错的地方应该是&#010;&gt; def inferSinkPhysicalSchema(&#010;&gt;     queryLogicalType: RowType,&#010;&gt;     sink: TableSink[_]): TableSchema = {&#010;&gt;   val withChangeFlag = sink match {&#010;&gt;     case _: RetractStreamTableSink[_] | _: UpsertStreamTableSink[_] =&gt; true&#010;&gt;     case _: StreamTableSink[_] =&gt; false&#010;&gt;     case dsts: DataStreamTableSink[_] =&gt; dsts.withChangeFlag&#010;&gt;   }&#010;&gt;   inferSinkPhysicalSchema(sink.getConsumedDataType, queryLogicalType, withChangeFlag)&#010;&gt; }&#010;&gt; sink没有match到，但是我的tablesink是实现了AppendStreamTableSink的&#010;&gt; 想远程debug调试一下，按照网上的方法[1]也没成功&#010;&gt; &#010;&gt; 大佬们有没有什么思路指导一下。感谢&#010;&gt; &#010;&gt; [1]https://blog.csdn.net/xianzhen376/article/details/80117637 &lt;https://blog.csdn.net/xianzhen376/article/details/80117637&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<87D0B776-D5DF-4535-8768-A779F560F913@163.com>"
    },
    {
        "id": "<26fa451.531c.1734b480dfe.Coremail.drewfranklin@163.com>",
        "from": "drewfranklin &lt;drewfrank...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:04:40 GMT",
        "subject": "flink cep 如何处理超时事件？",
        "content": "Hello all.&#010; 想请教下各位。&#010; &#010;我有个用户开户超时断点的场景。调研了一下，想通过flink cep 来实现。&#010; &#010;但是我定义pattern 后发现，我的这个没办法在一条事件数据上完成判定。必须借助和上一事件数据比较之后判断是不是超时。&#010;&#010;&#010;想知道该如何定义pattern 能够，取到排序之后前后两个两个事件。",
        "depth": "0",
        "reply": "<26fa451.531c.1734b480dfe.Coremail.drewfranklin@163.com>"
    },
    {
        "id": "<CAOMLN=b-emeHg=0r--LkP8GkLW-80UQH1abQNXzFhNze1DB0qQ@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:27:09 GMT",
        "subject": "Re: flink cep 如何处理超时事件？",
        "content": "Hi drewfranklin,&#013;&#010;&#013;&#010;flink使用event time，然后类似下面这样可以吗？&#013;&#010;Pattern.begin(\"a\").next(\"b\").within(Time.minutes(1));&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;drewfranklin &lt;drewfranklin@163.com&gt; 于2020年7月14日周二 上午11:05写道：&#013;&#010;&#013;&#010;&gt; Hello all.&#013;&#010;&gt;  想请教下各位。&#013;&#010;&gt;&#013;&#010;&gt; 我有个用户开户超时断点的场景。调研了一下，想通过flink cep 来实现。&#013;&#010;&gt;&#013;&#010;&gt; 但是我定义pattern 后发现，我的这个没办法在一条事件数据上完成判定。必须借助和上一事件数据比较之后判断是不是超时。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 想知道该如何定义pattern 能够，取到排序之后前后两个两个事件。&#013;&#010;",
        "depth": "1",
        "reply": "<26fa451.531c.1734b480dfe.Coremail.drewfranklin@163.com>"
    },
    {
        "id": "<CAEZk0438ohrd=O8oQg2NKr=CwXHEL90HugxD1baBqonUqe+AGQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:07:16 GMT",
        "subject": "flink1.9状态及作业迁移",
        "content": "hi：&#013;&#010;flink1.9的检查点或保存点中会保留hadoop集群的nameservice数据吗？现在想将一个集群的flink作业迁移到另一个集群，但两个集群的nameservice名称不一样，迁移会有问题吗？如果有问题的话对应状态保存的nameservice可以修改吗？或者说迁移的时候还有哪些其他需要注意的问题？&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk0438ohrd=O8oQg2NKr=CwXHEL90HugxD1baBqonUqe+AGQ@mail.gmail.com>"
    },
    {
        "id": "<DM5PR2001MB178514BE21E463B7182A95DCDA610@DM5PR2001MB1785.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:53:54 GMT",
        "subject": "Re: flink1.9状态及作业迁移",
        "content": "Checkpoint meta中存储的是完整路径，所以一般会把hdfs的namespace存储起来，导致没办法直接迁移。&#013;&#010;&#013;&#010;Flink-1.11 支持将savepoint（但是不支持Checkpoint）进行位置迁移 [1]，而对于Flink-1.9，二者均不支持。&#013;&#010;&#013;&#010;&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-5763&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;&#013;&#010;________________________________&#013;&#010;From: Dream-底限 &lt;zhangyu@akulaku.com&gt;&#013;&#010;Sent: Tuesday, July 14, 2020 11:07&#013;&#010;To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: flink1.9状态及作业迁移&#013;&#010;&#013;&#010;hi：&#013;&#010;flink1.9的检查点或保存点中会保留hadoop集群的nameservice数据吗？现在想将一个集群的flink作业迁移到另一个集群，但两个集群的nameservice名称不一样，迁移会有问题吗？如果有问题的话对应状态保存的nameservice可以修改吗？或者说迁移的时候还有哪些其他需要注意的问题？&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk0438ohrd=O8oQg2NKr=CwXHEL90HugxD1baBqonUqe+AGQ@mail.gmail.com>"
    },
    {
        "id": "<CAEZk043ta+1g7SZWLF1S+Cx0xZ9DLTg2VqoJsbWqpNMkaCm5bQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:57:50 GMT",
        "subject": "Re: flink1.9状态及作业迁移",
        "content": "hi、&#013;&#010;请问对于下面的情况，Checkpoint meta中存储的hdfs namespace可以修改吗&#013;&#010;》》Checkpoint meta中存储的是完整路径，所以一般会把hdfs的namespace存储起来，导致没办法直接迁移。&#013;&#010;&#013;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:54写道：&#013;&#010;&#013;&#010;&gt; Checkpoint meta中存储的是完整路径，所以一般会把hdfs的namespace存储起来，导致没办法直接迁移。&#013;&#010;&gt;&#013;&#010;&gt; Flink-1.11 支持将savepoint（但是不支持Checkpoint）进行位置迁移 [1]，而对于Flink-1.9，二者均不支持。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-5763&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; 唐云&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Dream-底限 &lt;zhangyu@akulaku.com&gt;&#013;&#010;&gt; Sent: Tuesday, July 14, 2020 11:07&#013;&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; Subject: flink1.9状态及作业迁移&#013;&#010;&gt;&#013;&#010;&gt; hi：&#013;&#010;&gt;&#013;&#010;&gt; flink1.9的检查点或保存点中会保留hadoop集群的nameservice数据吗？现在想将一个集群的flink作业迁移到另一个集群，但两个集群的nameservice名称不一样，迁移会有问题吗？如果有问题的话对应状态保存的nameservice可以修改吗？或者说迁移的时候还有哪些其他需要注意的问题？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk0438ohrd=O8oQg2NKr=CwXHEL90HugxD1baBqonUqe+AGQ@mail.gmail.com>"
    },
    {
        "id": "<DM5PR2001MB17850D9CD0282DA301CBDA3BDA610@DM5PR2001MB1785.namprd20.prod.outlook.com>",
        "from": "Yun Tang &lt;myas...@live.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:15:42 GMT",
        "subject": "Re: flink1.9状态及作业迁移",
        "content": "对于Flink本身机制不支持的场景，可以通过直接修改Checkpoint meta 文件同时将meta以及data文件迁移到新HDFS集群也能做到，加载Checkpoint的具体代码可以参照Checkpoints#loadAndValidateCheckpoint&#010;[1]，而存储Checkpoint的代码可以参照Checkpoints#storeCheckpointMetadata [2]&#013;&#010;&#013;&#010;&#013;&#010;[1] https://github.com/apache/flink/blob/5125b1123dfcfff73b5070401dfccb162959080c/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/Checkpoints.java#L124&#013;&#010;[2] https://github.com/apache/flink/blob/5125b1123dfcfff73b5070401dfccb162959080c/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/Checkpoints.java#L81&#013;&#010;&#013;&#010;&#013;&#010;祝好&#013;&#010;唐云&#013;&#010;________________________________&#013;&#010;From: Dream-底限 &lt;zhangyu@akulaku.com&gt;&#013;&#010;Sent: Tuesday, July 14, 2020 11:57&#013;&#010;To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;Subject: Re: flink1.9状态及作业迁移&#013;&#010;&#013;&#010;hi、&#013;&#010;请问对于下面的情况，Checkpoint meta中存储的hdfs namespace可以修改吗&#013;&#010;》》Checkpoint meta中存储的是完整路径，所以一般会把hdfs的namespace存储起来，导致没办法直接迁移。&#013;&#010;&#013;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:54写道：&#013;&#010;&#013;&#010;&gt; Checkpoint meta中存储的是完整路径，所以一般会把hdfs的namespace存储起来，导致没办法直接迁移。&#013;&#010;&gt;&#013;&#010;&gt; Flink-1.11 支持将savepoint（但是不支持Checkpoint）进行位置迁移 [1]，而对于Flink-1.9，二者均不支持。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-5763&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; 唐云&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; From: Dream-底限 &lt;zhangyu@akulaku.com&gt;&#013;&#010;&gt; Sent: Tuesday, July 14, 2020 11:07&#013;&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; Subject: flink1.9状态及作业迁移&#013;&#010;&gt;&#013;&#010;&gt; hi：&#013;&#010;&gt;&#013;&#010;&gt; flink1.9的检查点或保存点中会保留hadoop集群的nameservice数据吗？现在想将一个集群的flink作业迁移到另一个集群，但两个集群的nameservice名称不一样，迁移会有问题吗？如果有问题的话对应状态保存的nameservice可以修改吗？或者说迁移的时候还有哪些其他需要注意的问题？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk0438ohrd=O8oQg2NKr=CwXHEL90HugxD1baBqonUqe+AGQ@mail.gmail.com>"
    },
    {
        "id": "<6e960a46.bcc9.1734b89e36b.Coremail.chq19970719@163.com>",
        "from": "成欢晴 &lt;chq19970...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:16:34 GMT",
        "subject": "回复：flink1.9状态及作业迁移",
        "content": "退订&#010;&#010;&#010;&#010;&#010;| |&#010;chq19970719&#010;|&#010;|&#010;邮箱：chq19970719@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月14日 12:15，Yun Tang 写道：&#010;对于Flink本身机制不支持的场景，可以通过直接修改Checkpoint meta 文件同时将meta以及data文件迁移到新HDFS集群也能做到，加载Checkpoint的具体代码可以参照Checkpoints#loadAndValidateCheckpoint&#010;[1]，而存储Checkpoint的代码可以参照Checkpoints#storeCheckpointMetadata [2]&#010;&#010;&#010;[1] https://github.com/apache/flink/blob/5125b1123dfcfff73b5070401dfccb162959080c/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/Checkpoints.java#L124&#010;[2] https://github.com/apache/flink/blob/5125b1123dfcfff73b5070401dfccb162959080c/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/Checkpoints.java#L81&#010;&#010;&#010;祝好&#010;唐云&#010;________________________________&#010;From: Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;Sent: Tuesday, July 14, 2020 11:57&#010;To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;Subject: Re: flink1.9状态及作业迁移&#010;&#010;hi、&#010;请问对于下面的情况，Checkpoint meta中存储的hdfs namespace可以修改吗&#010;》》Checkpoint meta中存储的是完整路径，所以一般会把hdfs的namespace存储起来，导致没办法直接迁移。&#010;&#010;Yun Tang &lt;myasuka@live.com&gt; 于2020年7月14日周二 上午11:54写道：&#010;&#010;&gt; Checkpoint meta中存储的是完整路径，所以一般会把hdfs的namespace存储起来，导致没办法直接迁移。&#010;&gt;&#010;&gt; Flink-1.11 支持将savepoint（但是不支持Checkpoint）进行位置迁移 [1]，而对于Flink-1.9，二者均不支持。&#010;&gt;&#010;&gt;&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-5763&#010;&gt;&#010;&gt; 祝好&#010;&gt; 唐云&#010;&gt;&#010;&gt; ________________________________&#010;&gt; From: Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;&gt; Sent: Tuesday, July 14, 2020 11:07&#010;&gt; To: user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&gt; Subject: flink1.9状态及作业迁移&#010;&gt;&#010;&gt; hi：&#010;&gt;&#010;&gt; flink1.9的检查点或保存点中会保留hadoop集群的nameservice数据吗？现在想将一个集群的flink作业迁移到另一个集群，但两个集群的nameservice名称不一样，迁移会有问题吗？如果有问题的话对应状态保存的nameservice可以修改吗？或者说迁移的时候还有哪些其他需要注意的问题？&#010;&gt;&#010;",
        "depth": "3",
        "reply": "<CAEZk0438ohrd=O8oQg2NKr=CwXHEL90HugxD1baBqonUqe+AGQ@mail.gmail.com>"
    },
    {
        "id": "<1594696795014-0.post@n8.nabble.com>",
        "from": "Robin Zhang &lt;vincent2015q...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:19:55 GMT",
        "subject": "Flink1.10 flinksql 多表join状态ttl不成功的问题",
        "content": "&lt;http://apache-flink.147419.n8.nabble.com/file/t447/ttl.png&gt; &#010;我用flink sql 设置了 空闲状态的清理时间，但是 状态还是一直增加，里面有&#010;多次 group by  和多次 流表的关联 。&#010;代码如下：&#010;   tEnv.getConfig()&#010;         .setIdleStateRetentionTime(Time.hours(minIdleStateRetentionTime), &#010;                                                &#010;Time.hours(maxIdleStateRetentionTime));&#010;&#010;程序运行一周之后状态现在2.2G. 最近几天越来越大，表现在ttl没有成功，请教一下各位大佬&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594696795014-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO931-MegnFEgSzFYoR7kkq1rXKGeifB6Myr5JbnmM8OZRsw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:35:48 GMT",
        "subject": "Re: Flink1.10 flinksql 多表join状态ttl不成功的问题",
        "content": "Hi,&#013;&#010;&#013;&#010;请问用的是什么版本，blink planner 还是 old planner？&#013;&#010;有没有简化一点的 query 也能复现这个 ttl 问题的？ 比如一层 groupby？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Tue, 14 Jul 2020 at 15:36, Robin Zhang &lt;vincent2015qdlg@outlook.com&gt;&#013;&#010;wrote:&#013;&#010;&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t447/ttl.png&gt;&#013;&#010;&gt; 我用flink sql 设置了 空闲状态的清理时间，但是 状态还是一直增加，里面有&#010;多次 group by  和多次 流表的关联 。&#013;&#010;&gt; 代码如下：&#013;&#010;&gt;    tEnv.getConfig()&#013;&#010;&gt;          .setIdleStateRetentionTime(Time.hours(minIdleStateRetentionTime),&#013;&#010;&gt;&#013;&#010;&gt; Time.hours(maxIdleStateRetentionTime));&#013;&#010;&gt;&#013;&#010;&gt; 程序运行一周之后状态现在2.2G. 最近几天越来越大，表现在ttl没有成功，请教一下各位大佬&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<1594696795014-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594698525908-0.post@n8.nabble.com>",
        "from": "Robin Zhang &lt;vincent2015q...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 03:48:45 GMT",
        "subject": "Flink1.10 flinksql 多表join状态ttl不成功的问题",
        "content": "&lt;http://apache-flink.147419.n8.nabble.com/file/t447/ttl.png&gt; &#010;我用flink sql 设置了 空闲状态的清理时间，但是 状态还是一直增加，里面有&#010;多次 group by  和多次 流表的关联 。&#010;代码如下：&#010;   tEnv.getConfig()&#010;         .setIdleStateRetentionTime(Time.hours(minIdleStateRetentionTime), &#010;                                                &#010;Time.hours(maxIdleStateRetentionTime));&#010;&#010;程序运行一周之后状态现在2.2G. 最近几天越来越大，表现在ttl没有成功，请教一下各位大佬&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594696795014-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAMhjQvivw358Tx5RBs2d70BCaa7+h8y5EoUGHaK-TfH7F-82FQ@mail.gmail.com>",
        "from": "zhisheng &lt;zhisheng2...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:36:25 GMT",
        "subject": "Re: Flink1.10 flinksql 多表join状态ttl不成功的问题",
        "content": "有没有窗口啊？&#013;&#010;&#013;&#010;Robin Zhang &lt;vincent2015qdlg@outlook.com&gt; 于2020年7月14日周二 上午11:48写道：&#013;&#010;&#013;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t447/ttl.png&gt;&#013;&#010;&gt; 我用flink sql 设置了 空闲状态的清理时间，但是 状态还是一直增加，里面有&#010;多次 group by  和多次 流表的关联 。&#013;&#010;&gt; 代码如下：&#013;&#010;&gt;    tEnv.getConfig()&#013;&#010;&gt;          .setIdleStateRetentionTime(Time.hours(minIdleStateRetentionTime),&#013;&#010;&gt;&#013;&#010;&gt; Time.hours(maxIdleStateRetentionTime));&#013;&#010;&gt;&#013;&#010;&gt; 程序运行一周之后状态现在2.2G. 最近几天越来越大，表现在ttl没有成功，请教一下各位大佬&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<1594696795014-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594705051673-0.post@n8.nabble.com>",
        "from": "Robin Zhang &lt;vincent2015q...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 05:37:31 GMT",
        "subject": "Re: Flink1.10 flinksql 多表join状态ttl不成功的问题",
        "content": "没有使用窗口呢，就多表关联，涉及到流表join流表，流表join维表，group&#010;by 、topN等&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "3",
        "reply": "<1594696795014-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594706724184-0.post@n8.nabble.com>",
        "from": "Robin Zhang &lt;vincent2015q...@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:05:24 GMT",
        "subject": "Re: Flink1.10 flinksql 多表join状态ttl不成功的问题",
        "content": "没有窗口，就简单的表join，有kafka流表 ，kudu维表，使用了group by&#010;&#010;&gt; Jul 14, 2020; 12:36pm — by zhisheng zhisheng&#010;&gt; 有没有窗口啊？&#010;&#010;Robin Zhang &lt;[hidden email]&gt; 于2020年7月14日周二 上午11:48写道：&#010;&#010;&gt; &lt;http://apache-flink.147419.n8.nabble.com/file/t447/ttl.png&gt;&#010;&gt; 我用flink sql 设置了 空闲状态的清理时间，但是 状态还是一直增加，里面有&#010;多次 group by  和多次 流表的关联 。&#010;&gt; 代码如下：&#010;&gt;    tEnv.getConfig()&#010;&gt;          .setIdleStateRetentionTime(Time.hours(minIdleStateRetentionTime),&#010;&gt;&#010;&gt; Time.hours(maxIdleStateRetentionTime));&#010;&gt;&#010;&gt; 程序运行一周之后状态现在2.2G. 最近几天越来越大，表现在ttl没有成功，请教一下各位大佬&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "3",
        "reply": "<1594696795014-0.post@n8.nabble.com>"
    },
    {
        "id": "<434f3f70.be7d.1734b9b2fb5.Coremail.chq19970719@163.com>",
        "from": "成欢晴 &lt;chq19970...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:35:28 GMT",
        "subject": "退出邮件组",
        "content": "&#010;&#010;&#010;&#010;| |&#010;chq19970719&#010;|&#010;|&#010;邮箱：chq19970719@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master",
        "depth": "0",
        "reply": "<434f3f70.be7d.1734b9b2fb5.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>",
        "from": "成欢晴 &lt;chq19970...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:36:10 GMT",
        "subject": "(无主题)",
        "content": "退订&#010;&#010;&#010;| |&#010;chq19970719&#010;|&#010;|&#010;邮箱：chq19970719@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master",
        "depth": "1",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<CABi+2jQ_euPP_pno1eN3PbCKrU=hhAhA0LjTEs_+k_UQovNMAQ@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 04:38:45 GMT",
        "subject": "Re: (无主题)",
        "content": "Hi&#010;&#010;退订应该发这个邮箱：user-zh-unsubscribe@flink.apache.org&#010;&#010;Best&#010;Jingsong&#010;&#010;On Tue, Jul 14, 2020 at 12:36 PM 成欢晴 &lt;chq19970719@163.com&gt; wrote:&#010;&#010;&gt; 退订&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; chq19970719&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：chq19970719@163.com&#010;&gt; |&#010;&gt;&#010;&gt; Signature is customized by Netease Mail Master&#010;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "2",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<29b62a72.216a.1736814a675.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sun, 19 Jul 2020 17:17:30 GMT",
        "subject": "(无主题)",
        "content": "&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "1",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<CAOMLN=Zv3s3niLKt+2sXLQ+J9ZrFsNiq_jsMOjmxEY-JsANUNw@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:47:27 GMT",
        "subject": "Re: (无主题)",
        "content": "Hi,&#013;&#010;&#013;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#013;&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#013;&#010;&gt; api，希望看到的大佬能帮我解惑一下，谢谢啦&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;",
        "depth": "2",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<5cb5c383.5056.1736ad72707.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 06:09:11 GMT",
        "subject": "回复： (无主题)",
        "content": "&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "3",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<CAOMLN=Y6uz8PzFR9uM62kQpyajBy9M1_dq32iCwX=s+OepVkfg@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 07:11:09 GMT",
        "subject": "Re: (无主题)",
        "content": "Hi,&#013;&#010;&#013;&#010;从你举的这个例子考虑，仍然可以使用ContinusEventTimeTrigger来持续触发结果更新的，只不过整个窗口可以根据结束条件考虑别的，比如Global窗口。&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午2:09写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#013;&#010;&gt; Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#013;&#010;&gt; api，希望看到的大佬能帮我解惑一下，谢谢啦&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<41c967b.7cac.1736c257457.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 12:14:20 GMT",
        "subject": "回复：(无主题)",
        "content": "好的，谢谢大佬，我用这个试试&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月20日 15:11，shizk233 写道：&#010;Hi,&#010;&#010;从你举的这个例子考虑，仍然可以使用ContinusEventTimeTrigger来持续触发结果更新的，只不过整个窗口可以根据结束条件考虑别的，比如Global窗口。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午2:09写道：&#010;&#010;&gt;&#010;&gt;&#010;&gt; 不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;&gt; | |&#010;&gt; 罗显宴&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：15927482803@163.com&#010;&gt; |&#010;&gt; 签名由网易邮箱大师定制&#010;&gt; 在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;&gt; Hi,&#010;&gt;&#010;&gt; 累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;&gt; Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&gt;&#010;&gt; Best,&#010;&gt; shizk233&#010;&gt;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;&gt; api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&gt;&#010;&gt; | |&#010;&gt; 罗显宴&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：15927482803@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;",
        "depth": "5",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<1b1e3646.7dbb.1736c390b21.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 12:35:44 GMT",
        "subject": "回复： (无主题)",
        "content": "     我运行的时候，他直接按1小时窗口输出了，并没有按20秒连续输出递增&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "4",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<a454095.7de3.1736c3b6104.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 12:38:17 GMT",
        "subject": "回复： (无主题)",
        "content": "不好意思，刚才发的快，没来得及解释，&#010;这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "4",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<472064d5.7e34.1736c40a8ee.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 12:44:03 GMT",
        "subject": "回复： (无主题)",
        "content": "&#010;&#010;大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;不好意思，刚才发的快，没来得及解释，&#010;这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "5",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<CAOMLN=YMEQQc7V2VHrTi1w_MC2GbU9zfpZWw4EPq05duuRa-jA@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:10:53 GMT",
        "subject": "Re: (无主题)",
        "content": "Hi,&#013;&#010;&#013;&#010;我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#013;&#010;而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#013;&#010;&#013;&#010;你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt;&#013;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=%E7%BD%97%E6%98%BE%E5%AE%B4&amp;uid=15927482803%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fwzpmmc%2Fba95f912526bde9c6f0f1ae6f5b9b114.jpg&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9A15927482803%40163.com%22%5D&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail81&gt;&#010;定制&#013;&#010;&gt; 在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; &lt;15927482803@163.com&gt;&#010;写道：&#013;&#010;&gt;&#013;&#010;&gt; 不好意思，刚才发的快，没来得及解释，&#013;&#010;&gt; 这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt;&#013;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=%E7%BD%97%E6%98%BE%E5%AE%B4&amp;uid=15927482803%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fwzpmmc%2Fba95f912526bde9c6f0f1ae6f5b9b114.jpg&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9A15927482803%40163.com%22%5D&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail81&gt;&#010;定制&#013;&#010;&gt; 在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; &lt;15927482803@163.com&gt;&#010;写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#013;&#010;&gt; Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#013;&#010;&gt; api，希望看到的大佬能帮我解惑一下，谢谢啦&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<731075cf.a592.1736f6acdc5.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:28:57 GMT",
        "subject": "回复： (无主题)",
        "content": "hi,&#010;CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;&#010;我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#010;而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#010;&#010;&#010;你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#010;&#010;&#010;&#010;大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;不好意思，刚才发的快，没来得及解释，&#010;这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;",
        "depth": "7",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<CAOMLN=ZDZmGLxdSQcxLUhY55LLtNC4JWu+84YRdB2OPD9Pz6ng@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:37:46 GMT",
        "subject": "Re: (无主题)",
        "content": "Hi,&#013;&#010;&#013;&#010;有点不太理解，能举个例子说明一下result那部分你预期的输入(几条sample&#010;数据就行)和期望的输出结果吗&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:29写道：&#013;&#010;&#013;&#010;&gt; hi,&#013;&#010;&gt;&#013;&#010;&gt; CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#013;&#010;&gt; 而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#013;&#010;&gt; 不好意思，刚才发的快，没来得及解释，&#013;&#010;&gt;&#013;&#010;&gt; 这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#013;&#010;&gt; Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#013;&#010;&gt; api，希望看到的大佬能帮我解惑一下，谢谢啦&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<35413b50.a803.1736f7c9256.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 03:48:21 GMT",
        "subject": "回复： (无主题)",
        "content": "好的，&#010;输入：&#010;心功能不全和心律失常用药，1，时间戳&#010;心功能不全和心律失常用药，1，时间戳&#010;抗利尿剂，1，时间戳&#010;血管收缩剂，1，时间戳&#010;血管紧张素II受体拮抗剂，1，时间戳&#010;&#010;&#010;这里的时间戳就是eventtime了&#010;比如前三条是在一个20秒窗口中，所以应该分为两个窗口：&#010;心功能不全和心律失常用药和抗利尿剂，但是我是计数药物的种类的&#010;所以不管窗口有多少元素我还是置为1，所以输出的接口就是窗口之和，即为2&#010;接下来20秒都多了2个窗口而且和前面的医药种类不一样,所以在原来基础上再加2&#010;输出4&#010;&#010;&#010;即输出：&#010;2020-7-20 19:00:00,2&#010;2020-7-20 19:00:20,4&#010;&#010;&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:37，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;有点不太理解，能举个例子说明一下result那部分你预期的输入(几条sample&#010;数据就行)和期望的输出结果吗&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:29写道：&#010;&#010;hi,&#010;&#010;CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;&#010;我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#010;而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#010;&#010;&#010;你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#010;&#010;&#010;&#010;大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;不好意思，刚才发的快，没来得及解释，&#010;&#010;这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;",
        "depth": "9",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<CAOMLN=ZntbPrsThEZ_6rq1bfQDhZZ9PVdjNz5Bw=gRmkA+av5A@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 05:58:50 GMT",
        "subject": "Re: (无主题)",
        "content": "Hi,&#013;&#010;&#013;&#010;首先统一一下表述，当前只有1小时的滚动窗口，不存在20分钟的窗口，trigger只是输出结果，trigger的间隔和窗口大小无关。&#013;&#010;&#013;&#010;按目前的设计，在11-12点的窗口里，输入x条类型A的数据，agg都记为1条，但1小时窗口会触发3次20s的trigger，&#013;&#010;也就是会最多输出3条(A类型,12点)的数据（同一个1小时窗口，WindowEnd都是12点）。&#013;&#010;这3条数据进入MapState后，写下了((A类型,12点)，1)的记录并都注册了12点+1的timer，&#013;&#010;那么timer在12点的时候会输出的结果就是（12点，1）。&#013;&#010;&#013;&#010;如果12-13点里，A类型做相同的输入，MapState新增一条（(A类型,13点)，1）的记录，在13点得到最终结果（13点，2）。&#013;&#010;&#013;&#010;而这个结果和我理解的你的需求不太一样，我理解的情况应该是12点输出（12点，1），13点输出（13点，1），因为目前只有A类型的数据。&#013;&#010;期望的输出应该是无限时间上的去重类型统计，每隔1小时输出（几点，当前所有的类型总数）。&#013;&#010;&#013;&#010;我觉得目前的设计可能和描述的需求不太一致？你看看是不是这么回事儿&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;shizk233&#013;&#010;&#013;&#010;&#013;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:53写道：&#013;&#010;&#013;&#010;&gt; 好的，&#013;&#010;&gt; 输入：&#013;&#010;&gt; 心功能不全和心律失常用药，1，时间戳&#013;&#010;&gt; 心功能不全和心律失常用药，1，时间戳&#013;&#010;&gt; 抗利尿剂，1，时间戳&#013;&#010;&gt; 血管收缩剂，1，时间戳&#013;&#010;&gt; 血管紧张素II受体拮抗剂，1，时间戳&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 这里的时间戳就是eventtime了&#013;&#010;&gt; 比如前三条是在一个20秒窗口中，所以应该分为两个窗口：&#013;&#010;&gt; 心功能不全和心律失常用药和抗利尿剂，但是我是计数药物的种类的&#013;&#010;&gt; 所以不管窗口有多少元素我还是置为1，所以输出的接口就是窗口之和，即为2&#013;&#010;&gt; 接下来20秒都多了2个窗口而且和前面的医药种类不一样,所以在原来基础上再加2&#013;&#010;&gt; 输出4&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 即输出：&#013;&#010;&gt; 2020-7-20 19:00:00,2&#013;&#010;&gt; 2020-7-20 19:00:20,4&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月21日 11:37，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 有点不太理解，能举个例子说明一下result那部分你预期的输入(几条sample&#010;数据就行)和期望的输出结果吗&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:29写道：&#013;&#010;&gt;&#013;&#010;&gt; hi,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#013;&#010;&gt; 而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#013;&#010;&gt; 不好意思，刚才发的快，没来得及解释，&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#013;&#010;&gt; Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#013;&#010;&gt; api，希望看到的大佬能帮我解惑一下，谢谢啦&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "10",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<69cc9c86.b3d1.1737002d78c.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 06:15:01 GMT",
        "subject": "回复： (无主题)",
        "content": "&#010;&#010;hi，&#010;我觉得你说的是对的，我刚才没有理解trigger，我以为trigger当成一个1小时窗口的20分钟的小窗口了，其实我要的结果就是每20分钟有多少个窗口比如当前20分钟有A类型、B类型和C类型三个窗口，那么输出就是3，后来20分钟有A类型、B类型和D类型的结果，那么A类型和B类型是重复的只有D不是重复的，结果为4&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 13:58，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;首先统一一下表述，当前只有1小时的滚动窗口，不存在20分钟的窗口，trigger只是输出结果，trigger的间隔和窗口大小无关。&#010;&#010;按目前的设计，在11-12点的窗口里，输入x条类型A的数据，agg都记为1条，但1小时窗口会触发3次20s的trigger，&#010;也就是会最多输出3条(A类型,12点)的数据（同一个1小时窗口，WindowEnd都是12点）。&#010;这3条数据进入MapState后，写下了((A类型,12点)，1)的记录并都注册了12点+1的timer，&#010;那么timer在12点的时候会输出的结果就是（12点，1）。&#010;&#010;如果12-13点里，A类型做相同的输入，MapState新增一条（(A类型,13点)，1）的记录，在13点得到最终结果（13点，2）。&#010;&#010;而这个结果和我理解的你的需求不太一样，我理解的情况应该是12点输出（12点，1），13点输出（13点，1），因为目前只有A类型的数据。&#010;期望的输出应该是无限时间上的去重类型统计，每隔1小时输出（几点，当前所有的类型总数）。&#010;&#010;我觉得目前的设计可能和描述的需求不太一致？你看看是不是这么回事儿&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:53写道：&#010;&#010;好的，&#010;输入：&#010;心功能不全和心律失常用药，1，时间戳&#010;心功能不全和心律失常用药，1，时间戳&#010;抗利尿剂，1，时间戳&#010;血管收缩剂，1，时间戳&#010;血管紧张素II受体拮抗剂，1，时间戳&#010;&#010;&#010;这里的时间戳就是eventtime了&#010;比如前三条是在一个20秒窗口中，所以应该分为两个窗口：&#010;心功能不全和心律失常用药和抗利尿剂，但是我是计数药物的种类的&#010;所以不管窗口有多少元素我还是置为1，所以输出的接口就是窗口之和，即为2&#010;接下来20秒都多了2个窗口而且和前面的医药种类不一样,所以在原来基础上再加2&#010;输出4&#010;&#010;&#010;即输出：&#010;2020-7-20 19:00:00,2&#010;2020-7-20 19:00:20,4&#010;&#010;&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:37，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;有点不太理解，能举个例子说明一下result那部分你预期的输入(几条sample&#010;数据就行)和期望的输出结果吗&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:29写道：&#010;&#010;hi,&#010;&#010;&#010;CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;&#010;我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#010;而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#010;&#010;&#010;你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#010;&#010;&#010;&#010;大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;不好意思，刚才发的快，没来得及解释，&#010;&#010;&#010;这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;&#010;",
        "depth": "11",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<69092db3.ba3a.17370303463.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 07:04:34 GMT",
        "subject": "回复： (无主题)",
        "content": "hi，我想到解决办法了，可以用全局window，我一直以为是要分区在做窗口运算其实可以直接用timewindowAll来算，然后用状态保存就够了&#010;val result = num.timeWindowAll(Time.seconds(20))&#010;//        .trigger(ContinuousEventTimeTrigger.of(Time.seconds(20)))&#010;.process(new ProcessAllWindowFunction[IncreaseNumPerHour,IncreasePerHour,TimeWindow] {&#010;&#010;private var itemState: MapState[String,Int] = _&#010;&#010;override def open(parameters: Configuration): Unit = {&#010;itemState = getRuntimeContext.getMapState(new MapStateDescriptor[String,Int](\"item-state\",TypeInformation.of(classOf[String]),TypeInformation.of(classOf[Int])))&#010;          }&#010;&#010;override def process(context: Context, elements: Iterable[IncreaseNumPerHour], out: Collector[IncreasePerHour]):&#010;Unit = {&#010;var timestamp:Long = 0L&#010;elements.foreach(kv =&gt; {&#010;itemState.put(kv.category, 1)&#010;timestamp = (kv.timestamp/2000+1)*2000&#010;})&#010;import scala.collection.JavaConversions._&#010;            out.collect(IncreasePerHour(new Timestamp( timestamp - 1 ).toString,itemState.keys().size))&#010;          }&#010;        })&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 14:15，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;hi，&#010;我觉得你说的是对的，我刚才没有理解trigger，我以为trigger当成一个1小时窗口的20分钟的小窗口了，其实我要的结果就是每20分钟有多少个窗口比如当前20分钟有A类型、B类型和C类型三个窗口，那么输出就是3，后来20分钟有A类型、B类型和D类型的结果，那么A类型和B类型是重复的只有D不是重复的，结果为4&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 13:58，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;首先统一一下表述，当前只有1小时的滚动窗口，不存在20分钟的窗口，trigger只是输出结果，trigger的间隔和窗口大小无关。&#010;&#010;按目前的设计，在11-12点的窗口里，输入x条类型A的数据，agg都记为1条，但1小时窗口会触发3次20s的trigger，&#010;也就是会最多输出3条(A类型,12点)的数据（同一个1小时窗口，WindowEnd都是12点）。&#010;这3条数据进入MapState后，写下了((A类型,12点)，1)的记录并都注册了12点+1的timer，&#010;那么timer在12点的时候会输出的结果就是（12点，1）。&#010;&#010;如果12-13点里，A类型做相同的输入，MapState新增一条（(A类型,13点)，1）的记录，在13点得到最终结果（13点，2）。&#010;&#010;而这个结果和我理解的你的需求不太一样，我理解的情况应该是12点输出（12点，1），13点输出（13点，1），因为目前只有A类型的数据。&#010;期望的输出应该是无限时间上的去重类型统计，每隔1小时输出（几点，当前所有的类型总数）。&#010;&#010;我觉得目前的设计可能和描述的需求不太一致？你看看是不是这么回事儿&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:53写道：&#010;&#010;好的，&#010;输入：&#010;心功能不全和心律失常用药，1，时间戳&#010;心功能不全和心律失常用药，1，时间戳&#010;抗利尿剂，1，时间戳&#010;血管收缩剂，1，时间戳&#010;血管紧张素II受体拮抗剂，1，时间戳&#010;&#010;&#010;这里的时间戳就是eventtime了&#010;比如前三条是在一个20秒窗口中，所以应该分为两个窗口：&#010;心功能不全和心律失常用药和抗利尿剂，但是我是计数药物的种类的&#010;所以不管窗口有多少元素我还是置为1，所以输出的接口就是窗口之和，即为2&#010;接下来20秒都多了2个窗口而且和前面的医药种类不一样,所以在原来基础上再加2&#010;输出4&#010;&#010;&#010;即输出：&#010;2020-7-20 19:00:00,2&#010;2020-7-20 19:00:20,4&#010;&#010;&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:37，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;有点不太理解，能举个例子说明一下result那部分你预期的输入(几条sample&#010;数据就行)和期望的输出结果吗&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:29写道：&#010;&#010;hi,&#010;&#010;&#010;CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;&#010;我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#010;而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#010;&#010;&#010;你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#010;&#010;&#010;&#010;大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;不好意思，刚才发的快，没来得及解释，&#010;&#010;&#010;这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;&#010;",
        "depth": "12",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<4a1b197f.5b9a.1737775d3e3.Coremail.15927482803@163.com>",
        "from": "罗显宴 &lt;15927482...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 16:57:57 GMT",
        "subject": "回复：(无主题)",
        "content": "感谢shizk233大佬，我这个问题终于得到解决，我主要是通过全窗口加mapstate实现的&#010;best&#010;shizk233&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月21日 15:04，罗显宴 写道：&#010;hi，我想到解决办法了，可以用全局window，我一直以为是要分区在做窗口运算其实可以直接用timewindowAll来算，然后用状态保存就够了&#010;val result = num.timeWindowAll(Time.seconds(20))&#010;//        .trigger(ContinuousEventTimeTrigger.of(Time.seconds(20)))&#010;.process(new ProcessAllWindowFunction[IncreaseNumPerHour,IncreasePerHour,TimeWindow] {&#010;&#010;private var itemState: MapState[String,Int] = _&#010;&#010;override def open(parameters: Configuration): Unit = {&#010;itemState = getRuntimeContext.getMapState(new MapStateDescriptor[String,Int](\"item-state\",TypeInformation.of(classOf[String]),TypeInformation.of(classOf[Int])))&#010;         }&#010;&#010;override def process(context: Context, elements: Iterable[IncreaseNumPerHour], out: Collector[IncreasePerHour]):&#010;Unit = {&#010;var timestamp:Long = 0L&#010;elements.foreach(kv =&gt; {&#010;itemState.put(kv.category, 1)&#010;timestamp = (kv.timestamp/2000+1)*2000&#010;})&#010;import scala.collection.JavaConversions._&#010;           out.collect(IncreasePerHour(new Timestamp( timestamp - 1 ).toString,itemState.keys().size))&#010;         }&#010;       })&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 14:15，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;hi，&#010;我觉得你说的是对的，我刚才没有理解trigger，我以为trigger当成一个1小时窗口的20分钟的小窗口了，其实我要的结果就是每20分钟有多少个窗口比如当前20分钟有A类型、B类型和C类型三个窗口，那么输出就是3，后来20分钟有A类型、B类型和D类型的结果，那么A类型和B类型是重复的只有D不是重复的，结果为4&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 13:58，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;首先统一一下表述，当前只有1小时的滚动窗口，不存在20分钟的窗口，trigger只是输出结果，trigger的间隔和窗口大小无关。&#010;&#010;按目前的设计，在11-12点的窗口里，输入x条类型A的数据，agg都记为1条，但1小时窗口会触发3次20s的trigger，&#010;也就是会最多输出3条(A类型,12点)的数据（同一个1小时窗口，WindowEnd都是12点）。&#010;这3条数据进入MapState后，写下了((A类型,12点)，1)的记录并都注册了12点+1的timer，&#010;那么timer在12点的时候会输出的结果就是（12点，1）。&#010;&#010;如果12-13点里，A类型做相同的输入，MapState新增一条（(A类型,13点)，1）的记录，在13点得到最终结果（13点，2）。&#010;&#010;而这个结果和我理解的你的需求不太一样，我理解的情况应该是12点输出（12点，1），13点输出（13点，1），因为目前只有A类型的数据。&#010;期望的输出应该是无限时间上的去重类型统计，每隔1小时输出（几点，当前所有的类型总数）。&#010;&#010;我觉得目前的设计可能和描述的需求不太一致？你看看是不是这么回事儿&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:53写道：&#010;&#010;好的，&#010;输入：&#010;心功能不全和心律失常用药，1，时间戳&#010;心功能不全和心律失常用药，1，时间戳&#010;抗利尿剂，1，时间戳&#010;血管收缩剂，1，时间戳&#010;血管紧张素II受体拮抗剂，1，时间戳&#010;&#010;&#010;这里的时间戳就是eventtime了&#010;比如前三条是在一个20秒窗口中，所以应该分为两个窗口：&#010;心功能不全和心律失常用药和抗利尿剂，但是我是计数药物的种类的&#010;所以不管窗口有多少元素我还是置为1，所以输出的接口就是窗口之和，即为2&#010;接下来20秒都多了2个窗口而且和前面的医药种类不一样,所以在原来基础上再加2&#010;输出4&#010;&#010;&#010;即输出：&#010;2020-7-20 19:00:00,2&#010;2020-7-20 19:00:20,4&#010;&#010;&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:37，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;有点不太理解，能举个例子说明一下result那部分你预期的输入(几条sample&#010;数据就行)和期望的输出结果吗&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:29写道：&#010;&#010;hi,&#010;&#010;&#010;CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;&#010;我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#010;而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#010;&#010;&#010;你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#010;&#010;&#010;Best,&#010;shizk233&#010;&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#010;&#010;&#010;&#010;大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;不好意思，刚才发的快，没来得及解释，&#010;&#010;&#010;这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#010;&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#010;&#010;&#010;不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;签名由网易邮箱大师定制&#010;在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#010;Hi,&#010;&#010;累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#010;Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#010;&#010;Best,&#010;shizk233&#010;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#010;&#010;&#010;&#010;&#010;大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#010;api，希望看到的大佬能帮我解惑一下，谢谢啦&#010;&#010;| |&#010;罗显宴&#010;|&#010;|&#010;邮箱：15927482803@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;&#010;",
        "depth": "13",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<CAOMLN=ZBUDPBFGWeggFnjbXKukREdhwWuvKQ9Jp_8Q02WB0JUA@mail.gmail.com>",
        "from": "shizk233 &lt;wangwangdaxian...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 10:50:17 GMT",
        "subject": "Re: (无主题)",
        "content": "恭喜！&#013;&#010;&#013;&#010;罗显宴 &lt;15927482803@163.com&gt; 于2020年7月23日周四 上午1:14写道：&#013;&#010;&#013;&#010;&gt; 感谢shizk233大佬，我这个问题终于得到解决，我主要是通过全窗口加mapstate实现的&#013;&#010;&gt; best&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; 在2020年07月21日 15:04，罗显宴 写道：&#013;&#010;&gt; hi，我想到解决办法了，可以用全局window，我一直以为是要分区在做窗口运算其实可以直接用timewindowAll来算，然后用状态保存就够了&#013;&#010;&gt; val result = num.timeWindowAll(Time.seconds(20))&#013;&#010;&gt; //        .trigger(ContinuousEventTimeTrigger.of(Time.seconds(20)))&#013;&#010;&gt; .process(new&#013;&#010;&gt; ProcessAllWindowFunction[IncreaseNumPerHour,IncreasePerHour,TimeWindow] {&#013;&#010;&gt;&#013;&#010;&gt; private var itemState: MapState[String,Int] = _&#013;&#010;&gt;&#013;&#010;&gt; override def open(parameters: Configuration): Unit = {&#013;&#010;&gt; itemState = getRuntimeContext.getMapState(new&#013;&#010;&gt; MapStateDescriptor[String,Int](\"item-state\",TypeInformation.of(classOf[String]),TypeInformation.of(classOf[Int])))&#013;&#010;&gt;          }&#013;&#010;&gt;&#013;&#010;&gt; override def process(context: Context, elements:&#013;&#010;&gt; Iterable[IncreaseNumPerHour], out: Collector[IncreasePerHour]): Unit = {&#013;&#010;&gt; var timestamp:Long = 0L&#013;&#010;&gt; elements.foreach(kv =&gt; {&#013;&#010;&gt; itemState.put(kv.category, 1)&#013;&#010;&gt; timestamp = (kv.timestamp/2000+1)*2000&#013;&#010;&gt; })&#013;&#010;&gt; import scala.collection.JavaConversions._&#013;&#010;&gt;            out.collect(IncreasePerHour(new Timestamp( timestamp - 1&#013;&#010;&gt; ).toString,itemState.keys().size))&#013;&#010;&gt;          }&#013;&#010;&gt;        })&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月21日 14:15，罗显宴&lt;15927482803@163.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi，&#013;&#010;&gt;&#013;&#010;&gt; 我觉得你说的是对的，我刚才没有理解trigger，我以为trigger当成一个1小时窗口的20分钟的小窗口了，其实我要的结果就是每20分钟有多少个窗口比如当前20分钟有A类型、B类型和C类型三个窗口，那么输出就是3，后来20分钟有A类型、B类型和D类型的结果，那么A类型和B类型是重复的只有D不是重复的，结果为4&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月21日 13:58，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 首先统一一下表述，当前只有1小时的滚动窗口，不存在20分钟的窗口，trigger只是输出结果，trigger的间隔和窗口大小无关。&#013;&#010;&gt;&#013;&#010;&gt; 按目前的设计，在11-12点的窗口里，输入x条类型A的数据，agg都记为1条，但1小时窗口会触发3次20s的trigger，&#013;&#010;&gt; 也就是会最多输出3条(A类型,12点)的数据（同一个1小时窗口，WindowEnd都是12点）。&#013;&#010;&gt; 这3条数据进入MapState后，写下了((A类型,12点)，1)的记录并都注册了12点+1的timer，&#013;&#010;&gt; 那么timer在12点的时候会输出的结果就是（12点，1）。&#013;&#010;&gt;&#013;&#010;&gt; 如果12-13点里，A类型做相同的输入，MapState新增一条（(A类型,13点)，1）的记录，在13点得到最终结果（13点，2）。&#013;&#010;&gt;&#013;&#010;&gt; 而这个结果和我理解的你的需求不太一样，我理解的情况应该是12点输出（12点，1），13点输出（13点，1），因为目前只有A类型的数据。&#013;&#010;&gt; 期望的输出应该是无限时间上的去重类型统计，每隔1小时输出（几点，当前所有的类型总数）。&#013;&#010;&gt;&#013;&#010;&gt; 我觉得目前的设计可能和描述的需求不太一致？你看看是不是这么回事儿&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:53写道：&#013;&#010;&gt;&#013;&#010;&gt; 好的，&#013;&#010;&gt; 输入：&#013;&#010;&gt; 心功能不全和心律失常用药，1，时间戳&#013;&#010;&gt; 心功能不全和心律失常用药，1，时间戳&#013;&#010;&gt; 抗利尿剂，1，时间戳&#013;&#010;&gt; 血管收缩剂，1，时间戳&#013;&#010;&gt; 血管紧张素II受体拮抗剂，1，时间戳&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 这里的时间戳就是eventtime了&#013;&#010;&gt; 比如前三条是在一个20秒窗口中，所以应该分为两个窗口：&#013;&#010;&gt; 心功能不全和心律失常用药和抗利尿剂，但是我是计数药物的种类的&#013;&#010;&gt; 所以不管窗口有多少元素我还是置为1，所以输出的接口就是窗口之和，即为2&#013;&#010;&gt; 接下来20秒都多了2个窗口而且和前面的医药种类不一样,所以在原来基础上再加2&#013;&#010;&gt; 输出4&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 即输出：&#013;&#010;&gt; 2020-7-20 19:00:00,2&#013;&#010;&gt; 2020-7-20 19:00:20,4&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月21日 11:37，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 有点不太理解，能举个例子说明一下result那部分你预期的输入(几条sample&#010;数据就行)和期望的输出结果吗&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月21日周二 上午11:29写道：&#013;&#010;&gt;&#013;&#010;&gt; hi,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CountAgg是对一个窗口进行聚合，而一个窗口中的元素都是根据医药类别category分区而来的，都是一样的，所以我做累加就直接置为1了，你的意思是让我在CuontAgg上做累加吗&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月21日 11:10，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我猜是因为设的1小时滚动窗口，WindowFunction里拿到的WindowEnd就是1小时的END，&#013;&#010;&gt; 而acc其实没有变化，也就是每隔20s触发拿到的结果是一样的，在MapState里也会忽略重复值。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你可以让acc做个累加，然后结果输出里把acc的值带上看看。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 下午8:44写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大佬，不好意思，可能图片看不到，我把代码发一次，刚学习flink半个月，好多不懂，希望大佬莫嫌烦&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 20:38，罗显宴&lt;15927482803@163.com&gt; 写道：&#013;&#010;&gt; 不好意思，刚才发的快，没来得及解释，&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 这里aggregate算子主要做了一个预聚合，把窗口的个数置为一，然后用windowResult输出结果，然后对窗口分区，最后用mapState处理递增&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 14:09，罗显宴&lt;15927482803@163.com&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 不是，是连续累计，比如我在某个医药网站上爬取有关药物，每小时统计爬取到的新增药物种类，然后一直这样进行下去，然后这个网站爬完了，可以换另一个网站，&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt; 签名由网易邮箱大师定制&#013;&#010;&gt; 在2020年7月20日 11:47，shizk233&lt;wangwangdaxian233@gmail.com&gt; 写道：&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; 累计是仅在一天之内累计吗，这样的话可以开个一天的Tumbling&#013;&#010;&gt; Window，然后使用ContinusEventTimeTrigger每小时触发一下输出结果。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; shizk233&#013;&#010;&gt;&#013;&#010;&gt; 罗显宴 &lt;15927482803@163.com&gt; 于2020年7月20日周一 上午1:18写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 大家好，怎么实现一个滚动窗口内的连续递增元素个数，比如每小时累计用户登录数，比如说凌晨1点登录了500人，到了两点就累计到1200，到3点累计到，，，，这样实现一个递增曲线，在网上看到云邪大佬写的sql但是不会写datastream&#013;&#010;&gt; api，希望看到的大佬能帮我解惑一下，谢谢啦&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; 罗显宴&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：15927482803@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "14",
        "reply": "<3004174a.be89.1734b9bd46f.Coremail.chq19970719@163.com>"
    },
    {
        "id": "<tencent_181C0913B245ADC8A1314567707CF754D906@qq.com>",
        "from": "&quot;ゞ野蠻遊戲χ&quot; &lt;zhoujiazhi1...@vip.qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 05:09:35 GMT",
        "subject": "State 0点清除的问题",
        "content": "大家好：&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;我想问下，在ProcessAllWindowFunction中，在每天的0点清除state如何清除？&#013;&#010;&#013;&#010;&#013;&#010;Thanks&#013;&#010;嘉治",
        "depth": "0",
        "reply": "<tencent_181C0913B245ADC8A1314567707CF754D906@qq.com>"
    },
    {
        "id": "<CAA8tFvse9bYUMoZ0kp68jbKWWqdqwwSs9df5bi6E4-XQCVrapA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 05:46:25 GMT",
        "subject": "Re: State 0点清除的问题",
        "content": "Hi&#013;&#010;如果你需要精确的控制每天 0 点清除  state 的话，或许你可以考虑使用&#010;processFunction[1], 然后自己使用 timer&#013;&#010;实现相关逻辑&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/stream/operators/process_function.html&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;ゞ野蠻遊戲χ &lt;zhoujiazhi1985@vip.qq.com&gt; 于2020年7月14日周二 下午1:10写道：&#013;&#010;&#013;&#010;&gt; 大家好：&amp;nbsp; &amp;nbsp; &amp;nbsp;&#013;&#010;&gt; &amp;nbsp;我想问下，在ProcessAllWindowFunction中，在每天的0点清除state如何清除？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Thanks&#013;&#010;&gt; 嘉治&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_181C0913B245ADC8A1314567707CF754D906@qq.com>"
    },
    {
        "id": "<202007141437518255817@163.com>",
        "from": "&quot;amenhub@163.com&quot; &lt;amen...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:37:57 GMT",
        "subject": "flink-1.11使用executeSql()执行DDL语句问题",
        "content": "hi, everyone&#013;&#010;&#013;&#010;环境信息：flink-1.11.0, blink-planner, 本地ide开发测试（IDEA）&#013;&#010;&#013;&#010;问题描述：使用executeSql()方法执行DDL语句，控制台打印如下异常信息。（flink-connector-kafka_2.11依赖已添加）&#013;&#010;&#013;&#010;我不确定是否还有某个必要的依赖没有添加，还是有其他的地方没有考虑完整，请大佬赐教。&#013;&#010;&#013;&#010;--------------------------------------------------------------分割线-------------------------------------------------------------&#013;&#010;Exception in thread \"main\" org.apache.flink.table.api.ValidationException: Unable to create&#010;a source for reading table 'default_catalog.default_database.kafka_out'.&#013;&#010;&#013;&#010;Table options are:&#013;&#010;&#013;&#010;'connector'='kafka'&#013;&#010;'format'='json'&#013;&#010;'properties.bootstrap.servers'='localhost:9092'&#013;&#010;'properties.group.id'='flink-1.11'&#013;&#010;'scan.startup.mode'='group-offsets'&#013;&#010;'topic'='flink-kafka'&#013;&#010;at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)&#013;&#010;at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)&#013;&#010;at org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2178)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#013;&#010;at org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#013;&#010;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#013;&#010;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#013;&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#013;&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#013;&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#013;&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#013;&#010;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#013;&#010;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#013;&#010;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#013;&#010;at example.Example.main(Example.java:77)&#013;&#010;Caused by: org.apache.flink.table.api.ValidationException: Could not find any factories that&#010;implement 'org.apache.flink.table.factories.DeserializationFormatFactory' in the classpath.&#013;&#010;at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:229)&#013;&#010;at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverOptionalFormatFactory(FactoryUtil.java:538)&#013;&#010;at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverOptionalDecodingFormat(FactoryUtil.java:426)&#013;&#010;at org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverDecodingFormat(FactoryUtil.java:413)&#013;&#010;at org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryBase.createDynamicTableSource(KafkaDynamicTableFactoryBase.java:73)&#013;&#010;at org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)&#013;&#010;... 25 more&#013;&#010;&#013;&#010;--------------------------------------------------------------分割线-------------------------------------------------------------&#013;&#010;&#013;&#010;祝好！&#013;&#010;amenhub&#013;&#010;",
        "depth": "0",
        "reply": "<202007141437518255817@163.com>"
    },
    {
        "id": "<CABKuJ_TqH6=z_iiafiBRHGHMLqcr=T3g8+gaYOgq-RHPbm8hbQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:45:13 GMT",
        "subject": "Re: flink-1.11使用executeSql()执行DDL语句问题",
        "content": "看起来是format找不到实现，你可以添加一下flink-json的依赖看一下。&#010;&#010;amenhub@163.com &lt;amenhub@163.com&gt; 于2020年7月14日周二 下午2:38写道：&#010;&#010;&gt; hi, everyone&#010;&gt;&#010;&gt; 环境信息：flink-1.11.0, blink-planner, 本地ide开发测试（IDEA）&#010;&gt;&#010;&gt; 问题描述：使用executeSql()方法执行DDL语句，控制台打印如下异常信息。（flink-connector-kafka_2.11依赖已添加）&#010;&gt;&#010;&gt; 我不确定是否还有某个必要的依赖没有添加，还是有其他的地方没有考虑完整，请大佬赐教。&#010;&gt;&#010;&gt;&#010;&gt; --------------------------------------------------------------分割线-------------------------------------------------------------&#010;&gt; Exception in thread \"main\" org.apache.flink.table.api.ValidationException:&#010;&gt; Unable to create a source for reading table&#010;&gt; 'default_catalog.default_database.kafka_out'.&#010;&gt;&#010;&gt; Table options are:&#010;&gt;&#010;&gt; 'connector'='kafka'&#010;&gt; 'format'='json'&#010;&gt; 'properties.bootstrap.servers'='localhost:9092'&#010;&gt; 'properties.group.id'='flink-1.11'&#010;&gt; 'scan.startup.mode'='group-offsets'&#010;&gt; 'topic'='flink-kafka'&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2178)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;&gt; at example.Example.main(Example.java:77)&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;&gt; any factories that implement&#010;&gt; 'org.apache.flink.table.factories.DeserializationFormatFactory' in the&#010;&gt; classpath.&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:229)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverOptionalFormatFactory(FactoryUtil.java:538)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverOptionalDecodingFormat(FactoryUtil.java:426)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverDecodingFormat(FactoryUtil.java:413)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryBase.createDynamicTableSource(KafkaDynamicTableFactoryBase.java:73)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)&#010;&gt; ... 25 more&#010;&gt;&#010;&gt;&#010;&gt; --------------------------------------------------------------分割线-------------------------------------------------------------&#010;&gt;&#010;&gt; 祝好！&#010;&gt; amenhub&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "1",
        "reply": "<202007141437518255817@163.com>"
    },
    {
        "id": "<CABi+2jSQSXcPQZwW3fYRrp-K8zUumxb+-OPdkcyNDLYQMN3hYg@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:45:17 GMT",
        "subject": "Re: flink-1.11使用executeSql()执行DDL语句问题",
        "content": "还要添加flink-json&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Tue, Jul 14, 2020 at 2:38 PM amenhub@163.com &lt;amenhub@163.com&gt; wrote:&#010;&#010;&gt; hi, everyone&#010;&gt;&#010;&gt; 环境信息：flink-1.11.0, blink-planner, 本地ide开发测试（IDEA）&#010;&gt;&#010;&gt; 问题描述：使用executeSql()方法执行DDL语句，控制台打印如下异常信息。（flink-connector-kafka_2.11依赖已添加）&#010;&gt;&#010;&gt; 我不确定是否还有某个必要的依赖没有添加，还是有其他的地方没有考虑完整，请大佬赐教。&#010;&gt;&#010;&gt;&#010;&gt; --------------------------------------------------------------分割线-------------------------------------------------------------&#010;&gt; Exception in thread \"main\" org.apache.flink.table.api.ValidationException:&#010;&gt; Unable to create a source for reading table&#010;&gt; 'default_catalog.default_database.kafka_out'.&#010;&gt;&#010;&gt; Table options are:&#010;&gt;&#010;&gt; 'connector'='kafka'&#010;&gt; 'format'='json'&#010;&gt; 'properties.bootstrap.servers'='localhost:9092'&#010;&gt; 'properties.group.id'='flink-1.11'&#010;&gt; 'scan.startup.mode'='group-offsets'&#010;&gt; 'topic'='flink-kafka'&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:125)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.plan.schema.CatalogSourceTable.buildTableScan(CatalogSourceTable.scala:135)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.plan.schema.CatalogSourceTable.toRel(CatalogSourceTable.scala:78)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.toRel(SqlToRelConverter.java:3492)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertIdentifier(SqlToRelConverter.java:2415)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2102)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2178)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertFrom(SqlToRelConverter.java:2051)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelectImpl(SqlToRelConverter.java:661)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertSelect(SqlToRelConverter.java:642)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQueryRecursive(SqlToRelConverter.java:3345)&#010;&gt; at&#010;&gt; org.apache.calcite.sql2rel.SqlToRelConverter.convertQuery(SqlToRelConverter.java:568)&#010;&gt; at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$rel(FlinkPlannerImpl.scala:164)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.rel(FlinkPlannerImpl.scala:151)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.toQueryOperation(SqlToOperationConverter.java:773)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlQuery(SqlToOperationConverter.java:745)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:238)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convertSqlInsert(SqlToOperationConverter.java:527)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:204)&#010;&gt; at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;&gt; at example.Example.main(Example.java:77)&#010;&gt; Caused by: org.apache.flink.table.api.ValidationException: Could not find&#010;&gt; any factories that implement&#010;&gt; 'org.apache.flink.table.factories.DeserializationFormatFactory' in the&#010;&gt; classpath.&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:229)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverOptionalFormatFactory(FactoryUtil.java:538)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverOptionalDecodingFormat(FactoryUtil.java:426)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil$TableFactoryHelper.discoverDecodingFormat(FactoryUtil.java:413)&#010;&gt; at&#010;&gt; org.apache.flink.streaming.connectors.kafka.table.KafkaDynamicTableFactoryBase.createDynamicTableSource(KafkaDynamicTableFactoryBase.java:73)&#010;&gt; at&#010;&gt; org.apache.flink.table.factories.FactoryUtil.createTableSource(FactoryUtil.java:122)&#010;&gt; ... 25 more&#010;&gt;&#010;&gt;&#010;&gt; --------------------------------------------------------------分割线-------------------------------------------------------------&#010;&gt;&#010;&gt; 祝好！&#010;&gt; amenhub&#010;&gt;&#010;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "1",
        "reply": "<202007141437518255817@163.com>"
    },
    {
        "id": "<488a95e6.84ab.1734c18c82d.Coremail.lixin58688@163.com>",
        "from": "李宇彬 &lt;lixin58...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 06:52:39 GMT",
        "subject": "如果解决写hdfs的文件描述符问题？",
        "content": "hi, everyone&#010;&#010;&#010;&#010;    环境信息：flink-1.10.0、hadoop 2.6.0&#010;&#010;    我从kafka topic消费数据，通过BucketSink写到hdfs，在0点的时候会遇到这样的问题，要同时消费当天和昨天的数据，这样在写hdfs时会产生两倍的文件描述符，对hdfs造成很大的压力，请问在不增加文件描述符上限的情况下，如何解决？",
        "depth": "0",
        "reply": "<488a95e6.84ab.1734c18c82d.Coremail.lixin58688@163.com>"
    },
    {
        "id": "<CAHOPYeVvoJLkZSJNS2raBZe88T6H5YRZQ7L9tf1vML3VTqU8FQ@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 07:31:45 GMT",
        "subject": "[flink-sql] 如何在sql运行时动态修改kafka的scan.startup.mode",
        "content": "hi  all&#013;&#010;现在有个需求，就是一段用sql-client提交的sql任务需要动态修改kafka的scan.startup.mode，以支持不同的消费需求。请问有什么好的办法吗？&#013;&#010;谢谢&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "0",
        "reply": "<CAHOPYeVvoJLkZSJNS2raBZe88T6H5YRZQ7L9tf1vML3VTqU8FQ@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_R4Zggw=QXXxHUoHi5UeswNrZh22UPX+9+=MxXqF1dm6Q@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 07:37:48 GMT",
        "subject": "Re: [flink-sql] 如何在sql运行时动态修改kafka的scan.startup.mode",
        "content": "可以尝试下1.11中引入的LIKE[1]或者是Table Hint[2]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/create.html#create-table&#013;&#010;[2]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/hints.html&#013;&#010;&#013;&#010;Harold.Miao &lt;miaohonghit@gmail.com&gt; 于2020年7月14日周二 下午3:32写道：&#013;&#010;&#013;&#010;&gt; hi  all&#013;&#010;&gt;&#013;&#010;&gt; 现在有个需求，就是一段用sql-client提交的sql任务需要动态修改kafka的scan.startup.mode，以支持不同的消费需求。请问有什么好的办法吗？&#013;&#010;&gt; 谢谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best Regards,&#013;&#010;&gt; Harold Miao&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<CAHOPYeVvoJLkZSJNS2raBZe88T6H5YRZQ7L9tf1vML3VTqU8FQ@mail.gmail.com>"
    },
    {
        "id": "<202007141547551571989@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 07:47:56 GMT",
        "subject": "不能实时读取实时写入到 Hive 的数据",
        "content": "&#013;&#010;试验了一下 Flink-1.11  hive streaming 的功能&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#013;&#010;&#013;&#010;创建 kafka 表，通过 SQL 实时写入 Hive.&#013;&#010;&#013;&#010;但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<626F1908-3C0B-4E76-8B83-66872C158088@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:17:34 GMT",
        "subject": "Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "HI, wanglei&#010;&#010;你开启了 streaming-source.enable 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints 方便地指定参数。&#010;&#010;SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-05-20')&#010;*/;&#010;&#010;就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#010;&gt; &#010;&gt; &#010;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#010;&gt; &#010;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#010;&gt; &#010;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#010;&gt; &#010;&gt; 谢谢，&#010;&gt; 王磊&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; wanglei2@geekplus.com.cn &#010;&gt; &#010;&#010;&#010;",
        "depth": "1",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<2020071416403836301514@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:40:39 GMT",
        "subject": "回复: Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "&#013;&#010;我加上了这个 tablehint 。&#013;&#010;任务提交上去了，但客户端还是没有任何返回显示。 &#013;&#010;我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是 .part&#010;开头的 inprogress 文件。 &#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;发件人： Leonard Xu&#013;&#010;发送时间： 2020-07-14 16:17&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;HI, wanglei&#013;&#010; &#013;&#010;你开启了 streaming-source.enable 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints 方便地指定参数。&#013;&#010; &#013;&#010;SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-05-20')&#010;*/;&#013;&#010; &#013;&#010;就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#013;&#010; &#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010; &#013;&#010;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#013;&#010;&gt; &#013;&#010;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#013;&#010;&gt; &#013;&#010;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#013;&#010;&gt; &#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; wanglei2@geekplus.com.cn &#013;&#010;&gt; &#013;&#010; &#013;&#010;",
        "depth": "2",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<3df1ceeb-b958-4ab0-a12a-17843ad94803.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:43:58 GMT",
        "subject": "回复：Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "你好,&#010;这说明写入的hive文件没有进行rollup,可以贴下SQL么&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#010;发送时间：2020年7月14日(星期二) 16:40&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#010;主　题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#010;&#010;&#010;我加上了这个 tablehint 。&#010;任务提交上去了，但客户端还是没有任何返回显示。 &#010;我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是 .part&#010;开头的 inprogress 文件。 &#010;&#010;谢谢，&#010;王磊&#010;&#010;&#010;&#010;wanglei2@geekplus.com.cn &#010;&#010;发件人： Leonard Xu&#010;发送时间： 2020-07-14 16:17&#010;收件人： user-zh&#010;主题： Re: 不能实时读取实时写入到 Hive 的数据&#010;HI, wanglei&#010;&#010;你开启了 streaming-source.enable 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints 方便地指定参数。&#010;&#010;SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-05-20')&#010;*/;&#010;&#010;就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#010;&gt; &#010;&gt; &#010;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#010;&gt; &#010;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#010;&gt; &#010;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#010;&gt; &#010;&gt; 谢谢，&#010;&gt; 王磊&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; wanglei2@geekplus.com.cn &#010;&gt; &#010;&#010;",
        "depth": "1",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<2020071416500244902417@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:50:03 GMT",
        "subject": "Re: 回复: 不能实时读取实时写入到 Hive 的数据",
        "content": "应该是我没有理解 partitiion-commit 的意思，我看这里有文档：https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#partition-commit&#013;&#010;&#013;&#010;&#013;&#010;CREATE TABLE kafka_ods_wms_pick_order (&#013;&#010; order_no STRING,&#013;&#010; status INT,&#013;&#010; dispatch_time TIMESTAMP(3)&#013;&#010;) WITH (&#013;&#010; 'connector' = 'kafka',&#013;&#010; 'topic' = 'ods_wms_pick_order',&#013;&#010; 'properties.bootstrap.servers' = 'xxxx:9092',&#013;&#010; 'properties.group.id' = 'testGroup',&#013;&#010; 'format' = 'json',&#013;&#010; 'scan.startup.mode' = 'latest-offset'&#013;&#010;)&#013;&#010;&#013;&#010;&#013;&#010;CREATE TABLE hive_ods_wms_pick_order (&#013;&#010;  order_no STRING,&#013;&#010;  status INT,&#013;&#010;  dispatch_time TIMESTAMP&#013;&#010;) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#013;&#010;  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#013;&#010;  'sink.partition-commit.trigger'='partition-time',&#013;&#010;  'sink.partition-commit.delay'='1 h',&#013;&#010;  'sink.partition-commit.policy.kind'='metastore,success-file'&#013;&#010;);&#013;&#010;&#013;&#010;INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status, dispatch_time, DATE_FORMAT(dispatch_time,&#010;'yyyy-MM-dd'), DATE_FORMAT(dispatch_time, 'HH') FROM kafka_ods_wms_pick_order;&#013;&#010;SELECT * FROM hive_ods_wms_pick_order /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-07-24')&#010;*/;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010; &#013;&#010;Sender: 夏帅&#013;&#010;Send Time: 2020-07-14 16:43&#013;&#010;Receiver: user-zh; xbjtdcq&#013;&#010;Subject: 回复：Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;你好,&#013;&#010;这说明写入的hive文件没有进行rollup,可以贴下SQL么&#013;&#010; &#013;&#010; &#013;&#010;------------------------------------------------------------------&#013;&#010;发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;发送时间：2020年7月14日(星期二) 16:40&#013;&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#013;&#010;主　题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010; &#013;&#010; &#013;&#010;我加上了这个 tablehint 。&#013;&#010;任务提交上去了，但客户端还是没有任何返回显示。 &#013;&#010;我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是 .part&#010;开头的 inprogress 文件。 &#013;&#010; &#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;wanglei2@geekplus.com.cn &#013;&#010; &#013;&#010;发件人： Leonard Xu&#013;&#010;发送时间： 2020-07-14 16:17&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;HI, wanglei&#013;&#010; &#013;&#010;你开启了 streaming-source.enable 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints 方便地指定参数。&#013;&#010; &#013;&#010;SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-05-20')&#010;*/;&#013;&#010; &#013;&#010;就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#013;&#010; &#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010; &#013;&#010;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#013;&#010;&gt; &#013;&#010;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#013;&#010;&gt; &#013;&#010;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#013;&#010;&gt; &#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; wanglei2@geekplus.com.cn &#013;&#010;&gt; &#013;&#010; &#013;&#010;",
        "depth": "2",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<6121de37-e84f-4df9-9bd9-238f3da03b64.jkillers@dingtalk.com>",
        "from": "&quot;夏帅&quot; &lt;jkill...@dingtalk.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 08:59:06 GMT",
        "subject": "回复：回复: 不能实时读取实时写入到 Hive 的数据",
        "content": "你好,&#010;可以参考下这个问题的解决&#010;http://apache-flink.147419.n8.nabble.com/Table-options-do-not-contain-an-option-key-connector-for-discovering-a-connector-td4767.html&#010;&#010;&#010;------------------------------------------------------------------&#010;发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#010;发送时间：2020年7月14日(星期二) 16:50&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;; 夏帅 &lt;jkillers@dingtalk.com&gt;;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#010;主　题：Re: 回复: 不能实时读取实时写入到 Hive 的数据&#010;&#010;&#010;应该是我没有理解 partitiion-commit 的意思，我看这里有文档：https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#partition-commit&#010;&#010;&#010;CREATE TABLE kafka_ods_wms_pick_order (&#010; order_no STRING,&#010; status INT,&#010; dispatch_time TIMESTAMP(3)&#010;) WITH (&#010; 'connector' = 'kafka',&#010; 'topic' = 'ods_wms_pick_order',&#010; 'properties.bootstrap.servers' = 'xxxx:9092',&#010; 'properties.group.id' = 'testGroup',&#010; 'format' = 'json',&#010; 'scan.startup.mode' = 'latest-offset'&#010;)&#010;&#010;&#010;CREATE TABLE hive_ods_wms_pick_order (&#010;  order_no STRING,&#010;  status INT,&#010;  dispatch_time TIMESTAMP&#010;) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;  'sink.partition-commit.trigger'='partition-time',&#010;  'sink.partition-commit.delay'='1 h',&#010;  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;);&#010;&#010;INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status, dispatch_time, DATE_FORMAT(dispatch_time,&#010;'yyyy-MM-dd'), DATE_FORMAT(dispatch_time, 'HH') FROM kafka_ods_wms_pick_order;&#010;SELECT * FROM hive_ods_wms_pick_order /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-07-24')&#010;*/;&#010;&#010;&#010;&#010;&#010;wanglei2@geekplus.com.cn&#010;&#010;&#010;Sender: 夏帅&#010;Send Time: 2020-07-14 16:43&#010;Receiver: user-zh; xbjtdcq&#010;Subject: 回复：Re: 不能实时读取实时写入到 Hive 的数据&#010;你好,&#010;这说明写入的hive文件没有进行rollup,可以贴下SQL么&#010;------------------------------------------------------------------&#010;发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#010;发送时间：2020年7月14日(星期二) 16:40&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#010;主　题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#010;我加上了这个 tablehint 。&#010;任务提交上去了，但客户端还是没有任何返回显示。 &#010;我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是 .part&#010;开头的 inprogress 文件。 &#010;谢谢，&#010;王磊&#010;wanglei2@geekplus.com.cn &#010;发件人： Leonard Xu&#010;发送时间： 2020-07-14 16:17&#010;收件人： user-zh&#010;主题： Re: 不能实时读取实时写入到 Hive 的数据&#010;HI, wanglei&#010;你开启了 streaming-source.enable 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints 方便地指定参数。&#010;SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-05-20')&#010;*/;&#010;就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#010;祝好，&#010;Leonard Xu&#010;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#010;&gt; &#010;&gt; &#010;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#010;&gt; &#010;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#010;&gt; &#010;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#010;&gt; &#010;&gt; 谢谢，&#010;&gt; 王磊&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; wanglei2@geekplus.com.cn &#010;&gt; &#010;",
        "depth": "3",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<C2218A78-FB5C-458B-979A-5073E4D2BB82@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:20:26 GMT",
        "subject": "Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "&#010;Hi, wanglei&#010;&#010;这个参数 'sink.partition-commit.delay'='1 h’会在cp 完成后 + 你设置的1h delay后才会提交&#010;hive 的分区已完成信息(通过metastore或success文件).&#010;&#010;你看下夏帅贴的邮件，检查下 checkpoint 和  partition-commit的设置&#010;&#010;祝好,&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月14日，16:59，夏帅 &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#010;&gt; &#010;&gt; 你好,&#010;&gt; 可以参考下这个问题的解决&#010;&gt; http://apache-flink.147419.n8.nabble.com/Table-options-do-not-contain-an-option-key-connector-for-discovering-a-connector-td4767.html&#010;&gt; &#010;&gt; &#010;&gt; ------------------------------------------------------------------&#010;&gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#010;&gt; 发送时间：2020年7月14日(星期二) 16:50&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; 夏帅 &lt;jkillers@dingtalk.com&gt;;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#010;&gt; 主　题：Re: 回复: 不能实时读取实时写入到 Hive 的数据&#010;&gt; &#010;&gt; &#010;&gt; 应该是我没有理解 partitiion-commit 的意思，我看这里有文档：https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#partition-commit&#010;&gt; &#010;&gt; &#010;&gt; CREATE TABLE kafka_ods_wms_pick_order (&#010;&gt; order_no STRING,&#010;&gt; status INT,&#010;&gt; dispatch_time TIMESTAMP(3)&#010;&gt; ) WITH (&#010;&gt; 'connector' = 'kafka',&#010;&gt; 'topic' = 'ods_wms_pick_order',&#010;&gt; 'properties.bootstrap.servers' = 'xxxx:9092',&#010;&gt; 'properties.group.id' = 'testGroup',&#010;&gt; 'format' = 'json',&#010;&gt; 'scan.startup.mode' = 'latest-offset'&#010;&gt; )&#010;&gt; &#010;&gt; &#010;&gt; CREATE TABLE hive_ods_wms_pick_order (&#010;&gt;  order_no STRING,&#010;&gt;  status INT,&#010;&gt;  dispatch_time TIMESTAMP&#010;&gt; ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;&gt;  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;  'sink.partition-commit.trigger'='partition-time',&#010;&gt;  'sink.partition-commit.delay'='1 h',&#010;&gt;  'sink.partition-commit.policy.kind'='metastore,success-file'&#010;&gt; );&#010;&gt; &#010;&gt; INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status, dispatch_time, DATE_FORMAT(dispatch_time,&#010;'yyyy-MM-dd'), DATE_FORMAT(dispatch_time, 'HH') FROM kafka_ods_wms_pick_order;&#010;&gt; SELECT * FROM hive_ods_wms_pick_order /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-07-24')&#010;*/;&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; wanglei2@geekplus.com.cn&#010;&gt; &#010;&gt; &#010;&gt; Sender: 夏帅&#010;&gt; Send Time: 2020-07-14 16:43&#010;&gt; Receiver: user-zh; xbjtdcq&#010;&gt; Subject: 回复：Re: 不能实时读取实时写入到 Hive 的数据&#010;&gt; 你好,&#010;&gt; 这说明写入的hive文件没有进行rollup,可以贴下SQL么&#010;&gt; ------------------------------------------------------------------&#010;&gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#010;&gt; 发送时间：2020年7月14日(星期二) 16:40&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#010;&gt; 主　题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#010;&gt; 我加上了这个 tablehint 。&#010;&gt; 任务提交上去了，但客户端还是没有任何返回显示。 &#010;&gt; 我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是&#010;.part 开头的 inprogress 文件。 &#010;&gt; 谢谢，&#010;&gt; 王磊&#010;&gt; wanglei2@geekplus.com.cn &#010;&gt; 发件人： Leonard Xu&#010;&gt; 发送时间： 2020-07-14 16:17&#010;&gt; 收件人： user-zh&#010;&gt; 主题： Re: 不能实时读取实时写入到 Hive 的数据&#010;&gt; HI, wanglei&#010;&gt; 你开启了 streaming-source.enable 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints 方便地指定参数。&#010;&gt; SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-05-20')&#010;*/;&#010;&gt; 就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#010;&gt; 祝好，&#010;&gt; Leonard Xu&#010;&gt;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#010;&gt;&gt; &#010;&gt;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#010;&gt;&gt; &#010;&gt;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#010;&gt;&gt; &#010;&gt;&gt; 谢谢，&#010;&gt;&gt; 王磊&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; wanglei2@geekplus.com.cn &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "4",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<CADH6UNTTGgkqVqpiAJbH+m-ZSyrEEebuoKGkAzCCX0zinRxoLA@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:24:39 GMT",
        "subject": "Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "还可以在hive那边验证一下数据是否commit了，比如从hive CLI端执行一下show&#010;partitions，或者读一点数据&#013;&#010;&#013;&#010;On Tue, Jul 14, 2020 at 5:20 PM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; Hi, wanglei&#013;&#010;&gt;&#013;&#010;&gt; 这个参数 'sink.partition-commit.delay'='1 h’会在cp 完成后 + 你设置的1h&#010;delay后才会提交 hive&#013;&#010;&gt; 的分区已完成信息(通过metastore或success文件).&#013;&#010;&gt;&#013;&#010;&gt; 你看下夏帅贴的邮件，检查下 checkpoint 和  partition-commit的设置&#013;&#010;&gt;&#013;&#010;&gt; 祝好,&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月14日，16:59，夏帅 &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 你好,&#013;&#010;&gt; &gt; 可以参考下这个问题的解决&#013;&#010;&gt; &gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/Table-options-do-not-contain-an-option-key-connector-for-discovering-a-connector-td4767.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ------------------------------------------------------------------&#013;&#010;&gt; &gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;&gt; &gt; 发送时间：2020年7月14日(星期二) 16:50&#013;&#010;&gt; &gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; 夏帅 &lt;jkillers@dingtalk.com&gt;;&#013;&#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#013;&#010;&gt; &gt; 主 题：Re: 回复: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 应该是我没有理解 partitiion-commit 的意思，我看这里有文档：&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#partition-commit&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; CREATE TABLE kafka_ods_wms_pick_order (&#013;&#010;&gt; &gt; order_no STRING,&#013;&#010;&gt; &gt; status INT,&#013;&#010;&gt; &gt; dispatch_time TIMESTAMP(3)&#013;&#010;&gt; &gt; ) WITH (&#013;&#010;&gt; &gt; 'connector' = 'kafka',&#013;&#010;&gt; &gt; 'topic' = 'ods_wms_pick_order',&#013;&#010;&gt; &gt; 'properties.bootstrap.servers' = 'xxxx:9092',&#013;&#010;&gt; &gt; 'properties.group.id' = 'testGroup',&#013;&#010;&gt; &gt; 'format' = 'json',&#013;&#010;&gt; &gt; 'scan.startup.mode' = 'latest-offset'&#013;&#010;&gt; &gt; )&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; CREATE TABLE hive_ods_wms_pick_order (&#013;&#010;&gt; &gt;  order_no STRING,&#013;&#010;&gt; &gt;  status INT,&#013;&#010;&gt; &gt;  dispatch_time TIMESTAMP&#013;&#010;&gt; &gt; ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#013;&#010;&gt; &gt;  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#013;&#010;&gt; &gt;  'sink.partition-commit.trigger'='partition-time',&#013;&#010;&gt; &gt;  'sink.partition-commit.delay'='1 h',&#013;&#010;&gt; &gt;  'sink.partition-commit.policy.kind'='metastore,success-file'&#013;&#010;&gt; &gt; );&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status,&#013;&#010;&gt; dispatch_time, DATE_FORMAT(dispatch_time, 'yyyy-MM-dd'),&#013;&#010;&gt; DATE_FORMAT(dispatch_time, 'HH') FROM kafka_ods_wms_pick_order;&#013;&#010;&gt; &gt; SELECT * FROM hive_ods_wms_pick_order /*+&#013;&#010;&gt; OPTIONS('streaming-source.enable'='true',&#013;&#010;&gt; 'streaming-source.consume-start-offset'='2020-07-24') */;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Sender: 夏帅&#013;&#010;&gt; &gt; Send Time: 2020-07-14 16:43&#013;&#010;&gt; &gt; Receiver: user-zh; xbjtdcq&#013;&#010;&gt; &gt; Subject: 回复：Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; &gt; 你好,&#013;&#010;&gt; &gt; 这说明写入的hive文件没有进行rollup,可以贴下SQL么&#013;&#010;&gt; &gt; ------------------------------------------------------------------&#013;&#010;&gt; &gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;&gt; &gt; 发送时间：2020年7月14日(星期二) 16:40&#013;&#010;&gt; &gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#013;&#010;&gt; &gt; 主 题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; &gt; 我加上了这个 tablehint 。&#013;&#010;&gt; &gt; 任务提交上去了，但客户端还是没有任何返回显示。&#013;&#010;&gt; &gt; 我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是&#010;.part 开头的 inprogress 文件。&#013;&#010;&gt; &gt; 谢谢，&#013;&#010;&gt; &gt; 王磊&#013;&#010;&gt; &gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; &gt; 发件人： Leonard Xu&#013;&#010;&gt; &gt; 发送时间： 2020-07-14 16:17&#013;&#010;&gt; &gt; 收件人： user-zh&#013;&#010;&gt; &gt; 主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; &gt; HI, wanglei&#013;&#010;&gt; &gt; 你开启了 streaming-source.enable&#013;&#010;&gt; 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints&#013;&#010;&gt; 方便地指定参数。&#013;&#010;&gt; &gt; SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true',&#013;&#010;&gt; 'streaming-source.consume-start-offset'='2020-05-20') */;&#013;&#010;&gt; &gt; 就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#013;&#010;&gt; &gt; 祝好，&#013;&#010;&gt; &gt; Leonard Xu&#013;&#010;&gt; &gt;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink&#013;&#010;&gt; webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 谢谢，&#013;&#010;&gt; &gt;&gt; 王磊&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "5",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<2020071417493613931426@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:49:37 GMT",
        "subject": "回复: Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "我把问题简化一下，创建 Hive 表时不带任何参数&#013;&#010;&#013;&#010;CREATE TABLE hive_ods_wms_pick_order (&#013;&#010;  order_no STRING,&#013;&#010;  status INT,&#013;&#010;  dispatch_time TIMESTAMP&#013;&#010;)  STORED AS parquet&#013;&#010;&#013;&#010;INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status, dispatch_time FROM kafka_ods_wms_pick_order;&#013;&#010;&#013;&#010;我用的 sql-client 客户端，15 分钟过去了 hive 表对应的 hdfs 目录为什么还只是有一个大小为&#010;0 的 .part 文件呢？&#013;&#010;我在 flink 客户端 SELECT order_no, status, dispatch_time FROM kafka_ods_wms_pick_order&#010;确实是有数据返回的。&#013;&#010;&#013;&#010;我在 flink web ui 看了下这个 job 的 Checkpoint Counts 是 0.&#013;&#010;是需要让 job 做 checkpoint 才能写到 hdfs 上吗？&#013;&#010;我用 Flink sql-client 客户端怎么设置做 checkpoint 的频率呢？&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010; &#013;&#010;发件人： Leonard Xu&#013;&#010;发送时间： 2020-07-14 17:20&#013;&#010;收件人： user-zh; 夏帅&#013;&#010;抄送： wanglei2@geekplus.com.cn&#013;&#010;主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&#013;&#010;Hi, wanglei&#013;&#010;&#013;&#010;这个参数 'sink.partition-commit.delay'='1 h’会在cp 完成后 + 你设置的1h delay后才会提交&#010;hive 的分区已完成信息(通过metastore或success文件).&#013;&#010;&#013;&#010;你看下夏帅贴的邮件，检查下 checkpoint 和  partition-commit的设置&#013;&#010;&#013;&#010;祝好,&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;在 2020年7月14日，16:59，夏帅 &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#013;&#010;&#013;&#010;你好,&#013;&#010;可以参考下这个问题的解决&#013;&#010;http://apache-flink.147419.n8.nabble.com/Table-options-do-not-contain-an-option-key-connector-for-discovering-a-connector-td4767.html&#013;&#010;&#013;&#010;&#013;&#010;------------------------------------------------------------------&#013;&#010;发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;发送时间：2020年7月14日(星期二) 16:50&#013;&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;; 夏帅 &lt;jkillers@dingtalk.com&gt;;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#013;&#010;主　题：Re: 回复: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&#013;&#010;&#013;&#010;应该是我没有理解 partitiion-commit 的意思，我看这里有文档：https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#partition-commit&#013;&#010;&#013;&#010;&#013;&#010;CREATE TABLE kafka_ods_wms_pick_order (&#013;&#010;order_no STRING,&#013;&#010;status INT,&#013;&#010;dispatch_time TIMESTAMP(3)&#013;&#010;) WITH (&#013;&#010;'connector' = 'kafka',&#013;&#010;'topic' = 'ods_wms_pick_order',&#013;&#010;'properties.bootstrap.servers' = 'xxxx:9092',&#013;&#010;'properties.group.id' = 'testGroup',&#013;&#010;'format' = 'json',&#013;&#010;'scan.startup.mode' = 'latest-offset'&#013;&#010;)&#013;&#010;&#013;&#010;&#013;&#010;CREATE TABLE hive_ods_wms_pick_order (&#013;&#010; order_no STRING,&#013;&#010; status INT,&#013;&#010; dispatch_time TIMESTAMP&#013;&#010;) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#013;&#010; 'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#013;&#010; 'sink.partition-commit.trigger'='partition-time',&#013;&#010; 'sink.partition-commit.delay'='1 h',&#013;&#010; 'sink.partition-commit.policy.kind'='metastore,success-file'&#013;&#010;);&#013;&#010;&#013;&#010;INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status, dispatch_time, DATE_FORMAT(dispatch_time,&#010;'yyyy-MM-dd'), DATE_FORMAT(dispatch_time, 'HH') FROM kafka_ods_wms_pick_order;&#013;&#010;SELECT * FROM hive_ods_wms_pick_order /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-07-24')&#010;*/;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn&#013;&#010;&#013;&#010;&#013;&#010;Sender: 夏帅&#013;&#010;Send Time: 2020-07-14 16:43&#013;&#010;Receiver: user-zh; xbjtdcq&#013;&#010;Subject: 回复：Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;你好,&#013;&#010;这说明写入的hive文件没有进行rollup,可以贴下SQL么&#013;&#010;------------------------------------------------------------------&#013;&#010;发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;发送时间：2020年7月14日(星期二) 16:40&#013;&#010;收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#013;&#010;主　题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;我加上了这个 tablehint 。&#013;&#010;任务提交上去了，但客户端还是没有任何返回显示。 &#013;&#010;我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是 .part&#010;开头的 inprogress 文件。 &#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;发件人： Leonard Xu&#013;&#010;发送时间： 2020-07-14 16:17&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;HI, wanglei&#013;&#010;你开启了 streaming-source.enable 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints 方便地指定参数。&#013;&#010;SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true', 'streaming-source.consume-start-offset'='2020-05-20')&#010;*/;&#013;&#010;就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#013;&#010;&#013;&#010;&#013;&#010;试验了一下 Flink-1.11  hive streaming 的功能&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#013;&#010;&#013;&#010;创建 kafka 表，通过 SQL 实时写入 Hive.&#013;&#010;&#013;&#010;但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;&#013;&#010;",
        "depth": "1",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<CADH6UNTgxt9bOiHbT+5WossMQrbPKwJzvwh56Rp14Fn15Ez3BQ@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:29:52 GMT",
        "subject": "Re: Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "流数据写hive时，不管是分区表还是非分区表，commit都是通过checkpoint触发的。用SQL&#013;&#010;client的话可以在flink-conf.yaml里设置execution.checkpointing.interval来开启checkpoint&#013;&#010;&#013;&#010;On Tue, Jul 14, 2020 at 5:49 PM wanglei2@geekplus.com.cn &lt;&#013;&#010;wanglei2@geekplus.com.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; 我把问题简化一下，创建 Hive 表时不带任何参数&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE hive_ods_wms_pick_order (&#013;&#010;&gt;   order_no STRING,&#013;&#010;&gt;   status INT,&#013;&#010;&gt;   dispatch_time TIMESTAMP&#013;&#010;&gt; )  STORED AS parquet&#013;&#010;&gt;&#013;&#010;&gt; INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status,&#013;&#010;&gt; dispatch_time FROM kafka_ods_wms_pick_order;&#013;&#010;&gt;&#013;&#010;&gt; 我用的 sql-client 客户端，15 分钟过去了 hive 表对应的 hdfs 目录为什么还只是有一个大小为&#010;0 的 .part 文件呢？&#013;&#010;&gt; 我在 flink 客户端 SELECT order_no, status, dispatch_time FROM&#013;&#010;&gt; kafka_ods_wms_pick_order 确实是有数据返回的。&#013;&#010;&gt;&#013;&#010;&gt; 我在 flink web ui 看了下这个 job 的 Checkpoint Counts 是 0.&#013;&#010;&gt; 是需要让 job 做 checkpoint 才能写到 hdfs 上吗？&#013;&#010;&gt; 我用 Flink sql-client 客户端怎么设置做 checkpoint 的频率呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 发件人： Leonard Xu&#013;&#010;&gt; 发送时间： 2020-07-14 17:20&#013;&#010;&gt; 收件人： user-zh; 夏帅&#013;&#010;&gt; 抄送： wanglei2@geekplus.com.cn&#013;&#010;&gt; 主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt;&#013;&#010;&gt; Hi, wanglei&#013;&#010;&gt;&#013;&#010;&gt; 这个参数 'sink.partition-commit.delay'='1 h’会在cp 完成后 + 你设置的1h&#010;delay后才会提交 hive&#013;&#010;&gt; 的分区已完成信息(通过metastore或success文件).&#013;&#010;&gt;&#013;&#010;&gt; 你看下夏帅贴的邮件，检查下 checkpoint 和  partition-commit的设置&#013;&#010;&gt;&#013;&#010;&gt; 祝好,&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020年7月14日，16:59，夏帅 &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt; 你好,&#013;&#010;&gt; 可以参考下这个问题的解决&#013;&#010;&gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/Table-options-do-not-contain-an-option-key-connector-for-discovering-a-connector-td4767.html&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------------------------------------------------------&#013;&#010;&gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;&gt; 发送时间：2020年7月14日(星期二) 16:50&#013;&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; 夏帅 &lt;jkillers@dingtalk.com&gt;;&#013;&#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#013;&#010;&gt; 主 题：Re: 回复: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 应该是我没有理解 partitiion-commit 的意思，我看这里有文档：&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#partition-commit&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE kafka_ods_wms_pick_order (&#013;&#010;&gt; order_no STRING,&#013;&#010;&gt; status INT,&#013;&#010;&gt; dispatch_time TIMESTAMP(3)&#013;&#010;&gt; ) WITH (&#013;&#010;&gt; 'connector' = 'kafka',&#013;&#010;&gt; 'topic' = 'ods_wms_pick_order',&#013;&#010;&gt; 'properties.bootstrap.servers' = 'xxxx:9092',&#013;&#010;&gt; 'properties.group.id' = 'testGroup',&#013;&#010;&gt; 'format' = 'json',&#013;&#010;&gt; 'scan.startup.mode' = 'latest-offset'&#013;&#010;&gt; )&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE hive_ods_wms_pick_order (&#013;&#010;&gt;  order_no STRING,&#013;&#010;&gt;  status INT,&#013;&#010;&gt;  dispatch_time TIMESTAMP&#013;&#010;&gt; ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#013;&#010;&gt;  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#013;&#010;&gt;  'sink.partition-commit.trigger'='partition-time',&#013;&#010;&gt;  'sink.partition-commit.delay'='1 h',&#013;&#010;&gt;  'sink.partition-commit.policy.kind'='metastore,success-file'&#013;&#010;&gt; );&#013;&#010;&gt;&#013;&#010;&gt; INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status,&#013;&#010;&gt; dispatch_time, DATE_FORMAT(dispatch_time, 'yyyy-MM-dd'),&#013;&#010;&gt; DATE_FORMAT(dispatch_time, 'HH') FROM kafka_ods_wms_pick_order;&#013;&#010;&gt; SELECT * FROM hive_ods_wms_pick_order /*+&#013;&#010;&gt; OPTIONS('streaming-source.enable'='true',&#013;&#010;&gt; 'streaming-source.consume-start-offset'='2020-07-24') */;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Sender: 夏帅&#013;&#010;&gt; Send Time: 2020-07-14 16:43&#013;&#010;&gt; Receiver: user-zh; xbjtdcq&#013;&#010;&gt; Subject: 回复：Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; 你好,&#013;&#010;&gt; 这说明写入的hive文件没有进行rollup,可以贴下SQL么&#013;&#010;&gt; ------------------------------------------------------------------&#013;&#010;&gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;&gt; 发送时间：2020年7月14日(星期二) 16:40&#013;&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#013;&#010;&gt; 主 题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; 我加上了这个 tablehint 。&#013;&#010;&gt; 任务提交上去了，但客户端还是没有任何返回显示。&#013;&#010;&gt; 我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是&#010;.part 开头的 inprogress 文件。&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; 发件人： Leonard Xu&#013;&#010;&gt; 发送时间： 2020-07-14 16:17&#013;&#010;&gt; 收件人： user-zh&#013;&#010;&gt; 主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; HI, wanglei&#013;&#010;&gt; 你开启了 streaming-source.enable&#013;&#010;&gt; 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints&#013;&#010;&gt; 方便地指定参数。&#013;&#010;&gt; SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true',&#013;&#010;&gt; 'streaming-source.consume-start-offset'='2020-05-20') */;&#013;&#010;&gt; 就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#013;&#010;&gt; 祝好，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#013;&#010;&gt;&#013;&#010;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#013;&#010;&gt;&#013;&#010;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink&#013;&#010;&gt; webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "2",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<2020071418495504683228@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:49:56 GMT",
        "subject": "Re: Re: 不能实时读取实时写入到 Hive 的数据",
        "content": "&#013;&#010;谢谢，根本原因就是  flink sql-client 客户端默认没有设置 checkpoint 导致的。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010; &#013;&#010;Sender: Rui Li&#013;&#010;Send Time: 2020-07-14 18:29&#013;&#010;Receiver: user-zh&#013;&#010;cc: Leonard Xu; 夏帅&#013;&#010;Subject: Re: Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;流数据写hive时，不管是分区表还是非分区表，commit都是通过checkpoint触发的。用SQL&#013;&#010;client的话可以在flink-conf.yaml里设置execution.checkpointing.interval来开启checkpoint&#013;&#010; &#013;&#010;On Tue, Jul 14, 2020 at 5:49 PM wanglei2@geekplus.com.cn &lt;&#013;&#010;wanglei2@geekplus.com.cn&gt; wrote:&#013;&#010; &#013;&#010;&gt; 我把问题简化一下，创建 Hive 表时不带任何参数&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE hive_ods_wms_pick_order (&#013;&#010;&gt;   order_no STRING,&#013;&#010;&gt;   status INT,&#013;&#010;&gt;   dispatch_time TIMESTAMP&#013;&#010;&gt; )  STORED AS parquet&#013;&#010;&gt;&#013;&#010;&gt; INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status,&#013;&#010;&gt; dispatch_time FROM kafka_ods_wms_pick_order;&#013;&#010;&gt;&#013;&#010;&gt; 我用的 sql-client 客户端，15 分钟过去了 hive 表对应的 hdfs 目录为什么还只是有一个大小为&#010;0 的 .part 文件呢？&#013;&#010;&gt; 我在 flink 客户端 SELECT order_no, status, dispatch_time FROM&#013;&#010;&gt; kafka_ods_wms_pick_order 确实是有数据返回的。&#013;&#010;&gt;&#013;&#010;&gt; 我在 flink web ui 看了下这个 job 的 Checkpoint Counts 是 0.&#013;&#010;&gt; 是需要让 job 做 checkpoint 才能写到 hdfs 上吗？&#013;&#010;&gt; 我用 Flink sql-client 客户端怎么设置做 checkpoint 的频率呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 发件人： Leonard Xu&#013;&#010;&gt; 发送时间： 2020-07-14 17:20&#013;&#010;&gt; 收件人： user-zh; 夏帅&#013;&#010;&gt; 抄送： wanglei2@geekplus.com.cn&#013;&#010;&gt; 主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt;&#013;&#010;&gt; Hi, wanglei&#013;&#010;&gt;&#013;&#010;&gt; 这个参数 'sink.partition-commit.delay'='1 h’会在cp 完成后 + 你设置的1h&#010;delay后才会提交 hive&#013;&#010;&gt; 的分区已完成信息(通过metastore或success文件).&#013;&#010;&gt;&#013;&#010;&gt; 你看下夏帅贴的邮件，检查下 checkpoint 和  partition-commit的设置&#013;&#010;&gt;&#013;&#010;&gt; 祝好,&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020年7月14日，16:59，夏帅 &lt;jkillers@dingtalk.com.INVALID&gt; 写道：&#013;&#010;&gt;&#013;&#010;&gt; 你好,&#013;&#010;&gt; 可以参考下这个问题的解决&#013;&#010;&gt;&#013;&#010;&gt; http://apache-flink.147419.n8.nabble.com/Table-options-do-not-contain-an-option-key-connector-for-discovering-a-connector-td4767.html&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------------------------------------------------------&#013;&#010;&gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;&gt; 发送时间：2020年7月14日(星期二) 16:50&#013;&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; 夏帅 &lt;jkillers@dingtalk.com&gt;;&#013;&#010;&gt; Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#013;&#010;&gt; 主 题：Re: 回复: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 应该是我没有理解 partitiion-commit 的意思，我看这里有文档：&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#partition-commit&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE kafka_ods_wms_pick_order (&#013;&#010;&gt; order_no STRING,&#013;&#010;&gt; status INT,&#013;&#010;&gt; dispatch_time TIMESTAMP(3)&#013;&#010;&gt; ) WITH (&#013;&#010;&gt; 'connector' = 'kafka',&#013;&#010;&gt; 'topic' = 'ods_wms_pick_order',&#013;&#010;&gt; 'properties.bootstrap.servers' = 'xxxx:9092',&#013;&#010;&gt; 'properties.group.id' = 'testGroup',&#013;&#010;&gt; 'format' = 'json',&#013;&#010;&gt; 'scan.startup.mode' = 'latest-offset'&#013;&#010;&gt; )&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE hive_ods_wms_pick_order (&#013;&#010;&gt;  order_no STRING,&#013;&#010;&gt;  status INT,&#013;&#010;&gt;  dispatch_time TIMESTAMP&#013;&#010;&gt; ) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#013;&#010;&gt;  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#013;&#010;&gt;  'sink.partition-commit.trigger'='partition-time',&#013;&#010;&gt;  'sink.partition-commit.delay'='1 h',&#013;&#010;&gt;  'sink.partition-commit.policy.kind'='metastore,success-file'&#013;&#010;&gt; );&#013;&#010;&gt;&#013;&#010;&gt; INSERT INTO TABLE hive_ods_wms_pick_order SELECT order_no, status,&#013;&#010;&gt; dispatch_time, DATE_FORMAT(dispatch_time, 'yyyy-MM-dd'),&#013;&#010;&gt; DATE_FORMAT(dispatch_time, 'HH') FROM kafka_ods_wms_pick_order;&#013;&#010;&gt; SELECT * FROM hive_ods_wms_pick_order /*+&#013;&#010;&gt; OPTIONS('streaming-source.enable'='true',&#013;&#010;&gt; 'streaming-source.consume-start-offset'='2020-07-24') */;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Sender: 夏帅&#013;&#010;&gt; Send Time: 2020-07-14 16:43&#013;&#010;&gt; Receiver: user-zh; xbjtdcq&#013;&#010;&gt; Subject: 回复：Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; 你好,&#013;&#010;&gt; 这说明写入的hive文件没有进行rollup,可以贴下SQL么&#013;&#010;&gt; ------------------------------------------------------------------&#013;&#010;&gt; 发件人：wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt;&#013;&#010;&gt; 发送时间：2020年7月14日(星期二) 16:40&#013;&#010;&gt; 收件人：user-zh &lt;user-zh@flink.apache.org&gt;; xbjtdcq &lt;xbjtdcq@gmail.com&gt;&#013;&#010;&gt; 主 题：回复: Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; 我加上了这个 tablehint 。&#013;&#010;&gt; 任务提交上去了，但客户端还是没有任何返回显示。&#013;&#010;&gt; 我到 hadoop 集群上看了下 hive 表所在的这个目录，所有的文件都是&#010;.part 开头的 inprogress 文件。&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; 发件人： Leonard Xu&#013;&#010;&gt; 发送时间： 2020-07-14 16:17&#013;&#010;&gt; 收件人： user-zh&#013;&#010;&gt; 主题： Re: 不能实时读取实时写入到 Hive 的数据&#013;&#010;&gt; HI, wanglei&#013;&#010;&gt; 你开启了 streaming-source.enable&#013;&#010;&gt; 吗？这个参数用于指定如何读取是batch读，还是stream读，如果你要实时读的话应该把这个值设定为true,&#010;可以使用tablehints&#013;&#010;&gt; 方便地指定参数。&#013;&#010;&gt; SELECT * FROM hive_table /*+ OPTIONS('streaming-source.enable'='true',&#013;&#010;&gt; 'streaming-source.consume-start-offset'='2020-05-20') */;&#013;&#010;&gt; 就在你看得这个页面应该有对应的文档说明如何读取hive数据。&#013;&#010;&gt; 祝好，&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; 在 2020年7月14日，15:47，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 试验了一下 Flink-1.11  hive streaming 的功能&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/hive_streaming.html&#013;&#010;&gt;&#013;&#010;&gt; 创建 kafka 表，通过 SQL 实时写入 Hive.&#013;&#010;&gt;&#013;&#010;&gt; 但我再通过 flink sql-client 客户端 select * from hive_table 客户端没有任何返回，通过&#010;flink&#013;&#010;&gt; webUI 页面观察 这个 select * from hive_table 的 job 已经结束了。&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010; &#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "1",
        "reply": "<202007141547551571989@geekplus.com.cn>"
    },
    {
        "id": "<1594719520194-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:38:40 GMT",
        "subject": "flink 1.11 自定义RichFlatMapFunction中使用JdbcRowDataOutputFormat 写pgsql数据问题，RuntimeContext初始化问题，空指针或RuntimeContext未初始化，哪里用的不对！",
        "content": "代码，编译没问题，但运行的时候，RichFlatMapFunction在open的时候，JdbcRowDataOutputFormat.open会core，说RuntimeContext为空，如果去掉outputFormatStatus.setRuntimeContext(this.getRuntimeContext())，又会提示没有初始化？&#010;&#010;麻烦大佬帮看看，什么问题啊，是我哪里用的不对吗？&#010;&#010;&#010;        at&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;Caused by: java.lang.NullPointerException&#010;        at&#010;org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.createSimpleRowDataExecutor(JdbcRowDataOutputFormat.java:198)&#010;        at&#010;org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.lambda$new$2d156164$1(JdbcRowDataOutputFormat.java:94)&#010;        at&#010;org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.createAndOpenStatementExecutor(JdbcBatchingOutputFormat.java:131)&#010;        at&#010;org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.open(JdbcBatchingOutputFormat.java:113)&#010;        at&#010;org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.open(JdbcRowDataOutputFormat.java:103)&#010;        at&#010;com.qqmusic.quku.cdcSync.PostgresSinkMapFunction.open(PostgresSinkMapFunction.java:132)&#010;        at&#010;org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)&#010;        at&#010;org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)&#010;        at&#010;org.apache.flink.streaming.api.operators.StreamFlatMap.open(StreamFlatMap.java:43)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:291)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:473)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)&#010;        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#010;        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#010;        at java.lang.Thread.run(Thread.java:748)&#010;&#010;&#010;&#010;代码=====&gt;&#010;&#010;public class PostgresSinkMapFunction extends RichFlatMapFunction&lt;String,&#010;String&gt; {&#010;    private static String driverClass = \"org.postgresql.Driver\";&#010;    private static String dbUrl =&#010;\"jdbc:postgresql://localhost:5432/ai_audio_lyric_task\";&#010;    private static String userNmae = \"postgres\";&#010;    private static String passWord = \"1\";&#010;&#010;    // 表status&#010;    private static JdbcRowDataOutputFormat outputFormatStatus;&#010;    private static String[] fieldNames = new String[] {\"id\", \"name\"};&#010;    private static DataType[] fieldDataTypes = new DataType[]{&#010;            DataTypes.INT(),&#010;            DataTypes.STRING()};&#010;&#010;    private static RowType rowType = RowType.of(&#010;            Arrays.stream(fieldDataTypes)&#010;                    .map(DataType::getLogicalType)&#010;                    .toArray(LogicalType[]::new),&#010;            fieldNames);&#010;    private static RowDataTypeInfo rowDataTypeInfo =&#010;RowDataTypeInfo.of(rowType);&#010;&#010;    @Override&#010;    public void flatMap(String s, Collector&lt;String&gt; collector) throws&#010;Exception {&#010;            GenericRowData row = new GenericRowData(2);&#010;&#010;             row.setRowKind(INSERT);&#010;             row.setField(0, count);&#010;             row.setField(1, \"jindy\" + Integer.toString(count));&#010;&#010;            outputFormatStatus.writeRecord(row);&#010;&#010;    }&#010;&#010;    public void open(Configuration parameters) throws Exception {&#010;        super.open(parameters);&#010;&#010;        JdbcOptions jdbcOptions = JdbcOptions.builder()&#010;                .setDriverName(driverClass)&#010;                .setDBUrl(dbUrl)&#010;                .setTableName(\"status_mirror\")&#010;                .setUsername(userNmae)&#010;                .setPassword(passWord)&#010;                .build();&#010;&#010;        JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()&#010;                .withTableName(jdbcOptions.getTableName())&#010;                .withDialect(jdbcOptions.getDialect())&#010;                .withFieldNames(fieldNames)&#010;                .build();&#010;&#010;        outputFormatStatus =&#010;JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()&#010;                .setJdbcOptions(jdbcOptions)&#010;                .setFieldDataTypes(fieldDataTypes)&#010;                .setJdbcDmlOptions(dmlOptions)&#010;               &#010;.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())&#010;                .build();&#010;&#010;        // set context,这里有问题！！！！！！！！！！！！！！！！！！&#010;        outputFormatStatus.setRuntimeContext(this.getRuntimeContext());&#010;        outputFormatStatus.open(0, 1);&#010;    }&#010;&#010;    public void close() throws Exception {&#010;        super.close();&#010;        outputFormatStatus.close();&#010;    }&#010;}&#010;&#010;&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1594719520194-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO933UhsjM5miZcBmTmOXhJKbHM9w92YBJ0Q86xJvSKAE69Q@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:33:14 GMT",
        "subject": "Re: flink 1.11 自定义RichFlatMapFunction中使用JdbcRowDataOutputFormat 写pgsql数据问题，RuntimeContext初始化问题，空指针或RuntimeContext未初始化，哪里用的不对！",
        "content": "Hi，&#010;&#010;从异常来看，应该是少了如下这一行：&#010;&#010;       outputFormatStatus =&#010;JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()&#010;               .setJdbcOptions(jdbcOptions)&#010;               .setFieldDataTypes(fieldDataTypes)&#010;               .setJdbcDmlOptions(dmlOptions)&#010;&#010; .setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())&#010;               .setRowDataTypeInfo(rowDataTypeInfo) // 少了这一行&#010;               .build();&#010;&#010;顺便提醒下， `RowDataTypeInfo` 和 JdbcRowDataOutputFormat&#010;都是内部类，不保证跨版本的兼容性（其实，在下个版本，这两个类都被重构了）。&#010;&#010;Best,&#010;Jark&#010;&#010;&#010;On Tue, 14 Jul 2020 at 17:38, jindy_liu &lt;286729788@qq.com&gt; wrote:&#010;&#010;&gt;&#010;&gt; 代码，编译没问题，但运行的时候，RichFlatMapFunction在open的时候，JdbcRowDataOutputFormat.open会core，说RuntimeContext为空，如果去掉outputFormatStatus.setRuntimeContext(this.getRuntimeContext())，又会提示没有初始化？&#010;&gt;&#010;&gt; 麻烦大佬帮看看，什么问题啊，是我哪里用的不对吗？&#010;&gt;&#010;&gt;&#010;&gt;         at&#010;&gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; Caused by: java.lang.NullPointerException&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.createSimpleRowDataExecutor(JdbcRowDataOutputFormat.java:198)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.lambda$new$2d156164$1(JdbcRowDataOutputFormat.java:94)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.createAndOpenStatementExecutor(JdbcBatchingOutputFormat.java:131)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.open(JdbcBatchingOutputFormat.java:113)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.open(JdbcRowDataOutputFormat.java:103)&#010;&gt;         at&#010;&gt;&#010;&gt; com.qqmusic.quku.cdcSync.PostgresSinkMapFunction.open(PostgresSinkMapFunction.java:132)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.streaming.api.operators.StreamFlatMap.open(StreamFlatMap.java:43)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.OperatorChain.initializeStateAndOpenOperators(OperatorChain.java:291)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:473)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:92)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:469)&#010;&gt;         at&#010;&gt;&#010;&gt; org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:522)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:721)&#010;&gt;         at org.apache.flink.runtime.taskmanager.Task.run(Task.java:546)&#010;&gt;         at java.lang.Thread.run(Thread.java:748)&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 代码=====&gt;&#010;&gt;&#010;&gt; public class PostgresSinkMapFunction extends RichFlatMapFunction&lt;String,&#010;&gt; String&gt; {&#010;&gt;     private static String driverClass = \"org.postgresql.Driver\";&#010;&gt;     private static String dbUrl =&#010;&gt; \"jdbc:postgresql://localhost:5432/ai_audio_lyric_task\";&#010;&gt;     private static String userNmae = \"postgres\";&#010;&gt;     private static String passWord = \"1\";&#010;&gt;&#010;&gt;     // 表status&#010;&gt;     private static JdbcRowDataOutputFormat outputFormatStatus;&#010;&gt;     private static String[] fieldNames = new String[] {\"id\", \"name\"};&#010;&gt;     private static DataType[] fieldDataTypes = new DataType[]{&#010;&gt;             DataTypes.INT(),&#010;&gt;             DataTypes.STRING()};&#010;&gt;&#010;&gt;     private static RowType rowType = RowType.of(&#010;&gt;             Arrays.stream(fieldDataTypes)&#010;&gt;                     .map(DataType::getLogicalType)&#010;&gt;                     .toArray(LogicalType[]::new),&#010;&gt;             fieldNames);&#010;&gt;     private static RowDataTypeInfo rowDataTypeInfo =&#010;&gt; RowDataTypeInfo.of(rowType);&#010;&gt;&#010;&gt;     @Override&#010;&gt;     public void flatMap(String s, Collector&lt;String&gt; collector) throws&#010;&gt; Exception {&#010;&gt;             GenericRowData row = new GenericRowData(2);&#010;&gt;&#010;&gt;              row.setRowKind(INSERT);&#010;&gt;              row.setField(0, count);&#010;&gt;              row.setField(1, \"jindy\" + Integer.toString(count));&#010;&gt;&#010;&gt;             outputFormatStatus.writeRecord(row);&#010;&gt;&#010;&gt;     }&#010;&gt;&#010;&gt;     public void open(Configuration parameters) throws Exception {&#010;&gt;         super.open(parameters);&#010;&gt;&#010;&gt;         JdbcOptions jdbcOptions = JdbcOptions.builder()&#010;&gt;                 .setDriverName(driverClass)&#010;&gt;                 .setDBUrl(dbUrl)&#010;&gt;                 .setTableName(\"status_mirror\")&#010;&gt;                 .setUsername(userNmae)&#010;&gt;                 .setPassword(passWord)&#010;&gt;                 .build();&#010;&gt;&#010;&gt;         JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()&#010;&gt;                 .withTableName(jdbcOptions.getTableName())&#010;&gt;                 .withDialect(jdbcOptions.getDialect())&#010;&gt;                 .withFieldNames(fieldNames)&#010;&gt;                 .build();&#010;&gt;&#010;&gt;         outputFormatStatus =&#010;&gt; JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()&#010;&gt;                 .setJdbcOptions(jdbcOptions)&#010;&gt;                 .setFieldDataTypes(fieldDataTypes)&#010;&gt;                 .setJdbcDmlOptions(dmlOptions)&#010;&gt;&#010;&gt; .setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())&#010;&gt;                 .build();&#010;&gt;&#010;&gt;         // set context,这里有问题！！！！！！！！！！！！！！！！！！&#010;&gt;         outputFormatStatus.setRuntimeContext(this.getRuntimeContext());&#010;&gt;         outputFormatStatus.open(0, 1);&#010;&gt;     }&#010;&gt;&#010;&gt;     public void close() throws Exception {&#010;&gt;         super.close();&#010;&gt;         outputFormatStatus.close();&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<1594719520194-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594802652850-0.post@n8.nabble.com>",
        "from": "jindy_liu &lt;286729...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 08:44:12 GMT",
        "subject": "Re: flink 1.11 自定义RichFlatMapFunction中使用JdbcRowDataOutputFormat 写pgsql数据问题，RuntimeContext初始化问题，空指针或RuntimeContext未初始化，哪里用的不对！",
        "content": "确实是这行导致的，&#013;&#010;如果都重构了，那应该怎么用较好的？&#013;&#010;我需要知道每一行对应的是insert, update还是delete事件。&#013;&#010;或者问题变一下，对于这种api，一般遵守什么规则，flink的版本兼容性会更好？&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "2",
        "reply": "<1594719520194-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:56:39 GMT",
        "subject": "flink 1.11 upsert结果出错",
        "content": "各位大佬好，请教一个问题flink从Kafka读数，写入mysql，对mysql结果根据主键进行数据更新，看官网是支持“on&#010;DUPLICATE”的，但是在执行中报错是这个导致的语法问题。完整代码如下，是在linux下，直接python&#010;*.py执行的。请问下这个是不支持吗，还是怎么写呢！&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;source=\"\"\"&#013;&#010;CREATE TABLE kafka_source_tab (&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;trck_id VARCHAR,&#013;&#010;&amp;nbsp;score&amp;nbsp; INT&#013;&#010;&#013;&#010;&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'kafka',&#013;&#010;&amp;nbsp;'topic' = 'alarm_test_g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'scan.startup.mode' = 'earliest-offset', &#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:2181',&#013;&#010;&amp;nbsp;'properties.bootstrap.servers' = '10.2.2.73:9092',&#013;&#010;&amp;nbsp;'format' = 'json'&amp;nbsp; &#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;&#013;&#010;sink=\"\"\"&#013;&#010;CREATE TABLE g_source_tab (&#013;&#010;&amp;nbsp;trck_id VARCHAR,&#013;&#010;&amp;nbsp;score&amp;nbsp; INT,&#013;&#010;&#013;&#010;PRIMARY KEY (trck_id) NOT ENFORCED&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp; &#013;&#010;&amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = '123456t',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;t_env.execute_sql(source)&#013;&#010;t_env.execute_sql(sink)&#013;&#010;&#013;&#010;&#013;&#010;table_result1=t_env.execute_sql('''Insert into g_source_tab (`trck_id`,`score`) VALUES (select&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp;trck_id,score from kafka_source_tab ) ON DUPLICATE KEY UPDATE&#010;score=score+1''')&#013;&#010;&#013;&#010;table_result1.get_job_client().get_job_execution_result().result()",
        "depth": "0",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<3D487A2C-953F-4988-994F-10E0045C5E3A@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:15:30 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "Hello,&#010;&#010;&gt; 在 2020年7月14日，17:56，小学生 &lt;201782053@qq.com&gt; 写道：&#010;&gt; &#010;&gt; ON DUPLICATE KEY UPDATE &#010;&#010;这个语法 Flink 还不支持的，官网上说的 Flink 的 JDBC connector 实现 幂等写入[1]的方式，就是有相同pk的数据在写入数据库时，翻译成数据库&#010;upsert SQL的方式，这里说的语法是数据库的 SQL 语法 。&#010;&#010;&#010;Best,&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#idempotent-writes&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#idempotent-writes&gt;&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<tencent_D331FB90AB817820D71FD933D9267573A807@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:21:44 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "嗯嗯，谢谢大佬的解答，还有一个问题就是sql自己的语法是支持增量式的比如score=score+1,现在flink1.11特性反应成数据库&#010;upsert SQL的方式，其实是全量的更新同Pk的记录吧，并达不到增量的情况吧。",
        "depth": "2",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<9C6B7468-85BC-442F-AE85-2D99159B3B4E@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:28:42 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "是的，目前是更新相同pk的记录，如果需要统计相同pk的记录， Flink表不声明PK就是append&#010;写入，就会有写入多条记录，(DB里的表也不声明pk，不然insert会报错)。&#013;&#010;&#013;&#010;祝好&#013;&#010;&#013;&#010;&#013;&#010;&gt; 在 2020年7月14日，18:21，小学生 &lt;201782053@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 嗯嗯，谢谢大佬的解答，还有一个问题就是sql自己的语法是支持增量式的比如score=score+1,现在flink1.11特性反应成数据库&#010;upsert SQL的方式，其实是全量的更新同Pk的记录吧，并达不到增量的情况吧。&#013;&#010;&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<tencent_E5F6655725F1C4AEE5B27FB2AEEA9D500F08@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 03:05:03 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "嗯嗯，谢谢大佬的理解，还有一个问题，就是除了update，这个我看新性能也支持delete的，但是没找到相关的部分，delete这个是否类似：delete&#010;table1 where score=1;烦请大佬帮忙解答下，不胜感激。",
        "depth": "4",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<94959E83-4AA2-47FE-8610-E74453F38526@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 03:23:08 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "Hi，&#010;&#010;基本类似的，具体拼delete sql会根据 pk 来, 可以看下delete executor的代码[1]&#010;&#010;祝好，&#010;Leonard Xu&#010;【1】https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java#L89&#010;&lt;https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/executor/BufferReduceStatementExecutor.java#L89&gt;&#010;&#010;&gt; 在 2020年7月15日，11:05，小学生 &lt;201782053@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 嗯嗯，谢谢大佬的理解，还有一个问题，就是除了update，这个我看新性能也支持delete的，但是没找到相关的部分，delete这个是否类似：delete&#010;table1 where score=1;烦请大佬帮忙解答下，不胜感激。&#010;&#010;&#010;",
        "depth": "5",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<tencent_20EA1C50B2F61C5772F12EAEA0B29CB2DF08@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 05:32:50 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "嗯嗯，麻烦问下Python版本的相关资料有吗",
        "depth": "6",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<tencent_FBF2B215B23FEA5F5042F6915761F1CBAE08@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:04:36 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "各位大佬好，由于不是特别懂java,所以麻烦问下pyflink里面有相关mysql的delete吗，官网没看到，谢谢！",
        "depth": "6",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<39510291-35EB-476F-9828-A0A2EBCABC68@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:08:57 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "Hi，&#013;&#010;&#013;&#010;我理解 pyflink 底层也会走到你看到的java代码, 我对 pyflink 不是很熟，&#010;cc xingbo 补充下。&#013;&#010;&#013;&#010;祝好&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&gt; 在 2020年7月16日，11:04，小学生 &lt;201782053@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 各位大佬好，由于不是特别懂java,所以麻烦问下pyflink里面有相关mysql的delete吗，官网没看到，谢谢！&#013;&#010;&#013;&#010;",
        "depth": "7",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<CAPxmL=EERrBe+hwk+UjXG-H-KgP2WfLW2Ejc2y1zE-uDUOos4A@mail.gmail.com>",
        "from": "Xingbo Huang &lt;hxbks...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:11:36 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "Hi,&#013;&#010;Leonard 说的是对的，除了udf的部分，pyflink的所有的api都是调用的java端的功能，如果java端没有，pyflink就不支持&#013;&#010;&#013;&#010;Best,&#013;&#010;Xingbo&#013;&#010;&#013;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月16日周四 上午11:09写道：&#013;&#010;&#013;&#010;&gt; Hi，&#013;&#010;&gt;&#013;&#010;&gt; 我理解 pyflink 底层也会走到你看到的java代码, 我对 pyflink 不是很熟，&#010;cc xingbo 补充下。&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月16日，11:04，小学生 &lt;201782053@qq.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 各位大佬好，由于不是特别懂java,所以麻烦问下pyflink里面有相关mysql的delete吗，官网没看到，谢谢！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<tencent_DD2ED966F56FA8677BA8ABBE780ED7255907@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:14:34 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "谢谢两位大佬的解答，但是理解有点抽象，不太清楚，有没有pyflink下一个简单例子呢。",
        "depth": "9",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<CAPxmL=HOaHn-tJeAFErSZKvCw=NMxWunwERXKhBexjiS5mqCRg@mail.gmail.com>",
        "from": "Xingbo Huang &lt;hxbks...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:22:50 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "Hi,&#013;&#010;你需要什么样的例子，如果你用的table/sql的话，在官方文档对应的地方都有java/scala/python的对应写法。如果是python&#013;&#010;udf相关的东西，你可以参考[1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/python/&#013;&#010;&#013;&#010;Best,&#013;&#010;Xingbo&#013;&#010;&#013;&#010;小学生 &lt;201782053@qq.com&gt; 于2020年7月16日周四 上午11:14写道：&#013;&#010;&#013;&#010;&gt; 谢谢两位大佬的解答，但是理解有点抽象，不太清楚，有没有pyflink下一个简单例子呢。&#013;&#010;",
        "depth": "10",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<tencent_D5DB5551FCB9EBAB6C926CA77A20548CBD06@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:44:36 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "您好，比如说我这个例子，我使用delete就出错了，我想知道是啥原因呢，&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic, CheckpointingMode&#013;&#010;from pyflink.table import StreamTableEnvironment, EnvironmentSettings&#013;&#010;source=\"\"\"&#013;&#010;CREATE TABLE source_tab (&#013;&#010;&amp;nbsp;trck_id VARCHAR,&#013;&#010;&amp;nbsp;score&amp;nbsp; INT,&#013;&#010;PRIMARY KEY (trck_id) NOT ENFORCED&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp;&#013;&#010;&amp;nbsp;'table-name' = 'g',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = '123456t',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;sink=\"\"\"&#013;&#010;CREATE TABLE sink_tab (&#013;&#010;&amp;nbsp;trck_id VARCHAR,&#013;&#010;&amp;nbsp;score&amp;nbsp; INT,&#013;&#010;PRIMARY KEY (trck_id) NOT ENFORCED&#013;&#010;) WITH (&#013;&#010;&amp;nbsp;'connector' = 'jdbc',&#013;&#010;&amp;nbsp;'url' = 'jdbc:mysql://10.2.2.77:3306/bdt?useSSL=false',&amp;nbsp;&#013;&#010;&amp;nbsp;'table-name' = 'g_copy',&amp;nbsp; &amp;nbsp;&#013;&#010;&amp;nbsp;'username' = 'root',&#013;&#010;&amp;nbsp;'password' = '123456t',&#013;&#010;&amp;nbsp;'sink.buffer-flush.interval' = '1s'&#013;&#010;)&#013;&#010;\"\"\"&#013;&#010;env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;env.set_parallelism(1)&#013;&#010;env_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()&#013;&#010;t_env = StreamTableEnvironment.create(env, environment_settings=env_settings)&#013;&#010;&#013;&#010;&#013;&#010;t_env.execute_sql(source)&#013;&#010;t_env.execute_sql(sink)&#013;&#010;&#013;&#010;&#013;&#010;t_env.execute_sql('''delete from source_tab where trck_id='aew'&amp;nbsp; ''')&#013;&#010;table_result1=t_env.execute_sql('''insert into&amp;nbsp; sink_tab select * from source_tab&#010;''')&#013;&#010;table_result1.get_job_client().get_job_execution_result().result()",
        "depth": "11",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<509655C6-BF49-4352-86E3-29BBC8399D97@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 04:08:39 GMT",
        "subject": "Re: flink 1.11 upsert结果出错",
        "content": "&#010;&#010;&gt; 在 2020年7月16日，11:44，小学生 &lt;201782053@qq.com&gt; 写道：&#010;&gt; &#010;&gt; t_env.execute_sql('''delete from source_tab where trck_id='aew'&amp;nbsp; ''')&#010;&#010;你这张表定义的是 Flink 中的表，这张表对应的是你外部系统(MySQL数据库)中的表，Flink&#010;不支持 表上 的DELETE [1]， Flink 是一个计算引擎,&#010;主要场景是读取、写入外部系统，修改外部系统的数据目前只发生在写入（insert）的时候，并且主要是为了保证数据一致性语义，需要往下游系统发Delete消息，&#010;这个delete的消息的处理都是各个connector自己处理的，用户不用显示地调用delete,&#010;你可以参考[2]了解更多。&#010;&#010;祝好&#010;[1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/ &lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/&gt;&#010;[2]https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/streaming/dynamic_tables.html&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/streaming/dynamic_tables.html&gt;&#010;&#010;",
        "depth": "12",
        "reply": "<tencent_0E7DD9357510A83BFBBFAB95784F57425907@qq.com>"
    },
    {
        "id": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>",
        "from": "wldd &lt;wldd1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 09:58:39 GMT",
        "subject": "flink1.11.0读取mysql数据decimal类型会强转成decimal(38,18)的问题",
        "content": "hi，all：&#010;现在遇到一个问题，通过sql-client读取mysql数据时，decimal类型会强转成decimal(38,18)&#010;mysql ddl：&#010;CREATE TABLE `test2` (&#010;  `money` decimal(10,2) DEFAULT NULL&#010;) ENGINE=InnoDB DEFAULT CHARSET=utf8;&#010;&#010;&#010;insert into test2 values(10.22);&#010;&#010;&#010;flink ddl：&#010;CREATE TABLE test2 (&#010;    money decimal(10, 2)&#010;) WITH (&#010;    'connector.type' = 'jdbc',&#010;    'connector.url' = 'jdbc:mysql://localhost:3306/test',&#010;    'connector.table' = 'test2',&#010;    'connector.username' = 'root',&#010;    'connector.password' = 'root'&#010;);&#010;&#010;&#010;flink查询结果，streaming模式：&#010;sql：select * from test2;&#010;&#010;&#010;debug信息：&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;--&#010;&#010;Best，&#010;wldd",
        "depth": "0",
        "reply": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>"
    },
    {
        "id": "<A10874FF-57E6-4C58-996A-FEBFE2760F85@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:08:41 GMT",
        "subject": "Re: flink1.11.0读取mysql数据decimal类型会强转成decimal(38,18)的问题",
        "content": "Hi,&#010;&#010;SQL client 读取mysql的部分想当于一个connector,  这个connector只支持 DECIMAL（38，18）的，所有DECIMAL（p,&#010;s）都会转到这个类型，这是因为SQL Client还是用的legacy 的数据类型。&#010;你可以用其他 connector 如 Filesystem、Kafka等connector读取， 社区已经有一个issue[1]&#010;在跟进了。&#010;&#010;祝好，&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-17948 &lt;https://issues.apache.org/jira/browse/FLINK-17948&gt;&#010;&#010;&gt; 在 2020年7月14日，17:58，wldd &lt;wldd1116@163.com&gt; 写道：&#010;&gt; &#010;&gt; sql-client&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>"
    },
    {
        "id": "<54ce7bf2.7d10.1734cd7c048.Coremail.wldd1116@163.com>",
        "from": "wldd  &lt;wldd1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:21:14 GMT",
        "subject": "Re:Re: flink1.11.0读取mysql数据decimal类型会强转成decimal(38,18)的问题",
        "content": "Hi，&#010;batch模式用的不是用的legacy 的数据类型么，batch模式并没有对decimal进行强转&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;--&#010;&#010;Best，&#010;wldd&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 18:08:41，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;&#010;&gt;SQL client 读取mysql的部分想当于一个connector,  这个connector只支持 DECIMAL（38，18）的，所有DECIMAL（p,&#010;s）都会转到这个类型，这是因为SQL Client还是用的legacy 的数据类型。&#010;&gt;你可以用其他 connector 如 Filesystem、Kafka等connector读取， 社区已经有一个issue[1]&#010;在跟进了。&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;&gt;[1] https://issues.apache.org/jira/browse/FLINK-17948 &lt;https://issues.apache.org/jira/browse/FLINK-17948&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月14日，17:58，wldd &lt;wldd1116@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; sql-client&#010;&gt;&#010;",
        "depth": "2",
        "reply": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>"
    },
    {
        "id": "<51C27B77-B076-436D-B130-D432D081051B@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:31:33 GMT",
        "subject": "Re: flink1.11.0读取mysql数据decimal类型会强转成decimal(38,18)的问题",
        "content": "Hi,&#010;&#010;前面邮件图都挂了，理论上 SQL Client 都是会强转的，可以发个图床链接上或者贴下可以复现的代码吗？&#010;&#010;祝好&#010;&#010;&gt; 在 2020年7月14日，18:21，wldd &lt;wldd1116@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi，&#010;&gt; batch模式用的不是用的legacy 的数据类型么，batch模式并没有对decimal进行强转&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; --&#010;&gt; Best，&#010;&gt; wldd&#010;&gt; &#010;&gt; &#010;&gt; 在 2020-07-14 18:08:41，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &gt;Hi,&#010;&gt; &gt;&#010;&gt; &gt;SQL client 读取mysql的部分想当于一个connector,  这个connector只支持&#010;DECIMAL（38，18）的，所有DECIMAL（p, s）都会转到这个类型，这是因为SQL&#010;Client还是用的legacy 的数据类型。&#010;&gt; &gt;你可以用其他 connector 如 Filesystem、Kafka等connector读取， 社区已经有一个issue[1]&#010;在跟进了。&#010;&gt; &gt;&#010;&gt; &gt;祝好，&#010;&gt; &gt;Leonard Xu&#010;&gt; &gt;[1] https://issues.apache.org/jira/browse/FLINK-17948 &lt;https://issues.apache.org/jira/browse/FLINK-17948&gt;&#010;&gt; &gt;&#010;&gt; &gt;&gt; 在 2020年7月14日，17:58，wldd &lt;wldd1116@163.com&gt; 写道：&#010;&gt; &gt;&gt; &#010;&gt; &gt;&gt; sql-client&#010;&gt; &gt;&#010;&gt; &#010;&gt; &#010;&gt;  &#010;&#010;&#010;",
        "depth": "3",
        "reply": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>"
    },
    {
        "id": "<6ea8ce69.7f6c.1734cec4384.Coremail.wldd1116@163.com>",
        "from": "wldd  &lt;wldd1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:43:39 GMT",
        "subject": "Re:Re: flink1.11.0读取mysql数据decimal类型会强转成decimal(38,18)的问题",
        "content": "Hi：&#010;图片的内容没展示出来，图片的内容就是个查询结果，&#010;&#010;&#010;error日志这是batch模式的debug日志：&#010;2020-07-14 18:33:23,180 DEBUG org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat&#010;[] - No input splitting configured (data will be read with parallelism 1).&#010;2020-07-14 18:33:23,181 DEBUG org.apache.calcite.sql2rel                                 &#010; [] - Plan after converting SqlNode to RelNode&#010;LogicalProject(money=[$0])&#010;  LogicalTableScan(table=[[mydb, test, test2",
        "depth": "4",
        "reply": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>"
    },
    {
        "id": "<546cfcc3.8081.1734cf75c8c.Coremail.wldd1116@163.com>",
        "from": "wldd  &lt;wldd1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:55:46 GMT",
        "subject": "Re:Re:Re: flink1.11.0读取mysql数据decimal类型会强转成decimal(38,18)的问题",
        "content": "Hi：&#010;batchi模式执行结果：&#010;https://imgchr.com/i/UUqec6&#010;batch模式日志：&#010;https://imgchr.com/i/UUboX8&#010;streaming模式日志：&#010;https://imgchr.com/i/UUbYmF&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;--&#010;&#010;Best，&#010;wldd&#010;&#010;&#010;&#010;&#010;At 2020-07-14 18:43:39, \"wldd\" &lt;wldd1116@163.com&gt; wrote:&#010;&#010;Hi：&#010;图片的内容没展示出来，图片的内容就是个查询结果，&#010;&#010;&#010;error日志这是batch模式的debug日志：&#010;2020-07-14 18:33:23,180 DEBUG org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat&#010;[] - No input splitting configured (data will be read with parallelism 1).&#010;2020-07-14 18:33:23,181 DEBUG org.apache.calcite.sql2rel                                 &#010; [] - Plan after converting SqlNode to RelNode&#010;LogicalProject(money=[$0])&#010;  LogicalTableScan(table=[[mydb, test, test2",
        "depth": "5",
        "reply": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>"
    },
    {
        "id": "<4C1DA1F3-0452-402B-9044-091B15D8C391@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 11:55:00 GMT",
        "subject": "Re: flink1.11.0读取mysql数据decimal类型会强转成decimal(38,18)的问题",
        "content": "Hi,&#010;看了下代码，Stream 模式 确实有这个问题， batch 没有，原因是：&#010;&#010;CollectStreamTableSink 实现的是 TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; getOutputType()&#010;CollectBatchTableSink 实现的是 DataType getConsumedDataType()&#010;&#010;刚刚搜了下，社区有个 issue [1] 在彻底解这个问题，Godgrey 已经开PR了，这应该会把这两个CollectSink都去掉，使用&#010;TableResult#collect()来收集数据。&#010;&#010;Best，&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18550 &lt;https://issues.apache.org/jira/browse/FLINK-18550&gt;&#010;&#010;&#010;&#010;&gt; 在 2020年7月14日，18:55，wldd &lt;wldd1116@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi：&#010;&gt; batchi模式执行结果：&#010;&gt; https://imgchr.com/i/UUqec6&#010;&gt; batch模式日志：&#010;&gt; https://imgchr.com/i/UUboX8&#010;&gt; streaming模式日志：&#010;&gt; https://imgchr.com/i/UUbYmF&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; --&#010;&gt; &#010;&gt; Best，&#010;&gt; wldd&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; At 2020-07-14 18:43:39, \"wldd\" &lt;wldd1116@163.com&gt; wrote:&#010;&gt; &#010;&gt; Hi：&#010;&gt; 图片的内容没展示出来，图片的内容就是个查询结果，&#010;&gt; &#010;&gt; &#010;&gt; error日志这是batch模式的debug日志：&#010;&gt; 2020-07-14 18:33:23,180 DEBUG org.apache.flink.connector.jdbc.table.JdbcRowDataInputFormat&#010;[] - No input splitting configured (data will be read with parallelism 1).&#010;&gt; 2020-07-14 18:33:23,181 DEBUG org.apache.calcite.sql2rel                            &#010;      [] - Plan after converting SqlNode to RelNode&#010;&gt; LogicalProject(money=[$0])&#010;&gt;  LogicalTableScan(table=[[mydb, test, test2",
        "depth": "6",
        "reply": "<e78f0ff.79cd.1734cc310bb.Coremail.wldd1116@163.com>"
    },
    {
        "id": "<tencent_81DF811FFC23000E73B17805FE00BEF90708@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 10:56:05 GMT",
        "subject": "flink-1.11 DDL 设置chk目录问题",
        "content": "目前我只会设置streameEnv.setStateBackend(new FsStateBackend(checkpointPath));&#013;&#010;但是DDL时候应该如何设置呢？&#013;&#010;tableEnv.getConfig().getConfiguration().set(&#013;&#010;        ExecutionCheckpointingOptions.CHECKPOINTING_MODE, CheckpointingMode.EXACTLY_ONCE);&#013;&#010;tableEnv.getConfig().getConfiguration().set(&#013;&#010;        ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL, Duration.ofSeconds(10));",
        "depth": "0",
        "reply": "<tencent_81DF811FFC23000E73B17805FE00BEF90708@qq.com>"
    },
    {
        "id": "<FFF8FDFB-7148-401E-9457-87B9E2EF42AA@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 12:33:50 GMT",
        "subject": "Re: flink-1.11 DDL 设置chk目录问题",
        "content": "Hi，&#010;&#010;没有太理解在DDL中设置，TableConfig上也可以设置 StreamEexecutionEnvironment&#010;的 配置，你要的是这个吗？&#010;&#010;tableEnv.getConfig().getConfiguration().set(CHECKPOINTS_DIRECTORY, \"your-cp-path\");&#010;&#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月14日，18:56，kcz &lt;573693104@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 目前我只会设置streameEnv.setStateBackend(new FsStateBackend(checkpointPath));&#010;&gt; 但是DDL时候应该如何设置呢？&#010;&gt; tableEnv.getConfig().getConfiguration().set(&#010;&gt;        ExecutionCheckpointingOptions.CHECKPOINTING_MODE, CheckpointingMode.EXACTLY_ONCE);&#010;&gt; tableEnv.getConfig().getConfiguration().set(&#010;&gt;        ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL, Duration.ofSeconds(10));&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_81DF811FFC23000E73B17805FE00BEF90708@qq.com>"
    },
    {
        "id": "<tencent_0E3F5FE51BF0A377A4DE0125D6B980144005@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 13:44:33 GMT",
        "subject": "回复：flink-1.11 DDL 设置chk目录问题",
        "content": "谢谢 我一直用的是 streamEnv去设置config 今天看到table也可以，如果我用stream去设置&#010;也是可以的吧&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: Leonard Xu &lt;xbjtdcq@gmail.com&amp;gt;&#013;&#010;发送时间: 2020年7月14日 20:34&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：flink-1.11 DDL 设置chk目录问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi，&#013;&#010;&#013;&#010;没有太理解在DDL中设置，TableConfig上也可以设置 StreamEexecutionEnvironment&#010;的 配置，你要的是这个吗？&#013;&#010;&#013;&#010;tableEnv.getConfig().getConfiguration().set(CHECKPOINTS_DIRECTORY, \"your-cp-path\");&#013;&#010;&#013;&#010;&#013;&#010;祝好，&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;&amp;gt; 在 2020年7月14日，18:56，kcz &lt;573693104@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 目前我只会设置streameEnv.setStateBackend(new FsStateBackend(checkpointPath));&#013;&#010;&amp;gt; 但是DDL时候应该如何设置呢？&#013;&#010;&amp;gt; tableEnv.getConfig().getConfiguration().set(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ExecutionCheckpointingOptions.CHECKPOINTING_MODE,&#010;CheckpointingMode.EXACTLY_ONCE);&#013;&#010;&amp;gt; tableEnv.getConfig().getConfiguration().set(&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,&#010;Duration.ofSeconds(10));",
        "depth": "2",
        "reply": "<tencent_81DF811FFC23000E73B17805FE00BEF90708@qq.com>"
    },
    {
        "id": "<7046f795.92f7.1734d2afe9e.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 11:52:10 GMT",
        "subject": "flink1.11 sink hive error",
        "content": "hi all,&#010;flink1.11 sql sink hive table 报错：&#010;&#010;&#010;java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;Could not execute application.&#010;&#009;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_161]&#010;&#009;at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]&#010;&#009;at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;[data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;[data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [qile-data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [data-flow-1.0.jar:?]&#010;Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could&#010;not execute application.&#010;&#009;... 11 more&#010;Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused&#010;an error: Embedded metastore is not allowed. Make sure you have set a valid value for hive.metastore.uris&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;Caused by: java.lang.IllegalArgumentException: Embedded metastore is not allowed. Make sure&#010;you have set a valid value for hive.metastore.uris&#010;&#009;at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:139) ~[data-flow-1.0.jar:?]&#010;&#009;at org.apache.flink.table.catalog.hive.HiveCatalog.&lt;init&gt;(HiveCatalog.java:171) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.catalog.hive.HiveCatalog.&lt;init&gt;(HiveCatalog.java:157) ~[flink-sql-connector-hive-2.2.0_2.11-1.11.0.jar:1.11.0]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:27)&#010;~[data-flow-1.0.jar:?]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[data-flow-1.0.jar:?]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more",
        "depth": "0",
        "reply": "<7046f795.92f7.1734d2afe9e.Coremail.wander669@163.com>"
    },
    {
        "id": "<EB888BCE-2DA8-471C-90C2-726691C0B3D4@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 12:03:11 GMT",
        "subject": "Re: flink1.11 sink hive error",
        "content": "Hello&#010;&#010;&#010;&gt; 在 2020年7月14日，19:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; : Embedded metastore is not allowed.&#010;&#010;Flink 集成 Hive 时，不支持 embedded metastore 的, 你需要起一个hive metastore&#010;并在conf文件配置 hive.metastore.uris, 支持的 metastore 版本 参考[1]&#010;&#010;Best,&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&gt;&#010;",
        "depth": "1",
        "reply": "<7046f795.92f7.1734d2afe9e.Coremail.wander669@163.com>"
    },
    {
        "id": "<4a4de1ce.94b4.1734d4d1e97.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 12:29:26 GMT",
        "subject": "Re:Re: flink1.11 sink hive error",
        "content": "Hi，&#010;&#010;&#010;是在flink的conf文件配置hive.metastore.uris吗&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 20:03:11，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hello&#010;&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月14日，19:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; : Embedded metastore is not allowed.&#010;&gt;&#010;&gt;Flink 集成 Hive 时，不支持 embedded metastore 的, 你需要起一个hive metastore&#010;并在conf文件配置 hive.metastore.uris, 支持的 metastore 版本 参考[1]&#010;&gt;&#010;&gt;Best,&#010;&gt;Leonard Xu&#010;&gt;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&gt;&#010;",
        "depth": "2",
        "reply": "<7046f795.92f7.1734d2afe9e.Coremail.wander669@163.com>"
    },
    {
        "id": "<F65D8CFA-C8D9-419F-A5B8-7AF8DB4C5FAF@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 12:42:16 GMT",
        "subject": "Re: flink1.11 sink hive error",
        "content": "Hi,&#010;你安装 hive 的 metastore 后，在你 hivehome/conf/hive-site.xml 文件中添加这样一个配置：&#010;  &lt;property&gt;&#010;    &lt;name&gt;hive.metastore.uris&lt;/name&gt;&#010;    &lt;value&gt;thrift://xxxx:9083&lt;/value&gt;&#010;    &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect&#010;to remote metastore.&lt;/description&gt;&#010;  &lt;/property&gt;&#010;一般生产环境应该也是这样配置，&#010;然后 Flink 对接到hive配置参考[1]，应该和你之前用的没啥变化，就是不支持&#010;embedded 的 metastore&#010;&#010;祝好，&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive&gt;&#010;&#010;&gt; 在 2020年7月14日，20:29，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi，&#010;&gt; &#010;&gt; &#010;&gt; 是在flink的conf文件配置hive.metastore.uris吗&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在 2020-07-14 20:03:11，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;&gt; Hello&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月14日，19:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; : Embedded metastore is not allowed.&#010;&gt;&gt; &#010;&gt;&gt; Flink 集成 Hive 时，不支持 embedded metastore 的, 你需要起一个hive&#010;metastore 并在conf文件配置 hive.metastore.uris, 支持的 metastore 版本 参考[1]&#010;&gt;&gt; &#010;&gt;&gt; Best,&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt; [1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&gt;&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<7046f795.92f7.1734d2afe9e.Coremail.wander669@163.com>"
    },
    {
        "id": "<9e3411c.9628.1734d6a1550.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 14 Jul 2020 13:01:04 GMT",
        "subject": "Re:Re: flink1.11 sink hive error",
        "content": "Hi,&#010;&#010;&#010;我刚才把flink sink的hive table，hive hdfs目录都删了，hbase表数据也清空了（hbase&#010;通过hue hive table方式查询），然后重启程序，就可以了，&#010;等再出问题，我试下你这种方法，感谢答疑！&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-14 20:42:16，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hi,&#010;&gt;你安装 hive 的 metastore 后，在你 hivehome/conf/hive-site.xml 文件中添加这样一个配置：&#010;&gt;  &lt;property&gt;&#010;&gt;    &lt;name&gt;hive.metastore.uris&lt;/name&gt;&#010;&gt;    &lt;value&gt;thrift://xxxx:9083&lt;/value&gt;&#010;&gt;    &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to&#010;connect to remote metastore.&lt;/description&gt;&#010;&gt;  &lt;/property&gt;&#010;&gt;一般生产环境应该也是这样配置，&#010;&gt;然后 Flink 对接到hive配置参考[1]，应该和你之前用的没啥变化，就是不支持&#010;embedded 的 metastore&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;&gt;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#connecting-to-hive&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月14日，20:29，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; Hi，&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 是在flink的conf文件配置hive.metastore.uris吗&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 在 2020-07-14 20:03:11，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;&gt;&gt; Hello&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; 在 2020年7月14日，19:52，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt;&gt;&gt; &#010;&gt;&gt;&gt;&gt; : Embedded metastore is not allowed.&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Flink 集成 Hive 时，不支持 embedded metastore 的, 你需要起一个hive&#010;metastore 并在conf文件配置 hive.metastore.uris, 支持的 metastore 版本 参考[1]&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Best,&#010;&gt;&gt;&gt; Leonard Xu&#010;&gt;&gt;&gt; [1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/hive/#using-bundled-hive-jar&gt;&#010;&gt;&#010;",
        "depth": "4",
        "reply": "<7046f795.92f7.1734d2afe9e.Coremail.wander669@163.com>"
    },
    {
        "id": "<202007150919537622622@163.com>",
        "from": "王双利 &lt;all...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 01:19:57 GMT",
        "subject": "TableEnvironment 里面无法执行 Hop等窗口函数",
        "content": "TableEnvironment 里面无法执行 Hop等窗口函数，使用StreamTableEnvironment 则可以执行，Flink版本1.10&#013;&#010;是否是这样的&#013;&#010;",
        "depth": "0",
        "reply": "<202007150919537622622@163.com>"
    },
    {
        "id": "<578e93a0.bc7f.1735076d457.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 03:13:51 GMT",
        "subject": "回复：TableEnvironment 里面无法执行 Hop等窗口函数",
        "content": "hi&#010;滑动窗口只有在stream里面才会有&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月15日 09:19，王双利 写道：&#010;TableEnvironment 里面无法执行 Hop等窗口函数，使用StreamTableEnvironment 则可以执行，Flink版本1.10&#010;是否是这样的&#010;",
        "depth": "1",
        "reply": "<202007150919537622622@163.com>"
    },
    {
        "id": "<CAGR9zpZJuNebSqff5CaXjM7AFm0Lg=7w8sYiep0Omtn60BYijw@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 01:54:08 GMT",
        "subject": "flink sql 1.10 insert into tb select 复杂schema 失败",
        "content": "Hello，&#010;&#010;         在使用flink sql 1.10.0 时候，当source table 中含有复杂schema，比如&#010;create table xxx (&#010;a string,&#010;b row(&#010; c row(d string)&#010;  )&#010;)&#010;&#010;当c 中有值的时候，sql 如下 insert into select * from xxx会出现下面错误&#010;&#010;Caused by: java.lang.ClassCastException:&#010;org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.NullNode&#010;cannot be cast to&#010;org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode&#010;at&#010;org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:337)&#010;at&#010;org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;at&#010;org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;at&#010;org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;at&#010;org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;at&#010;org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;at&#010;org.apache.flink.formats.json.JsonRowSerializationSchema.serialize(JsonRowSerializationSchema.java:138)&#010;... 38 more&#010;&#010;&#010;Best wishes.&#010;&#010;",
        "depth": "0",
        "reply": "<CAGR9zpZJuNebSqff5CaXjM7AFm0Lg=7w8sYiep0Omtn60BYijw@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_Qn0rUrVXFaJ0Cd1E2iqD+ZeY_dO5U15ETwG2e_3QDzeQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:25:16 GMT",
        "subject": "Re: flink sql 1.10 insert into tb select 复杂schema 失败",
        "content": "Hi Peihui,&#010;&#010;这是一个已知bug[1]，已经在1.10.1和1.11.0中修复了，可以尝试下这两个版本。&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-16220&#010;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月15日周三 上午9:54写道：&#010;&#010;&gt; Hello，&#010;&gt;&#010;&gt;          在使用flink sql 1.10.0 时候，当source table 中含有复杂schema，比如&#010;&gt; create table xxx (&#010;&gt; a string,&#010;&gt; b row(&#010;&gt;  c row(d string)&#010;&gt;   )&#010;&gt; )&#010;&gt;&#010;&gt; 当c 中有值的时候，sql 如下 insert into select * from xxx会出现下面错误&#010;&gt;&#010;&gt; Caused by: java.lang.ClassCastException:&#010;&gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.NullNode&#010;&gt; cannot be cast to&#010;&gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:337)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; at&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.serialize(JsonRowSerializationSchema.java:138)&#010;&gt; ... 38 more&#010;&gt;&#010;&gt;&#010;&gt; Best wishes.&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "1",
        "reply": "<CAGR9zpZJuNebSqff5CaXjM7AFm0Lg=7w8sYiep0Omtn60BYijw@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpbdzj=e7_BKzjqJZZSGM1_85BuKFBxjqd=eHLr1d-Kgpw@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 04:03:31 GMT",
        "subject": "Re: flink sql 1.10 insert into tb select 复杂schema 失败",
        "content": "Hi BenChao,&#010;&#010;刚才尝试了flink 1.10.1 但是问题还是存在，看了&#010;&#010;[1] https://issues.apache.org/jira/browse/FLINK-16628&#010;&#010;&#010;这个bug fix没有我给的 table 复杂，&#010;&#010;CREATE TABLE source_kafka_sasl (&#010;    svt STRING,&#010;    ops ROW&lt;a ROW(b STRING)&gt;&#010;) WITH ()&#010;&#010;&#010;我的是在原有的ops 里面又前嵌套了row。&#010;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月15日周三 上午10:25写道：&#010;&#010;&gt; Hi Peihui,&#010;&gt;&#010;&gt; 这是一个已知bug[1]，已经在1.10.1和1.11.0中修复了，可以尝试下这两个版本。&#010;&gt;&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-16220&#010;&gt;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月15日周三 上午9:54写道：&#010;&gt;&#010;&gt; &gt; Hello，&#010;&gt; &gt;&#010;&gt; &gt;          在使用flink sql 1.10.0 时候，当source table 中含有复杂schema，比如&#010;&gt; &gt; create table xxx (&#010;&gt; &gt; a string,&#010;&gt; &gt; b row(&#010;&gt; &gt;  c row(d string)&#010;&gt; &gt;   )&#010;&gt; &gt; )&#010;&gt; &gt;&#010;&gt; &gt; 当c 中有值的时候，sql 如下 insert into select * from xxx会出现下面错误&#010;&gt; &gt;&#010;&gt; &gt; Caused by: java.lang.ClassCastException:&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.NullNode&#010;&gt; &gt; cannot be cast to&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:337)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.serialize(JsonRowSerializationSchema.java:138)&#010;&gt; &gt; ... 38 more&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best wishes.&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CAGR9zpZJuNebSqff5CaXjM7AFm0Lg=7w8sYiep0Omtn60BYijw@mail.gmail.com>"
    },
    {
        "id": "<CAGR9zpbH-czYVAK_SMDoSBTCqQ+nBmFLmkOpeLR0daquQVfgnw@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 05:16:40 GMT",
        "subject": "Re: flink sql 1.10 insert into tb select 复杂schema 失败",
        "content": "Hi BenChao,&#010;&#010;换成1.10.1 就可以了。刚才那封邮件不行，是因为依赖flink-kafka的依赖版本没有修改过来。&#010;Thank you.&#010;&#010;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月15日周三 上午10:25写道：&#010;&#010;&gt; Hi Peihui,&#010;&gt;&#010;&gt; 这是一个已知bug[1]，已经在1.10.1和1.11.0中修复了，可以尝试下这两个版本。&#010;&gt;&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-16220&#010;&gt;&#010;&gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月15日周三 上午9:54写道：&#010;&gt;&#010;&gt; &gt; Hello，&#010;&gt; &gt;&#010;&gt; &gt;          在使用flink sql 1.10.0 时候，当source table 中含有复杂schema，比如&#010;&gt; &gt; create table xxx (&#010;&gt; &gt; a string,&#010;&gt; &gt; b row(&#010;&gt; &gt;  c row(d string)&#010;&gt; &gt;   )&#010;&gt; &gt; )&#010;&gt; &gt;&#010;&gt; &gt; 当c 中有值的时候，sql 如下 insert into select * from xxx会出现下面错误&#010;&gt; &gt;&#010;&gt; &gt; Caused by: java.lang.ClassCastException:&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.NullNode&#010;&gt; &gt; cannot be cast to&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:337)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.serialize(JsonRowSerializationSchema.java:138)&#010;&gt; &gt; ... 38 more&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best wishes.&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CAGR9zpZJuNebSqff5CaXjM7AFm0Lg=7w8sYiep0Omtn60BYijw@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_SFqKqDvgURYOifm=3uwU7o6jFAf22spTxgi+fc_vR5-Q@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 05:49:42 GMT",
        "subject": "Re: flink sql 1.10 insert into tb select 复杂schema 失败",
        "content": "赞👍 （不过这个应该只是json format的bug，跟connector没有关系）&#010;&#010;Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月15日周三 下午1:16写道：&#010;&#010;&gt; Hi BenChao,&#010;&gt;&#010;&gt; 换成1.10.1 就可以了。刚才那封邮件不行，是因为依赖flink-kafka的依赖版本没有修改过来。&#010;&gt; Thank you.&#010;&gt;&#010;&gt;&#010;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月15日周三 上午10:25写道：&#010;&gt;&#010;&gt; &gt; Hi Peihui,&#010;&gt; &gt;&#010;&gt; &gt; 这是一个已知bug[1]，已经在1.10.1和1.11.0中修复了，可以尝试下这两个版本。&#010;&gt; &gt;&#010;&gt; &gt; [1] https://issues.apache.org/jira/browse/FLINK-16220&#010;&gt; &gt;&#010;&gt; &gt; Peihui He &lt;peihuihe@gmail.com&gt; 于2020年7月15日周三 上午9:54写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; Hello，&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;          在使用flink sql 1.10.0 时候，当source table 中含有复杂schema，比如&#010;&gt; &gt; &gt; create table xxx (&#010;&gt; &gt; &gt; a string,&#010;&gt; &gt; &gt; b row(&#010;&gt; &gt; &gt;  c row(d string)&#010;&gt; &gt; &gt;   )&#010;&gt; &gt; &gt; )&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 当c 中有值的时候，sql 如下 insert into select * from xxx会出现下面错误&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Caused by: java.lang.ClassCastException:&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.NullNode&#010;&gt; &gt; &gt; cannot be cast to&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:337)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$assembleRowConverter$dd344700$1(JsonRowSerializationSchema.java:345)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.lambda$wrapIntoNullableConverter$1fa09b5b$1(JsonRowSerializationSchema.java:189)&#010;&gt; &gt; &gt; at&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.formats.json.JsonRowSerializationSchema.serialize(JsonRowSerializationSchema.java:138)&#010;&gt; &gt; &gt; ... 38 more&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best wishes.&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Benchao Li&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "3",
        "reply": "<CAGR9zpZJuNebSqff5CaXjM7AFm0Lg=7w8sYiep0Omtn60BYijw@mail.gmail.com>"
    },
    {
        "id": "<tencent_D83575359E08485E1740691F53A7563FA206@qq.com>",
        "from": "&quot;Z-Z&quot; &lt;zz9876543...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:47:39 GMT",
        "subject": "【求助】Flink Hadoop依赖问题",
        "content": "我在使用Flink 1.11.0版本中，使用docker-compose搭建，docker-compose文件如下：&#013;&#010;version: \"2.1\"&#013;&#010;services:&#013;&#010;&amp;nbsp; jobmanager:&#013;&#010;&amp;nbsp; &amp;nbsp; image: flink:1.11.0-scala_2.12&#013;&#010;&amp;nbsp; &amp;nbsp; expose:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6123\"&#013;&#010;&amp;nbsp; &amp;nbsp; ports:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"8081:8081\"&#013;&#010;&amp;nbsp; &amp;nbsp; command: jobmanager&#013;&#010;&amp;nbsp; &amp;nbsp; environment:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - JOB_MANAGER_RPC_ADDRESS=jobmanager&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - HADOOP_CLASSPATH=/data/hadoop-2.9.2/etc/hadoop:/data/hadoop-2.9.2/share/hadoop/common/lib/*:/data/hadoop-2.9.2/share/hadoop/common/*:/data/hadoop-2.9.2/share/hadoop/hdfs:/data/hadoop-2.9.2/share/hadoop/hdfs/lib/*:/data/hadoop-2.9.2/share/hadoop/hdfs/*:/data/hadoop-2.9.2/share/hadoop/yarn:/data/hadoop-2.9.2/share/hadoop/yarn/lib/*:/data/hadoop-2.9.2/share/hadoop/yarn/*:/data/hadoop-2.9.2/share/hadoop/mapreduce/lib/*:/data/hadoop-2.9.2/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar&#013;&#010;&amp;nbsp; &amp;nbsp; volumes:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./jobmanager/conf:/opt/flink/conf&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./data:/data&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; taskmanager:&#013;&#010;&amp;nbsp; &amp;nbsp; image: flink:1.11.0-scala_2.12&#013;&#010;&amp;nbsp; &amp;nbsp; expose:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6121\"&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6122\"&#013;&#010;&amp;nbsp; &amp;nbsp; depends_on:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - jobmanager&#013;&#010;&amp;nbsp; &amp;nbsp; command: taskmanager&#013;&#010;&amp;nbsp; &amp;nbsp; links:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"jobmanager:jobmanager\"&#013;&#010;&amp;nbsp; &amp;nbsp; environment:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - JOB_MANAGER_RPC_ADDRESS=jobmanager&#013;&#010;&amp;nbsp; &amp;nbsp; volumes:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./taskmanager/conf:/opt/flink/conf&#013;&#010;networks:&#013;&#010;&amp;nbsp; default:&#013;&#010;&amp;nbsp; &amp;nbsp; external:&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; name: flink-network&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hadoop-2.9.2已经放在data目录了，且已经在jobmanager和taskmanager的环境变量里添加了HADOOP_CLASSPATH，但通过cli提交和webui提交，jobmanager还是提示报Could&#010;not find a file system implementation for scheme 'hdfs'。有谁知道是怎么回事吗？",
        "depth": "0",
        "reply": "<tencent_D83575359E08485E1740691F53A7563FA206@qq.com>"
    },
    {
        "id": "<6a6b0623.5530.17351ba8406.Coremail.flinker@126.com>",
        "from": "&quot;Roc Marshal&quot; &lt;flin...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 09:07:24 GMT",
        "subject": "Re:【求助】Flink Hadoop依赖问题",
        "content": "&#010;&#010;&#010;你好，Z-Z,&#010;&#010;可以尝试在 https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/&#010;下载对应的uber jar包，并就将下载后的jar文件放到flink镜像的 ${FLINK_HOME}/lib&#010;路径下，之后启动编排的容器。&#010;祝好。&#010;Roc Marshal.&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-15 10:47:39，\"Z-Z\" &lt;zz9876543210@qq.com&gt; 写道：&#010;&gt;我在使用Flink 1.11.0版本中，使用docker-compose搭建，docker-compose文件如下：&#010;&gt;version: \"2.1\"&#010;&gt;services:&#010;&gt;&amp;nbsp; jobmanager:&#010;&gt;&amp;nbsp; &amp;nbsp; image: flink:1.11.0-scala_2.12&#010;&gt;&amp;nbsp; &amp;nbsp; expose:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6123\"&#010;&gt;&amp;nbsp; &amp;nbsp; ports:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"8081:8081\"&#010;&gt;&amp;nbsp; &amp;nbsp; command: jobmanager&#010;&gt;&amp;nbsp; &amp;nbsp; environment:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - JOB_MANAGER_RPC_ADDRESS=jobmanager&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - HADOOP_CLASSPATH=/data/hadoop-2.9.2/etc/hadoop:/data/hadoop-2.9.2/share/hadoop/common/lib/*:/data/hadoop-2.9.2/share/hadoop/common/*:/data/hadoop-2.9.2/share/hadoop/hdfs:/data/hadoop-2.9.2/share/hadoop/hdfs/lib/*:/data/hadoop-2.9.2/share/hadoop/hdfs/*:/data/hadoop-2.9.2/share/hadoop/yarn:/data/hadoop-2.9.2/share/hadoop/yarn/lib/*:/data/hadoop-2.9.2/share/hadoop/yarn/*:/data/hadoop-2.9.2/share/hadoop/mapreduce/lib/*:/data/hadoop-2.9.2/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar&#010;&gt;&amp;nbsp; &amp;nbsp; volumes:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./jobmanager/conf:/opt/flink/conf&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./data:/data&#010;&gt;&#010;&gt;&#010;&gt;&amp;nbsp; taskmanager:&#010;&gt;&amp;nbsp; &amp;nbsp; image: flink:1.11.0-scala_2.12&#010;&gt;&amp;nbsp; &amp;nbsp; expose:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6121\"&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6122\"&#010;&gt;&amp;nbsp; &amp;nbsp; depends_on:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - jobmanager&#010;&gt;&amp;nbsp; &amp;nbsp; command: taskmanager&#010;&gt;&amp;nbsp; &amp;nbsp; links:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"jobmanager:jobmanager\"&#010;&gt;&amp;nbsp; &amp;nbsp; environment:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - JOB_MANAGER_RPC_ADDRESS=jobmanager&#010;&gt;&amp;nbsp; &amp;nbsp; volumes:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./taskmanager/conf:/opt/flink/conf&#010;&gt;networks:&#010;&gt;&amp;nbsp; default:&#010;&gt;&amp;nbsp; &amp;nbsp; external:&#010;&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; name: flink-network&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;hadoop-2.9.2已经放在data目录了，且已经在jobmanager和taskmanager的环境变量里添加了HADOOP_CLASSPATH，但通过cli提交和webui提交，jobmanager还是提示报Could&#010;not find a file system implementation for scheme 'hdfs'。有谁知道是怎么回事吗？&#010;",
        "depth": "1",
        "reply": "<tencent_D83575359E08485E1740691F53A7563FA206@qq.com>"
    },
    {
        "id": "<CAP+gf37ySJEA5mUHPxDdmcr-TF8_iiG9P_594wgnm8o7Fm+vOg@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 06:35:43 GMT",
        "subject": "Re: 【求助】Flink Hadoop依赖问题",
        "content": "你可以在Pod里面确认一下/data目录是否正常挂载，另外需要在Pod里ps看一下&#013;&#010;起的JVM进程里的classpath是什么，有没有包括hadoop的jar&#013;&#010;&#013;&#010;&#013;&#010;当然，使用Roc Marshal建议的增加flink-shaded-hadoop并且放到$FLINK_HOME/lib下也可以解决问题&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;Roc Marshal &lt;flinker@126.com&gt; 于2020年7月15日周三 下午5:09写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 你好，Z-Z,&#013;&#010;&gt;&#013;&#010;&gt; 可以尝试在&#013;&#010;&gt; https://repo1.maven.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/&#013;&#010;&gt; 下载对应的uber jar包，并就将下载后的jar文件放到flink镜像的 ${FLINK_HOME}/lib&#010;路径下，之后启动编排的容器。&#013;&#010;&gt; 祝好。&#013;&#010;&gt; Roc Marshal.&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-15 10:47:39，\"Z-Z\" &lt;zz9876543210@qq.com&gt; 写道：&#013;&#010;&gt; &gt;我在使用Flink 1.11.0版本中，使用docker-compose搭建，docker-compose文件如下：&#013;&#010;&gt; &gt;version: \"2.1\"&#013;&#010;&gt; &gt;services:&#013;&#010;&gt; &gt;&amp;nbsp; jobmanager:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; image: flink:1.11.0-scala_2.12&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; expose:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6123\"&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; ports:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"8081:8081\"&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; command: jobmanager&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; environment:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - JOB_MANAGER_RPC_ADDRESS=jobmanager&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; -&#013;&#010;&gt; HADOOP_CLASSPATH=/data/hadoop-2.9.2/etc/hadoop:/data/hadoop-2.9.2/share/hadoop/common/lib/*:/data/hadoop-2.9.2/share/hadoop/common/*:/data/hadoop-2.9.2/share/hadoop/hdfs:/data/hadoop-2.9.2/share/hadoop/hdfs/lib/*:/data/hadoop-2.9.2/share/hadoop/hdfs/*:/data/hadoop-2.9.2/share/hadoop/yarn:/data/hadoop-2.9.2/share/hadoop/yarn/lib/*:/data/hadoop-2.9.2/share/hadoop/yarn/*:/data/hadoop-2.9.2/share/hadoop/mapreduce/lib/*:/data/hadoop-2.9.2/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; volumes:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./jobmanager/conf:/opt/flink/conf&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./data:/data&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&amp;nbsp; taskmanager:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; image: flink:1.11.0-scala_2.12&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; expose:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6121\"&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"6122\"&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; depends_on:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - jobmanager&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; command: taskmanager&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; links:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - \"jobmanager:jobmanager\"&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; environment:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - JOB_MANAGER_RPC_ADDRESS=jobmanager&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; volumes:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; - ./taskmanager/conf:/opt/flink/conf&#013;&#010;&gt; &gt;networks:&#013;&#010;&gt; &gt;&amp;nbsp; default:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; external:&#013;&#010;&gt; &gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; name: flink-network&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;hadoop-2.9.2已经放在data目录了，且已经在jobmanager和taskmanager的环境变量里添加了HADOOP_CLASSPATH，但通过cli提交和webui提交，jobmanager还是提示报Could&#013;&#010;&gt; not find a file system implementation for scheme 'hdfs'。有谁知道是怎么回事吗？&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_D83575359E08485E1740691F53A7563FA206@qq.com>"
    },
    {
        "id": "<tencent_8CDB2E7CCA8478CE8796164F@qq.com>",
        "from": "&quot;dmt312_2010&quot;&lt;dmt312_2...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:48:41 GMT",
        "subject": "flink1.11.0  java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread",
        "content": "Hi,&#010;大家好，请教各位大佬一个问题，我在验证flink 1.11.0时，遇到如下问题：&#010;&#010;&#010;报错信息：&#010;&#010;&#010;[ERROR] Could not execute SQL statement. Reason:&#010;java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.&lt;init&gt;(Lorg/slf4j/Logger;Lorg/apache/flink/streaming/connectors/kafka/internal/Handover;Ljava/util/Properties;&#010;Lorg/apache/flink/streaming/connectors/kafka/internals/ClosableBlockingQueue;Ljava/lang/String;&#010;JZLorg/apache/flink/metrics/MetricGroup;Lorg/apache/flink/metrics/MetricGroup;)V&#010;&#010;&#010;请问下是缺少某些需要的包吗？&#010;&#010;&#010;&#010;&#010;环境信息：&#010;&#010;&#010;版本：flink 1.11.0&#010;启动方式：flink on yarn (集成到CDH中)&#010;&#010;&#010;Flink Home：&#010;/opt/cloudera/parcels/FLINK/lib/flink&#010;&#010;&#010;${FLINK_HOME}/lib/ jar包信息:&#010;&#010;&#010;-rw-r--r-- 1 root   root      53820 Mar  9  2017 commons-cli-1.4.jar&#010;-rw-r--r-- 1 root   root     284220 Jan 17  2011 commons-lang-2.6.jar&#010;-rw-r--r-- 1 root   root      61829 Jul  6  2014 commons-logging-1.2.jar&#010;-rw-r--r-- 1 root   root     197130 Jun 30 12:45 flink-connector-jdbc_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      47655 Jun 30 12:41 flink-connector-kafka-0.10_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      60151 Jun 30 12:42 flink-connector-kafka-0.11_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root     109660 Jun 30 12:46 flink-connector-kafka_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root     122794 Jun 30 12:41 flink-connector-kafka-base_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      90782 Jun 30 16:40 flink-csv-1.11.0.jar&#010;-rw-r--r-- 1 root   root   99461460 Jun 30 16:46 flink-dist_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      94863 Jun 30 16:40 flink-json-1.11.0.jar&#010;-rw-rw-r-- 1 root   root    19127 Jul 14 19:12 flink-metrics-core-1.11.0.jar&#010;-rw-r--r-- 1 root   root     108120 Jul 14 19:35 flink-metrics-prometheus-1.11.0.jar&#010;-rw-r--r-- 1 root   root    7712156 Jun 18 10:42 flink-shaded-zookeeper-3.4.14.jar&#010;-rw-r--r-- 1 root   root   31924588 Jun 30 16:45 flink-table_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root   34817036 Jun 30 16:45 flink-table-blink_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root    2740491 Oct 18  2019 kafka-clients-2.3.1.jar&#010;-rw-r--r-- 1 root   root      67114 Apr 20 20:47 log4j-1.2-api-2.12.1.jar&#010;-rw-r--r-- 1 root   root     276771 Apr 20 20:47 log4j-api-2.12.1.jar&#010;-rw-r--r-- 1 root   root    1674433 Apr 20 20:47 log4j-core-2.12.1.jar&#010;-rw-r--r-- 1 root   root      23518 Apr 20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;-rw-r--r-- 1 root   root    1006904 Apr 20 11:10 mysql-connector-java-5.1.49.jar&#010;-rw-r--r-- 1 root   root      26084 Jul 14 18:54 slf4j-api-1.7.5.jar&#010;-rw-r--r-- 1 root   root       8869 Jul 14 18:54 slf4j-log4j12-1.7.5.jar&#010;-rwxr-xr-x 1 root   root      10680 Jul 14 18:54 slf4j-simple-1.7.5.jar&#010;&#010;&#010;&#010;&#010;&#010;&#010;执行语句&#010;&#010;&#010;bin/sql-client.sh embedded&#010;&#010;&#010;&#010;&#010;CREATE TABLE TZT_PAYMENT_ORDER (&#010;  `op_type` STRING,&#010;  `op_ts` STRING,&#010;  `current_ts` STRING,&#010;  `pos` STRING, &#010;  `ID` BIGINT,&#010;  `TRACE_ID` STRING,&#010;  `BIZ_SYSTEM_CODE` STRING,&#010;  `MERCHANT_NO` STRING,&#010;  `REQUEST_NO` STRING,&#010;  `PRODUCT_NAME` STRING,&#010;  `BANK_CODE` STRING,&#010;  `CREATE_TIME` STRING,&#010;  `row_ts` as TO_TIMESTAMP(`CREATE_TIME`),&#010;   WATERMARK FOR row_ts AS row_ts - INTERVAL '1' MINUTE&#010;) WITH (&#010;   'format' = 'json',&#010;   'connector' = 'kafka',&#010;   'topic' = 'TZT_TZT_PAYMENT_ORDER',&#010;   'properties.bootstrap.servers' = 'xxxx',&#010;   'properties.group.id' = 'TZT_TZT_PAYMENT_ORDER_TEST_WWX',&#010;   'scan.startup.mode' = 'earliest-offset'&#010;  );&#010;&#010;&#010;select *  from  TZT_PAYMENT_ORDER;",
        "depth": "0",
        "reply": "<tencent_8CDB2E7CCA8478CE8796164F@qq.com>"
    },
    {
        "id": "<2BB1FB8E-438C-4022-8F5E-E549B4CD6901@gmail.com>",
        "from": "Paul Lam &lt;paullin3...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:56:46 GMT",
        "subject": "Re: flink1.11.0  java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread",
        "content": "Hi,&#010;&#010;看起来是 Kafka connector class 冲突了，flink-connector-kafka_2.12-1.11.0.jar 和 flink-connector-kafka-0.10_2.12-1.11.0.jar&#010;不能同时加到 classpath 里。&#010;&#010;Best,&#010;Paul Lam&#010;&#010;&gt; 2020年7月15日 10:48，dmt312_2010 &lt;dmt312_2010@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,&#010;&gt; 大家好，请教各位大佬一个问题，我在验证flink 1.11.0时，遇到如下问题：&#010;&gt; &#010;&gt; &#010;&gt; 报错信息：&#010;&gt; &#010;&gt; &#010;&gt; [ERROR] Could not execute SQL statement. Reason:&#010;&gt; java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.&lt;init&gt;(Lorg/slf4j/Logger;Lorg/apache/flink/streaming/connectors/kafka/internal/Handover;Ljava/util/Properties;&#010;&gt; Lorg/apache/flink/streaming/connectors/kafka/internals/ClosableBlockingQueue;Ljava/lang/String;&#010;&gt; JZLorg/apache/flink/metrics/MetricGroup;Lorg/apache/flink/metrics/MetricGroup;)V&#010;&gt; &#010;&gt; &#010;&gt; 请问下是缺少某些需要的包吗？&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 环境信息：&#010;&gt; &#010;&gt; &#010;&gt; 版本：flink 1.11.0&#010;&gt; 启动方式：flink on yarn (集成到CDH中)&#010;&gt; &#010;&gt; &#010;&gt; Flink Home：&#010;&gt; /opt/cloudera/parcels/FLINK/lib/flink&#010;&gt; &#010;&gt; &#010;&gt; ${FLINK_HOME}/lib/ jar包信息:&#010;&gt; &#010;&gt; &#010;&gt; -rw-r--r-- 1 root   root      53820 Mar  9  2017 commons-cli-1.4.jar&#010;&gt; -rw-r--r-- 1 root   root     284220 Jan 17  2011 commons-lang-2.6.jar&#010;&gt; -rw-r--r-- 1 root   root      61829 Jul  6  2014 commons-logging-1.2.jar&#010;&gt; -rw-r--r-- 1 root   root     197130 Jun 30 12:45 flink-connector-jdbc_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root      47655 Jun 30 12:41 flink-connector-kafka-0.10_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root      60151 Jun 30 12:42 flink-connector-kafka-0.11_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root     109660 Jun 30 12:46 flink-connector-kafka_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root     122794 Jun 30 12:41 flink-connector-kafka-base_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root      90782 Jun 30 16:40 flink-csv-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root   99461460 Jun 30 16:46 flink-dist_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root      94863 Jun 30 16:40 flink-json-1.11.0.jar&#010;&gt; -rw-rw-r-- 1 root   root    19127 Jul 14 19:12 flink-metrics-core-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root     108120 Jul 14 19:35 flink-metrics-prometheus-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root    7712156 Jun 18 10:42 flink-shaded-zookeeper-3.4.14.jar&#010;&gt; -rw-r--r-- 1 root   root   31924588 Jun 30 16:45 flink-table_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root   34817036 Jun 30 16:45 flink-table-blink_2.12-1.11.0.jar&#010;&gt; -rw-r--r-- 1 root   root    2740491 Oct 18  2019 kafka-clients-2.3.1.jar&#010;&gt; -rw-r--r-- 1 root   root      67114 Apr 20 20:47 log4j-1.2-api-2.12.1.jar&#010;&gt; -rw-r--r-- 1 root   root     276771 Apr 20 20:47 log4j-api-2.12.1.jar&#010;&gt; -rw-r--r-- 1 root   root    1674433 Apr 20 20:47 log4j-core-2.12.1.jar&#010;&gt; -rw-r--r-- 1 root   root      23518 Apr 20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;&gt; -rw-r--r-- 1 root   root    1006904 Apr 20 11:10 mysql-connector-java-5.1.49.jar&#010;&gt; -rw-r--r-- 1 root   root      26084 Jul 14 18:54 slf4j-api-1.7.5.jar&#010;&gt; -rw-r--r-- 1 root   root       8869 Jul 14 18:54 slf4j-log4j12-1.7.5.jar&#010;&gt; -rwxr-xr-x 1 root   root      10680 Jul 14 18:54 slf4j-simple-1.7.5.jar&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 执行语句&#010;&gt; &#010;&gt; &#010;&gt; bin/sql-client.sh embedded&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; CREATE TABLE TZT_PAYMENT_ORDER (&#010;&gt;  `op_type` STRING,&#010;&gt;  `op_ts` STRING,&#010;&gt;  `current_ts` STRING,&#010;&gt;  `pos` STRING, &#010;&gt;  `ID` BIGINT,&#010;&gt;  `TRACE_ID` STRING,&#010;&gt;  `BIZ_SYSTEM_CODE` STRING,&#010;&gt;  `MERCHANT_NO` STRING,&#010;&gt;  `REQUEST_NO` STRING,&#010;&gt;  `PRODUCT_NAME` STRING,&#010;&gt;  `BANK_CODE` STRING,&#010;&gt;  `CREATE_TIME` STRING,&#010;&gt;  `row_ts` as TO_TIMESTAMP(`CREATE_TIME`),&#010;&gt;   WATERMARK FOR row_ts AS row_ts - INTERVAL '1' MINUTE&#010;&gt; ) WITH (&#010;&gt;   'format' = 'json',&#010;&gt;   'connector' = 'kafka',&#010;&gt;   'topic' = 'TZT_TZT_PAYMENT_ORDER',&#010;&gt;   'properties.bootstrap.servers' = 'xxxx',&#010;&gt;   'properties.group.id' = 'TZT_TZT_PAYMENT_ORDER_TEST_WWX',&#010;&gt;   'scan.startup.mode' = 'earliest-offset'&#010;&gt;  );&#010;&gt; &#010;&gt; &#010;&gt; select *  from  TZT_PAYMENT_ORDER;&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<tencent_8CDB2E7CCA8478CE8796164F@qq.com>"
    },
    {
        "id": "<CC21E54A-0A43-4D80-88F7-2B17D1BAB0A8@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 02:59:16 GMT",
        "subject": "Re: flink1.11.0  java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread",
        "content": "Hi,&#010;两个kafka connector是会冲突的，还有一点是 SQL client 里应该用 sql 的connector依赖吧，&#010;flink-sql-connector-kafka_2.12-1.11.0.jar &#010;&#010;祝好，&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月15日，10:56，Paul Lam &lt;paullin3280@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,&#010;&gt; &#010;&gt; 看起来是 Kafka connector class 冲突了，flink-connector-kafka_2.12-1.11.0.jar&#010;和 flink-connector-kafka-0.10_2.12-1.11.0.jar 不能同时加到 classpath 里。&#010;&gt; &#010;&gt; Best,&#010;&gt; Paul Lam&#010;&gt; &#010;&gt;&gt; 2020年7月15日 10:48，dmt312_2010 &lt;dmt312_2010@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; Hi,&#010;&gt;&gt; 大家好，请教各位大佬一个问题，我在验证flink 1.11.0时，遇到如下问题：&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 报错信息：&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; [ERROR] Could not execute SQL statement. Reason:&#010;&gt;&gt; java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.&lt;init&gt;(Lorg/slf4j/Logger;Lorg/apache/flink/streaming/connectors/kafka/internal/Handover;Ljava/util/Properties;&#010;&gt;&gt; Lorg/apache/flink/streaming/connectors/kafka/internals/ClosableBlockingQueue;Ljava/lang/String;&#010;&gt;&gt; JZLorg/apache/flink/metrics/MetricGroup;Lorg/apache/flink/metrics/MetricGroup;)V&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 请问下是缺少某些需要的包吗？&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 环境信息：&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 版本：flink 1.11.0&#010;&gt;&gt; 启动方式：flink on yarn (集成到CDH中)&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; Flink Home：&#010;&gt;&gt; /opt/cloudera/parcels/FLINK/lib/flink&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; ${FLINK_HOME}/lib/ jar包信息:&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; -rw-r--r-- 1 root   root      53820 Mar  9  2017 commons-cli-1.4.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root     284220 Jan 17  2011 commons-lang-2.6.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      61829 Jul  6  2014 commons-logging-1.2.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root     197130 Jun 30 12:45 flink-connector-jdbc_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      47655 Jun 30 12:41 flink-connector-kafka-0.10_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      60151 Jun 30 12:42 flink-connector-kafka-0.11_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root     109660 Jun 30 12:46 flink-connector-kafka_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root     122794 Jun 30 12:41 flink-connector-kafka-base_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      90782 Jun 30 16:40 flink-csv-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root   99461460 Jun 30 16:46 flink-dist_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      94863 Jun 30 16:40 flink-json-1.11.0.jar&#010;&gt;&gt; -rw-rw-r-- 1 root   root    19127 Jul 14 19:12 flink-metrics-core-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root     108120 Jul 14 19:35 flink-metrics-prometheus-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root    7712156 Jun 18 10:42 flink-shaded-zookeeper-3.4.14.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root   31924588 Jun 30 16:45 flink-table_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root   34817036 Jun 30 16:45 flink-table-blink_2.12-1.11.0.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root    2740491 Oct 18  2019 kafka-clients-2.3.1.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      67114 Apr 20 20:47 log4j-1.2-api-2.12.1.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root     276771 Apr 20 20:47 log4j-api-2.12.1.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root    1674433 Apr 20 20:47 log4j-core-2.12.1.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      23518 Apr 20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root    1006904 Apr 20 11:10 mysql-connector-java-5.1.49.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root      26084 Jul 14 18:54 slf4j-api-1.7.5.jar&#010;&gt;&gt; -rw-r--r-- 1 root   root       8869 Jul 14 18:54 slf4j-log4j12-1.7.5.jar&#010;&gt;&gt; -rwxr-xr-x 1 root   root      10680 Jul 14 18:54 slf4j-simple-1.7.5.jar&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 执行语句&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; bin/sql-client.sh embedded&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; CREATE TABLE TZT_PAYMENT_ORDER (&#010;&gt;&gt; `op_type` STRING,&#010;&gt;&gt; `op_ts` STRING,&#010;&gt;&gt; `current_ts` STRING,&#010;&gt;&gt; `pos` STRING, &#010;&gt;&gt; `ID` BIGINT,&#010;&gt;&gt; `TRACE_ID` STRING,&#010;&gt;&gt; `BIZ_SYSTEM_CODE` STRING,&#010;&gt;&gt; `MERCHANT_NO` STRING,&#010;&gt;&gt; `REQUEST_NO` STRING,&#010;&gt;&gt; `PRODUCT_NAME` STRING,&#010;&gt;&gt; `BANK_CODE` STRING,&#010;&gt;&gt; `CREATE_TIME` STRING,&#010;&gt;&gt; `row_ts` as TO_TIMESTAMP(`CREATE_TIME`),&#010;&gt;&gt;  WATERMARK FOR row_ts AS row_ts - INTERVAL '1' MINUTE&#010;&gt;&gt; ) WITH (&#010;&gt;&gt;  'format' = 'json',&#010;&gt;&gt;  'connector' = 'kafka',&#010;&gt;&gt;  'topic' = 'TZT_TZT_PAYMENT_ORDER',&#010;&gt;&gt;  'properties.bootstrap.servers' = 'xxxx',&#010;&gt;&gt;  'properties.group.id' = 'TZT_TZT_PAYMENT_ORDER_TEST_WWX',&#010;&gt;&gt;  'scan.startup.mode' = 'earliest-offset'&#010;&gt;&gt; );&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; select *  from  TZT_PAYMENT_ORDER;&#010;&gt; &#010;&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_8CDB2E7CCA8478CE8796164F@qq.com>"
    },
    {
        "id": "<5c0c0e72.bb6d.173506eaef1.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 03:04:57 GMT",
        "subject": "Re: flink1.11.0 java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread",
        "content": "这个很明显是jar包冲突了 只保留flink-sql那个包就行了&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;On 07/15/2020 10:48, dmt312_2010 wrote:&#010;Hi,&#010;大家好，请教各位大佬一个问题，我在验证flink 1.11.0时，遇到如下问题：&#010;&#010;&#010;报错信息：&#010;&#010;&#010;[ERROR] Could not execute SQL statement. Reason:&#010;java.lang.NoSuchMethodError: org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.&lt;init&gt;(Lorg/slf4j/Logger;Lorg/apache/flink/streaming/connectors/kafka/internal/Handover;Ljava/util/Properties;&#010;Lorg/apache/flink/streaming/connectors/kafka/internals/ClosableBlockingQueue;Ljava/lang/String;&#010;JZLorg/apache/flink/metrics/MetricGroup;Lorg/apache/flink/metrics/MetricGroup;)V&#010;&#010;&#010;请问下是缺少某些需要的包吗？&#010;&#010;&#010;&#010;&#010;环境信息：&#010;&#010;&#010;版本：flink 1.11.0&#010;启动方式：flink on yarn (集成到CDH中)&#010;&#010;&#010;Flink Home：&#010;/opt/cloudera/parcels/FLINK/lib/flink&#010;&#010;&#010;${FLINK_HOME}/lib/ jar包信息:&#010;&#010;&#010;-rw-r--r-- 1 root   root      53820 Mar  9  2017 commons-cli-1.4.jar&#010;-rw-r--r-- 1 root   root     284220 Jan 17  2011 commons-lang-2.6.jar&#010;-rw-r--r-- 1 root   root      61829 Jul  6  2014 commons-logging-1.2.jar&#010;-rw-r--r-- 1 root   root     197130 Jun 30 12:45 flink-connector-jdbc_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      47655 Jun 30 12:41 flink-connector-kafka-0.10_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      60151 Jun 30 12:42 flink-connector-kafka-0.11_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root     109660 Jun 30 12:46 flink-connector-kafka_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root     122794 Jun 30 12:41 flink-connector-kafka-base_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      90782 Jun 30 16:40 flink-csv-1.11.0.jar&#010;-rw-r--r-- 1 root   root   99461460 Jun 30 16:46 flink-dist_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root      94863 Jun 30 16:40 flink-json-1.11.0.jar&#010;-rw-rw-r-- 1 root   root    19127 Jul 14 19:12 flink-metrics-core-1.11.0.jar&#010;-rw-r--r-- 1 root   root     108120 Jul 14 19:35 flink-metrics-prometheus-1.11.0.jar&#010;-rw-r--r-- 1 root   root    7712156 Jun 18 10:42 flink-shaded-zookeeper-3.4.14.jar&#010;-rw-r--r-- 1 root   root   31924588 Jun 30 16:45 flink-table_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root   34817036 Jun 30 16:45 flink-table-blink_2.12-1.11.0.jar&#010;-rw-r--r-- 1 root   root    2740491 Oct 18  2019 kafka-clients-2.3.1.jar&#010;-rw-r--r-- 1 root   root      67114 Apr 20 20:47 log4j-1.2-api-2.12.1.jar&#010;-rw-r--r-- 1 root   root     276771 Apr 20 20:47 log4j-api-2.12.1.jar&#010;-rw-r--r-- 1 root   root    1674433 Apr 20 20:47 log4j-core-2.12.1.jar&#010;-rw-r--r-- 1 root   root      23518 Apr 20 20:47 log4j-slf4j-impl-2.12.1.jar&#010;-rw-r--r-- 1 root   root    1006904 Apr 20 11:10 mysql-connector-java-5.1.49.jar&#010;-rw-r--r-- 1 root   root      26084 Jul 14 18:54 slf4j-api-1.7.5.jar&#010;-rw-r--r-- 1 root   root       8869 Jul 14 18:54 slf4j-log4j12-1.7.5.jar&#010;-rwxr-xr-x 1 root   root      10680 Jul 14 18:54 slf4j-simple-1.7.5.jar&#010;&#010;&#010;&#010;&#010;&#010;&#010;执行语句&#010;&#010;&#010;bin/sql-client.sh embedded&#010;&#010;&#010;&#010;&#010;CREATE TABLE TZT_PAYMENT_ORDER (&#010; `op_type` STRING,&#010; `op_ts` STRING,&#010; `current_ts` STRING,&#010; `pos` STRING,&#010; `ID` BIGINT,&#010; `TRACE_ID` STRING,&#010; `BIZ_SYSTEM_CODE` STRING,&#010; `MERCHANT_NO` STRING,&#010; `REQUEST_NO` STRING,&#010; `PRODUCT_NAME` STRING,&#010; `BANK_CODE` STRING,&#010; `CREATE_TIME` STRING,&#010; `row_ts` as TO_TIMESTAMP(`CREATE_TIME`),&#010;  WATERMARK FOR row_ts AS row_ts - INTERVAL '1' MINUTE&#010;) WITH (&#010;  'format' = 'json',&#010;  'connector' = 'kafka',&#010;  'topic' = 'TZT_TZT_PAYMENT_ORDER',&#010;  'properties.bootstrap.servers' = 'xxxx',&#010;  'properties.group.id' = 'TZT_TZT_PAYMENT_ORDER_TEST_WWX',&#010;  'scan.startup.mode' = 'earliest-offset'&#010; );&#010;&#010;&#010;select *  from  TZT_PAYMENT_ORDER;",
        "depth": "1",
        "reply": "<tencent_8CDB2E7CCA8478CE8796164F@qq.com>"
    },
    {
        "id": "<tencent_B16F79C82DB8698DF28CA95A@qq.com>",
        "from": "&quot;dmt312_2010&quot;&lt;dmt312_2...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 03:18:39 GMT",
        "subject": "Re: flink1.11.0 java.lang.NoSuchMethodError:org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread",
        "content": "谢谢，确实是jar包冲突的问题，替换完包以后已经可以了&#010;&#010;&#010; 原始邮件 &#010;发件人: JasonLee&lt;17610775726@163.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月15日(周三) 11:04&#010;主题: Re: flink1.11.0 java.lang.NoSuchMethodError:org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread&#010;&#010;&#010;这个很明显是jar包冲突了 只保留flink-sql那个包就行了 | | JasonLee | | 邮箱：17610775726@163.com&#010;| Signature is customized by Netease Mail Master On 07/15/2020 10:48, dmt312_2010 wrote: Hi,&#010;大家好，请教各位大佬一个问题，我在验证flink 1.11.0时，遇到如下问题：&#010;报错信息： [ERROR] Could not execute SQL statement. Reason: java.lang.NoSuchMethodError:&#010;org.apache.flink.streaming.connectors.kafka.internal.KafkaConsumerThread.&lt;init&gt;(Lorg/slf4j/Logger;Lorg/apache/flink/streaming/connectors/kafka/internal/Handover;Ljava/util/Properties;&#010;Lorg/apache/flink/streaming/connectors/kafka/internals/ClosableBlockingQueue;Ljava/lang/String;&#010;JZLorg/apache/flink/metrics/MetricGroup;Lorg/apache/flink/metrics/MetricGroup;)V 请问下是缺少某些需要的包吗？&#010;环境信息： 版本：flink 1.11.0 启动方式：flink on yarn (集成到CDH中) Flink&#010;Home： /opt/cloudera/parcels/FLINK/lib/flink ${FLINK_HOME}/lib/ jar包信息: -rw-r--r--&#010;1 root root 53820 Mar 9 2017 commons-cli-1.4.jar -rw-r--r-- 1 root root 284220 Jan 17 2011&#010;commons-lang-2.6.jar -rw-r--r-- 1 root root 61829 Jul 6 2014 commons-logging-1.2.jar -rw-r--r--&#010;1 root root 197130 Jun 30 12:45 flink-connector-jdbc_2.12-1.11.0.jar -rw-r--r-- 1 root root&#010;47655 Jun 30 12:41 flink-connector-kafka-0.10_2.12-1.11.0.jar -rw-r--r-- 1 root root 60151&#010;Jun 30 12:42 flink-connector-kafka-0.11_2.12-1.11.0.jar -rw-r--r-- 1 root root 109660 Jun&#010;30 12:46 flink-connector-kafka_2.12-1.11.0.jar -rw-r--r-- 1 root root 122794 Jun 30 12:41&#010;flink-connector-kafka-base_2.12-1.11.0.jar -rw-r--r-- 1 root root 90782 Jun 30 16:40 flink-csv-1.11.0.jar&#010;-rw-r--r-- 1 root root 99461460 Jun 30 16:46 flink-dist_2.12-1.11.0.jar -rw-r--r-- 1 root&#010;root 94863 Jun 30 16:40 flink-json-1.11.0.jar -rw-rw-r-- 1 root root 19127 Jul 14 19:12 flink-metrics-core-1.11.0.jar&#010;-rw-r--r-- 1 root root 108120 Jul 14 19:35 flink-metrics-prometheus-1.11.0.jar -rw-r--r--&#010;1 root root 7712156 Jun 18 10:42 flink-shaded-zookeeper-3.4.14.jar -rw-r--r-- 1 root root&#010;31924588 Jun 30 16:45 flink-table_2.12-1.11.0.jar -rw-r--r-- 1 root root 34817036 Jun 30 16:45&#010;flink-table-blink_2.12-1.11.0.jar -rw-r--r-- 1 root root 2740491 Oct 18 2019 kafka-clients-2.3.1.jar&#010;-rw-r--r-- 1 root root 67114 Apr 20 20:47 log4j-1.2-api-2.12.1.jar -rw-r--r-- 1 root root&#010;276771 Apr 20 20:47 log4j-api-2.12.1.jar -rw-r--r-- 1 root root 1674433 Apr 20 20:47 log4j-core-2.12.1.jar&#010;-rw-r--r-- 1 root root 23518 Apr 20 20:47 log4j-slf4j-impl-2.12.1.jar -rw-r--r-- 1 root root&#010;1006904 Apr 20 11:10 mysql-connector-java-5.1.49.jar -rw-r--r-- 1 root root 26084 Jul 14 18:54&#010;slf4j-api-1.7.5.jar -rw-r--r-- 1 root root 8869 Jul 14 18:54 slf4j-log4j12-1.7.5.jar -rwxr-xr-x&#010;1 root root 10680 Jul 14 18:54 slf4j-simple-1.7.5.jar 执行语句 bin/sql-client.sh embedded&#010;CREATE TABLE TZT_PAYMENT_ORDER ( `op_type` STRING, `op_ts` STRING, `current_ts` STRING, `pos`&#010;STRING, `ID` BIGINT, `TRACE_ID` STRING, `BIZ_SYSTEM_CODE` STRING, `MERCHANT_NO` STRING, `REQUEST_NO`&#010;STRING, `PRODUCT_NAME` STRING, `BANK_CODE` STRING, `CREATE_TIME` STRING, `row_ts` as TO_TIMESTAMP(`CREATE_TIME`),&#010;WATERMARK FOR row_ts AS row_ts - INTERVAL '1' MINUTE ) WITH ( 'format' = 'json', 'connector'&#010;= 'kafka', 'topic' = 'TZT_TZT_PAYMENT_ORDER', 'properties.bootstrap.servers' = 'xxxx', 'properties.group.id'&#010;= 'TZT_TZT_PAYMENT_ORDER_TEST_WWX', 'scan.startup.mode' = 'earliest-offset' ); select * from&#010;TZT_PAYMENT_ORDER;",
        "depth": "0",
        "reply": "<tencent_B16F79C82DB8698DF28CA95A@qq.com>"
    },
    {
        "id": "<2020071515035482453110@163.com>",
        "from": "&quot;18500348251@163.com&quot; &lt;18500348...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 07:03:55 GMT",
        "subject": "flink1.11 sql kafka 抽取事件时间",
        "content": "大家好！&#013;&#010;&#013;&#010;使用flink1.11 sql接入kafka ，format为csv&#013;&#010;从eventTime字段中抽取事件时间&#013;&#010;rowtime AS TO_TIMESTAMP(FROM_UNIXTIME(eventTime / 1000, 'yyyy-MM-dd HH:mm:ss'))&#013;&#010;eventTime可能存在脏数据（非13位的毫秒时间戳），设置了 'csv.ignore-parse-errors'&#010;= 'true', 那么eventTime会被设置为null，此时会报一个异常：&#013;&#010;Caused by: java.lang.RuntimeException: RowTime field should not be null, please convert it&#010;to a non-null long value.&#013;&#010;&#013;&#010;有没有什么好的方式可以解决&#013;&#010;&#013;&#010;&#013;&#010;祝好！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;18500348251@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<2020071515035482453110@163.com>"
    },
    {
        "id": "<CABKuJ_SHzm_4D=E6Y415=DhmDe3Y00LEV-YoWYKL0dSp2UWSiA@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 11:00:17 GMT",
        "subject": "Re: flink1.11 sql kafka 抽取事件时间",
        "content": "我感觉可以通过计算列的方式来解决呀，你只需要在计算rowtime这个列的时候保证它不是null即可，如果是null，可以设置一个默认值之类的？&#013;&#010;&#013;&#010;18500348251@163.com &lt;18500348251@163.com&gt; 于2020年7月15日周三 下午3:04写道：&#013;&#010;&#013;&#010;&gt; 大家好！&#013;&#010;&gt;&#013;&#010;&gt; 使用flink1.11 sql接入kafka ，format为csv&#013;&#010;&gt; 从eventTime字段中抽取事件时间&#013;&#010;&gt; rowtime AS TO_TIMESTAMP(FROM_UNIXTIME(eventTime / 1000, 'yyyy-MM-dd&#013;&#010;&gt; HH:mm:ss'))&#013;&#010;&gt; eventTime可能存在脏数据（非13位的毫秒时间戳），设置了 'csv.ignore-parse-errors'&#010;= 'true',&#013;&#010;&gt; 那么eventTime会被设置为null，此时会报一个异常：&#013;&#010;&gt; Caused by: java.lang.RuntimeException: RowTime field should not be null,&#013;&#010;&gt; please convert it to a non-null long value.&#013;&#010;&gt;&#013;&#010;&gt; 有没有什么好的方式可以解决&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 祝好！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 18500348251@163.com&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<2020071515035482453110@163.com>"
    },
    {
        "id": "<2020071516344371288836@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 08:34:44 GMT",
        "subject": "FlinkSQL 入到 MySQL后汉字乱码",
        "content": "&#013;&#010;KafkaTable：        kafka 消息&#013;&#010;MySQL_tableA：  维表，维表里 value 是汉字&#013;&#010;MySQL_tableB：  join后的结果表。和 MySQL_tableA 不在同一台服务器上。&#013;&#010;&#013;&#010;我直接在 flink sql client   SELECT 是可以正常显示， 但 INSERT INTO MySQL_tableB&#010;SELECT 后到 MySQL_tableB 里去查看，汉字就乱码了。&#013;&#010;大家有什么建议吗？&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;",
        "depth": "0",
        "reply": "<2020071516344371288836@geekplus.com.cn>"
    },
    {
        "id": "<2020071516473460392938@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 08:47:38 GMT",
        "subject": "回复: FlinkSQL 入到 MySQL后汉字乱码",
        "content": "&#013;&#010;是 MySQL_tableB 所在的 server 端字符设置有问题。&#013;&#010;配置中加上下面的配置就好了。&#013;&#010;&#013;&#010;&#013;&#010;[mysqld] character-set-server=utf8 [client] default-character-set=utf8 [mysql] default-character-set=utf8&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;发件人： wanglei2@geekplus.com.cn&#013;&#010;发送时间： 2020-07-15 16:34&#013;&#010;收件人： user-zh&#013;&#010;主题： FlinkSQL 入到 MySQL后汉字乱码&#013;&#010; &#013;&#010;KafkaTable：        kafka 消息&#013;&#010;MySQL_tableA：  维表，维表里 value 是汉字&#013;&#010;MySQL_tableB：  join后的结果表。和 MySQL_tableA 不在同一台服务器上。&#013;&#010; &#013;&#010;我直接在 flink sql client   SELECT 是可以正常显示， 但 INSERT INTO MySQL_tableB&#010;SELECT 后到 MySQL_tableB 里去查看，汉字就乱码了。&#013;&#010;大家有什么建议吗？&#013;&#010; &#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;",
        "depth": "1",
        "reply": "<2020071516344371288836@geekplus.com.cn>"
    },
    {
        "id": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 09:04:18 GMT",
        "subject": "Flink 1.11 submit job timed out",
        "content": "&#010;Hi&#010;&#010;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。&#010;job 并行度120.提交作业的时候出现大量的No hostname could be resolved for the&#010;IP address，JM time out，作业提交失败。web ui也会卡主无响应。&#010;&#010;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&#010;&#010;部分日志如下：&#010;&#010;2020-07-15 16:58:46,460 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation   &#010; [] - No hostname could be resolved for the IP address 10.32.160.7, using IP address as host&#010;name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-15 16:58:46,460 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation   &#010; [] - No hostname could be resolved for the IP address 10.44.224.7, using IP address as host&#010;name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-15 16:58:46,461 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation   &#010; [] - No hostname could be resolved for the IP address 10.40.32.9, using IP address as host&#010;name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&#010;2020-07-15 16:59:10,236 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] - The heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed out.&#010;2020-07-15 16:59:10,236 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2&#010;for job e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&#010;&#010;how to deal with ？&#010;&#010;&#010;beset ！&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制",
        "depth": "0",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<6543a1ca.56d3.17351c2b59c.Coremail.flinker@126.com>",
        "from": "&quot;Roc Marshal&quot; &lt;flin...@126.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 09:16:21 GMT",
        "subject": "Re:Flink 1.11 submit job timed out",
        "content": "Hi，SmileSmile.&#010;个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;希望这对你有帮助。&#010;&#010;&#010;祝好。&#010;Roc Marshal&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt;&#010;&gt;Hi&#010;&gt;&#010;&gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。&#010;job 并行度120.提交作业的时候出现大量的No hostname could be resolved for the&#010;IP address，JM time out，作业提交失败。web ui也会卡主无响应。&#010;&gt;&#010;&gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt;&#010;&gt;&#010;&gt;部分日志如下：&#010;&gt;&#010;&gt;2020-07-15 16:58:46,460 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation&#010;    [] - No hostname could be resolved for the IP address 10.32.160.7, using IP address as&#010;host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&gt;2020-07-15 16:58:46,460 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation&#010;    [] - No hostname could be resolved for the IP address 10.44.224.7, using IP address as&#010;host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&gt;2020-07-15 16:58:46,461 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation&#010;    [] - No hostname could be resolved for the IP address 10.40.32.9, using IP address as&#010;host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&gt;&#010;&gt;2020-07-15 16:59:10,236 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] - The heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed out.&#010;&gt;2020-07-15 16:59:10,236 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2&#010;for job e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt;&#010;&gt;&#010;&gt;how to deal with ？&#010;&gt;&#010;&gt;&#010;&gt;beset ！&#010;&gt;&#010;&gt;| |&#010;&gt;a511955993&#010;&gt;|&#010;&gt;|&#010;&gt;邮箱：a511955993@163.com&#010;&gt;|&#010;&gt;&#010;&gt;签名由 网易邮箱大师 定制&#010;",
        "depth": "1",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<5ce3b341.eb59.17351c5eec5.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 09:19:53 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "Hi Roc&#010;&#010;该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/15/2020 17:16, Roc Marshal wrote:&#010;Hi，SmileSmile.&#010;个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;希望这对你有帮助。&#010;&#010;&#010;祝好。&#010;Roc Marshal&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt;&#010;&gt;Hi&#010;&gt;&#010;&gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。&#010;job 并行度120.提交作业的时候出现大量的No hostname could be resolved for the&#010;IP address，JM time out，作业提交失败。web ui也会卡主无响应。&#010;&gt;&#010;&gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt;&#010;&gt;&#010;&gt;部分日志如下：&#010;&gt;&#010;&gt;2020-07-15 16:58:46,460 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation&#010;    [] - No hostname could be resolved for the IP address 10.32.160.7, using IP address as&#010;host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&gt;2020-07-15 16:58:46,460 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation&#010;    [] - No hostname could be resolved for the IP address 10.44.224.7, using IP address as&#010;host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&gt;2020-07-15 16:58:46,461 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation&#010;    [] - No hostname could be resolved for the IP address 10.40.32.9, using IP address as&#010;host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&gt;&#010;&gt;2020-07-15 16:59:10,236 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] - The heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed out.&#010;&gt;2020-07-15 16:59:10,236 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2&#010;for job e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt;&#010;&gt;&#010;&gt;how to deal with ？&#010;&gt;&#010;&gt;&#010;&gt;beset ！&#010;&gt;&#010;&gt;| |&#010;&gt;a511955993&#010;&gt;|&#010;&gt;|&#010;&gt;邮箱：a511955993@163.com&#010;&gt;|&#010;&gt;&#010;&gt;签名由 网易邮箱大师 定制&#010;",
        "depth": "2",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAA8tFvv4Djru6UjdRh8vKcezYfNCTi0U+ROXh+uKyWoodR68ug@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 05:17:33 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "Hi&#013;&#010;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了&#010;HA 也可以看一下 zk 的日志。之前遇到过一次在 Yarn&#013;&#010;环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk 日志发现的原因。&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#013;&#010;&#013;&#010;&gt; Hi Roc&#013;&#010;&gt;&#013;&#010;&gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; On 07/15/2020 17:16, Roc Marshal wrote:&#013;&#010;&gt; Hi，SmileSmile.&#013;&#010;&gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#013;&#010;&gt; 希望这对你有帮助。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 祝好。&#013;&#010;&gt; Roc Marshal&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Hi&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#013;&#010;&gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved for the&#010;IP address，JM time&#013;&#010;&gt; out，作业提交失败。web ui也会卡主无响应。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;部分日志如下：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; hostname could be resolved for the IP address 10.32.160.7, using IP address&#013;&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#013;&#010;&gt; impacted.&#013;&#010;&gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; hostname could be resolved for the IP address 10.44.224.7, using IP address&#013;&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#013;&#010;&gt; impacted.&#013;&#010;&gt; &gt;2020-07-15 16:58:46,461 WARN&#013;&#010;&gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; hostname could be resolved for the IP address 10.40.32.9, using IP address&#013;&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#013;&#010;&gt; impacted.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - The&#013;&#010;&gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed out.&#013;&#010;&gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#013;&#010;&gt; Disconnect job manager 00000000000000000000000000000000&#013;&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#013;&#010;&gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;how to deal with ？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;beset ！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;| |&#013;&#010;&gt; &gt;a511955993&#013;&#010;&gt; &gt;|&#013;&#010;&gt; &gt;|&#013;&#010;&gt; &gt;邮箱：a511955993@163.com&#013;&#010;&gt; &gt;|&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<1a831ae3.a2fa.173703d04fa.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 07:18:33 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "Hi，Congxian&#010;&#010;因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no&#010;hostname could be resolved，jm失联，作业提交失败。 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&#010;在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&#010;&#010;是否有其他排查思路？&#010;&#010;Best！&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/16/2020 13:17, Congxian Qiu wrote:&#010;Hi&#010;  如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了&#010;HA 也可以看一下 zk 的日志。之前遇到过一次在 Yarn&#010;环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk 日志发现的原因。&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&#010;&gt; Hi Roc&#010;&gt;&#010;&gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; On 07/15/2020 17:16, Roc Marshal wrote:&#010;&gt; Hi，SmileSmile.&#010;&gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;&gt; 希望这对你有帮助。&#010;&gt;&#010;&gt;&#010;&gt; 祝好。&#010;&gt; Roc Marshal&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt; &gt;&#010;&gt; &gt;Hi&#010;&gt; &gt;&#010;&gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#010;&gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved for the&#010;IP address，JM time&#010;&gt; out，作业提交失败。web ui也会卡主无响应。&#010;&gt; &gt;&#010;&gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;部分日志如下：&#010;&gt; &gt;&#010;&gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.32.160.7, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.44.224.7, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; &gt;2020-07-15 16:58:46,461 WARN&#010;&gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.40.32.9, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; &gt;&#010;&gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - The&#010;&gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed out.&#010;&gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;how to deal with ？&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;beset ！&#010;&gt; &gt;&#010;&gt; &gt;| |&#010;&gt; &gt;a511955993&#010;&gt; &gt;|&#010;&gt; &gt;|&#010;&gt; &gt;邮箱：a511955993@163.com&#010;&gt; &gt;|&#010;&gt; &gt;&#010;&gt; &gt;签名由 网易邮箱大师 定制&#010;&gt;&#010;",
        "depth": "4",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAA8tFvu8pRy5n6cC5kGptdd-Akb-K3kLFko2eQ=BYCpoueLAxQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 11:19:35 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "Hi&#013;&#010;   不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个&#010;pod&#013;&#010;的完整日志有没有什么发现&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#013;&#010;&#013;&#010;&gt; Hi，Congxian&#013;&#010;&gt;&#013;&#010;&gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no&#010;hostname could be&#013;&#010;&gt; resolved，jm失联，作业提交失败。&#013;&#010;&gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#013;&#010;&gt;&#013;&#010;&gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 是否有其他排查思路？&#013;&#010;&gt;&#013;&#010;&gt; Best！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; | |&#013;&#010;&gt; a511955993&#013;&#010;&gt; |&#013;&#010;&gt; |&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt; |&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt;&#013;&#010;&gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#013;&#010;&gt; Hi&#013;&#010;&gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了&#010;HA 也可以看一下 zk 的日志。之前遇到过一次在 Yarn&#013;&#010;&gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk&#010;日志发现的原因。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Roc&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#013;&#010;&gt; &gt; Hi，SmileSmile.&#013;&#010;&gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#013;&#010;&gt; &gt; 希望这对你有帮助。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 祝好。&#013;&#010;&gt; &gt; Roc Marshal&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;Hi&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#013;&#010;&gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved for&#010;the IP address，JM&#013;&#010;&gt; time&#013;&#010;&gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;部分日志如下：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using IP&#013;&#010;&gt; address&#013;&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#013;&#010;&gt; be&#013;&#010;&gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using IP&#013;&#010;&gt; address&#013;&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#013;&#010;&gt; be&#013;&#010;&gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#013;&#010;&gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using IP&#013;&#010;&gt; address&#013;&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#013;&#010;&gt; be&#013;&#010;&gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#013;&#010;&gt; The&#013;&#010;&gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed&#013;&#010;&gt; out.&#013;&#010;&gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#013;&#010;&gt; &gt; Disconnect job manager 00000000000000000000000000000000&#013;&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#013;&#010;&gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;how to deal with ？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;beset ！&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;| |&#013;&#010;&gt; &gt; &gt;a511955993&#013;&#010;&gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt;邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "5",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf35502upmj5-8DsTP-t7id4Mpq1CfHU=62tadgAc5R2-YQ@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 10:18:47 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#013;&#010;有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#013;&#010;可能是coredns有问题&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#013;&#010;&#013;&#010;&gt; Hi&#013;&#010;&gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个&#010;pod&#013;&#010;&gt; 的完整日志有没有什么发现&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi，Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no&#010;hostname could be&#013;&#010;&gt; &gt; resolved，jm失联，作业提交失败。&#013;&#010;&gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 是否有其他排查思路？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了&#010;HA 也可以看一下 zk 的日志。之前遇到过一次在&#013;&#010;&gt; Yarn&#013;&#010;&gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及&#010;zk 日志发现的原因。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi Roc&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; | |&#013;&#010;&gt; &gt; &gt; a511955993&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#013;&#010;&gt; &gt; &gt; Hi，SmileSmile.&#013;&#010;&gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#013;&#010;&gt; &gt; &gt; 希望这对你有帮助。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 祝好。&#013;&#010;&gt; &gt; &gt; Roc Marshal&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;Hi&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#013;&#010;&gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved&#010;for the IP address，JM&#013;&#010;&gt; &gt; time&#013;&#010;&gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;部分日志如下：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using IP&#013;&#010;&gt; &gt; address&#013;&#010;&gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#013;&#010;&gt; &gt; be&#013;&#010;&gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using IP&#013;&#010;&gt; &gt; address&#013;&#010;&gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#013;&#010;&gt; &gt; be&#013;&#010;&gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#013;&#010;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using IP&#013;&#010;&gt; &gt; address&#013;&#010;&gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#013;&#010;&gt; &gt; be&#013;&#010;&gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#013;&#010;&gt; &gt; The&#013;&#010;&gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed&#013;&#010;&gt; &gt; out.&#013;&#010;&gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#013;&#010;&gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#013;&#010;&gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#013;&#010;&gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;how to deal with ？&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;beset ！&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;| |&#013;&#010;&gt; &gt; &gt; &gt;a511955993&#013;&#010;&gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<c5580da.444e.1737661e48f.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 11:56:33 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "&#010;Hi，Yang Wang！&#010;&#010;很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&#010;在JM报错的地方，No hostname could be resolved for ip address xxxxx ，报出来的ip是k8s分配给flink&#010;pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#010;&#010;Best！&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/22/2020 18:18, Yang Wang wrote:&#010;如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#010;有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;可能是coredns有问题&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#010;&#010;&gt; Hi&#010;&gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个&#010;pod&#010;&gt; 的完整日志有没有什么发现&#010;&gt; Best,&#010;&gt; Congxian&#010;&gt;&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#010;&gt;&#010;&gt; &gt; Hi，Congxian&#010;&gt; &gt;&#010;&gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no&#010;hostname could be&#010;&gt; &gt; resolved，jm失联，作业提交失败。&#010;&gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&gt; &gt;&#010;&gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 是否有其他排查思路？&#010;&gt; &gt;&#010;&gt; &gt; Best！&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; | |&#010;&gt; &gt; a511955993&#010;&gt; &gt; |&#010;&gt; &gt; |&#010;&gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; |&#010;&gt; &gt;&#010;&gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt;&#010;&gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#010;&gt; &gt; Hi&#010;&gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了&#010;HA 也可以看一下 zk 的日志。之前遇到过一次在&#010;&gt; Yarn&#010;&gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及&#010;zk 日志发现的原因。&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Congxian&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; Hi Roc&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; | |&#010;&gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#010;&gt; &gt; &gt; Hi，SmileSmile.&#010;&gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;&gt; &gt; &gt; 希望这对你有帮助。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 祝好。&#010;&gt; &gt; &gt; Roc Marshal&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;Hi&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#010;&gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved&#010;for the IP address，JM&#010;&gt; &gt; time&#010;&gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;部分日志如下：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using IP&#010;&gt; &gt; address&#010;&gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; &gt; be&#010;&gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using IP&#010;&gt; &gt; address&#010;&gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; &gt; be&#010;&gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#010;&gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using IP&#010;&gt; &gt; address&#010;&gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; &gt; be&#010;&gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; The&#010;&gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a timed&#010;&gt; &gt; out.&#010;&gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;how to deal with ？&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;beset ！&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt;a511955993&#010;&gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;",
        "depth": "7",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf35yP1PVGBbeFV4s4_qx0zjhgaz5fQr-A8HSDASMR3XECA@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 02:11:08 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#013;&#010;在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#013;&#010;问题了&#013;&#010;&#013;&#010;kubectl run -i -t busybox --image=busybox --restart=Never&#013;&#010;&#013;&#010;你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; Hi，Yang Wang！&#013;&#010;&gt;&#013;&#010;&gt; 很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#013;&#010;&gt;&#013;&#010;&gt; 在JM报错的地方，No hostname could be resolved for ip address xxxxx&#013;&#010;&gt; ，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#013;&#010;&gt;&#013;&#010;&gt; Best！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; a511955993&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt;&#013;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&gt;&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt;&#010;定制&#013;&#010;&gt;&#013;&#010;&gt; On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#013;&#010;&gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#013;&#010;&gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#013;&#010;&gt; 可能是coredns有问题&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi&#013;&#010;&gt; &gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM&#010;日志一样，如果有的话，可以尝试看一下这个 pod&#013;&#010;&gt; &gt; 的完整日志有没有什么发现&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Congxian&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi，Congxian&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no&#010;hostname could be&#013;&#010;&gt; &gt; &gt; resolved，jm失联，作业提交失败。&#013;&#010;&gt; &gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 是否有其他排查思路？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best！&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; | |&#013;&#010;&gt; &gt; &gt; a511955993&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#013;&#010;&gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod&#010;的相关日志，如果开启了 HA 也可以看一下 zk 的日志。之前遇到过一次在&#013;&#010;&gt; &gt; Yarn&#013;&#010;&gt; &gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及&#010;zk 日志发现的原因。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hi Roc&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; | |&#013;&#010;&gt; &gt; &gt; &gt; a511955993&#013;&#010;&gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#013;&#010;&gt; &gt; &gt; &gt; Hi，SmileSmile.&#013;&#010;&gt; &gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#013;&#010;&gt; &gt; &gt; &gt; 希望这对你有帮助。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 祝好。&#013;&#010;&gt; &gt; &gt; &gt; Roc Marshal&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;Hi&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#013;&#010;&gt; &gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be&#010;resolved for the IP&#013;&#010;&gt; address，JM&#013;&#010;&gt; &gt; &gt; time&#013;&#010;&gt; &gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;部分日志如下：&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using IP&#013;&#010;&gt; &gt; &gt; address&#013;&#010;&gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files)&#013;&#010;&gt; may&#013;&#010;&gt; &gt; &gt; be&#013;&#010;&gt; &gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using IP&#013;&#010;&gt; &gt; &gt; address&#013;&#010;&gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files)&#013;&#010;&gt; may&#013;&#010;&gt; &gt; &gt; be&#013;&#010;&gt; &gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#013;&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#013;&#010;&gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using IP&#013;&#010;&gt; &gt; &gt; address&#013;&#010;&gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files)&#013;&#010;&gt; may&#013;&#010;&gt; &gt; &gt; be&#013;&#010;&gt; &gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#013;&#010;&gt; [] -&#013;&#010;&gt; &gt; &gt; The&#013;&#010;&gt; &gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a&#013;&#010;&gt; timed&#013;&#010;&gt; &gt; &gt; out.&#013;&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#013;&#010;&gt; [] -&#013;&#010;&gt; &gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#013;&#010;&gt; &gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for&#013;&#010;&gt; job&#013;&#010;&gt; &gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;how to deal with ？&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;beset ！&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;| |&#013;&#010;&gt; &gt; &gt; &gt; &gt;a511955993&#013;&#010;&gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "8",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<6c755266.67e3.17379b883df.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 03:30:02 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "&#010;Hi Yang Wang&#010;&#010;刚刚在测试环境测试了一下，taskManager没有办法nslookup出来，JM可以nslookup，这两者的差别在于是否有service。&#010;&#010;解决方案：我这边给集群加上了taskmanager-query-state-service.yaml（按照官网上是可选服务）。就不会刷No&#010;hostname could be resolved for ip address，将NodePort改为ClusterIp，作业就可以成功提交，不会出现time&#010;out的问题了，问题得到了解决。&#010;&#010;&#010;1. 如果按照上面的情况，那么这个配置文件是必须配置的？&#010;&#010;2. 在1.11的更新中，发现有 [Flink-15911][Flink-15154] 支持分别配置用于本地监听绑定的网络接口和外部访问的地址和端口。是否是这块的改动，&#010;需要JM去通过TM上报的ip反向解析出service？&#010;&#010;&#010;Bset！&#010;&#010;&#010;[1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/23/2020 10:11, Yang Wang wrote:&#010;我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#010;在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#010;问题了&#010;&#010;kubectl run -i -t busybox --image=busybox --restart=Never&#010;&#010;你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#010;&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#010;&#010;&gt;&#010;&gt; Hi，Yang Wang！&#010;&gt;&#010;&gt; 很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&gt;&#010;&gt; 在JM报错的地方，No hostname could be resolved for ip address xxxxx&#010;&gt; ，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#010;&gt;&#010;&gt; Best！&#010;&gt;&#010;&gt;&#010;&gt; a511955993&#010;&gt; 邮箱：a511955993@163.com&#010;&gt;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt;&#010;定制&#010;&gt;&#010;&gt; On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP address，应该是集群的coredns&#010;&gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;&gt; 可能是coredns有问题&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#010;&gt;&#010;&gt; &gt; Hi&#010;&gt; &gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM&#010;日志一样，如果有的话，可以尝试看一下这个 pod&#010;&gt; &gt; 的完整日志有没有什么发现&#010;&gt; &gt; Best,&#010;&gt; &gt; Congxian&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; Hi，Congxian&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no&#010;hostname could be&#010;&gt; &gt; &gt; resolved，jm失联，作业提交失败。&#010;&gt; &gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 是否有其他排查思路？&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best！&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; | |&#010;&gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; |&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#010;&gt; &gt; &gt; Hi&#010;&gt; &gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod&#010;的相关日志，如果开启了 HA 也可以看一下 zk 的日志。之前遇到过一次在&#010;&gt; &gt; Yarn&#010;&gt; &gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及&#010;zk 日志发现的原因。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Hi Roc&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#010;&gt; &gt; &gt; &gt; Hi，SmileSmile.&#010;&gt; &gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;&gt; &gt; &gt; &gt; 希望这对你有帮助。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 祝好。&#010;&gt; &gt; &gt; &gt; Roc Marshal&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;Hi&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#010;&gt; &gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be&#010;resolved for the IP&#010;&gt; address，JM&#010;&gt; &gt; &gt; time&#010;&gt; &gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;部分日志如下：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using IP&#010;&gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files)&#010;&gt; may&#010;&gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using IP&#010;&gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files)&#010;&gt; may&#010;&gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using IP&#010;&gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS files)&#010;&gt; may&#010;&gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; [] -&#010;&gt; &gt; &gt; The&#010;&gt; &gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a&#010;&gt; timed&#010;&gt; &gt; &gt; out.&#010;&gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; [] -&#010;&gt; &gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for&#010;&gt; job&#010;&gt; &gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;how to deal with ？&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;beset ！&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt; &gt;a511955993&#010;&gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;",
        "depth": "9",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf34rXDdrLAuL1pbF-Wqijqe=PbBt0XFRO9AXQ83a0FXLGg@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 05:26:47 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "很高兴你的问题解决了，但我觉得根本原因应该不是加上了taskmanager-query-state-service.yaml的关系。&#013;&#010;我这边不创建这个服务也是正常的，而且nslookup {tm_ip_address}是可以正常反解析到hostname的。&#013;&#010;&#013;&#010;注意这里不是解析hostname，而是通过ip地址来反解析进行验证&#013;&#010;&#013;&#010;&#013;&#010;回答你说的两个问题：&#013;&#010;1. 不是必须的，我这边验证不需要创建，集群也是可以正常运行任务的。Rest&#013;&#010;service的暴露方式是ClusterIP、NodePort、LoadBalancer都正常&#013;&#010;2. 如果没有配置taskmanager.bind-host，&#013;&#010;[Flink-15911][Flink-15154]这两个JIRA并不会影响TM向RM注册时候的使用的地址&#013;&#010;&#013;&#010;如果你想找到根本原因，那可能需要你这边提供JM/TM的完整log，这样方便分析&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 上午11:30写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; Hi Yang Wang&#013;&#010;&gt;&#013;&#010;&gt; 刚刚在测试环境测试了一下，taskManager没有办法nslookup出来，JM可以nslookup，这两者的差别在于是否有service。&#013;&#010;&gt;&#013;&#010;&gt; 解决方案：我这边给集群加上了taskmanager-query-state-service.yaml（按照官网上是可选服务）。就不会刷No&#013;&#010;&gt; hostname could be resolved for ip&#013;&#010;&gt; address，将NodePort改为ClusterIp，作业就可以成功提交，不会出现time&#010;out的问题了，问题得到了解决。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 1. 如果按照上面的情况，那么这个配置文件是必须配置的？&#013;&#010;&gt;&#013;&#010;&gt; 2. 在1.11的更新中，发现有 [Flink-15911][Flink-15154]&#013;&#010;&gt; 支持分别配置用于本地监听绑定的网络接口和外部访问的地址和端口。是否是这块的改动，&#013;&#010;&gt; 需要JM去通过TM上报的ip反向解析出service？&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Bset！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html&#013;&#010;&gt;&#013;&#010;&gt; a511955993&#013;&#010;&gt; 邮箱：a511955993@163.com&#013;&#010;&gt;&#013;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&gt;&#013;&#010;&gt;&#013;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt;&#010;定制&#013;&#010;&gt;&#013;&#010;&gt; On 07/23/2020 10:11, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#013;&#010;&gt; 我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#013;&#010;&gt; 在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#013;&#010;&gt; 问题了&#013;&#010;&gt;&#013;&#010;&gt; kubectl run -i -t busybox --image=busybox --restart=Never&#013;&#010;&gt;&#013;&#010;&gt; 你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hi，Yang Wang！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在JM报错的地方，No hostname could be resolved for ip address xxxxx&#013;&#010;&gt; &gt; ，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; a511955993&#013;&#010;&gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &lt;&#013;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt;&#010;定制&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#013;&#010;&gt; &gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP&#013;&#010;&gt; address，应该是集群的coredns&#013;&#010;&gt; &gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#013;&#010;&gt; &gt; 可能是coredns有问题&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Yang&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的&#010;NM 日志一样，如果有的话，可以尝试看一下这个 pod&#013;&#010;&gt; &gt; &gt; 的完整日志有没有什么发现&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Hi，Congxian&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no&#010;hostname could be&#013;&#010;&gt; &gt; &gt; &gt; resolved，jm失联，作业提交失败。&#013;&#010;&gt; &gt; &gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 是否有其他排查思路？&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Best！&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; | |&#013;&#010;&gt; &gt; &gt; &gt; a511955993&#013;&#010;&gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#013;&#010;&gt; &gt; &gt; &gt; Hi&#013;&#010;&gt; &gt; &gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下&#010;pod 的相关日志，如果开启了 HA 也可以看一下 zk&#013;&#010;&gt; 的日志。之前遇到过一次在&#013;&#010;&gt; &gt; &gt; Yarn&#013;&#010;&gt; &gt; &gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM&#010;日志以及 zk 日志发现的原因。&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; &gt; Congxian&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; Hi Roc&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; | |&#013;&#010;&gt; &gt; &gt; &gt; &gt; a511955993&#013;&#010;&gt; &gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; &gt; &gt; |&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#013;&#010;&gt; &gt; &gt; &gt; &gt; Hi，SmileSmile.&#013;&#010;&gt; &gt; &gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#013;&#010;&gt; &gt; &gt; &gt; &gt; 希望这对你有帮助。&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 祝好。&#013;&#010;&gt; &gt; &gt; &gt; &gt; Roc Marshal&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt;&#010;写道：&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;Hi&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM&#010;4个slot。 job&#013;&#010;&gt; &gt; &gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could&#010;be resolved for the IP&#013;&#010;&gt; &gt; address，JM&#013;&#010;&gt; &gt; &gt; &gt; time&#013;&#010;&gt; &gt; &gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;部分日志如下：&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] -&#013;&#010;&gt; No&#013;&#010;&gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using&#013;&#010;&gt; IP&#013;&#010;&gt; &gt; &gt; &gt; address&#013;&#010;&gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#013;&#010;&gt; files)&#013;&#010;&gt; &gt; may&#013;&#010;&gt; &gt; &gt; &gt; be&#013;&#010;&gt; &gt; &gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#013;&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] -&#013;&#010;&gt; No&#013;&#010;&gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using&#013;&#010;&gt; IP&#013;&#010;&gt; &gt; &gt; &gt; address&#013;&#010;&gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#013;&#010;&gt; files)&#013;&#010;&gt; &gt; may&#013;&#010;&gt; &gt; &gt; &gt; be&#013;&#010;&gt; &gt; &gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#013;&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] -&#013;&#010;&gt; No&#013;&#010;&gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using IP&#013;&#010;&gt; &gt; &gt; &gt; address&#013;&#010;&gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#013;&#010;&gt; files)&#013;&#010;&gt; &gt; may&#013;&#010;&gt; &gt; &gt; &gt; be&#013;&#010;&gt; &gt; &gt; &gt; &gt; impacted.&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#013;&#010;&gt; &gt; [] -&#013;&#010;&gt; &gt; &gt; &gt; The&#013;&#010;&gt; &gt; &gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a&#013;&#010;&gt; &gt; timed&#013;&#010;&gt; &gt; &gt; &gt; out.&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#013;&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#013;&#010;&gt; &gt; [] -&#013;&#010;&gt; &gt; &gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#013;&#010;&gt; &gt; &gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for&#013;&#010;&gt; &gt; job&#013;&#010;&gt; &gt; &gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;how to deal with ？&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;beset ！&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;| |&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;a511955993&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#013;&#010;&gt; &gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "10",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<663a304.764f.1737a68a4b3.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 06:42:25 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "Hi Yang Wang&#010;&#010;先分享下我这边的环境版本&#010;&#010;&#010;kubernetes：1.17.4.   CNI： weave  &#010;&#010;&#010;1 2 3 是我的一些疑惑&#010;&#010;4 是JM日志&#010;&#010;&#010;1. 去掉taskmanager-query-state-service.yaml后确实不行  nslookup&#010;&#010;kubectl exec -it busybox2 -- /bin/sh&#010;/ # nslookup 10.47.96.2&#010;Server:          10.96.0.10&#010;Address:     10.96.0.10:53&#010;&#010;** server can't find 2.96.47.10.in-addr.arpa: NXDOMAIN&#010;&#010;&#010;&#010;2. Flink1.11和Flink1.10&#010;&#010;detail subtasks taskmanagers xxx x 这行  1.11变成了172-20-0-50。1.10是flink-taskmanager-7b5d6958b6-sfzlk:36459。这块的改动是？（目前这个集群跑着1.10和1.11,1.10可以正常运行，如果coredns有问题，1.10版本的flink应该也有一样的情况吧？）&#010;&#010;3. coredns是否特殊配置？&#010;&#010;在容器中解析域名是正常的，只是反向解析没有service才会有问题。coredns是否有什么需要配置？&#010;&#010;&#010;4. time out时候的JM日志如下：&#010;&#010;&#010;&#010;2020-07-23 13:53:00,228 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - ResourceManager akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_0 was granted leadership with fencing token 00000000000000000000000000000000&#010;2020-07-23 13:53:00,232 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka://flink/user/rpc/dispatcher_1 .&#010;2020-07-23 13:53:00,233 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Starting the SlotManager.&#010;2020-07-23 13:53:03,472 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 1f9ae0cd95a28943a73be26323588696 (akka.tcp://flink@10.34.128.9:6122/user/rpc/taskmanager_0) at ResourceManager&#010;2020-07-23 13:53:03,777 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID cac09e751264e61615329c20713a84b4 (akka.tcp://flink@10.32.160.6:6122/user/rpc/taskmanager_0) at ResourceManager&#010;2020-07-23 13:53:03,787 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 93c72d01d09f9ae427c5fc980ed4c1e4 (akka.tcp://flink@10.39.0.8:6122/user/rpc/taskmanager_0) at ResourceManager&#010;2020-07-23 13:53:04,044 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 8adf2f8e81b77a16d5418a9e252c61e2 (akka.tcp://flink@10.38.64.7:6122/user/rpc/taskmanager_0) at ResourceManager&#010;2020-07-23 13:53:04,099 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 23e9d2358f6eb76b9ae718d879d4f330 (akka.tcp://flink@10.42.160.6:6122/user/rpc/taskmanager_0) at ResourceManager&#010;2020-07-23 13:53:04,146 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID 092f8dee299e32df13db3111662b61f8 (akka.tcp://flink@10.33.192.14:6122/user/rpc/taskmanager_0) at ResourceManager&#010;&#010;&#010;2020-07-23 13:55:44,220 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;2020-07-23 13:55:44,222 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;2020-07-23 13:55:44,251 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka://flink/user/rpc/jobmanager_2 .&#010;2020-07-23 13:55:44,260 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job JobTest (99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:44,278 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:44,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job JobTest (99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:44,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 0 ms.&#010;2020-07-23 13:55:44,428 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 25 ms&#010;2020-07-23 13:55:44,437 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Loading state backend via factory org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory&#010;2020-07-23 13:55:44,456 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using predefined options: DEFAULT.&#010;2020-07-23 13:55:44,457 INFO  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using default options factory: DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;2020-07-23 13:55:44,466 WARN  org.apache.flink.runtime.util.HadoopUtils                    [] - Could not find Hadoop configuration via any of the supported methods (Flink configuration, environment variables).&#010;2020-07-23 13:55:45,276 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@72bd8533 for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:45,280 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] - JobManager runner for job JobTest (99a030d0e3f428490a501c0132f27a56) was granted leadership with session id 00000000-0000-0000-0000-000000000000 at akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2.&#010;2020-07-23 13:55:45,286 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;&#010;&#010;&#010;2020-07-23 13:55:45,436 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}]&#010;2020-07-23 13:55:45,436 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}]&#010;2020-07-23 13:55:45,436 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}]&#010;2020-07-23 13:55:45,437 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{e559485ea7b0b7e17367816882538d90}]&#010;2020-07-23 13:55:45,437 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{7be8f6c1aedb27b04e7feae68078685c}]&#010;2020-07-23 13:55:45,437 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{582a86197884206652dff3aea2306bb3}]&#010;2020-07-23 13:55:45,437 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{0cc24260eda3af299a0b321feefaf2cb}]&#010;2020-07-23 13:55:45,437 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{240ca6f3d3b5ece6a98243ec8cadf616}]&#010;2020-07-23 13:55:45,438 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{c35033d598a517acc108424bb9f809fb}]&#010;2020-07-23 13:55:45,438 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{ad35013c3b532d4b4df1be62395ae0cf}]&#010;2020-07-23 13:55:45,438 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId{c929bd5e8daf432d01fad1ece3daec1a}]&#010;2020-07-23 13:55:45,487 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;2020-07-23 13:55:45,492 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration&#010;2020-07-23 13:55:45,493 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 13:55:45,499 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 13:55:45,501 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.&#010;2020-07-23 13:55:45,501 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,502 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Request slot with profile ResourceProfile{UNKNOWN} for job 99a030d0e3f428490a501c0132f27a56 with allocation id d420d08bf2654d9ea76955c70db18b69.&#010;2020-07-23 13:55:45,502 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{e7e422409acebdb385014a9634af6a90}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{14ac08438e79c8db8d25d93b99d62725}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;&#010;2020-07-23 13:55:45,514 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Request slot with profile ResourceProfile{UNKNOWN} for job 99a030d0e3f428490a501c0132f27a56 with allocation id fce526bbe3e1be91caa3e4b536b20e35.&#010;2020-07-23 13:55:45,514 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{40c7abbb12514c405323b0569fb21647}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,514 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{a4985a9647b65b30a571258b45c8f2ce}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,515 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{c52a6eb2fa58050e71e7903590019fd1}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;&#010;2020-07-23 13:55:45,517 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Request slot with profile ResourceProfile{UNKNOWN} for job 99a030d0e3f428490a501c0132f27a56 with allocation id 18ac7ec802ebfcfed8c05ee9324a55a4.&#010;&#010;2020-07-23 13:55:45,518 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Request slot with profile ResourceProfile{UNKNOWN} for job 99a030d0e3f428490a501c0132f27a56 with allocation id 7ec76cbe689eb418b63599e90ade19be.&#010;2020-07-23 13:55:45,518 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{46d65692a8b5aad11b51f9a74a666a74}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{3670bb4f345eedf941cc18e477ba1e9d}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{4a12467d76b9e3df8bc3412c0be08e14}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,519 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId{e559485ea7b0b7e17367816882538d90}] and profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,519 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Request slot with profile ResourceProfile{UNKNOWN} for job 99a030d0e3f428490a501c0132f27a56 with allocation id b78837a29b4032924ac25be70ed21a3c.&#010;&#010;&#010;2020-07-23 13:58:18,037 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.47.96.2, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:22,192 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.34.64.14, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:22,358 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.34.128.9, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:24,562 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.32.160.6, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:25,487 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.38.64.7, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:27,636 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.42.160.6, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:27,767 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.43.64.12, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:29,651 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - The heartbeat of JobManager with id 456a18b6c404cb11a359718e16de1c6b timed out.&#010;2020-07-23 13:58:29,651 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;2020-07-23 13:58:29,854 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.39.0.8, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:33,623 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.35.0.10, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:35,756 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.36.32.8, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;2020-07-23 13:58:36,694 WARN  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No hostname could be resolved for the IP address 10.42.128.6, using IP address as host name. Local input split assignment (such as for HDFS files) may be impacted.&#010;&#010;&#010;2020-07-23 14:01:17,814 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 83b1ff14900abfd54418e7fa3efb3f8a: The heartbeat of JobManager with id 456a18b6c404cb11a359718e16de1c6b timed out..&#010;2020-07-23 14:01:17,815 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;2020-07-23 14:01:17,816 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration&#010;2020-07-23 14:01:17,816 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:17,836 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source: host_relation -&gt; Timestamps/Watermarks -&gt; Map (1/1) (302ca9640e2d209a543d843f2996ccd2) switched from SCHEDULED to FAILED on not deployed.&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;     at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_242]&#010;     ... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;     ... 23 more&#010;2020-07-23 14:01:17,848 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - Calculating tasks to restart to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;2020-07-23 14:01:17,910 INFO  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy [] - 902 tasks should be restarted to recover the failed task cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;2020-07-23 14:01:17,913 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job JobTest (99a030d0e3f428490a501c0132f27a56) switched from state RUNNING to FAILING.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy&#010;     at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     ... 45 more&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;     at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_242]&#010;     ... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;     ... 23 more&#010;&#010;&#010;&#010;2020-07-23 14:01:18,109 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 1809eb912d69854f2babedeaf879df6a.&#010;2020-07-23 14:01:18,110 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job JobTest (99a030d0e3f428490a501c0132f27a56) switched from state FAILING to FAILED.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy&#010;     at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_242]&#010;     at org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.Actor$class.aroundReceive(Actor.scala:517) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.actor.ActorCell.invoke(ActorCell.scala:561) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.run(Mailbox.scala:225) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.Mailbox.exec(Mailbox.scala:235) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Could not allocate the required slot within slot request timeout. Please make sure that the cluster has enough resources.&#010;     at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441) ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;     ... 45 more&#010;Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException&#010;     at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_242]&#010;     at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_242]&#010;     ... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;     ... 23 more&#010;2020-07-23 14:01:18,114 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping checkpoint coordinator for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,117 INFO  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore [] - Shutting down&#010;2020-07-23 14:01:18,118 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Discarding the results produced by task execution 302ca9640e2d209a543d843f2996ccd2.&#010;2020-07-23 14:01:18,120 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] timed out.&#010;2020-07-23 14:01:18,120 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] timed out.&#010;2020-07-23 14:01:18,120 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{e7e422409acebdb385014a9634af6a90}] timed out.&#010;2020-07-23 14:01:18,121 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] timed out.&#010;2020-07-23 14:01:18,121 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] timed out.&#010;2020-07-23 14:01:18,121 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] timed out.&#010;2020-07-23 14:01:18,122 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] timed out.&#010;2020-07-23 14:01:18,122 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending slot request [&#010;&#010;&#010;2020-07-23 14:01:18,151 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,157 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,157 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,157 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job 99a030d0e3f428490a501c0132f27a56 reached globally terminal state FAILED.&#010;2020-07-23 14:01:18,162 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,162 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 00000000000000000000000000000000.&#010;2020-07-23 14:01:18,225 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job JobTest(99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 14:01:18,381 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Suspending SlotPool.&#010;2020-07-23 14:01:18,382 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 83b1ff14900abfd54418e7fa3efb3f8a: JobManager is shutting down..&#010;2020-07-23 14:01:18,382 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping SlotPool.&#010;2020-07-23 14:01:18,382 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager 00000000000000000000000000000000@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/23/2020 13:26, Yang Wang wrote:&#010;很高兴你的问题解决了，但我觉得根本原因应该不是加上了taskmanager-query-state-service.yaml的关系。&#010;我这边不创建这个服务也是正常的，而且nslookup {tm_ip_address}是可以正常反解析到hostname的。&#010;&#010;注意这里不是解析hostname，而是通过ip地址来反解析进行验证&#010;&#010;&#010;回答你说的两个问题：&#010;1. 不是必须的，我这边验证不需要创建，集群也是可以正常运行任务的。Rest&#010;service的暴露方式是ClusterIP、NodePort、LoadBalancer都正常&#010;2. 如果没有配置taskmanager.bind-host，&#010;[Flink-15911][Flink-15154]这两个JIRA并不会影响TM向RM注册时候的使用的地址&#010;&#010;如果你想找到根本原因，那可能需要你这边提供JM/TM的完整log，这样方便分析&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 上午11:30写道：&#010;&#010;&gt;&#010;&gt; Hi Yang Wang&#010;&gt;&#010;&gt; 刚刚在测试环境测试了一下，taskManager没有办法nslookup出来，JM可以nslookup，这两者的差别在于是否有service。&#010;&gt;&#010;&gt; 解决方案：我这边给集群加上了taskmanager-query-state-service.yaml（按照官网上是可选服务）。就不会刷No&#010;&gt; hostname could be resolved for ip&#010;&gt; address，将NodePort改为ClusterIp，作业就可以成功提交，不会出现time out的问题了，问题得到了解决。&#010;&gt;&#010;&gt;&#010;&gt; 1. 如果按照上面的情况，那么这个配置文件是必须配置的？&#010;&gt;&#010;&gt; 2. 在1.11的更新中，发现有 [Flink-15911][Flink-15154]&#010;&gt; 支持分别配置用于本地监听绑定的网络接口和外部访问的地址和端口。是否是这块的改动，&#010;&gt; 需要JM去通过TM上报的ip反向解析出service？&#010;&gt;&#010;&gt;&#010;&gt; Bset！&#010;&gt;&#010;&gt;&#010;&gt; [1]&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html&#010;&gt;&#010;&gt; a511955993&#010;&gt; 邮箱：a511955993@163.com&#010;&gt;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt;&#010;&gt; On 07/23/2020 10:11, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; 我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#010;&gt; 在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#010;&gt; 问题了&#010;&gt;&#010;&gt; kubectl run -i -t busybox --image=busybox --restart=Never&#010;&gt;&#010;&gt; 你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; Hi，Yang Wang！&#010;&gt; &gt;&#010;&gt; &gt; 很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&gt; &gt;&#010;&gt; &gt; 在JM报错的地方，No hostname could be resolved for ip address xxxxx&#010;&gt; &gt; ，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#010;&gt; &gt;&#010;&gt; &gt; Best！&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; a511955993&#010;&gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt;&#010;&gt; &gt; &lt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt;&#010;&gt; &gt; On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP&#010;&gt; address，应该是集群的coredns&#010;&gt; &gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;&gt; &gt; 可能是coredns有问题&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Yang&#010;&gt; &gt;&#010;&gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; Hi&#010;&gt; &gt; &gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个 pod&#010;&gt; &gt; &gt; 的完整日志有没有什么发现&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Hi，Congxian&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no hostname could be&#010;&gt; &gt; &gt; &gt; resolved，jm失联，作业提交失败。&#010;&gt; &gt; &gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 是否有其他排查思路？&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Best！&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#010;&gt; &gt; &gt; &gt; Hi&#010;&gt; &gt; &gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了 HA 也可以看一下 zk&#010;&gt; 的日志。之前遇到过一次在&#010;&gt; &gt; &gt; Yarn&#010;&gt; &gt; &gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk 日志发现的原因。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Hi Roc&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#010;&gt; &gt; &gt; &gt; &gt; Hi，SmileSmile.&#010;&gt; &gt; &gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;&gt; &gt; &gt; &gt; &gt; 希望这对你有帮助。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 祝好。&#010;&gt; &gt; &gt; &gt; &gt; Roc Marshal&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;Hi&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。 job&#010;&gt; &gt; &gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved for the IP&#010;&gt; &gt; address，JM&#010;&gt; &gt; &gt; &gt; time&#010;&gt; &gt; &gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;部分日志如下：&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] -&#010;&gt; No&#010;&gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using&#010;&gt; IP&#010;&gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; files)&#010;&gt; &gt; may&#010;&gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] -&#010;&gt; No&#010;&gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using&#010;&gt; IP&#010;&gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; files)&#010;&gt; &gt; may&#010;&gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] -&#010;&gt; No&#010;&gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using IP&#010;&gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; files)&#010;&gt; &gt; may&#010;&gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; The&#010;&gt; &gt; &gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a&#010;&gt; &gt; timed&#010;&gt; &gt; &gt; &gt; out.&#010;&gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; &gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for&#010;&gt; &gt; job&#010;&gt; &gt; &gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;how to deal with ？&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;beset ！&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt; &gt; &gt;a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;",
        "depth": "11",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf36ZwU84340Q+Y2geMdbf-MWx=eYLbemx1WMdLUbtk3R+Q@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 03:36:04 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "看你这个任务，失败的根本原因并不是“No hostname could be resolved&#010;”，这个WARNING的原因可以单独讨论（如果在1.10里面不存在的话）。&#010;你可以本地起一个Standalone的集群，也会有这样的WARNING，并不影响正常使用&#010;&#010;&#010;失败的原因是slot 5分钟申请超时了，你给的日志里面2020-07-23 13:55:45,519到2020-07-23&#010;13:58:18,037是空白的，没有进行省略吧？&#010;这段时间按理应该是task开始deploy了。在日志里看到了JM-&gt;RM的心跳超时，同一个Pod里面的同一个进程通信也超时了&#010;所以怀疑JM一直在FullGC，这个需要你确认一下&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 下午2:43写道：&#010;&#010;&gt; Hi Yang Wang&#010;&gt;&#010;&gt; 先分享下我这边的环境版本&#010;&gt;&#010;&gt;&#010;&gt; kubernetes：1.17.4.   CNI： weave&#010;&gt;&#010;&gt;&#010;&gt; 1 2 3 是我的一些疑惑&#010;&gt;&#010;&gt; 4 是JM日志&#010;&gt;&#010;&gt;&#010;&gt; 1. 去掉taskmanager-query-state-service.yaml后确实不行  nslookup&#010;&gt;&#010;&gt; kubectl exec -it busybox2 -- /bin/sh&#010;&gt; / # nslookup 10.47.96.2&#010;&gt; Server:          10.96.0.10&#010;&gt; Address:     10.96.0.10:53&#010;&gt;&#010;&gt; ** server can't find 2.96.47.10.in-addr.arpa: NXDOMAIN&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2. Flink1.11和Flink1.10&#010;&gt;&#010;&gt; detail subtasks taskmanagers xxx x 这行&#010;&gt;  1.11变成了172-20-0-50。1.10是flink-taskmanager-7b5d6958b6-sfzlk:36459。这块的改动是？（目前这个集群跑着1.10和1.11,1.10可以正常运行，如果coredns有问题，1.10版本的flink应该也有一样的情况吧？）&#010;&gt;&#010;&gt; 3. coredns是否特殊配置？&#010;&gt;&#010;&gt; 在容器中解析域名是正常的，只是反向解析没有service才会有问题。coredns是否有什么需要配置？&#010;&gt;&#010;&gt;&#010;&gt; 4. time out时候的JM日志如下：&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:53:00,228 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; ResourceManager akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_0&#010;&gt; was granted leadership with fencing token 00000000000000000000000000000000&#010;&gt; 2020-07-23 13:53:00,232 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher&#010;&gt; at akka://flink/user/rpc/dispatcher_1 .&#010;&gt; 2020-07-23 13:53:00,233 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] -&#010;&gt; Starting the SlotManager.&#010;&gt; 2020-07-23 13:53:03,472 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 1f9ae0cd95a28943a73be26323588696&#010;&gt; (akka.tcp://flink@10.34.128.9:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:03,777 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID cac09e751264e61615329c20713a84b4&#010;&gt; (akka.tcp://flink@10.32.160.6:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:03,787 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 93c72d01d09f9ae427c5fc980ed4c1e4&#010;&gt; (akka.tcp://flink@10.39.0.8:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:04,044 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 8adf2f8e81b77a16d5418a9e252c61e2&#010;&gt; (akka.tcp://flink@10.38.64.7:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:04,099 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 23e9d2358f6eb76b9ae718d879d4f330&#010;&gt; (akka.tcp://flink@10.42.160.6:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:04,146 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 092f8dee299e32df13db3111662b61f8&#010;&gt; (akka.tcp://flink@10.33.192.14:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:55:44,220 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received&#010;&gt; JobGraph submission 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;&gt; 2020-07-23 13:55:44,222 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] -&#010;&gt; Submitting job 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;&gt; 2020-07-23 13:55:44,251 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at&#010;&gt; akka://flink/user/rpc/jobmanager_2 .&#010;&gt; 2020-07-23 13:55:44,260 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Initializing job JobTest&#010;&gt; (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:44,278 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Using restart back off time strategy&#010;&gt; NoRestartBackoffTimeStrategy for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:44,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Running initialization on master for job JobTest&#010;&gt; (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:44,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Successfully ran initialization on master in 0 ms.&#010;&gt; 2020-07-23 13:55:44,428 INFO&#010;&gt;  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] -&#010;&gt; Built 1 pipelined regions in 25 ms&#010;&gt; 2020-07-23 13:55:44,437 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Loading state backend via factory&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory&#010;&gt; 2020-07-23 13:55:44,456 INFO&#010;&gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; predefined options: DEFAULT.&#010;&gt; 2020-07-23 13:55:44,457 INFO&#010;&gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; default options factory:&#010;&gt; DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;&gt; 2020-07-23 13:55:44,466 WARN  org.apache.flink.runtime.util.HadoopUtils&#010;&gt;                  [] - Could not find Hadoop configuration via any of the&#010;&gt; supported methods (Flink configuration, environment variables).&#010;&gt; 2020-07-23 13:55:45,276 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Using failover strategy&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@72bd8533&#010;&gt; for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:45,280 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] -&#010;&gt; JobManager runner for job JobTest (99a030d0e3f428490a501c0132f27a56) was&#010;&gt; granted leadership with session id 00000000-0000-0000-0000-000000000000 at&#010;&gt; akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2.&#010;&gt; 2020-07-23 13:55:45,286 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Starting scheduling with scheduling strategy&#010;&gt; [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,436 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}]&#010;&gt; 2020-07-23 13:55:45,436 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}]&#010;&gt; 2020-07-23 13:55:45,436 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{e559485ea7b0b7e17367816882538d90}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{7be8f6c1aedb27b04e7feae68078685c}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{582a86197884206652dff3aea2306bb3}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{0cc24260eda3af299a0b321feefaf2cb}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{240ca6f3d3b5ece6a98243ec8cadf616}]&#010;&gt; 2020-07-23 13:55:45,438 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{c35033d598a517acc108424bb9f809fb}]&#010;&gt; 2020-07-23 13:55:45,438 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{ad35013c3b532d4b4df1be62395ae0cf}]&#010;&gt; 2020-07-23 13:55:45,438 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{c929bd5e8daf432d01fad1ece3daec1a}]&#010;&gt; 2020-07-23 13:55:45,487 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Connecting to ResourceManager&#010;&gt; akka.tcp://flink@flink-jobmanager&#010;&gt; :6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; 2020-07-23 13:55:45,492 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Resolved ResourceManager address, beginning&#010;&gt; registration&#010;&gt; 2020-07-23 13:55:45,493 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 13:55:45,499 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 13:55:45,501 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - JobManager successfully registered at ResourceManager,&#010;&gt; leader id: 00000000000000000000000000000000.&#010;&gt; 2020-07-23 13:55:45,501 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,502 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; d420d08bf2654d9ea76955c70db18b69.&#010;&gt; 2020-07-23 13:55:45,502 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{e7e422409acebdb385014a9634af6a90}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{14ac08438e79c8db8d25d93b99d62725}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,514 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; fce526bbe3e1be91caa3e4b536b20e35.&#010;&gt; 2020-07-23 13:55:45,514 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{40c7abbb12514c405323b0569fb21647}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,514 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{a4985a9647b65b30a571258b45c8f2ce}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,515 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{c52a6eb2fa58050e71e7903590019fd1}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,517 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; 18ac7ec802ebfcfed8c05ee9324a55a4.&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; 7ec76cbe689eb418b63599e90ade19be.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{46d65692a8b5aad11b51f9a74a666a74}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{3670bb4f345eedf941cc18e477ba1e9d}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{4a12467d76b9e3df8bc3412c0be08e14}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,519 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{e559485ea7b0b7e17367816882538d90}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,519 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; b78837a29b4032924ac25be70ed21a3c.&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:58:18,037 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.47.96.2, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:22,192 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.34.64.14, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:22,358 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.34.128.9, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:24,562 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.32.160.6, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:25,487 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.38.64.7, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:27,636 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.42.160.6, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:27,767 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.43.64.12, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:29,651 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; The heartbeat of JobManager with id 456a18b6c404cb11a359718e16de1c6b timed&#010;&gt; out.&#010;&gt; 2020-07-23 13:58:29,651 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&gt; 2020-07-23 13:58:29,854 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.39.0.8, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:33,623 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.35.0.10, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:35,756 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.36.32.8, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:36,694 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.42.128.6, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 14:01:17,814 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Close ResourceManager connection&#010;&gt; 83b1ff14900abfd54418e7fa3efb3f8a: The heartbeat of JobManager with id&#010;&gt; 456a18b6c404cb11a359718e16de1c6b timed out..&#010;&gt; 2020-07-23 14:01:17,815 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Connecting to ResourceManager&#010;&gt; akka.tcp://flink@flink-jobmanager&#010;&gt; :6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; 2020-07-23 14:01:17,816 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Resolved ResourceManager address, beginning&#010;&gt; registration&#010;&gt; 2020-07-23 14:01:17,816 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:17,836 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; host_relation -&gt; Timestamps/Watermarks -&gt; Map (1/1)&#010;&gt; (302ca9640e2d209a543d843f2996ccd2) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt;      ... 23 more&#010;&gt; 2020-07-23 14:01:17,848 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;&gt; 2020-07-23 14:01:17,910 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 902 tasks should be restarted to recover the failed task&#010;&gt; cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;&gt; 2020-07-23 14:01:17,913 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; JobTest (99a030d0e3f428490a501c0132f27a56) switched from state RUNNING to&#010;&gt; FAILING.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; NoRestartBackoffTimeStrategy&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt;      ... 23 more&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 14:01:18,109 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 1809eb912d69854f2babedeaf879df6a.&#010;&gt; 2020-07-23 14:01:18,110 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; JobTest (99a030d0e3f428490a501c0132f27a56) switched from state FAILING to&#010;&gt; FAILED.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; NoRestartBackoffTimeStrategy&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt;      ... 23 more&#010;&gt; 2020-07-23 14:01:18,114 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping&#010;&gt; checkpoint coordinator for job 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,117 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore []&#010;&gt; - Shutting down&#010;&gt; 2020-07-23 14:01:18,118 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 302ca9640e2d209a543d843f2996ccd2.&#010;&gt; 2020-07-23 14:01:18,120 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] timed out.&#010;&gt; 2020-07-23 14:01:18,120 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] timed out.&#010;&gt; 2020-07-23 14:01:18,120 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{e7e422409acebdb385014a9634af6a90}] timed out.&#010;&gt; 2020-07-23 14:01:18,121 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] timed out.&#010;&gt; 2020-07-23 14:01:18,121 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] timed out.&#010;&gt; 2020-07-23 14:01:18,121 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] timed out.&#010;&gt; 2020-07-23 14:01:18,122 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] timed out.&#010;&gt; 2020-07-23 14:01:18,122 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 14:01:18,151 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,157 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,157 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,157 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 reached globally terminal state FAILED.&#010;&gt; 2020-07-23 14:01:18,162 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,162 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - JobManager successfully registered at ResourceManager,&#010;&gt; leader id: 00000000000000000000000000000000.&#010;&gt; 2020-07-23 14:01:18,225 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Stopping the JobMaster for job&#010;&gt; JobTest(99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 14:01:18,381 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Suspending SlotPool.&#010;&gt; 2020-07-23 14:01:18,382 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Close ResourceManager connection&#010;&gt; 83b1ff14900abfd54418e7fa3efb3f8a: JobManager is shutting down..&#010;&gt; 2020-07-23 14:01:18,382 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping&#010;&gt; SlotPool.&#010;&gt; 2020-07-23 14:01:18,382 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&gt;&#010;&gt; a511955993&#010;&gt; 邮箱：a511955993@163.com&#010;&gt;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&gt;&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&gt; 定制&#010;&gt;&#010;&gt; On 07/23/2020 13:26, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; 很高兴你的问题解决了，但我觉得根本原因应该不是加上了taskmanager-query-state-service.yaml的关系。&#010;&gt; 我这边不创建这个服务也是正常的，而且nslookup {tm_ip_address}是可以正常反解析到hostname的。&#010;&gt;&#010;&gt; 注意这里不是解析hostname，而是通过ip地址来反解析进行验证&#010;&gt;&#010;&gt;&#010;&gt; 回答你说的两个问题：&#010;&gt; 1. 不是必须的，我这边验证不需要创建，集群也是可以正常运行任务的。Rest&#010;&gt; service的暴露方式是ClusterIP、NodePort、LoadBalancer都正常&#010;&gt; 2. 如果没有配置taskmanager.bind-host，&#010;&gt; [Flink-15911][Flink-15154]这两个JIRA并不会影响TM向RM注册时候的使用的地址&#010;&gt;&#010;&gt; 如果你想找到根本原因，那可能需要你这边提供JM/TM的完整log，这样方便分析&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 上午11:30写道：&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; Hi Yang Wang&#010;&gt; &gt;&#010;&gt; &gt; 刚刚在测试环境测试了一下，taskManager没有办法nslookup出来，JM可以nslookup，这两者的差别在于是否有service。&#010;&gt; &gt;&#010;&gt; &gt; 解决方案：我这边给集群加上了taskmanager-query-state-service.yaml（按照官网上是可选服务）。就不会刷No&#010;&gt; &gt; hostname could be resolved for ip&#010;&gt; &gt; address，将NodePort改为ClusterIp，作业就可以成功提交，不会出现time out的问题了，问题得到了解决。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 1. 如果按照上面的情况，那么这个配置文件是必须配置的？&#010;&gt; &gt;&#010;&gt; &gt; 2. 在1.11的更新中，发现有 [Flink-15911][Flink-15154]&#010;&gt; &gt; 支持分别配置用于本地监听绑定的网络接口和外部访问的地址和端口。是否是这块的改动，&#010;&gt; &gt; 需要JM去通过TM上报的ip反向解析出service？&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Bset！&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; [1]&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html&#010;&gt; &gt;&#010;&gt; &gt; a511955993&#010;&gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt;&#010;&gt; &gt; &lt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt;&#010;&gt; &gt; On 07/23/2020 10:11, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; 我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#010;&gt; &gt; 在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#010;&gt; &gt; 问题了&#010;&gt; &gt;&#010;&gt; &gt; kubectl run -i -t busybox --image=busybox --restart=Never&#010;&gt; &gt;&#010;&gt; &gt; 你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Yang&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Hi，Yang Wang！&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 在JM报错的地方，No hostname could be resolved for ip address xxxxx&#010;&gt; &gt; &gt; ，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best！&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &lt;&#010;&gt; &gt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; &gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP&#010;&gt; &gt; address，应该是集群的coredns&#010;&gt; &gt; &gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;&gt; &gt; &gt; 可能是coredns有问题&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Yang&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Hi&#010;&gt; &gt; &gt; &gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个 pod&#010;&gt; &gt; &gt; &gt; 的完整日志有没有什么发现&#010;&gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Hi，Congxian&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no hostname could be&#010;&gt; &gt; &gt; &gt; &gt; resolved，jm失联，作业提交失败。&#010;&gt; &gt; &gt; &gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 是否有其他排查思路？&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Best！&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#010;&gt; &gt; &gt; &gt; &gt; Hi&#010;&gt; &gt; &gt; &gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了 HA 也可以看一下 zk&#010;&gt; &gt; 的日志。之前遇到过一次在&#010;&gt; &gt; &gt; &gt; Yarn&#010;&gt; &gt; &gt; &gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk 日志发现的原因。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; Hi Roc&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#010;&gt; &gt; &gt; &gt; &gt; &gt; Hi，SmileSmile.&#010;&gt; &gt; &gt; &gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;&gt; &gt; &gt; &gt; &gt; &gt; 希望这对你有帮助。&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 祝好。&#010;&gt; &gt; &gt; &gt; &gt; &gt; Roc Marshal&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;Hi&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。&#010;&gt; job&#010;&gt; &gt; &gt; &gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved for the IP&#010;&gt; &gt; &gt; address，JM&#010;&gt; &gt; &gt; &gt; &gt; time&#010;&gt; &gt; &gt; &gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no&#010;&gt; hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;部分日志如下：&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; -&#010;&gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using&#010;&gt; &gt; IP&#010;&gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; files)&#010;&gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; -&#010;&gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using&#010;&gt; &gt; IP&#010;&gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; files)&#010;&gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; -&#010;&gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using&#010;&gt; IP&#010;&gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; files)&#010;&gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; &gt; The&#010;&gt; &gt; &gt; &gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a&#010;&gt; &gt; &gt; timed&#010;&gt; &gt; &gt; &gt; &gt; out.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; &gt; &gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2&#010;&gt; for&#010;&gt; &gt; &gt; job&#010;&gt; &gt; &gt; &gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;how to deal with ？&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;beset ！&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "12",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<164deeac.6627.1738ed23435.Coremail.a511955993@163.com>",
        "from": "SmileSmile &lt;a511955...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 05:50:07 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "Hi，Yang Wang&#010;&#010;因为日志太长了，删了一些重复的内容。&#010;一开始怀疑过jm gc的问题，将jm的内存调整为10g也是一样的情况。&#010;&#010;Best&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/27/2020 11:36, Yang Wang wrote:&#010;看你这个任务，失败的根本原因并不是“No hostname could be resolved&#010;”，这个WARNING的原因可以单独讨论（如果在1.10里面不存在的话）。&#010;你可以本地起一个Standalone的集群，也会有这样的WARNING，并不影响正常使用&#010;&#010;&#010;失败的原因是slot 5分钟申请超时了，你给的日志里面2020-07-23 13:55:45,519到2020-07-23&#010;13:58:18,037是空白的，没有进行省略吧？&#010;这段时间按理应该是task开始deploy了。在日志里看到了JM-&gt;RM的心跳超时，同一个Pod里面的同一个进程通信也超时了&#010;所以怀疑JM一直在FullGC，这个需要你确认一下&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 下午2:43写道：&#010;&#010;&gt; Hi Yang Wang&#010;&gt;&#010;&gt; 先分享下我这边的环境版本&#010;&gt;&#010;&gt;&#010;&gt; kubernetes：1.17.4.   CNI： weave&#010;&gt;&#010;&gt;&#010;&gt; 1 2 3 是我的一些疑惑&#010;&gt;&#010;&gt; 4 是JM日志&#010;&gt;&#010;&gt;&#010;&gt; 1. 去掉taskmanager-query-state-service.yaml后确实不行  nslookup&#010;&gt;&#010;&gt; kubectl exec -it busybox2 -- /bin/sh&#010;&gt; / # nslookup 10.47.96.2&#010;&gt; Server:          10.96.0.10&#010;&gt; Address:     10.96.0.10:53&#010;&gt;&#010;&gt; ** server can't find 2.96.47.10.in-addr.arpa: NXDOMAIN&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2. Flink1.11和Flink1.10&#010;&gt;&#010;&gt; detail subtasks taskmanagers xxx x 这行&#010;&gt;  1.11变成了172-20-0-50。1.10是flink-taskmanager-7b5d6958b6-sfzlk:36459。这块的改动是？（目前这个集群跑着1.10和1.11,1.10可以正常运行，如果coredns有问题，1.10版本的flink应该也有一样的情况吧？）&#010;&gt;&#010;&gt; 3. coredns是否特殊配置？&#010;&gt;&#010;&gt; 在容器中解析域名是正常的，只是反向解析没有service才会有问题。coredns是否有什么需要配置？&#010;&gt;&#010;&gt;&#010;&gt; 4. time out时候的JM日志如下：&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:53:00,228 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; ResourceManager akka.tcp://flink@flink-jobmanager:6123/user/rpc/resourcemanager_0&#010;&gt; was granted leadership with fencing token 00000000000000000000000000000000&#010;&gt; 2020-07-23 13:53:00,232 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher&#010;&gt; at akka://flink/user/rpc/dispatcher_1 .&#010;&gt; 2020-07-23 13:53:00,233 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] -&#010;&gt; Starting the SlotManager.&#010;&gt; 2020-07-23 13:53:03,472 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 1f9ae0cd95a28943a73be26323588696&#010;&gt; (akka.tcp://flink@10.34.128.9:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:03,777 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID cac09e751264e61615329c20713a84b4&#010;&gt; (akka.tcp://flink@10.32.160.6:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:03,787 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 93c72d01d09f9ae427c5fc980ed4c1e4&#010;&gt; (akka.tcp://flink@10.39.0.8:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:04,044 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 8adf2f8e81b77a16d5418a9e252c61e2&#010;&gt; (akka.tcp://flink@10.38.64.7:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:04,099 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 23e9d2358f6eb76b9ae718d879d4f330&#010;&gt; (akka.tcp://flink@10.42.160.6:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt; 2020-07-23 13:53:04,146 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering TaskManager with ResourceID 092f8dee299e32df13db3111662b61f8&#010;&gt; (akka.tcp://flink@10.33.192.14:6122/user/rpc/taskmanager_0) at&#010;&gt; ResourceManager&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:55:44,220 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received&#010;&gt; JobGraph submission 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;&gt; 2020-07-23 13:55:44,222 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] -&#010;&gt; Submitting job 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;&gt; 2020-07-23 13:55:44,251 INFO&#010;&gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting&#010;&gt; RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at&#010;&gt; akka://flink/user/rpc/jobmanager_2 .&#010;&gt; 2020-07-23 13:55:44,260 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Initializing job JobTest&#010;&gt; (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:44,278 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Using restart back off time strategy&#010;&gt; NoRestartBackoffTimeStrategy for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:44,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Running initialization on master for job JobTest&#010;&gt; (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:44,319 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Successfully ran initialization on master in 0 ms.&#010;&gt; 2020-07-23 13:55:44,428 INFO&#010;&gt;  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] -&#010;&gt; Built 1 pipelined regions in 25 ms&#010;&gt; 2020-07-23 13:55:44,437 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Loading state backend via factory&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory&#010;&gt; 2020-07-23 13:55:44,456 INFO&#010;&gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; predefined options: DEFAULT.&#010;&gt; 2020-07-23 13:55:44,457 INFO&#010;&gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; default options factory:&#010;&gt; DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;&gt; 2020-07-23 13:55:44,466 WARN  org.apache.flink.runtime.util.HadoopUtils&#010;&gt;                  [] - Could not find Hadoop configuration via any of the&#010;&gt; supported methods (Flink configuration, environment variables).&#010;&gt; 2020-07-23 13:55:45,276 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Using failover strategy&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@72bd8533&#010;&gt; for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 13:55:45,280 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] -&#010;&gt; JobManager runner for job JobTest (99a030d0e3f428490a501c0132f27a56) was&#010;&gt; granted leadership with session id 00000000-0000-0000-0000-000000000000 at&#010;&gt; akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2.&#010;&gt; 2020-07-23 13:55:45,286 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Starting scheduling with scheduling strategy&#010;&gt; [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,436 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}]&#010;&gt; 2020-07-23 13:55:45,436 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}]&#010;&gt; 2020-07-23 13:55:45,436 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{e559485ea7b0b7e17367816882538d90}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{7be8f6c1aedb27b04e7feae68078685c}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{582a86197884206652dff3aea2306bb3}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{0cc24260eda3af299a0b321feefaf2cb}]&#010;&gt; 2020-07-23 13:55:45,437 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{240ca6f3d3b5ece6a98243ec8cadf616}]&#010;&gt; 2020-07-23 13:55:45,438 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{c35033d598a517acc108424bb9f809fb}]&#010;&gt; 2020-07-23 13:55:45,438 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{ad35013c3b532d4b4df1be62395ae0cf}]&#010;&gt; 2020-07-23 13:55:45,438 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; serve slot request, no ResourceManager connected. Adding as pending request&#010;&gt; [SlotRequestId{c929bd5e8daf432d01fad1ece3daec1a}]&#010;&gt; 2020-07-23 13:55:45,487 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Connecting to ResourceManager&#010;&gt; akka.tcp://flink@flink-jobmanager&#010;&gt; :6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; 2020-07-23 13:55:45,492 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Resolved ResourceManager address, beginning&#010;&gt; registration&#010;&gt; 2020-07-23 13:55:45,493 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 13:55:45,499 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 13:55:45,501 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - JobManager successfully registered at ResourceManager,&#010;&gt; leader id: 00000000000000000000000000000000.&#010;&gt; 2020-07-23 13:55:45,501 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,502 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; d420d08bf2654d9ea76955c70db18b69.&#010;&gt; 2020-07-23 13:55:45,502 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{e7e422409acebdb385014a9634af6a90}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,503 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{14ac08438e79c8db8d25d93b99d62725}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,514 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; fce526bbe3e1be91caa3e4b536b20e35.&#010;&gt; 2020-07-23 13:55:45,514 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{40c7abbb12514c405323b0569fb21647}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,514 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{a4985a9647b65b30a571258b45c8f2ce}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,515 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{c52a6eb2fa58050e71e7903590019fd1}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,517 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; 18ac7ec802ebfcfed8c05ee9324a55a4.&#010;&gt;&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; 7ec76cbe689eb418b63599e90ade19be.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{46d65692a8b5aad11b51f9a74a666a74}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{3670bb4f345eedf941cc18e477ba1e9d}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{4a12467d76b9e3df8bc3412c0be08e14}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,518 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,519 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Requesting new slot [SlotRequestId{e559485ea7b0b7e17367816882538d90}] and&#010;&gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; 2020-07-23 13:55:45,519 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; b78837a29b4032924ac25be70ed21a3c.&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 13:58:18,037 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.47.96.2, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:22,192 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.34.64.14, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:22,358 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.34.128.9, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:24,562 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.32.160.6, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:25,487 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.38.64.7, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:27,636 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.42.160.6, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:27,767 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.43.64.12, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:29,651 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; The heartbeat of JobManager with id 456a18b6c404cb11a359718e16de1c6b timed&#010;&gt; out.&#010;&gt; 2020-07-23 13:58:29,651 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&gt; 2020-07-23 13:58:29,854 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.39.0.8, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:33,623 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.35.0.10, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:35,756 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.36.32.8, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt; 2020-07-23 13:58:36,694 WARN&#010;&gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; hostname could be resolved for the IP address 10.42.128.6, using IP address&#010;&gt; as host name. Local input split assignment (such as for HDFS files) may be&#010;&gt; impacted.&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 14:01:17,814 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Close ResourceManager connection&#010;&gt; 83b1ff14900abfd54418e7fa3efb3f8a: The heartbeat of JobManager with id&#010;&gt; 456a18b6c404cb11a359718e16de1c6b timed out..&#010;&gt; 2020-07-23 14:01:17,815 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Connecting to ResourceManager&#010;&gt; akka.tcp://flink@flink-jobmanager&#010;&gt; :6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; 2020-07-23 14:01:17,816 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Resolved ResourceManager address, beginning&#010;&gt; registration&#010;&gt; 2020-07-23 14:01:17,816 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:17,836 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Source:&#010;&gt; host_relation -&gt; Timestamps/Watermarks -&gt; Map (1/1)&#010;&gt; (302ca9640e2d209a543d843f2996ccd2) switched from SCHEDULED to FAILED on not&#010;&gt; deployed.&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt;      ... 23 more&#010;&gt; 2020-07-23 14:01:17,848 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;&gt; 2020-07-23 14:01:17,910 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; [] - 902 tasks should be restarted to recover the failed task&#010;&gt; cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;&gt; 2020-07-23 14:01:17,913 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; JobTest (99a030d0e3f428490a501c0132f27a56) switched from state RUNNING to&#010;&gt; FAILING.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; NoRestartBackoffTimeStrategy&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt;      ... 23 more&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 14:01:18,109 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 1809eb912d69854f2babedeaf879df6a.&#010;&gt; 2020-07-23 14:01:18,110 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; JobTest (99a030d0e3f428490a501c0132f27a56) switched from state FAILING to&#010;&gt; FAILED.&#010;&gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; NoRestartBackoffTimeStrategy&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      at&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; Caused by:&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; make sure that the cluster has enough resources.&#010;&gt;      at&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt;      ... 45 more&#010;&gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; java.util.concurrent.TimeoutException&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      at&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; ~[?:1.8.0_242]&#010;&gt;      ... 25 more&#010;&gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt;      ... 23 more&#010;&gt; 2020-07-23 14:01:18,114 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Stopping&#010;&gt; checkpoint coordinator for job 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,117 INFO&#010;&gt;  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore []&#010;&gt; - Shutting down&#010;&gt; 2020-07-23 14:01:18,118 INFO&#010;&gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Discarding the results produced by task execution&#010;&gt; 302ca9640e2d209a543d843f2996ccd2.&#010;&gt; 2020-07-23 14:01:18,120 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] timed out.&#010;&gt; 2020-07-23 14:01:18,120 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] timed out.&#010;&gt; 2020-07-23 14:01:18,120 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{e7e422409acebdb385014a9634af6a90}] timed out.&#010;&gt; 2020-07-23 14:01:18,121 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] timed out.&#010;&gt; 2020-07-23 14:01:18,121 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] timed out.&#010;&gt; 2020-07-23 14:01:18,121 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] timed out.&#010;&gt; 2020-07-23 14:01:18,122 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] timed out.&#010;&gt; 2020-07-23 14:01:18,122 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Pending&#010;&gt; slot request [&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-23 14:01:18,151 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registering job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,157 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,157 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,157 INFO&#010;&gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 reached globally terminal state FAILED.&#010;&gt; 2020-07-23 14:01:18,162 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Registered job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; 2020-07-23 14:01:18,162 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - JobManager successfully registered at ResourceManager,&#010;&gt; leader id: 00000000000000000000000000000000.&#010;&gt; 2020-07-23 14:01:18,225 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Stopping the JobMaster for job&#010;&gt; JobTest(99a030d0e3f428490a501c0132f27a56).&#010;&gt; 2020-07-23 14:01:18,381 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Suspending SlotPool.&#010;&gt; 2020-07-23 14:01:18,382 INFO  org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt;                 [] - Close ResourceManager connection&#010;&gt; 83b1ff14900abfd54418e7fa3efb3f8a: JobManager is shutting down..&#010;&gt; 2020-07-23 14:01:18,382 INFO&#010;&gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping&#010;&gt; SlotPool.&#010;&gt; 2020-07-23 14:01:18,382 INFO&#010;&gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&gt;&#010;&gt; a511955993&#010;&gt; 邮箱：a511955993@163.com&#010;&gt;&#010;&gt; &lt;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt;&#010;&gt; On 07/23/2020 13:26, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; 很高兴你的问题解决了，但我觉得根本原因应该不是加上了taskmanager-query-state-service.yaml的关系。&#010;&gt; 我这边不创建这个服务也是正常的，而且nslookup {tm_ip_address}是可以正常反解析到hostname的。&#010;&gt;&#010;&gt; 注意这里不是解析hostname，而是通过ip地址来反解析进行验证&#010;&gt;&#010;&gt;&#010;&gt; 回答你说的两个问题：&#010;&gt; 1. 不是必须的，我这边验证不需要创建，集群也是可以正常运行任务的。Rest&#010;&gt; service的暴露方式是ClusterIP、NodePort、LoadBalancer都正常&#010;&gt; 2. 如果没有配置taskmanager.bind-host，&#010;&gt; [Flink-15911][Flink-15154]这两个JIRA并不会影响TM向RM注册时候的使用的地址&#010;&gt;&#010;&gt; 如果你想找到根本原因，那可能需要你这边提供JM/TM的完整log，这样方便分析&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 上午11:30写道：&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; Hi Yang Wang&#010;&gt; &gt;&#010;&gt; &gt; 刚刚在测试环境测试了一下，taskManager没有办法nslookup出来，JM可以nslookup，这两者的差别在于是否有service。&#010;&gt; &gt;&#010;&gt; &gt; 解决方案：我这边给集群加上了taskmanager-query-state-service.yaml（按照官网上是可选服务）。就不会刷No&#010;&gt; &gt; hostname could be resolved for ip&#010;&gt; &gt; address，将NodePort改为ClusterIp，作业就可以成功提交，不会出现time out的问题了，问题得到了解决。&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 1. 如果按照上面的情况，那么这个配置文件是必须配置的？&#010;&gt; &gt;&#010;&gt; &gt; 2. 在1.11的更新中，发现有 [Flink-15911][Flink-15154]&#010;&gt; &gt; 支持分别配置用于本地监听绑定的网络接口和外部访问的地址和端口。是否是这块的改动，&#010;&gt; &gt; 需要JM去通过TM上报的ip反向解析出service？&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Bset！&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; [1]&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html&#010;&gt; &gt;&#010;&gt; &gt; a511955993&#010;&gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt;&#010;&gt; &gt; &lt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt;&#010;&gt; &gt; On 07/23/2020 10:11, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; 我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#010;&gt; &gt; 在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#010;&gt; &gt; 问题了&#010;&gt; &gt;&#010;&gt; &gt; kubectl run -i -t busybox --image=busybox --restart=Never&#010;&gt; &gt;&#010;&gt; &gt; 你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Yang&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Hi，Yang Wang！&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 在JM报错的地方，No hostname could be resolved for ip address xxxxx&#010;&gt; &gt; &gt; ，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best！&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &lt;&#010;&gt; &gt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt;&#010;&gt;&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; &gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP&#010;&gt; &gt; address，应该是集群的coredns&#010;&gt; &gt; &gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;&gt; &gt; &gt; 可能是coredns有问题&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Yang&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Hi&#010;&gt; &gt; &gt; &gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个 pod&#010;&gt; &gt; &gt; &gt; 的完整日志有没有什么发现&#010;&gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Hi，Congxian&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no hostname could be&#010;&gt; &gt; &gt; &gt; &gt; resolved，jm失联，作业提交失败。&#010;&gt; &gt; &gt; &gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 是否有其他排查思路？&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Best！&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#010;&gt; &gt; &gt; &gt; &gt; Hi&#010;&gt; &gt; &gt; &gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了 HA 也可以看一下 zk&#010;&gt; &gt; 的日志。之前遇到过一次在&#010;&gt; &gt; &gt; &gt; Yarn&#010;&gt; &gt; &gt; &gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk 日志发现的原因。&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; Hi Roc&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#010;&gt; &gt; &gt; &gt; &gt; &gt; Hi，SmileSmile.&#010;&gt; &gt; &gt; &gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;&gt; &gt; &gt; &gt; &gt; &gt; 希望这对你有帮助。&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 祝好。&#010;&gt; &gt; &gt; &gt; &gt; &gt; Roc Marshal&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;Hi&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。&#010;&gt; job&#010;&gt; &gt; &gt; &gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved for the IP&#010;&gt; &gt; &gt; address，JM&#010;&gt; &gt; &gt; &gt; &gt; time&#010;&gt; &gt; &gt; &gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no&#010;&gt; hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;部分日志如下：&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; -&#010;&gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7, using&#010;&gt; &gt; IP&#010;&gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; files)&#010;&gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; -&#010;&gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7, using&#010;&gt; &gt; IP&#010;&gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; files)&#010;&gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; -&#010;&gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using&#010;&gt; IP&#010;&gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; files)&#010;&gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; &gt; The&#010;&gt; &gt; &gt; &gt; &gt; &gt; heartbeat of JobManager with id 69a0d460de468888a9f41c770d963c0a&#010;&gt; &gt; &gt; timed&#010;&gt; &gt; &gt; &gt; &gt; out.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; &gt; &gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2&#010;&gt; for&#010;&gt; &gt; &gt; job&#010;&gt; &gt; &gt; &gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;how to deal with ？&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;beset ！&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;",
        "depth": "13",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<CAP+gf35-d5+Lg9x8eS3eseK-CTDF2oXrhAkYqONB7RY8+u0oYg@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 07:01:10 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "建议先配置heartbeat.timeout的值大一些，然后把gc log打出来&#010;看看是不是经常发生fullGC，每次持续时间是多长，从你目前提供的log看，进程内JM-&gt;RM都会心跳超时&#010;怀疑还是和GC有关的&#010;&#010;env.java.opts.jobmanager: -Xloggc:&lt;LOG_DIR&gt;/jobmanager-gc.log&#010;-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=512M&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月27日周一 下午1:50写道：&#010;&#010;&gt; Hi，Yang Wang&#010;&gt;&#010;&gt; 因为日志太长了，删了一些重复的内容。&#010;&gt; 一开始怀疑过jm gc的问题，将jm的内存调整为10g也是一样的情况。&#010;&gt;&#010;&gt; Best&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; a511955993&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：a511955993@163.com&#010;&gt; |&#010;&gt;&#010;&gt; 签名由 网易邮箱大师 定制&#010;&gt;&#010;&gt; On 07/27/2020 11:36, Yang Wang wrote:&#010;&gt; 看你这个任务，失败的根本原因并不是“No hostname could be resolved&#010;&gt; ”，这个WARNING的原因可以单独讨论（如果在1.10里面不存在的话）。&#010;&gt; 你可以本地起一个Standalone的集群，也会有这样的WARNING，并不影响正常使用&#010;&gt;&#010;&gt;&#010;&gt; 失败的原因是slot 5分钟申请超时了，你给的日志里面2020-07-23 13:55:45,519到2020-07-23&#010;&gt; 13:58:18,037是空白的，没有进行省略吧？&#010;&gt; 这段时间按理应该是task开始deploy了。在日志里看到了JM-&gt;RM的心跳超时，同一个Pod里面的同一个进程通信也超时了&#010;&gt; 所以怀疑JM一直在FullGC，这个需要你确认一下&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Yang&#010;&gt;&#010;&gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 下午2:43写道：&#010;&gt;&#010;&gt; &gt; Hi Yang Wang&#010;&gt; &gt;&#010;&gt; &gt; 先分享下我这边的环境版本&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; kubernetes：1.17.4.   CNI： weave&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 1 2 3 是我的一些疑惑&#010;&gt; &gt;&#010;&gt; &gt; 4 是JM日志&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 1. 去掉taskmanager-query-state-service.yaml后确实不行  nslookup&#010;&gt; &gt;&#010;&gt; &gt; kubectl exec -it busybox2 -- /bin/sh&#010;&gt; &gt; / # nslookup 10.47.96.2&#010;&gt; &gt; Server:          10.96.0.10&#010;&gt; &gt; Address:     10.96.0.10:53&#010;&gt; &gt;&#010;&gt; &gt; ** server can't find 2.96.47.10.in-addr.arpa: NXDOMAIN&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2. Flink1.11和Flink1.10&#010;&gt; &gt;&#010;&gt; &gt; detail subtasks taskmanagers xxx x 这行&#010;&gt; &gt;&#010;&gt; 1.11变成了172-20-0-50。1.10是flink-taskmanager-7b5d6958b6-sfzlk:36459。这块的改动是？（目前这个集群跑着1.10和1.11,1.10可以正常运行，如果coredns有问题，1.10版本的flink应该也有一样的情况吧？）&#010;&gt; &gt;&#010;&gt; &gt; 3. coredns是否特殊配置？&#010;&gt; &gt;&#010;&gt; &gt; 在容器中解析域名是正常的，只是反向解析没有service才会有问题。coredns是否有什么需要配置？&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 4. time out时候的JM日志如下：&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 13:53:00,228 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; ResourceManager akka.tcp://flink@flink-jobmanager&#010;&gt; :6123/user/rpc/resourcemanager_0&#010;&gt; &gt; was granted leadership with fencing token&#010;&gt; 00000000000000000000000000000000&#010;&gt; &gt; 2020-07-23 13:53:00,232 INFO&#010;&gt; &gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] -&#010;&gt; Starting&#010;&gt; &gt; RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher&#010;&gt; &gt; at akka://flink/user/rpc/dispatcher_1 .&#010;&gt; &gt; 2020-07-23 13:53:00,233 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl []&#010;&gt; -&#010;&gt; &gt; Starting the SlotManager.&#010;&gt; &gt; 2020-07-23 13:53:03,472 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering TaskManager with ResourceID 1f9ae0cd95a28943a73be26323588696&#010;&gt; &gt; (akka.tcp://flink@10.34.128.9:6122/user/rpc/taskmanager_0) at&#010;&gt; &gt; ResourceManager&#010;&gt; &gt; 2020-07-23 13:53:03,777 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering TaskManager with ResourceID cac09e751264e61615329c20713a84b4&#010;&gt; &gt; (akka.tcp://flink@10.32.160.6:6122/user/rpc/taskmanager_0) at&#010;&gt; &gt; ResourceManager&#010;&gt; &gt; 2020-07-23 13:53:03,787 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering TaskManager with ResourceID 93c72d01d09f9ae427c5fc980ed4c1e4&#010;&gt; &gt; (akka.tcp://flink@10.39.0.8:6122/user/rpc/taskmanager_0) at&#010;&gt; &gt; ResourceManager&#010;&gt; &gt; 2020-07-23 13:53:04,044 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering TaskManager with ResourceID 8adf2f8e81b77a16d5418a9e252c61e2&#010;&gt; &gt; (akka.tcp://flink@10.38.64.7:6122/user/rpc/taskmanager_0) at&#010;&gt; &gt; ResourceManager&#010;&gt; &gt; 2020-07-23 13:53:04,099 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering TaskManager with ResourceID 23e9d2358f6eb76b9ae718d879d4f330&#010;&gt; &gt; (akka.tcp://flink@10.42.160.6:6122/user/rpc/taskmanager_0) at&#010;&gt; &gt; ResourceManager&#010;&gt; &gt; 2020-07-23 13:53:04,146 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering TaskManager with ResourceID 092f8dee299e32df13db3111662b61f8&#010;&gt; &gt; (akka.tcp://flink@10.33.192.14:6122/user/rpc/taskmanager_0) at&#010;&gt; &gt; ResourceManager&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 13:55:44,220 INFO&#010;&gt; &gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] -&#010;&gt; Received&#010;&gt; &gt; JobGraph submission 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;&gt; &gt; 2020-07-23 13:55:44,222 INFO&#010;&gt; &gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] -&#010;&gt; &gt; Submitting job 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;&gt; &gt; 2020-07-23 13:55:44,251 INFO&#010;&gt; &gt;  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] -&#010;&gt; Starting&#010;&gt; &gt; RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at&#010;&gt; &gt; akka://flink/user/rpc/jobmanager_2 .&#010;&gt; &gt; 2020-07-23 13:55:44,260 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Initializing job JobTest&#010;&gt; &gt; (99a030d0e3f428490a501c0132f27a56).&#010;&gt; &gt; 2020-07-23 13:55:44,278 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Using restart back off time strategy&#010;&gt; &gt; NoRestartBackoffTimeStrategy for JobTest&#010;&gt; (99a030d0e3f428490a501c0132f27a56).&#010;&gt; &gt; 2020-07-23 13:55:44,319 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Running initialization on master for job JobTest&#010;&gt; &gt; (99a030d0e3f428490a501c0132f27a56).&#010;&gt; &gt; 2020-07-23 13:55:44,319 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Successfully ran initialization on master in 0 ms.&#010;&gt; &gt; 2020-07-23 13:55:44,428 INFO&#010;&gt; &gt;  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] -&#010;&gt; &gt; Built 1 pipelined regions in 25 ms&#010;&gt; &gt; 2020-07-23 13:55:44,437 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Loading state backend via factory&#010;&gt; &gt; org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory&#010;&gt; &gt; 2020-07-23 13:55:44,456 INFO&#010;&gt; &gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; &gt; predefined options: DEFAULT.&#010;&gt; &gt; 2020-07-23 13:55:44,457 INFO&#010;&gt; &gt;  org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;&gt; &gt; default options factory:&#010;&gt; &gt; DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;&gt; &gt; 2020-07-23 13:55:44,466 WARN  org.apache.flink.runtime.util.HadoopUtils&#010;&gt; &gt;                  [] - Could not find Hadoop configuration via any of the&#010;&gt; &gt; supported methods (Flink configuration, environment variables).&#010;&gt; &gt; 2020-07-23 13:55:45,276 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Using failover strategy&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@72bd8533&#010;&gt; &gt; for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;&gt; &gt; 2020-07-23 13:55:45,280 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] -&#010;&gt; &gt; JobManager runner for job JobTest (99a030d0e3f428490a501c0132f27a56) was&#010;&gt; &gt; granted leadership with session id 00000000-0000-0000-0000-000000000000&#010;&gt; at&#010;&gt; &gt; akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2.&#010;&gt; &gt; 2020-07-23 13:55:45,286 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Starting scheduling with scheduling strategy&#010;&gt; &gt; [org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 13:55:45,436 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}]&#010;&gt; &gt; 2020-07-23 13:55:45,436 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}]&#010;&gt; &gt; 2020-07-23 13:55:45,436 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}]&#010;&gt; &gt; 2020-07-23 13:55:45,437 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{e559485ea7b0b7e17367816882538d90}]&#010;&gt; &gt; 2020-07-23 13:55:45,437 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{7be8f6c1aedb27b04e7feae68078685c}]&#010;&gt; &gt; 2020-07-23 13:55:45,437 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{582a86197884206652dff3aea2306bb3}]&#010;&gt; &gt; 2020-07-23 13:55:45,437 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{0cc24260eda3af299a0b321feefaf2cb}]&#010;&gt; &gt; 2020-07-23 13:55:45,437 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{240ca6f3d3b5ece6a98243ec8cadf616}]&#010;&gt; &gt; 2020-07-23 13:55:45,438 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{c35033d598a517acc108424bb9f809fb}]&#010;&gt; &gt; 2020-07-23 13:55:45,438 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{ad35013c3b532d4b4df1be62395ae0cf}]&#010;&gt; &gt; 2020-07-23 13:55:45,438 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;&gt; &gt; serve slot request, no ResourceManager connected. Adding as pending&#010;&gt; request&#010;&gt; &gt; [SlotRequestId{c929bd5e8daf432d01fad1ece3daec1a}]&#010;&gt; &gt; 2020-07-23 13:55:45,487 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Connecting to ResourceManager&#010;&gt; &gt; akka.tcp://flink@flink-jobmanager&#010;&gt; &gt; :6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; &gt; 2020-07-23 13:55:45,492 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Resolved ResourceManager address, beginning&#010;&gt; &gt; registration&#010;&gt; &gt; 2020-07-23 13:55:45,493 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 13:55:45,499 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registered job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 13:55:45,501 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - JobManager successfully registered at&#010;&gt; ResourceManager,&#010;&gt; &gt; leader id: 00000000000000000000000000000000.&#010;&gt; &gt; 2020-07-23 13:55:45,501 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,502 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; &gt; d420d08bf2654d9ea76955c70db18b69.&#010;&gt; &gt; 2020-07-23 13:55:45,502 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,503 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{e7e422409acebdb385014a9634af6a90}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,503 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,503 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,503 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,503 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,503 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{14ac08438e79c8db8d25d93b99d62725}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 13:55:45,514 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; &gt; fce526bbe3e1be91caa3e4b536b20e35.&#010;&gt; &gt; 2020-07-23 13:55:45,514 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{40c7abbb12514c405323b0569fb21647}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,514 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{a4985a9647b65b30a571258b45c8f2ce}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,515 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{c52a6eb2fa58050e71e7903590019fd1}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 13:55:45,517 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; &gt; 18ac7ec802ebfcfed8c05ee9324a55a4.&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 13:55:45,518 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; &gt; 7ec76cbe689eb418b63599e90ade19be.&#010;&gt; &gt; 2020-07-23 13:55:45,518 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{46d65692a8b5aad11b51f9a74a666a74}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,518 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{3670bb4f345eedf941cc18e477ba1e9d}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,518 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{4a12467d76b9e3df8bc3412c0be08e14}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,518 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,518 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,518 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,519 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Requesting new slot [SlotRequestId{e559485ea7b0b7e17367816882538d90}] and&#010;&gt; &gt; profile ResourceProfile{UNKNOWN} from resource manager.&#010;&gt; &gt; 2020-07-23 13:55:45,519 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Request slot with profile ResourceProfile{UNKNOWN} for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 with allocation id&#010;&gt; &gt; b78837a29b4032924ac25be70ed21a3c.&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 13:58:18,037 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.47.96.2, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:22,192 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.34.64.14, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:22,358 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.34.128.9, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:24,562 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.32.160.6, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:25,487 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.38.64.7, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:27,636 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.42.160.6, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:27,767 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.43.64.12, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:29,651 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; The heartbeat of JobManager with id 456a18b6c404cb11a359718e16de1c6b&#010;&gt; timed&#010;&gt; &gt; out.&#010;&gt; &gt; 2020-07-23 13:58:29,651 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&gt; &gt; 2020-07-23 13:58:29,854 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.39.0.8, using IP address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:33,623 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.35.0.10, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:35,756 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.36.32.8, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt; 2020-07-23 13:58:36,694 WARN&#010;&gt; &gt;  org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;&gt; &gt; hostname could be resolved for the IP address 10.42.128.6, using IP&#010;&gt; address&#010;&gt; &gt; as host name. Local input split assignment (such as for HDFS files) may&#010;&gt; be&#010;&gt; &gt; impacted.&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 14:01:17,814 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Close ResourceManager connection&#010;&gt; &gt; 83b1ff14900abfd54418e7fa3efb3f8a: The heartbeat of JobManager with id&#010;&gt; &gt; 456a18b6c404cb11a359718e16de1c6b timed out..&#010;&gt; &gt; 2020-07-23 14:01:17,815 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Connecting to ResourceManager&#010;&gt; &gt; akka.tcp://flink@flink-jobmanager&#010;&gt; &gt; :6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;&gt; &gt; 2020-07-23 14:01:17,816 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Resolved ResourceManager address, beginning&#010;&gt; &gt; registration&#010;&gt; &gt; 2020-07-23 14:01:17,816 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 14:01:17,836 INFO&#010;&gt; &gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; Source:&#010;&gt; &gt; host_relation -&gt; Timestamps/Watermarks -&gt; Map (1/1)&#010;&gt; &gt; (302ca9640e2d209a543d843f2996ccd2) switched from SCHEDULED to FAILED on&#010;&gt; not&#010;&gt; &gt; deployed.&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; &gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; &gt; make sure that the cluster has enough resources.&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; &gt; java.util.concurrent.TimeoutException&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      ... 25 more&#010;&gt; &gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; &gt;      ... 23 more&#010;&gt; &gt; 2020-07-23 14:01:17,848 INFO&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; &gt; [] - Calculating tasks to restart to recover the failed task&#010;&gt; &gt; cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;&gt; &gt; 2020-07-23 14:01:17,910 INFO&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;&gt; &gt; [] - 902 tasks should be restarted to recover the failed task&#010;&gt; &gt; cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;&gt; &gt; 2020-07-23 14:01:17,913 INFO&#010;&gt; &gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; &gt; JobTest (99a030d0e3f428490a501c0132f27a56) switched from state RUNNING to&#010;&gt; &gt; FAILING.&#010;&gt; &gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; &gt; NoRestartBackoffTimeStrategy&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt; Caused by:&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; &gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; &gt; make sure that the cluster has enough resources.&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      ... 45 more&#010;&gt; &gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; &gt; java.util.concurrent.TimeoutException&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      ... 25 more&#010;&gt; &gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; &gt;      ... 23 more&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 14:01:18,109 INFO&#010;&gt; &gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; &gt; Discarding the results produced by task execution&#010;&gt; &gt; 1809eb912d69854f2babedeaf879df6a.&#010;&gt; &gt; 2020-07-23 14:01:18,110 INFO&#010;&gt; &gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;&gt; &gt; JobTest (99a030d0e3f428490a501c0132f27a56) switched from state FAILING to&#010;&gt; &gt; FAILED.&#010;&gt; &gt; org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;&gt; &gt; NoRestartBackoffTimeStrategy&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt; akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;&gt; &gt; [flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt; Caused by:&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;&gt; &gt; Could not allocate the required slot within slot request timeout. Please&#010;&gt; &gt; make sure that the cluster has enough resources.&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;&gt; &gt; ~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;&gt; &gt;      ... 45 more&#010;&gt; &gt; Caused by: java.util.concurrent.CompletionException:&#010;&gt; &gt; java.util.concurrent.TimeoutException&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      at&#010;&gt; &gt;&#010;&gt; java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;&gt; &gt; ~[?:1.8.0_242]&#010;&gt; &gt;      ... 25 more&#010;&gt; &gt; Caused by: java.util.concurrent.TimeoutException&#010;&gt; &gt;      ... 23 more&#010;&gt; &gt; 2020-07-23 14:01:18,114 INFO&#010;&gt; &gt;  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;&gt; Stopping&#010;&gt; &gt; checkpoint coordinator for job 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 14:01:18,117 INFO&#010;&gt; &gt;  org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore&#010;&gt; []&#010;&gt; &gt; - Shutting down&#010;&gt; &gt; 2020-07-23 14:01:18,118 INFO&#010;&gt; &gt;  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;&gt; &gt; Discarding the results produced by task execution&#010;&gt; &gt; 302ca9640e2d209a543d843f2996ccd2.&#010;&gt; &gt; 2020-07-23 14:01:18,120 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] timed out.&#010;&gt; &gt; 2020-07-23 14:01:18,120 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] timed out.&#010;&gt; &gt; 2020-07-23 14:01:18,120 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [SlotRequestId{e7e422409acebdb385014a9634af6a90}] timed out.&#010;&gt; &gt; 2020-07-23 14:01:18,121 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] timed out.&#010;&gt; &gt; 2020-07-23 14:01:18,121 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] timed out.&#010;&gt; &gt; 2020-07-23 14:01:18,121 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] timed out.&#010;&gt; &gt; 2020-07-23 14:01:18,122 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] timed out.&#010;&gt; &gt; 2020-07-23 14:01:18,122 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Pending&#010;&gt; &gt; slot request [&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 2020-07-23 14:01:18,151 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registering job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 14:01:18,157 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registered job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 14:01:18,157 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registered job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 14:01:18,157 INFO&#010;&gt; &gt;  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 reached globally terminal state FAILED.&#010;&gt; &gt; 2020-07-23 14:01:18,162 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Registered job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56.&#010;&gt; &gt; 2020-07-23 14:01:18,162 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - JobManager successfully registered at&#010;&gt; ResourceManager,&#010;&gt; &gt; leader id: 00000000000000000000000000000000.&#010;&gt; &gt; 2020-07-23 14:01:18,225 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Stopping the JobMaster for job&#010;&gt; &gt; JobTest(99a030d0e3f428490a501c0132f27a56).&#010;&gt; &gt; 2020-07-23 14:01:18,381 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; &gt; Suspending SlotPool.&#010;&gt; &gt; 2020-07-23 14:01:18,382 INFO&#010;&gt; org.apache.flink.runtime.jobmaster.JobMaster&#010;&gt; &gt;                 [] - Close ResourceManager connection&#010;&gt; &gt; 83b1ff14900abfd54418e7fa3efb3f8a: JobManager is shutting down..&#010;&gt; &gt; 2020-07-23 14:01:18,382 INFO&#010;&gt; &gt;  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;&gt; Stopping&#010;&gt; &gt; SlotPool.&#010;&gt; &gt; 2020-07-23 14:01:18,382 INFO&#010;&gt; &gt;  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;&gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;&gt; &gt; 99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&gt; &gt;&#010;&gt; &gt; a511955993&#010;&gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt;&#010;&gt; &gt; &lt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt&#010;&gt; ;&#010;&gt; &gt;&#010;&gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt;&#010;&gt; &gt; On 07/23/2020 13:26, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; 很高兴你的问题解决了，但我觉得根本原因应该不是加上了taskmanager-query-state-service.yaml的关系。&#010;&gt; &gt; 我这边不创建这个服务也是正常的，而且nslookup {tm_ip_address}是可以正常反解析到hostname的。&#010;&gt; &gt;&#010;&gt; &gt; 注意这里不是解析hostname，而是通过ip地址来反解析进行验证&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; 回答你说的两个问题：&#010;&gt; &gt; 1. 不是必须的，我这边验证不需要创建，集群也是可以正常运行任务的。Rest&#010;&gt; &gt; service的暴露方式是ClusterIP、NodePort、LoadBalancer都正常&#010;&gt; &gt; 2. 如果没有配置taskmanager.bind-host，&#010;&gt; &gt; [Flink-15911][Flink-15154]这两个JIRA并不会影响TM向RM注册时候的使用的地址&#010;&gt; &gt;&#010;&gt; &gt; 如果你想找到根本原因，那可能需要你这边提供JM/TM的完整log，这样方便分析&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Yang&#010;&gt; &gt;&#010;&gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 上午11:30写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Hi Yang Wang&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 刚刚在测试环境测试了一下，taskManager没有办法nslookup出来，JM可以nslookup，这两者的差别在于是否有service。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 解决方案：我这边给集群加上了taskmanager-query-state-service.yaml（按照官网上是可选服务）。就不会刷No&#010;&gt; &gt; &gt; hostname could be resolved for ip&#010;&gt; &gt; &gt; address，将NodePort改为ClusterIp，作业就可以成功提交，不会出现time out的问题了，问题得到了解决。&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 1. 如果按照上面的情况，那么这个配置文件是必须配置的？&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 2. 在1.11的更新中，发现有 [Flink-15911][Flink-15154]&#010;&gt; &gt; &gt; 支持分别配置用于本地监听绑定的网络接口和外部访问的地址和端口。是否是这块的改动，&#010;&gt; &gt; &gt; 需要JM去通过TM上报的ip反向解析出service？&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Bset！&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; [1]&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &lt;&#010;&gt; &gt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt&#010;&gt; ;&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On 07/23/2020 10:11, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; &gt; 我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#010;&gt; &gt; &gt; 在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#010;&gt; &gt; &gt; 问题了&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; kubectl run -i -t busybox --image=busybox --restart=Never&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Yang&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Hi，Yang Wang！&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 在JM报错的地方，No hostname could be resolved for ip address xxxxx&#010;&gt; &gt; &gt; &gt; ，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Best！&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &lt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt&#010;&gt; ;&#010;&gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;&gt; &gt; &gt; &gt; 如果你的日志里面一直在刷No hostname could be resolved for the IP&#010;&gt; &gt; &gt; address，应该是集群的coredns&#010;&gt; &gt; &gt; &gt; 有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;&gt; &gt; &gt; &gt; 可能是coredns有问题&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; Yang&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Hi&#010;&gt; &gt; &gt; &gt; &gt;    不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个 pod&#010;&gt; &gt; &gt; &gt; &gt; 的完整日志有没有什么发现&#010;&gt; &gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; Hi，Congxian&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no hostname could be&#010;&gt; &gt; &gt; &gt; &gt; &gt; resolved，jm失联，作业提交失败。&#010;&gt; &gt; &gt; &gt; &gt; &gt; 将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 是否有其他排查思路？&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; Best！&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; On 07/16/2020 13:17, Congxian Qiu wrote:&#010;&gt; &gt; &gt; &gt; &gt; &gt; Hi&#010;&gt; &gt; &gt; &gt; &gt; &gt;   如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了 HA 也可以看一下 zk&#010;&gt; &gt; &gt; 的日志。之前遇到过一次在&#010;&gt; &gt; &gt; &gt; &gt; Yarn&#010;&gt; &gt; &gt; &gt; &gt; &gt; 环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk 日志发现的原因。&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; &gt; &gt; Congxian&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; Hi Roc&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; | |&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; |&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; On 07/15/2020 17:16, Roc Marshal wrote:&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; Hi，SmileSmile.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 希望这对你有帮助。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 祝好。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; Roc Marshal&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;Hi&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。&#010;&gt; &gt; job&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; 并行度120.提交作业的时候出现大量的No hostname could be resolved for the IP&#010;&gt; &gt; &gt; &gt; address，JM&#010;&gt; &gt; &gt; &gt; &gt; &gt; time&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; out，作业提交失败。web ui也会卡主无响应。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;用wordCount，并行度只有1提交也会刷，no&#010;&gt; &gt; hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;部分日志如下：&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; &gt; -&#010;&gt; &gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.32.160.7,&#010;&gt; using&#010;&gt; &gt; &gt; IP&#010;&gt; &gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; &gt; files)&#010;&gt; &gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,460 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; &gt; -&#010;&gt; &gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.44.224.7,&#010;&gt; using&#010;&gt; &gt; &gt; IP&#010;&gt; &gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; &gt; files)&#010;&gt; &gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:58:46,461 WARN&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;&gt; &gt; -&#010;&gt; &gt; &gt; No&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; hostname could be resolved for the IP address 10.40.32.9, using&#010;&gt; &gt; IP&#010;&gt; &gt; &gt; &gt; &gt; &gt; address&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; as host name. Local input split assignment (such as for HDFS&#010;&gt; &gt; &gt; files)&#010;&gt; &gt; &gt; &gt; may&#010;&gt; &gt; &gt; &gt; &gt; &gt; be&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; impacted.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; &gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; &gt; &gt; The&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; heartbeat of JobManager with id&#010;&gt; 69a0d460de468888a9f41c770d963c0a&#010;&gt; &gt; &gt; &gt; timed&#010;&gt; &gt; &gt; &gt; &gt; &gt; out.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;2020-07-15 16:59:10,236 INFO&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;&gt; &gt; &gt; &gt; [] -&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; Disconnect job manager 00000000000000000000000000000000&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; @akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2&#010;&gt; &gt; for&#010;&gt; &gt; &gt; &gt; job&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;how to deal with ？&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;beset ！&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;| |&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;a511955993&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;邮箱：a511955993@163.com&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;|&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt;签名由 网易邮箱大师 定制&#010;&gt; &gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "14",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<3aecc979.4bf0.173a4b8ef54.Coremail.wangm92@163.com>",
        "from": "&quot;Matt Wang&quot; &lt;wang...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 31 Jul 2020 11:54:10 GMT",
        "subject": "Re: Flink 1.11 submit job timed out",
        "content": "遇到了同样的问题，也是启动了 taskmanager-query-state-service.yaml 这个服务后，作业才能正常提交的，另外我是在本地装的 k8s 集群进行测试的，如果是 GC 的问题，启不启动 TM service 应该不会有影响的&#010;&#010;&#010;--&#010;&#010;Best,&#010;Matt Wang&#010;&#010;&#010;On 07/27/2020 15:01，Yang Wang&lt;danrtsey.wy@gmail.com&gt; wrote：&#010;建议先配置heartbeat.timeout的值大一些，然后把gc log打出来&#010;看看是不是经常发生fullGC，每次持续时间是多长，从你目前提供的log看，进程内JM-&gt;RM都会心跳超时&#010;怀疑还是和GC有关的&#010;&#010;env.java.opts.jobmanager: -Xloggc:&lt;LOG_DIR&gt;/jobmanager-gc.log&#010;-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=2 -XX:GCLogFileSize=512M&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月27日周一 下午1:50写道：&#010;&#010;Hi，Yang Wang&#010;&#010;因为日志太长了，删了一些重复的内容。&#010;一开始怀疑过jm gc的问题，将jm的内存调整为10g也是一样的情况。&#010;&#010;Best&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/27/2020 11:36, Yang Wang wrote:&#010;看你这个任务，失败的根本原因并不是“No hostname could be resolved&#010;”，这个WARNING的原因可以单独讨论（如果在1.10里面不存在的话）。&#010;你可以本地起一个Standalone的集群，也会有这样的WARNING，并不影响正常使用&#010;&#010;&#010;失败的原因是slot 5分钟申请超时了，你给的日志里面2020-07-23 13:55:45,519到2020-07-23&#010;13:58:18,037是空白的，没有进行省略吧？&#010;这段时间按理应该是task开始deploy了。在日志里看到了JM-&gt;RM的心跳超时，同一个Pod里面的同一个进程通信也超时了&#010;所以怀疑JM一直在FullGC，这个需要你确认一下&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 下午2:43写道：&#010;&#010;Hi Yang Wang&#010;&#010;先分享下我这边的环境版本&#010;&#010;&#010;kubernetes：1.17.4.   CNI： weave&#010;&#010;&#010;1 2 3 是我的一些疑惑&#010;&#010;4 是JM日志&#010;&#010;&#010;1. 去掉taskmanager-query-state-service.yaml后确实不行  nslookup&#010;&#010;kubectl exec -it busybox2 -- /bin/sh&#010;/ # nslookup 10.47.96.2&#010;Server:          10.96.0.10&#010;Address:     10.96.0.10:53&#010;&#010;** server can't find 2.96.47.10.in-addr.arpa: NXDOMAIN&#010;&#010;&#010;&#010;2. Flink1.11和Flink1.10&#010;&#010;detail subtasks taskmanagers xxx x 这行&#010;&#010;1.11变成了172-20-0-50。1.10是flink-taskmanager-7b5d6958b6-sfzlk:36459。这块的改动是？（目前这个集群跑着1.10和1.11,1.10可以正常运行，如果coredns有问题，1.10版本的flink应该也有一样的情况吧？）&#010;&#010;3. coredns是否特殊配置？&#010;&#010;在容器中解析域名是正常的，只是反向解析没有service才会有问题。coredns是否有什么需要配置？&#010;&#010;&#010;4. time out时候的JM日志如下：&#010;&#010;&#010;&#010;2020-07-23 13:53:00,228 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;ResourceManager akka.tcp://flink@flink-jobmanager&#010;:6123/user/rpc/resourcemanager_0&#010;was granted leadership with fencing token&#010;00000000000000000000000000000000&#010;2020-07-23 13:53:00,232 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] -&#010;Starting&#010;RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher&#010;at akka://flink/user/rpc/dispatcher_1 .&#010;2020-07-23 13:53:00,233 INFO&#010;org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl []&#010;-&#010;Starting the SlotManager.&#010;2020-07-23 13:53:03,472 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering TaskManager with ResourceID 1f9ae0cd95a28943a73be26323588696&#010;(akka.tcp://flink@10.34.128.9:6122/user/rpc/taskmanager_0) at&#010;ResourceManager&#010;2020-07-23 13:53:03,777 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering TaskManager with ResourceID cac09e751264e61615329c20713a84b4&#010;(akka.tcp://flink@10.32.160.6:6122/user/rpc/taskmanager_0) at&#010;ResourceManager&#010;2020-07-23 13:53:03,787 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering TaskManager with ResourceID 93c72d01d09f9ae427c5fc980ed4c1e4&#010;(akka.tcp://flink@10.39.0.8:6122/user/rpc/taskmanager_0) at&#010;ResourceManager&#010;2020-07-23 13:53:04,044 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering TaskManager with ResourceID 8adf2f8e81b77a16d5418a9e252c61e2&#010;(akka.tcp://flink@10.38.64.7:6122/user/rpc/taskmanager_0) at&#010;ResourceManager&#010;2020-07-23 13:53:04,099 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering TaskManager with ResourceID 23e9d2358f6eb76b9ae718d879d4f330&#010;(akka.tcp://flink@10.42.160.6:6122/user/rpc/taskmanager_0) at&#010;ResourceManager&#010;2020-07-23 13:53:04,146 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering TaskManager with ResourceID 092f8dee299e32df13db3111662b61f8&#010;(akka.tcp://flink@10.33.192.14:6122/user/rpc/taskmanager_0) at&#010;ResourceManager&#010;&#010;&#010;2020-07-23 13:55:44,220 INFO&#010;org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] -&#010;Received&#010;JobGraph submission 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;2020-07-23 13:55:44,222 INFO&#010;org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] -&#010;Submitting job 99a030d0e3f428490a501c0132f27a56 (JobTest).&#010;2020-07-23 13:55:44,251 INFO&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] -&#010;Starting&#010;RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at&#010;akka://flink/user/rpc/jobmanager_2 .&#010;2020-07-23 13:55:44,260 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Initializing job JobTest&#010;(99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:44,278 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Using restart back off time strategy&#010;NoRestartBackoffTimeStrategy for JobTest&#010;(99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:44,319 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Running initialization on master for job JobTest&#010;(99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:44,319 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Successfully ran initialization on master in 0 ms.&#010;2020-07-23 13:55:44,428 INFO&#010;org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] -&#010;Built 1 pipelined regions in 25 ms&#010;2020-07-23 13:55:44,437 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Loading state backend via factory&#010;org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory&#010;2020-07-23 13:55:44,456 INFO&#010;org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;predefined options: DEFAULT.&#010;2020-07-23 13:55:44,457 INFO&#010;org.apache.flink.contrib.streaming.state.RocksDBStateBackend [] - Using&#010;default options factory:&#010;DefaultConfigurableOptionsFactory{configuredOptions={}}.&#010;2020-07-23 13:55:44,466 WARN  org.apache.flink.runtime.util.HadoopUtils&#010;[] - Could not find Hadoop configuration via any of the&#010;supported methods (Flink configuration, environment variables).&#010;2020-07-23 13:55:45,276 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Using failover strategy&#010;&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@72bd8533&#010;for JobTest (99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 13:55:45,280 INFO&#010;org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] -&#010;JobManager runner for job JobTest (99a030d0e3f428490a501c0132f27a56) was&#010;granted leadership with session id 00000000-0000-0000-0000-000000000000&#010;at&#010;akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2.&#010;2020-07-23 13:55:45,286 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Starting scheduling with scheduling strategy&#010;[org.apache.flink.runtime.scheduler.strategy.EagerSchedulingStrategy]&#010;&#010;&#010;&#010;2020-07-23 13:55:45,436 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}]&#010;2020-07-23 13:55:45,436 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{4ad15f417716c9e07fca383990c0f52a}]&#010;2020-07-23 13:55:45,436 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}]&#010;2020-07-23 13:55:45,437 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{e559485ea7b0b7e17367816882538d90}]&#010;2020-07-23 13:55:45,437 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{7be8f6c1aedb27b04e7feae68078685c}]&#010;2020-07-23 13:55:45,437 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{582a86197884206652dff3aea2306bb3}]&#010;2020-07-23 13:55:45,437 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{0cc24260eda3af299a0b321feefaf2cb}]&#010;2020-07-23 13:55:45,437 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{240ca6f3d3b5ece6a98243ec8cadf616}]&#010;2020-07-23 13:55:45,438 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{c35033d598a517acc108424bb9f809fb}]&#010;2020-07-23 13:55:45,438 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{ad35013c3b532d4b4df1be62395ae0cf}]&#010;2020-07-23 13:55:45,438 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot&#010;serve slot request, no ResourceManager connected. Adding as pending&#010;request&#010;[SlotRequestId{c929bd5e8daf432d01fad1ece3daec1a}]&#010;2020-07-23 13:55:45,487 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Connecting to ResourceManager&#010;akka.tcp://flink@flink-jobmanager&#010;:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;2020-07-23 13:55:45,492 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Resolved ResourceManager address, beginning&#010;registration&#010;2020-07-23 13:55:45,493 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 13:55:45,499 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registered job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 13:55:45,501 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - JobManager successfully registered at&#010;ResourceManager,&#010;leader id: 00000000000000000000000000000000.&#010;2020-07-23 13:55:45,501 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,502 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Request slot with profile ResourceProfile{UNKNOWN} for job&#010;99a030d0e3f428490a501c0132f27a56 with allocation id&#010;d420d08bf2654d9ea76955c70db18b69.&#010;2020-07-23 13:55:45,502 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{e7e422409acebdb385014a9634af6a90}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,503 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{14ac08438e79c8db8d25d93b99d62725}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;&#010;2020-07-23 13:55:45,514 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Request slot with profile ResourceProfile{UNKNOWN} for job&#010;99a030d0e3f428490a501c0132f27a56 with allocation id&#010;fce526bbe3e1be91caa3e4b536b20e35.&#010;2020-07-23 13:55:45,514 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{40c7abbb12514c405323b0569fb21647}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,514 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{a4985a9647b65b30a571258b45c8f2ce}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,515 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{c52a6eb2fa58050e71e7903590019fd1}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;&#010;2020-07-23 13:55:45,517 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Request slot with profile ResourceProfile{UNKNOWN} for job&#010;99a030d0e3f428490a501c0132f27a56 with allocation id&#010;18ac7ec802ebfcfed8c05ee9324a55a4.&#010;&#010;2020-07-23 13:55:45,518 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Request slot with profile ResourceProfile{UNKNOWN} for job&#010;99a030d0e3f428490a501c0132f27a56 with allocation id&#010;7ec76cbe689eb418b63599e90ade19be.&#010;2020-07-23 13:55:45,518 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{46d65692a8b5aad11b51f9a74a666a74}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{3670bb4f345eedf941cc18e477ba1e9d}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{4a12467d76b9e3df8bc3412c0be08e14}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{e092b12b96b0a98bbf057e71b9705c23}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{4ad15f417716c9e07fca383990c0f52a}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,518 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{345fdb427a893b7fc3f4f040f93445d2}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,519 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Requesting new slot [SlotRequestId{e559485ea7b0b7e17367816882538d90}] and&#010;profile ResourceProfile{UNKNOWN} from resource manager.&#010;2020-07-23 13:55:45,519 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Request slot with profile ResourceProfile{UNKNOWN} for job&#010;99a030d0e3f428490a501c0132f27a56 with allocation id&#010;b78837a29b4032924ac25be70ed21a3c.&#010;&#010;&#010;2020-07-23 13:58:18,037 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.47.96.2, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:22,192 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.34.64.14, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:22,358 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.34.128.9, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:24,562 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.32.160.6, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:25,487 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.38.64.7, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:27,636 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.42.160.6, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:27,767 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.43.64.12, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:29,651 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;The heartbeat of JobManager with id 456a18b6c404cb11a359718e16de1c6b&#010;timed&#010;out.&#010;2020-07-23 13:58:29,651 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Disconnect job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;2020-07-23 13:58:29,854 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.39.0.8, using IP address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:33,623 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.35.0.10, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:35,756 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.36.32.8, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;2020-07-23 13:58:36,694 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     [] - No&#010;hostname could be resolved for the IP address 10.42.128.6, using IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS files) may&#010;be&#010;impacted.&#010;&#010;&#010;2020-07-23 14:01:17,814 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Close ResourceManager connection&#010;83b1ff14900abfd54418e7fa3efb3f8a: The heartbeat of JobManager with id&#010;456a18b6c404cb11a359718e16de1c6b timed out..&#010;2020-07-23 14:01:17,815 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Connecting to ResourceManager&#010;akka.tcp://flink@flink-jobmanager&#010;:6123/user/rpc/resourcemanager_*(00000000000000000000000000000000)&#010;2020-07-23 14:01:17,816 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Resolved ResourceManager address, beginning&#010;registration&#010;2020-07-23 14:01:17,816 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:17,836 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Source:&#010;host_relation -&gt; Timestamps/Watermarks -&gt; Map (1/1)&#010;(302ca9640e2d209a543d843f2996ccd2) switched from SCHEDULED to FAILED on&#010;not&#010;deployed.&#010;&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;~[?:1.8.0_242]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-23 14:01:17,848 INFO&#010;&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - Calculating tasks to restart to recover the failed task&#010;cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;2020-07-23 14:01:17,910 INFO&#010;&#010;org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy&#010;[] - 902 tasks should be restarted to recover the failed task&#010;cbc357ccb763df2852fee8c4fc7d55f2_0.&#010;2020-07-23 14:01:17,913 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;JobTest (99a030d0e3f428490a501c0132f27a56) switched from state RUNNING to&#010;FAILING.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;NoRestartBackoffTimeStrategy&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;Caused by:&#010;&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;... 45 more&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;~[?:1.8.0_242]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;&#010;&#010;&#010;2020-07-23 14:01:18,109 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;1809eb912d69854f2babedeaf879df6a.&#010;2020-07-23 14:01:18,110 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job&#010;JobTest (99a030d0e3f428490a501c0132f27a56) switched from state FAILING to&#010;FAILED.&#010;org.apache.flink.runtime.JobException: Recovery is suppressed by&#010;NoRestartBackoffTimeStrategy&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:116)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:78)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:192)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:185)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:179)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:503)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.UpdateSchedulerNgOnInternalFailuresListener.notifyTaskFailure(UpdateSchedulerNgOnInternalFailuresListener.java:49)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph.notifySchedulerNgAboutInternalTaskFailure(ExecutionGraph.java:1710)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1287)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.Execution.processFail(Execution.java:1255)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.Execution.markFailed(Execution.java:1086)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.executiongraph.ExecutionVertex.markFailed(ExecutionVertex.java:748)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultExecutionVertexOperations.markFailed(DefaultExecutionVertexOperations.java:41)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskDeploymentFailure(DefaultScheduler.java:435)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.lambda$assignResourceOrHandleError$6(DefaultScheduler.java:422)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SchedulerImpl.lambda$internalAllocateSlot$0(SchedulerImpl.java:168)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$SingleTaskSlot.release(SlotSharingManager.java:726)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.release(SlotSharingManager.java:537)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotSharingManager$MultiTaskSlot.lambda$new$0(SlotSharingManager.java:432)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.concurrent.FutureUtils.lambda$forwardTo$21(FutureUtils.java:1120)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;org.apache.flink.runtime.concurrent.FutureUtils$Timeout.run(FutureUtils.java:1036)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRunAsync(AkkaRpcActor.java:402)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:195)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:152)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:26)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:21)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:21)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.Actor$class.aroundReceive(Actor.scala:517)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:225)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:592)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.actor.ActorCell.invoke(ActorCell.scala:561)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.run(Mailbox.scala:225)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.Mailbox.exec(Mailbox.scala:235)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;at&#010;&#010;akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;Caused by:&#010;&#010;org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException:&#010;Could not allocate the required slot within slot request timeout. Please&#010;make sure that the cluster has enough resources.&#010;at&#010;&#010;org.apache.flink.runtime.scheduler.DefaultScheduler.maybeWrapWithNoResourceAvailableException(DefaultScheduler.java:441)&#010;~[flink-dist_2.11-1.11.1.jar:1.11.1]&#010;... 45 more&#010;Caused by: java.util.concurrent.CompletionException:&#010;java.util.concurrent.TimeoutException&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)&#010;~[?:1.8.0_242]&#010;at&#010;&#010;java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)&#010;~[?:1.8.0_242]&#010;... 25 more&#010;Caused by: java.util.concurrent.TimeoutException&#010;... 23 more&#010;2020-07-23 14:01:18,114 INFO&#010;org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] -&#010;Stopping&#010;checkpoint coordinator for job 99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,117 INFO&#010;org.apache.flink.runtime.checkpoint.StandaloneCompletedCheckpointStore&#010;[]&#010;- Shutting down&#010;2020-07-23 14:01:18,118 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph       [] -&#010;Discarding the results produced by task execution&#010;302ca9640e2d209a543d843f2996ccd2.&#010;2020-07-23 14:01:18,120 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [SlotRequestId{15fd2a9565c2b080748c1d1592b1cbbc}] timed out.&#010;2020-07-23 14:01:18,120 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [SlotRequestId{8cd72cc16f0e319d915a9a096a1096d7}] timed out.&#010;2020-07-23 14:01:18,120 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [SlotRequestId{e7e422409acebdb385014a9634af6a90}] timed out.&#010;2020-07-23 14:01:18,121 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [SlotRequestId{cef1af73546ca1fc27ca7a3322e9e815}] timed out.&#010;2020-07-23 14:01:18,121 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [SlotRequestId{108fe0b3086567ad79275eccef2fdaf8}] timed out.&#010;2020-07-23 14:01:18,121 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [SlotRequestId{265e67985eab7a6dc08024e53bf2708d}] timed out.&#010;2020-07-23 14:01:18,122 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [SlotRequestId{7087497a17c441f1a1d6fefcbc7cd0ea}] timed out.&#010;2020-07-23 14:01:18,122 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Pending&#010;slot request [&#010;&#010;&#010;2020-07-23 14:01:18,151 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registering job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,157 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registered job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,157 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registered job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,157 INFO&#010;org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job&#010;99a030d0e3f428490a501c0132f27a56 reached globally terminal state FAILED.&#010;2020-07-23 14:01:18,162 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Registered job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56.&#010;2020-07-23 14:01:18,162 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - JobManager successfully registered at&#010;ResourceManager,&#010;leader id: 00000000000000000000000000000000.&#010;2020-07-23 14:01:18,225 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Stopping the JobMaster for job&#010;JobTest(99a030d0e3f428490a501c0132f27a56).&#010;2020-07-23 14:01:18,381 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Suspending SlotPool.&#010;2020-07-23 14:01:18,382 INFO&#010;org.apache.flink.runtime.jobmaster.JobMaster&#010;[] - Close ResourceManager connection&#010;83b1ff14900abfd54418e7fa3efb3f8a: JobManager is shutting down..&#010;2020-07-23 14:01:18,382 INFO&#010;org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] -&#010;Stopping&#010;SlotPool.&#010;2020-07-23 14:01:18,382 INFO&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] -&#010;Disconnect job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2 for job&#010;99a030d0e3f428490a501c0132f27a56 from the resource manager.&#010;&#010;a511955993&#010;邮箱：a511955993@163.com&#010;&#010;&lt;&#010;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt&#010;;&#010;&#010;签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&#010;On 07/23/2020 13:26, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;很高兴你的问题解决了，但我觉得根本原因应该不是加上了taskmanager-query-state-service.yaml的关系。&#010;我这边不创建这个服务也是正常的，而且nslookup {tm_ip_address}是可以正常反解析到hostname的。&#010;&#010;注意这里不是解析hostname，而是通过ip地址来反解析进行验证&#010;&#010;&#010;回答你说的两个问题：&#010;1. 不是必须的，我这边验证不需要创建，集群也是可以正常运行任务的。Rest&#010;service的暴露方式是ClusterIP、NodePort、LoadBalancer都正常&#010;2. 如果没有配置taskmanager.bind-host，&#010;[Flink-15911][Flink-15154]这两个JIRA并不会影响TM向RM注册时候的使用的地址&#010;&#010;如果你想找到根本原因，那可能需要你这边提供JM/TM的完整log，这样方便分析&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月23日周四 上午11:30写道：&#010;&#010;&#010;Hi Yang Wang&#010;&#010;刚刚在测试环境测试了一下，taskManager没有办法nslookup出来，JM可以nslookup，这两者的差别在于是否有service。&#010;&#010;解决方案：我这边给集群加上了taskmanager-query-state-service.yaml（按照官网上是可选服务）。就不会刷No&#010;hostname could be resolved for ip&#010;address，将NodePort改为ClusterIp，作业就可以成功提交，不会出现time out的问题了，问题得到了解决。&#010;&#010;&#010;1. 如果按照上面的情况，那么这个配置文件是必须配置的？&#010;&#010;2. 在1.11的更新中，发现有 [Flink-15911][Flink-15154]&#010;支持分别配置用于本地监听绑定的网络接口和外部访问的地址和端口。是否是这块的改动，&#010;需要JM去通过TM上报的ip反向解析出service？&#010;&#010;&#010;Bset！&#010;&#010;&#010;[1]&#010;&#010;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/kubernetes.html&#010;&#010;a511955993&#010;邮箱：a511955993@163.com&#010;&#010;&lt;&#010;&#010;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt&#010;;&#010;&#010;&#010;签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&#010;On 07/23/2020 10:11, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;我的意思就是你在Flink任务运行的过程中，然后下面的命令在集群里面起一个busybox的pod，&#010;在里面执行 nslookup {ip_address}，看看是否能够正常解析到。如果不能应该就是coredns的&#010;问题了&#010;&#010;kubectl run -i -t busybox --image=busybox --restart=Never&#010;&#010;你需要确认下集群的coredns pod是否正常，一般是部署在kube-system这个namespace下的&#010;&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月22日周三 下午7:57写道：&#010;&#010;&#010;Hi，Yang Wang！&#010;&#010;很开心可以收到你的回复，你的回复帮助很大，让我知道了问题的方向。我再补充些信息，希望可以帮我进一步判断一下问题根源。&#010;&#010;在JM报错的地方，No hostname could be resolved for ip address xxxxx&#010;，报出来的ip是k8s分配给flink pod的内网ip，不是宿主机的ip。请问这个问题可能出在哪里呢&#010;&#010;Best！&#010;&#010;&#010;a511955993&#010;邮箱：a511955993@163.com&#010;&#010;&lt;&#010;&#010;&#010;https://maas.mail.163.com/dashi-web-extend/html/proSignature.html?ftlId=1&amp;name=a511955993&amp;uid=a511955993%40163.com&amp;iconUrl=https%3A%2F%2Fmail-online.nosdn.127.net%2Fqiyelogo%2FdefaultAvatar.png&amp;items=%5B%22%E9%82%AE%E7%AE%B1%EF%BC%9Aa511955993%40163.com%22%5D&amp;gt&#010;;&#010;&#010;&#010;&#010;签名由 网易邮箱大师 &lt;https://mail.163.com/dashi/dlpro.html?from=mail88&amp;gt; 定制&#010;&#010;On 07/22/2020 18:18, Yang Wang &lt;danrtsey.wy@gmail.com&gt; wrote:&#010;如果你的日志里面一直在刷No hostname could be resolved for the IP&#010;address，应该是集群的coredns&#010;有问题，由ip地址反查hostname查不到。你可以起一个busybox验证一下是不是这个ip就解析不了，有&#010;可能是coredns有问题&#010;&#010;&#010;Best,&#010;Yang&#010;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月21日周二 下午7:29写道：&#010;&#010;Hi&#010;不确定 k8s 环境中能否看到 pod 的完整日志？类似 Yarn 的 NM 日志一样，如果有的话，可以尝试看一下这个 pod&#010;的完整日志有没有什么发现&#010;Best,&#010;Congxian&#010;&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月21日周二 下午3:19写道：&#010;&#010;Hi，Congxian&#010;&#010;因为是测试环境，没有配置HA，目前看到的信息，就是JM刷出来大量的no hostname could be&#010;resolved，jm失联，作业提交失败。&#010;将jm内存配置为10g也是一样的情况（jobmanager.memory.pprocesa.size：10240m）。&#010;&#010;在同一个环境将版本回退到1.10没有出现该问题，也不会刷如上报错。&#010;&#010;&#010;是否有其他排查思路？&#010;&#010;Best！&#010;&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/16/2020 13:17, Congxian Qiu wrote:&#010;Hi&#010;如果没有异常，GC 情况也正常的话，或许可以看一下 pod 的相关日志，如果开启了 HA 也可以看一下 zk&#010;的日志。之前遇到过一次在&#010;Yarn&#010;环境中类似的现象是由于其他原因导致的，通过看 NM 日志以及 zk 日志发现的原因。&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;SmileSmile &lt;a511955993@163.com&gt; 于2020年7月15日周三 下午5:20写道：&#010;&#010;Hi Roc&#010;&#010;该现象在1.10.1版本没有，在1.11版本才出现。请问这个该如何查比较合适&#010;&#010;&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;On 07/15/2020 17:16, Roc Marshal wrote:&#010;Hi，SmileSmile.&#010;个人之前有遇到过 类似 的host解析问题，可以从k8s的pod节点网络映射角度排查一下。&#010;希望这对你有帮助。&#010;&#010;&#010;祝好。&#010;Roc Marshal&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-15 17:04:18，\"SmileSmile\" &lt;a511955993@163.com&gt; 写道：&#010;&#010;Hi&#010;&#010;使用版本Flink 1.11，部署方式 kubernetes session。 TM个数30个，每个TM 4个slot。&#010;job&#010;并行度120.提交作业的时候出现大量的No hostname could be resolved for the IP&#010;address，JM&#010;time&#010;out，作业提交失败。web ui也会卡主无响应。&#010;&#010;用wordCount，并行度只有1提交也会刷，no&#010;hostname的日志会刷个几条，然后正常提交，如果并行度一上去，就会超时。&#010;&#010;&#010;部分日志如下：&#010;&#010;2020-07-15 16:58:46,460 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;-&#010;No&#010;hostname could be resolved for the IP address 10.32.160.7,&#010;using&#010;IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS&#010;files)&#010;may&#010;be&#010;impacted.&#010;2020-07-15 16:58:46,460 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;-&#010;No&#010;hostname could be resolved for the IP address 10.44.224.7,&#010;using&#010;IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS&#010;files)&#010;may&#010;be&#010;impacted.&#010;2020-07-15 16:58:46,461 WARN&#010;org.apache.flink.runtime.taskmanager.TaskManagerLocation     []&#010;-&#010;No&#010;hostname could be resolved for the IP address 10.40.32.9, using&#010;IP&#010;address&#010;as host name. Local input split assignment (such as for HDFS&#010;files)&#010;may&#010;be&#010;impacted.&#010;&#010;2020-07-15 16:59:10,236 INFO&#010;&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] -&#010;The&#010;heartbeat of JobManager with id&#010;69a0d460de468888a9f41c770d963c0a&#010;timed&#010;out.&#010;2020-07-15 16:59:10,236 INFO&#010;&#010;org.apache.flink.runtime.resourcemanager.StandaloneResourceManager&#010;[] -&#010;Disconnect job manager 00000000000000000000000000000000&#010;@akka.tcp://flink@flink-jobmanager:6123/user/rpc/jobmanager_2&#010;for&#010;job&#010;e1554c737e37ed79688a15c746b6e9ef from the resource manager.&#010;&#010;&#010;how to deal with ？&#010;&#010;&#010;beset ！&#010;&#010;| |&#010;a511955993&#010;|&#010;|&#010;邮箱：a511955993@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;",
        "depth": "15",
        "reply": "<2498bd4.e988.17351b7aa73.Coremail.a511955993@163.com>"
    },
    {
        "id": "<1594805964605-0.post@n8.nabble.com>",
        "from": "vw17 &lt;vwing...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 09:39:24 GMT",
        "subject": "springboot 2.3.1 + flink 1.11.0整合后如何从外部传入application.yml配置文件?",
        "content": "Hi,&#013;&#010;由于项目需要目前整合了springboot和flink,&#010;但一些项目的相关配置在application.yml，生产环境需要变更其中的一些配置。之前通常的做法的是启动时使用&#010;-Dspring.config.location=xxx 从外部指定需要的配置文件。现在想知道如果使用flink&#010;run&#010;启动jar能否支持这种指定方式? &#013;&#010;谢谢&#010;&#010;&#010;&#013;&#010;--&#013;&#010;Sent from: http://apache-flink.147419.n8.nabble.com/",
        "depth": "0",
        "reply": "<1594805964605-0.post@n8.nabble.com>"
    },
    {
        "id": "<758f2cbb.7073.17352a5e59f.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 13:24:30 GMT",
        "subject": "flink 1.11 sql类型问题",
        "content": "hi，&#010;我看1.11的java.sql.Timestamp 对应的是Flink的TIMESTAMP(9)，跟之前默认的TIMESTAMP(3)有区别，而且之前1.10的Timestamp(3)是带时区UTC的，现在这个类型不带时区了。想问下这个具体调整应该如何适配？",
        "depth": "0",
        "reply": "<758f2cbb.7073.17352a5e59f.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<703a87d.7589.173530f01f4.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 15 Jul 2020 15:19:19 GMT",
        "subject": "Re:flink 1.11 sql类型问题",
        "content": "hi,&#010;我通过flink sql 定义了一个es sink，其中有个字段类型定义为了 eventTime&#010;TIMESTAMP(9) WITH LOCAL TIME ZONE。&#010;在尝试写入时，报了如下的异常。看来json parser无法解析这种类型。请问下大神们，我应该怎么写入一个UTC日期的时间类型？格式类似&#010;2020-07-15T12:00:00.000Z &#010;&#010;&#010;&#010;java.lang.UnsupportedOperationException: Not support to parse type: TIMESTAMP(9) WITH LOCAL&#010;TIME ZONE&#010;&#010;at org.apache.flink.formats.json.JsonRowDataSerializationSchema.createNotNullConverter(JsonRowDataSerializationSchema.java:184)&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-15 21:24:30，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;hi，&#010;&gt;我看1.11的java.sql.Timestamp 对应的是Flink的TIMESTAMP(9)，跟之前默认的TIMESTAMP(3)有区别，而且之前1.10的Timestamp(3)是带时区UTC的，现在这个类型不带时区了。想问下这个具体调整应该如何适配？&#010;",
        "depth": "1",
        "reply": "<758f2cbb.7073.17352a5e59f.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<BDB014DE-67D5-407E-830F-3CDBA629B686@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 01:40:06 GMT",
        "subject": "Re: flink 1.11 sql类型问题",
        "content": "Hello&#010;&#010;json解析UTC时间是支持的，你with参数里指定下json中timestamp的类型试下，&#010;json.timestamp-format.standard = 'ISO-8601'&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&gt;&#010;&#010;&gt; 在 2020年7月15日，23:19，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt; &#010;&gt; hi,&#010;&gt; 我通过flink sql 定义了一个es sink，其中有个字段类型定义为了 eventTime&#010;TIMESTAMP(9) WITH LOCAL TIME ZONE。&#010;&gt; 在尝试写入时，报了如下的异常。看来json parser无法解析这种类型。请问下大神们，我应该怎么写入一个UTC日期的时间类型？格式类似&#010;2020-07-15T12:00:00.000Z &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; java.lang.UnsupportedOperationException: Not support to parse type: TIMESTAMP(9) WITH&#010;LOCAL TIME ZONE&#010;&gt; &#010;&gt; at org.apache.flink.formats.json.JsonRowDataSerializationSchema.createNotNullConverter(JsonRowDataSerializationSchema.java:184)&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在 2020-07-15 21:24:30，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt; hi，&#010;&gt;&gt; 我看1.11的java.sql.Timestamp 对应的是Flink的TIMESTAMP(9)，跟之前默认的TIMESTAMP(3)有区别，而且之前1.10的Timestamp(3)是带时区UTC的，现在这个类型不带时区了。想问下这个具体调整应该如何适配？&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<758f2cbb.7073.17352a5e59f.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<5ff0156e.15a3.173555b7e7d.Coremail.sunfulin0321@163.com>",
        "from": "sunfulin  &lt;sunfulin0...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 02:02:06 GMT",
        "subject": "Re:Re: flink 1.11 sql类型问题",
        "content": "&#010;&#010;&#010;hi, leonard&#010;感谢回复。我在es的ddl with参数里加了这个，貌似还是报错。我再简单描述下我的场景：&#010;我的es sink的ddl如下：&#010;create table es_sink (&#010;  a varchar,&#010;  b varchar,&#010;  c TIMESTAMP(9) WITH LOCAL TIME ZONE&#010;) with (&#010;  ....&#010;)&#010;&#010;&#010;我使用处理时间属性，将流里的proctime转成UTC格式的日期类型，作为c这个字段写入。现在能原生支持么？之前在1.10版本貌似是可以直接写的。但是到1.11写的不带时区了，导致不能兼容之前的格式。&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-16 09:40:06，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hello&#010;&gt;&#010;&gt;json解析UTC时间是支持的，你with参数里指定下json中timestamp的类型试下，&#010;json.timestamp-format.standard = 'ISO-8601'&#010;&gt;&#010;&gt;Best&#010;&gt;Leonard Xu&#010;&gt;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月15日，23:19，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; hi,&#010;&gt;&gt; 我通过flink sql 定义了一个es sink，其中有个字段类型定义为了&#010;eventTime TIMESTAMP(9) WITH LOCAL TIME ZONE。&#010;&gt;&gt; 在尝试写入时，报了如下的异常。看来json parser无法解析这种类型。请问下大神们，我应该怎么写入一个UTC日期的时间类型？格式类似&#010;2020-07-15T12:00:00.000Z &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; java.lang.UnsupportedOperationException: Not support to parse type: TIMESTAMP(9)&#010;WITH LOCAL TIME ZONE&#010;&gt;&gt; &#010;&gt;&gt; at org.apache.flink.formats.json.JsonRowDataSerializationSchema.createNotNullConverter(JsonRowDataSerializationSchema.java:184)&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 在 2020-07-15 21:24:30，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#010;&gt;&gt;&gt; hi，&#010;&gt;&gt;&gt; 我看1.11的java.sql.Timestamp 对应的是Flink的TIMESTAMP(9)，跟之前默认的TIMESTAMP(3)有区别，而且之前1.10的Timestamp(3)是带时区UTC的，现在这个类型不带时区了。想问下这个具体调整应该如何适配？&#010;&gt;&#010;",
        "depth": "3",
        "reply": "<758f2cbb.7073.17352a5e59f.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CAELO932F68jhNJEMpt2qtKSFjNKLLwkrEK7H1iWwS4vwSuRTvg@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 09:53:10 GMT",
        "subject": "Re: Re: flink 1.11 sql类型问题",
        "content": "你是说输出的时候想带 'Z' 后缀？&#013;&#010;如果这样的话，我觉得 json.timestamp-format.standard = 'ISO-8601' 这个参数应该能解决你的问题。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Thu, 16 Jul 2020 at 10:02, sunfulin &lt;sunfulin0321@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi, leonard&#013;&#010;&gt; 感谢回复。我在es的ddl with参数里加了这个，貌似还是报错。我再简单描述下我的场景：&#013;&#010;&gt; 我的es sink的ddl如下：&#013;&#010;&gt; create table es_sink (&#013;&#010;&gt;   a varchar,&#013;&#010;&gt;   b varchar,&#013;&#010;&gt;   c TIMESTAMP(9) WITH LOCAL TIME ZONE&#013;&#010;&gt; ) with (&#013;&#010;&gt;   ....&#013;&#010;&gt; )&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 我使用处理时间属性，将流里的proctime转成UTC格式的日期类型，作为c这个字段写入。现在能原生支持么？之前在1.10版本貌似是可以直接写的。但是到1.11写的不带时区了，导致不能兼容之前的格式。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-16 09:40:06，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;Hello&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;json解析UTC时间是支持的，你with参数里指定下json中timestamp的类型试下，&#013;&#010;&gt; json.timestamp-format.standard = 'ISO-8601'&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best&#013;&#010;&gt; &gt;Leonard Xu&#013;&#010;&gt; &gt;[1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&#013;&#010;&gt; &lt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; 在 2020年7月15日，23:19，sunfulin &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; hi,&#013;&#010;&gt; &gt;&gt; 我通过flink sql 定义了一个es sink，其中有个字段类型定义为了&#010;eventTime TIMESTAMP(9) WITH&#013;&#010;&gt; LOCAL TIME ZONE。&#013;&#010;&gt; &gt;&gt; 在尝试写入时，报了如下的异常。看来json parser无法解析这种类型。请问下大神们，我应该怎么写入一个UTC日期的时间类型？格式类似&#013;&#010;&gt; 2020-07-15T12:00:00.000Z&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; java.lang.UnsupportedOperationException: Not support to parse type:&#013;&#010;&gt; TIMESTAMP(9) WITH LOCAL TIME ZONE&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; at&#013;&#010;&gt; org.apache.flink.formats.json.JsonRowDataSerializationSchema.createNotNullConverter(JsonRowDataSerializationSchema.java:184)&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt; 在 2020-07-15 21:24:30，\"sunfulin\" &lt;sunfulin0321@163.com&gt; 写道：&#013;&#010;&gt; &gt;&gt;&gt; hi，&#013;&#010;&gt; &gt;&gt;&gt; 我看1.11的java.sql.Timestamp&#013;&#010;&gt; 对应的是Flink的TIMESTAMP(9)，跟之前默认的TIMESTAMP(3)有区别，而且之前1.10的Timestamp(3)是带时区UTC的，现在这个类型不带时区了。想问下这个具体调整应该如何适配？&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<758f2cbb.7073.17352a5e59f.Coremail.sunfulin0321@163.com>"
    },
    {
        "id": "<CABMA2UvUOMdTP_LERbUgzQ6J2vyUZZKp=AEuUx2+E3zTEs5zwg@mail.gmail.com>",
        "from": "Jim Chen &lt;chenshuai19950...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 02:10:34 GMT",
        "subject": "HELP，flink1.10 sql整合hbase，insert into时，提示validateSchemaAndApplyImplicitCast报错",
        "content": "Hi,&#010;&#010;我在使用flink1.10.1的sql功能，hbase的版本是1.4.3，写入hbase时，提示validateSchemaAndApplyImplicitCast报错，意思是Query的Schema和Sink的Schema不一致。主要是Query&#010;Schema中的Row(EXPR$0)，里面都是表达式。Sink&#010;Schema中是Row(device_id)这种。我不知道，在sql中如何写，才能和hbase的sink&#010;schema保持一致。&#010;&#010;我尝试了，类似于在select device_id as rowkey, ROW( device_id as 这里不能as )&#010;as&#010;f1，不写的话，Query 中ROW的 Schema都是表达式，不是具体定义的一个字段&#010;&#010;这里query和sink的字段个数，是对上的。每个字段的类型也是对应上的。就是Query的Schema中是表达式，没法保持一致&#010;&#010;报错信息如下：&#010;[image: image.png]&#010;&#010;关键代码：&#010;HBase sink ddl:&#010;String ddlSource = \"CREATE TABLE&#010;test_hive_catalog.test_dim.dw_common_mobile_device_user_mapping (\\n\" +&#010;                \"  rowkey STRING,\\n\" +&#010;                \"  f1 ROW&lt; \\n\" +&#010;                \"        device_id STRING,\\n\" +&#010;                \"        pass_id STRING,\\n\" +&#010;                \"        first_date STRING,\\n\" +&#010;                \"        first_channel_id STRING,\\n\" +&#010;                \"        first_app_version STRING,\\n\" +&#010;                \"        first_server_time STRING,\\n\" +&#010;                \"        first_server_hour STRING,\\n\" +&#010;                \"        first_ip_location STRING,\\n\" +&#010;                \"        first_login_time STRING,\\n\" +&#010;                \"        sys_can_uninstall STRING,\\n\" +&#010;                \"        update_date STRING,\\n\" +&#010;                \"        server_time BIGINT,\\n\" +&#010;                \"        last_pass_id STRING,\\n\" +&#010;                \"        last_channel_id STRING,\\n\" +&#010;                \"        last_app_version STRING,\\n\" +&#010;                \"        last_date STRING,\\n\" +&#010;                \"        os STRING,\\n\" +&#010;                \"        attribution_channel_id STRING,\\n\" +&#010;                \"        attribution_first_date STRING,\\n\" +&#010;                \"        p_product STRING,\\n\" +&#010;                \"        p_project STRING,\\n\" +&#010;                \"        p_dt STRING\\n\" +&#010;                \"        &gt;\\n\" +&#010;                \") WITH (\\n\" +&#010;                \"  'connector.type' = 'hbase',\\n\" +&#010;                \"  'connector.version' = '1.4.3',\\n\" + //&#010;即使绕过语法编译，换其他版本的hbase，还是有问题，如线上的版本就不行&#010;                \"  'connector.table-name' =&#010;'dw_common_mobile_device_user_mapping_new',\\n\" +&#010;                \"  'connector.zookeeper.quorum' = '\"+ zookeeperServers&#010;+\"',\\n\" +&#010;                \"  'connector.zookeeper.znode.parent' = '/hbase143',\\n\" +&#010;                \"  'connector.write.buffer-flush.max-size' = '2mb',\\n\" +&#010;                \"  'connector.write.buffer-flush.max-rows' = '1000',\\n\" +&#010;                \"  'connector.write.buffer-flush.interval' = '2s'\\n\" +&#010;                \")\";&#010;&#010;insert into sql:&#010;&#010;String bodyAndLocalSql = \"\" +&#010;//                \"insert into&#010;test_hive_catalog.test_dim.dw_common_mobile_device_user_mapping \" +&#010;                \"SELECT CAST(rowkey AS STRING) AS rowkey, \" +&#010;                \" ROW(\" +&#010;                \" device_id, pass_id, first_date, first_channel_id,&#010;first_app_version, first_server_time, first_server_hour, first_ip_location,&#010;first_login_time, sys_can_uninstall, update_date, server_time,&#010;last_pass_id, last_channel_id, last_app_version, last_date, os,&#010;attribution_channel_id, attribution_first_date, p_product, p_project, p_dt&#010;\" +&#010;                \") AS f1\" +&#010;                \" FROM \" +&#010;                \"(\" +&#010;                \" SELECT \" +&#010;                \" MD5(CONCAT_WS('|', kafka.uid, kafka.p_product,&#010;kafka.p_project)) AS rowkey, \" +&#010;                \" kafka.uid AS device_id \" +&#010;                \",kafka.pass_id \" +&#010;&#010;                // first_date&#010;                \",CASE WHEN COALESCE(hbase.server_time, 0) &lt;=&#010;kafka.server_time \" +&#010;                // 新用户&#010;                \" THEN FROM_UNIXTIME(kafka.server_time, 'yyyy-MM-dd') \" +&#010;                // 老用户&#010;                \" ELSE hbase.first_date END AS first_date \" +&#010;&#010;                // first_channel_id&#010;                \",CASE WHEN COALESCE(hbase.server_time, 0) &lt;=&#010;kafka.server_time \" +&#010;                // 新用户&#010;                \" THEN kafka.wlb_channel_id\" +&#010;                // 老用户&#010;                \" ELSE hbase.first_channel_id END AS first_channel_id \" +&#010;&#010;                // first_app_version&#010;                \",CASE WHEN COALESCE(hbase.server_time, 0) &lt;=&#010;kafka.server_time \" +&#010;                // 新用户&#010;                \" THEN kafka.app_version \" +&#010;                // 老用户&#010;                \" ELSE hbase.first_app_version END AS first_app_version \" +&#010;&#010;                // first_server_time&#010;                \",CASE WHEN COALESCE(hbase.server_time, 0) &lt;=&#010;kafka.server_time \" +&#010;                // 新用户&#010;                \" THEN FROM_UNIXTIME(kafka.server_time, 'yyyy-MM-dd&#010;HH:mm:ss') \" +&#010;                // 老用户&#010;                \" ELSE hbase.first_server_time END AS first_server_time \" +&#010;&#010;                // first_server_hour&#010;                \",CASE WHEN COALESCE(hbase.server_time, 0) &lt;=&#010;kafka.server_time \" +&#010;                // 新用户&#010;                \" THEN FROM_UNIXTIME(kafka.server_time, 'HH') \" +&#010;                // 老用户&#010;                \" ELSE hbase.first_server_hour END AS first_server_hour \" +&#010;&#010;                // first_ip_location&#010;                \",CASE WHEN COALESCE(hbase.server_time, 0) &lt;=&#010;kafka.server_time \" +&#010;                // 新用户&#010;                \" THEN kafka.ip_location \" +&#010;                // 老用户&#010;                \" ELSE hbase.first_ip_location END AS first_ip_location \" +&#010;&#010;                // first_login_time&#010;                \",CASE WHEN COALESCE(hbase.server_time, 0) &lt;=&#010;kafka.server_time \" +&#010;                // 新用户&#010;                \" THEN FROM_UNIXTIME(kafka.server_time, 'yyyy-MM-dd&#010;HH:mm:ss') \" +&#010;                // 老用户&#010;                \" ELSE hbase.first_login_time END AS first_login_time \" +&#010;&#010;                \",kafka.sys_can_uninstall \" +&#010;&#010;                // update_date&#010;                \",CASE WHEN hbase.pass_id = 0 \" +&#010;                \" THEN CAST(FROM_UNIXTIME(kafka.server_time, 'yyyy-MM-dd')&#010;AS string) \" +&#010;                \" END AS update_date \" + // VARCHAR(2000)&#010;&#010;                // server_time&#010;                \",kafka.server_time\" +&#010;&#010;                \", kafka.pass_id AS last_pass_id\" +&#010;                \", kafka.wlb_channel_id AS last_channel_id\" +&#010;                \", kafka.app_version AS last_app_version\" +&#010;                \", CAST(FROM_UNIXTIME(kafka.server_time, 'yyyy-MM-dd') AS&#010;STRING) AS last_date\" + // VARCHAR(2000)&#010;                \", kafka.os\" +&#010;                \", hbase.attribution_channel_id\" +&#010;                \", hbase.attribution_first_date\" +&#010;                \", kafka.p_product\" +&#010;                \", kafka.p_project\" +&#010;                \", kafka.p_dt\" +&#010;&#010;                \" FROM test_hive_catalog.test_ods.test_ods_header AS kafka&#010;\" +&#010;                \" FULL JOIN&#010;test_hive_catalog.test_dim.dw_common_mobile_device_user_mapping AS hbase \" +&#010;                \" ON kafka.uid = hbase.device_id \" + // TODO&#010;这里uid，后面要改成device_id&#010;                \" WHERE kafka.is_body=1 AND kafka.is_local=1\" +&#010;                \")\";&#010;&#010;",
        "depth": "0",
        "reply": "<CABMA2UvUOMdTP_LERbUgzQ6J2vyUZZKp=AEuUx2+E3zTEs5zwg@mail.gmail.com>"
    },
    {
        "id": "<2020071611304650705210@163.com>",
        "from": "&quot;amenhub@163.com&quot; &lt;amen...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:30:48 GMT",
        "subject": "Flink-1.11内置connector测试问题求解",
        "content": "hi, everyone&#013;&#010;&#013;&#010;小白在测试flink 1.11新特性新内置的三个connector时，在本地创建图片[1]中的任务并进行数据打印时，控制台只打印了表schema，而没有按内置的datagen&#010;connector规则产生数据，请问可能是什么原因呢？谢谢解答！&#013;&#010;&#013;&#010;&#013;&#010;[1] https://postimg.cc/PprT9XV6&#013;&#010;&#013;&#010;best,&#013;&#010;amenhub&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;amenhub@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<2020071611304650705210@163.com>"
    },
    {
        "id": "<CABi+2jSY6VcwgCbuDt6JEv2i-OYUE63nAJFkW423ahurZwMBvQ@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:36:38 GMT",
        "subject": "Re: Flink-1.11内置connector测试问题求解",
        "content": " tableResult.print需要有checkpoint&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Thu, Jul 16, 2020 at 11:31 AM amenhub@163.com &lt;amenhub@163.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi, everyone&#013;&#010;&gt;&#013;&#010;&gt; 小白在测试flink&#013;&#010;&gt; 1.11新特性新内置的三个connector时，在本地创建图片[1]中的任务并进行数据打印时，控制台只打印了表schema，而没有按内置的datagen&#013;&#010;&gt; connector规则产生数据，请问可能是什么原因呢？谢谢解答！&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; [1] https://postimg.cc/PprT9XV6&#013;&#010;&gt;&#013;&#010;&gt; best,&#013;&#010;&gt; amenhub&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; amenhub@163.com&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<2020071611304650705210@163.com>"
    },
    {
        "id": "<CADQYLGtOpBeboMD7V5Naq0FNLif3Os-Ff8x=hfpw0jPEaV-bqw@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 04:08:55 GMT",
        "subject": "Re: Flink-1.11内置connector测试问题求解",
        "content": "目前 1.11 版本中的 tableResult.print 只支持 exactly once 语义，需要配置 checkpoint。&#013;&#010;&#013;&#010;1.12 里准备支持 at least once 语义，用户可以不用配置 checkpoint。目前 pr&#010;[1] 正在reivew 。&#013;&#010;&#013;&#010;[1] https://github.com/apache/flink/pull/12867&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;Jingsong Li &lt;jingsonglee0@gmail.com&gt; 于2020年7月16日周四 上午11:36写道：&#013;&#010;&#013;&#010;&gt;  tableResult.print需要有checkpoint&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Thu, Jul 16, 2020 at 11:31 AM amenhub@163.com &lt;amenhub@163.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi, everyone&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 小白在测试flink&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.11新特性新内置的三个connector时，在本地创建图片[1]中的任务并进行数据打印时，控制台只打印了表schema，而没有按内置的datagen&#013;&#010;&gt; &gt; connector规则产生数据，请问可能是什么原因呢？谢谢解答！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1] https://postimg.cc/PprT9XV6&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; best,&#013;&#010;&gt; &gt; amenhub&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; amenhub@163.com&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<2020071611304650705210@163.com>"
    },
    {
        "id": "<f3e1eb45-8b1d-4c10-81e3-4784fde833b5.zhengbinbin@heint.cn>",
        "from": "&quot;郑斌斌&quot; &lt;zhengbin...@heint.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 03:42:56 GMT",
        "subject": "FlinkKafkaConsumer API 维表关联",
        "content": "各位好：&#010;&#010;请教一下，用FlinkKafkaConsumer API的话，如何支持SQL的方式，和维表关联。（之前用Kafka&#010;API&amp;StreamTableDescriptor API是可以的）&#010; \"select  a.id,b.name from kafka_table a \"&#010;        + \"join dim_table FOR SYSTEM_TIME AS OF a.proctime as b on a.id = b.user_id\";&#010;&#010;thanks &amp; Regards",
        "depth": "0",
        "reply": "<f3e1eb45-8b1d-4c10-81e3-4784fde833b5.zhengbinbin@heint.cn>"
    },
    {
        "id": "<CAELO930mCKMr_1o+3bZ4gRMDV7xXnORPMs955W5TtWYxwcNCgw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:42:10 GMT",
        "subject": "Re: FlinkKafkaConsumer API 维表关联",
        "content": "你需要用 DDL 去声明这张 kafka 表[1], 目前不建议使用 `Kafka` 和 `StreamTableDescriptor`&#010;API。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;[1]:&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html&#013;&#010;&#013;&#010;On Thu, 16 Jul 2020 at 11:43, 郑斌斌 &lt;zhengbinbin@heint.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; 各位好：&#013;&#010;&gt;&#013;&#010;&gt; 请教一下，用FlinkKafkaConsumer API的话，如何支持SQL的方式，和维表关联。（之前用Kafka&#013;&#010;&gt; API&amp;StreamTableDescriptor API是可以的）&#013;&#010;&gt;  \"select  a.id,b.name from kafka_table a \"&#013;&#010;&gt;         + \"join dim_table FOR SYSTEM_TIME AS OF a.proctime as b on a.id =&#013;&#010;&gt; b.user_id\";&#013;&#010;&gt;&#013;&#010;&gt; thanks &amp; Regards&#013;&#010;",
        "depth": "1",
        "reply": "<f3e1eb45-8b1d-4c10-81e3-4784fde833b5.zhengbinbin@heint.cn>"
    },
    {
        "id": "<3d8d7d3f.3c01.17355cdf87e.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 04:07:08 GMT",
        "subject": "flink sql 1.11 create hive table error",
        "content": "Hi all,&#010;flink sql 1.11 create table 是不是 不支持 IF NOT EXISTS&#010;&#010;&#010;Query：&#010;    val hiveConfDir = \"/etc/hive/conf\" &#010;    val hiveVersion = \"2.1.1\"&#010;&#010;    val odsCatalog = \"odsCatalog\"&#010;    val odsHiveCatalog = new HiveCatalog(odsCatalog, \"ods\", hiveConfDir, hiveVersion)&#010;    streamTableEnv.registerCatalog(odsCatalog, odsHiveCatalog)&#010;&#010;    streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE IF NOT EXISTS odsCatalog.ods.hive_table (&#010;        |  user_id STRING,&#010;        |  age INT&#010;        |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;        |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;        |  'sink.partition-commit.trigger'='partition-time',&#010;        |  'sink.partition-commit.delay'='0s',&#010;        |  'sink.partition-commit.policy.kind'='metastore'&#010;        |)&#010;        |&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;Could not execute application.&#010;&#009;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_161]&#010;&#009;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_161]&#010;&#009;at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]&#010;&#009;at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;[data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;[data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [data-flow-1.0.jar:?]&#010;&#009;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [data-flow-1.0.jar:?]&#010;Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException: Could&#010;not execute application.&#010;&#009;... 11 more&#010;Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method caused&#010;an error: SQL parse failed. Encountered \"NOT\" at line 3, column 17.&#010;Was expecting one of:&#010;    &lt;EOF&gt; &#010;    \"ROW\" ...&#010;    \"COMMENT\" ...&#010;    \"LOCATION\" ...&#010;    \"PARTITIONED\" ...&#010;    \"STORED\" ...&#010;    \"TBLPROPERTIES\" ...&#010;    \"(\" ...&#010;    \".\" ...&#010;    &#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;Caused by: org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered \"NOT\"&#010;at line 3, column 17.&#010;Was expecting one of:&#010;    &lt;EOF&gt; &#010;    \"ROW\" ...&#010;    \"COMMENT\" ...&#010;    \"LOCATION\" ...&#010;    \"PARTITIONED\" ...&#010;    \"STORED\" ...&#010;    \"TBLPROPERTIES\" ...&#010;    \"(\" ...&#010;    \".\" ...&#010;    &#010;&#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[qile-data-flow-1.0.jar:?]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered \"NOT\" at line 3, column&#010;17.&#010;Was expecting one of:&#010;    &lt;EOF&gt; &#010;    \"ROW\" ...&#010;    \"COMMENT\" ...&#010;    \"LOCATION\" ...&#010;    \"PARTITIONED\" ...&#010;    \"STORED\" ...&#010;    \"TBLPROPERTIES\" ...&#010;    \"(\" ...&#010;    \".\" ...&#010;    &#010;&#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.convertException(FlinkHiveSqlParserImpl.java:435)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.normalizeException(FlinkHiveSqlParserImpl.java:220)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:148) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:163) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[data-flow-1.0.jar:?]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[data-flow-1.0.jar:?]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;... 10 more&#010;Caused by: org.apache.flink.sql.parser.hive.impl.ParseException: Encountered \"NOT\" at line&#010;3, column 17.&#010;Was expecting one of:&#010;    &lt;EOF&gt; &#010;    \"ROW\" ...&#010;    \"COMMENT\" ...&#010;    \"LOCATION\" ...&#010;    \"PARTITIONED\" ...&#010;    \"STORED\" ...&#010;    \"TBLPROPERTIES\" ...&#010;    \"(\" ...&#010;    \".\" ...&#010;    &#010;&#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.generateParseException(FlinkHiveSqlParserImpl.java:37347)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.jj_consume_token(FlinkHiveSqlParserImpl.java:37158)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.SqlStmtEof(FlinkHiveSqlParserImpl.java:3962)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.parseSqlStmtEof(FlinkHiveSqlParserImpl.java:267)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:161) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[data-flow-1.0.jar:?]&#010;&#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[data-flow-1.0.jar:?]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0",
        "depth": "0",
        "reply": "<3d8d7d3f.3c01.17355cdf87e.Coremail.wander669@163.com>"
    },
    {
        "id": "<D2C48DC9-E207-4235-9619-A03EF1CC8275@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 04:16:40 GMT",
        "subject": "Re: flink sql 1.11 create hive table error",
        "content": "Hello, Zach&#010;&#010;是的, 1.12 会支持，PR[1]已经开了，在review中。&#010;&#010;祝好，&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18588 &lt;https://issues.apache.org/jira/browse/FLINK-18588&gt;&#010;&gt; 在 2020年7月16日，12:07，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi all,&#010;&gt; flink sql 1.11 create table 是不是 不支持 IF NOT EXISTS&#010;&gt; &#010;&gt; &#010;&gt; Query：&#010;&gt;    val hiveConfDir = \"/etc/hive/conf\" &#010;&gt;    val hiveVersion = \"2.1.1\"&#010;&gt; &#010;&gt;    val odsCatalog = \"odsCatalog\"&#010;&gt;    val odsHiveCatalog = new HiveCatalog(odsCatalog, \"ods\", hiveConfDir, hiveVersion)&#010;&gt;    streamTableEnv.registerCatalog(odsCatalog, odsHiveCatalog)&#010;&gt; &#010;&gt;    streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE IF NOT EXISTS odsCatalog.ods.hive_table (&#010;&gt;        |  user_id STRING,&#010;&gt;        |  age INT&#010;&gt;        |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;&gt;        |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;        |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;        |  'sink.partition-commit.delay'='0s',&#010;&gt;        |  'sink.partition-commit.policy.kind'='metastore'&#010;&gt;        |)&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin)&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;Could not execute application.&#010;&gt; &#009;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_161]&#010;&gt; &#009;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_161]&#010;&gt; &#009;at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943) ~[?:1.8.0_161]&#010;&gt; &#009;at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;~[?:1.8.0_161]&#010;&gt; &#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) ~[?:1.8.0_161]&#010;&gt; &#009;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_161]&#010;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_161]&#010;&gt; &#009;at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]&#010;&gt; &#009;at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;[data-flow-1.0.jar:?]&#010;&gt; &#009;at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [data-flow-1.0.jar:?]&#010;&gt; &#009;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;[data-flow-1.0.jar:?]&#010;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [data-flow-1.0.jar:?]&#010;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [data-flow-1.0.jar:?]&#010;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [data-flow-1.0.jar:?]&#010;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [data-flow-1.0.jar:?]&#010;&gt; Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;Could not execute application.&#010;&gt; &#009;... 11 more&#010;&gt; Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method&#010;caused an error: SQL parse failed. Encountered \"NOT\" at line 3, column 17.&#010;&gt; Was expecting one of:&#010;&gt;    &lt;EOF&gt; &#010;&gt;    \"ROW\" ...&#010;&gt;    \"COMMENT\" ...&#010;&gt;    \"LOCATION\" ...&#010;&gt;    \"PARTITIONED\" ...&#010;&gt;    \"STORED\" ...&#010;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;    \"(\" ...&#010;&gt;    \".\" ...&#010;&gt; &#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;... 10 more&#010;&gt; Caused by: org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered&#010;\"NOT\" at line 3, column 17.&#010;&gt; Was expecting one of:&#010;&gt;    &lt;EOF&gt; &#010;&gt;    \"ROW\" ...&#010;&gt;    \"COMMENT\" ...&#010;&gt;    \"LOCATION\" ...&#010;&gt;    \"PARTITIONED\" ...&#010;&gt;    \"STORED\" ...&#010;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;    \"(\" ...&#010;&gt;    \".\" ...&#010;&gt; &#010;&gt; &#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[qile-data-flow-1.0.jar:?]&#010;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[qile-data-flow-1.0.jar:?]&#010;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&gt; &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&gt; &#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;... 10 more&#010;&gt; Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered \"NOT\" at line&#010;3, column 17.&#010;&gt; Was expecting one of:&#010;&gt;    &lt;EOF&gt; &#010;&gt;    \"ROW\" ...&#010;&gt;    \"COMMENT\" ...&#010;&gt;    \"LOCATION\" ...&#010;&gt;    \"PARTITIONED\" ...&#010;&gt;    \"STORED\" ...&#010;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;    \"(\" ...&#010;&gt;    \".\" ...&#010;&gt; &#010;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.convertException(FlinkHiveSqlParserImpl.java:435)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.normalizeException(FlinkHiveSqlParserImpl.java:220)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:148) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:163) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[data-flow-1.0.jar:?]&#010;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[data-flow-1.0.jar:?]&#010;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&gt; &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&gt; &#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;... 10 more&#010;&gt; Caused by: org.apache.flink.sql.parser.hive.impl.ParseException: Encountered \"NOT\" at&#010;line 3, column 17.&#010;&gt; Was expecting one of:&#010;&gt;    &lt;EOF&gt; &#010;&gt;    \"ROW\" ...&#010;&gt;    \"COMMENT\" ...&#010;&gt;    \"LOCATION\" ...&#010;&gt;    \"PARTITIONED\" ...&#010;&gt;    \"STORED\" ...&#010;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;    \"(\" ...&#010;&gt;    \".\" ...&#010;&gt; &#010;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.generateParseException(FlinkHiveSqlParserImpl.java:37347)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.jj_consume_token(FlinkHiveSqlParserImpl.java:37158)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.SqlStmtEof(FlinkHiveSqlParserImpl.java:3962)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.parseSqlStmtEof(FlinkHiveSqlParserImpl.java:267)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:161) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[data-flow-1.0.jar:?]&#010;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[data-flow-1.0.jar:?]&#010;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]&#010;&gt; &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&gt; &#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<3d8d7d3f.3c01.17355cdf87e.Coremail.wander669@163.com>"
    },
    {
        "id": "<1018567c.3da4.17355f380aa.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 04:48:08 GMT",
        "subject": "Re:Re: flink sql 1.11 create hive table error",
        "content": "hi Leonard,&#010;感谢答疑！&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;At 2020-07-16 12:16:40, \"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; wrote:&#010;&gt;Hello, Zach&#010;&gt;&#010;&gt;是的, 1.12 会支持，PR[1]已经开了，在review中。&#010;&gt;&#010;&gt;祝好，&#010;&gt;Leonard Xu&#010;&gt;[1] https://issues.apache.org/jira/browse/FLINK-18588 &lt;https://issues.apache.org/jira/browse/FLINK-18588&gt;&#010;&gt;&gt; 在 2020年7月16日，12:07，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; Hi all,&#010;&gt;&gt; flink sql 1.11 create table 是不是 不支持 IF NOT EXISTS&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; Query：&#010;&gt;&gt;    val hiveConfDir = \"/etc/hive/conf\" &#010;&gt;&gt;    val hiveVersion = \"2.1.1\"&#010;&gt;&gt; &#010;&gt;&gt;    val odsCatalog = \"odsCatalog\"&#010;&gt;&gt;    val odsHiveCatalog = new HiveCatalog(odsCatalog, \"ods\", hiveConfDir, hiveVersion)&#010;&gt;&gt;    streamTableEnv.registerCatalog(odsCatalog, odsHiveCatalog)&#010;&gt;&gt; &#010;&gt;&gt;    streamTableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)&#010;&gt;&gt;    streamTableEnv.executeSql(&#010;&gt;&gt;      \"\"\"&#010;&gt;&gt;        |&#010;&gt;&gt;        |CREATE TABLE IF NOT EXISTS odsCatalog.ods.hive_table (&#010;&gt;&gt;        |  user_id STRING,&#010;&gt;&gt;        |  age INT&#010;&gt;&gt;        |) PARTITIONED BY (dt STRING, hr STRING) STORED AS parquet TBLPROPERTIES (&#010;&gt;&gt;        |  'partition.time-extractor.timestamp-pattern'='$dt $hr:00:00',&#010;&gt;&gt;        |  'sink.partition-commit.trigger'='partition-time',&#010;&gt;&gt;        |  'sink.partition-commit.delay'='0s',&#010;&gt;&gt;        |  'sink.partition-commit.policy.kind'='metastore'&#010;&gt;&gt;        |)&#010;&gt;&gt;        |&#010;&gt;&gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; java.util.concurrent.CompletionException: org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;Could not execute application.&#010;&gt;&gt; &#009;at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:943)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:245)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.lambda$runApplicationAsync$1(ApplicationDispatcherBootstrap.java:199)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_161]&#010;&gt;&gt; &#009;at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_161]&#010;&gt;&gt; &#009;at org.apache.flink.runtime.concurrent.akka.ActorSystemScheduledExecutorAdapter$ScheduledFutureTask.run(ActorSystemScheduledExecutorAdapter.java:154)&#010;[data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44)&#010;[data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;[data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&#010;[data-flow-1.0.jar:?]&#010;&gt;&gt; Caused by: org.apache.flink.client.deployment.application.ApplicationExecutionException:&#010;Could not execute application.&#010;&gt;&gt; &#009;... 11 more&#010;&gt;&gt; Caused by: org.apache.flink.client.program.ProgramInvocationException: The main method&#010;caused an error: SQL parse failed. Encountered \"NOT\" at line 3, column 17.&#010;&gt;&gt; Was expecting one of:&#010;&gt;&gt;    &lt;EOF&gt; &#010;&gt;&gt;    \"ROW\" ...&#010;&gt;&gt;    \"COMMENT\" ...&#010;&gt;&gt;    \"LOCATION\" ...&#010;&gt;&gt;    \"PARTITIONED\" ...&#010;&gt;&gt;    \"STORED\" ...&#010;&gt;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;&gt;    \"(\" ...&#010;&gt;&gt;    \".\" ...&#010;&gt;&gt; &#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:302)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;... 10 more&#010;&gt;&gt; Caused by: org.apache.flink.table.api.SqlParserException: SQL parse failed. Encountered&#010;\"NOT\" at line 3, column 17.&#010;&gt;&gt; Was expecting one of:&#010;&gt;&gt;    &lt;EOF&gt; &#010;&gt;&gt;    \"ROW\" ...&#010;&gt;&gt;    \"COMMENT\" ...&#010;&gt;&gt;    \"LOCATION\" ...&#010;&gt;&gt;    \"PARTITIONED\" ...&#010;&gt;&gt;    \"STORED\" ...&#010;&gt;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;&gt;    \"(\" ...&#010;&gt;&gt;    \".\" ...&#010;&gt;&gt; &#010;&gt;&gt; &#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:56)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[qile-data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&gt;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;... 10 more&#010;&gt;&gt; Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered \"NOT\" at&#010;line 3, column 17.&#010;&gt;&gt; Was expecting one of:&#010;&gt;&gt;    &lt;EOF&gt; &#010;&gt;&gt;    \"ROW\" ...&#010;&gt;&gt;    \"COMMENT\" ...&#010;&gt;&gt;    \"LOCATION\" ...&#010;&gt;&gt;    \"PARTITIONED\" ...&#010;&gt;&gt;    \"STORED\" ...&#010;&gt;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;&gt;    \"(\" ...&#010;&gt;&gt;    \".\" ...&#010;&gt;&gt; &#010;&gt;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.convertException(FlinkHiveSqlParserImpl.java:435)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.normalizeException(FlinkHiveSqlParserImpl.java:220)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.handleException(SqlParser.java:148) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:163) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&gt;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;... 10 more&#010;&gt;&gt; Caused by: org.apache.flink.sql.parser.hive.impl.ParseException: Encountered \"NOT\"&#010;at line 3, column 17.&#010;&gt;&gt; Was expecting one of:&#010;&gt;&gt;    &lt;EOF&gt; &#010;&gt;&gt;    \"ROW\" ...&#010;&gt;&gt;    \"COMMENT\" ...&#010;&gt;&gt;    \"LOCATION\" ...&#010;&gt;&gt;    \"PARTITIONED\" ...&#010;&gt;&gt;    \"STORED\" ...&#010;&gt;&gt;    \"TBLPROPERTIES\" ...&#010;&gt;&gt;    \"(\" ...&#010;&gt;&gt;    \".\" ...&#010;&gt;&gt; &#010;&gt;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.generateParseException(FlinkHiveSqlParserImpl.java:37347)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.jj_consume_token(FlinkHiveSqlParserImpl.java:37158)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.SqlStmtEof(FlinkHiveSqlParserImpl.java:3962)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.sql.parser.hive.impl.FlinkHiveSqlParserImpl.parseSqlStmtEof(FlinkHiveSqlParserImpl.java:267)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseQuery(SqlParser.java:161) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.calcite.sql.parser.SqlParser.parseStmt(SqlParser.java:188) ~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.planner.calcite.CalciteParser.parse(CalciteParser.java:54)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:76)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:678)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase$.main(FromKafkaSinkHiveAndHbase.scala:31)&#010;~[data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at cn.ibobei.qile.dataflow.sql.FromKafkaSinkHiveAndHbase.main(FromKafkaSinkHiveAndHbase.scala)&#010;~[data-flow-1.0.jar:?]&#010;&gt;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]&#010;&gt;&gt; &#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;~[?:1.8.0_161]&#010;&gt;&gt; &#009;at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]&#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:288)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:198)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:149) ~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&gt; &#009;at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrap.runApplicationEntryPoint(ApplicationDispatcherBootstrap.java:230)&#010;~[flink-clients_2.11-1.11.0.jar:1.11.0]&#010;&gt;&#010;",
        "depth": "2",
        "reply": "<3d8d7d3f.3c01.17355cdf87e.Coremail.wander669@163.com>"
    },
    {
        "id": "<CAHOPYeVU3Th3=qTibsDH5=9ufv-0EFJQT9+3_dyicSoRGVUYnA@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 05:27:40 GMT",
        "subject": "[sql-client] 通过sql-client提交sql怎么设置checkpointing.interval",
        "content": "hi flink users&#013;&#010;&#013;&#010;通过sql-client提交sql怎么设置checkpointing.interval？&#013;&#010;我看了一下sql-client-defaults.yaml中的execution， 并没有发现这个参数。请教大家一下。&#013;&#010;谢谢&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "0",
        "reply": "<CAHOPYeVU3Th3=qTibsDH5=9ufv-0EFJQT9+3_dyicSoRGVUYnA@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGvzkmrj46654FGo+R_HdGa620Vj35GRYYCBUuvtqXkNUg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 05:36:24 GMT",
        "subject": "Re: [sql-client] 通过sql-client提交sql怎么设置checkpointing.interval",
        "content": "现在还不支持在sql-client-defaults.yaml 里配置 checkpointing.interval,&#013;&#010;你可以配置在flink-conf.yaml里&#013;&#010;&#013;&#010;Harold.Miao &lt;miaohonghit@gmail.com&gt; 于2020年7月16日周四 下午1:27写道：&#013;&#010;&#013;&#010;&gt; hi flink users&#013;&#010;&gt;&#013;&#010;&gt; 通过sql-client提交sql怎么设置checkpointing.interval？&#013;&#010;&gt; 我看了一下sql-client-defaults.yaml中的execution， 并没有发现这个参数。请教大家一下。&#013;&#010;&gt; 谢谢&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best Regards,&#013;&#010;&gt; Harold Miao&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAHOPYeVU3Th3=qTibsDH5=9ufv-0EFJQT9+3_dyicSoRGVUYnA@mail.gmail.com>"
    },
    {
        "id": "<CAHOPYeX+-P4RA+wVYcZHmwyD=CfosgJzOGSSRvhv479cnphoTg@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 05:38:24 GMT",
        "subject": "Re: [sql-client] 通过sql-client提交sql怎么设置checkpointing.interval",
        "content": "是在flink-conf.yaml里面配置这个参数吗&#013;&#010;execution.checkpointing.interval&#013;&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/config.html#execution-checkpointing-interval&gt;&#013;&#010;&#013;&#010;godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月16日周四 下午1:37写道：&#013;&#010;&#013;&#010;&gt; 现在还不支持在sql-client-defaults.yaml 里配置 checkpointing.interval,&#013;&#010;&gt; 你可以配置在flink-conf.yaml里&#013;&#010;&gt;&#013;&#010;&gt; Harold.Miao &lt;miaohonghit@gmail.com&gt; 于2020年7月16日周四 下午1:27写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi flink users&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 通过sql-client提交sql怎么设置checkpointing.interval？&#013;&#010;&gt; &gt; 我看了一下sql-client-defaults.yaml中的execution， 并没有发现这个参数。请教大家一下。&#013;&#010;&gt; &gt; 谢谢&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best Regards,&#013;&#010;&gt; &gt; Harold Miao&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "2",
        "reply": "<CAHOPYeVU3Th3=qTibsDH5=9ufv-0EFJQT9+3_dyicSoRGVUYnA@mail.gmail.com>"
    },
    {
        "id": "<2020071616045599749946@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:06:02 GMT",
        "subject": "Re: [sql-client] 通过sql-client提交sql怎么设置checkpointing.interval",
        "content": "&#013;&#010;直接在 flink-conf.yaml 文件中加配置&#013;&#010;execution.checkpointing.interval: 60000&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;    &#013;&#010; &#013;&#010;Sender: Harold.Miao&#013;&#010;Send Time: 2020-07-16 13:27&#013;&#010;Receiver: user-zh&#013;&#010;Subject: [sql-client] 通过sql-client提交sql怎么设置checkpointing.interval&#013;&#010;hi flink users&#013;&#010; &#013;&#010;通过sql-client提交sql怎么设置checkpointing.interval？&#013;&#010;我看了一下sql-client-defaults.yaml中的execution， 并没有发现这个参数。请教大家一下。&#013;&#010;谢谢&#013;&#010; &#013;&#010; &#013;&#010; &#013;&#010;-- &#013;&#010; &#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "1",
        "reply": "<CAHOPYeVU3Th3=qTibsDH5=9ufv-0EFJQT9+3_dyicSoRGVUYnA@mail.gmail.com>"
    },
    {
        "id": "<tencent_E05BB0BB1E6A2A50813A256A9417FF10B80A@qq.com>",
        "from": "&quot;jiafu&quot; &lt;530496...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 05:48:48 GMT",
        "subject": "Trying to eagerly schedule a task whose inputs are not ready",
        "content": "flink任务运行时会有以下的报错&#013;&#010;org.apache.flink.runtime.executiongraph.ExecutionGraphException: Trying to eagerly schedule&#010;a task whose inputs are not ready (result type: PIPELINED_BOUNDED, partition consumable: false,&#010;producer state: SCHEDULED, producer slot: null). &#009;at org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor.fromEdges(InputChannelDeploymentDescriptor.java:145)&#010;&#009;at org.apache.flink.runtime.executiongraph.ExecutionVertex.createDeploymentDescriptor(ExecutionVertex.java:840)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.deploy(Execution.java:621) &#009;at org.apache.flink.util.function.ThrowingRunnable.lambda$unchecked$0(ThrowingRunnable.java:50)&#010;&#009;at java.util.concurrent.CompletableFuture.uniRun(CompletableFuture.java:705) &#009;at java.util.concurrent.CompletableFuture.uniRunStage(CompletableFuture.java:717)&#010;&#009;at java.util.concurrent.CompletableFuture.thenRun(CompletableFuture.java:2010) &#009;at org.apache.flink.runtime.executiongraph.Execution.scheduleForExecution(Execution.java:436)&#010;&#009;at org.apache.flink.runtime.executiongraph.ExecutionVertex.scheduleForExecution(ExecutionVertex.java:637)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.restart(FailoverRegion.java:229)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.reset(FailoverRegion.java:186)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.allVerticesInTerminalState(FailoverRegion.java:96)&#010;&#009;at org.apache.flink.runtime.executiongraph.failover.FailoverRegion.lambda$cancel$0(FailoverRegion.java:146)&#010;&#009;at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:656) &#009;at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632)&#010;&#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) &#009;at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)&#010;&#009;at org.apache.flink.runtime.concurrent.FutureUtils$WaitingConjunctFuture.handleCompletedFuture(FutureUtils.java:633)&#010;&#009;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) &#009;at&#010;java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)&#010;&#009;at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) &#009;at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1962)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.lambda$releaseAssignedResource$11(Execution.java:1350)&#010;&#009;at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760) &#009;at&#010;java.util.concurrent.CompletableFuture.uniWhenCompleteStage(CompletableFuture.java:778) &#009;at&#010;java.util.concurrent.CompletableFuture.whenComplete(CompletableFuture.java:2140) &#009;at org.apache.flink.runtime.executiongraph.Execution.releaseAssignedResource(Execution.java:1345)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.finishCancellation(Execution.java:1115)&#010;&#009;at org.apache.flink.runtime.executiongraph.Execution.completeCancelling(Execution.java:1094)&#010;&#009;at org.apache.flink.runtime.executiongraph.ExecutionGraph.updateState(ExecutionGraph.java:1628)&#010;&#009;at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:517)&#010;&#009;at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source) &#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498) &#009;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:274)&#010;&#009;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:189)&#010;&#009;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:74)&#010;&#009;at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:147) &#009;at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.onReceive(FencedAkkaRpcActor.java:40)&#010;&#009;at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165) &#009;at akka.actor.Actor$class.aroundReceive(Actor.scala:502)&#010;&#009;at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95) &#009;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)&#010;&#009;at akka.actor.ActorCell.invoke(ActorCell.scala:495) &#009;at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)&#010;&#009;at akka.dispatch.Mailbox.run(Mailbox.scala:224) &#009;at akka.dispatch.Mailbox.exec(Mailbox.scala:234)&#010;&#009;at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) &#009;at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&#010;&#009;at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) &#009;at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",
        "depth": "0",
        "reply": "<tencent_E05BB0BB1E6A2A50813A256A9417FF10B80A@qq.com>"
    },
    {
        "id": "<46616561.542f.1735664625e.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 06:51:26 GMT",
        "subject": "flink1.11 set yarn slots failed",
        "content": "Hi all,&#010;&#010;&#010;使用如下命令，设置Number of slots per TaskManager&#010;/opt/flink-1.11.0/bin/flink run-application -t yarn-application \\&#010;-Djobmanager.memory.process.size=1024m \\&#010;-Dtaskmanager.memory.process.size=2048m \\&#010; -ys 4 \\&#010;&#010;&#010;发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#010;of slots per TaskManager？&#010;另外，有哪些方式可以增Flink UI中的大Available Task Slots的值，现在每次提交作业都是0",
        "depth": "0",
        "reply": "<46616561.542f.1735664625e.Coremail.wander669@163.com>"
    },
    {
        "id": "<CAP+gf37_QypPYd11wTM2D=rp=Xe4BYh8+Cc5Jj2kmHvR924=dg@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 07:04:19 GMT",
        "subject": "Re: flink1.11 set yarn slots failed",
        "content": "-t是新引入的参数，是不支持以前的-yxxx参数的&#013;&#010;你需要使用-Dtaskmanager.numberOfTaskSlots=4这样来设置&#013;&#010;&#013;&#010;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月16日周四 下午2:51写道：&#013;&#010;&#013;&#010;&gt; Hi all,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 使用如下命令，设置Number of slots per TaskManager&#013;&#010;&gt; /opt/flink-1.11.0/bin/flink run-application -t yarn-application \\&#013;&#010;&gt; -Djobmanager.memory.process.size=1024m \\&#013;&#010;&gt; -Dtaskmanager.memory.process.size=2048m \\&#013;&#010;&gt;  -ys 4 \\&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#013;&#010;&gt; of slots per TaskManager？&#013;&#010;&gt; 另外，有哪些方式可以增Flink UI中的大Available Task Slots的值，现在每次提交作业都是0&#013;&#010;",
        "depth": "1",
        "reply": "<46616561.542f.1735664625e.Coremail.wander669@163.com>"
    },
    {
        "id": "<tencent_4339BB19B17BCD02C14A4132@qq.com>",
        "from": "&quot;flinkcx&quot;&lt;flin...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 07:03:14 GMT",
        "subject": "回复：flink1.11 set yarn slots failed",
        "content": "是不是应该用-D作为前缀来设置,比如-Dtaskmanager.numberOfTaskSlots=4&#010;&#010;&#010; 原始邮件 &#010;发件人: Zhou Zach&lt;wander669@163.com&gt;&#010;收件人: Flink user-zh mailing list&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月16日(周四) 14:51&#010;主题: flink1.11 set yarn slots failed&#010;&#010;&#010;Hi all, 使用如下命令，设置Number of slots per TaskManager /opt/flink-1.11.0/bin/flink&#010;run-application -t yarn-application \\ -Djobmanager.memory.process.size=1024m \\ -Dtaskmanager.memory.process.size=2048m&#010;\\ -ys 4 \\ 发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#010;of slots per TaskManager？ 另外，有哪些方式可以增Flink UI中的大Available Task&#010;Slots的值，现在每次提交作业都是0",
        "depth": "0",
        "reply": "<tencent_4339BB19B17BCD02C14A4132@qq.com>"
    },
    {
        "id": "<3c168e08.5e86.1735685dc89.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 07:28:00 GMT",
        "subject": "Re:回复：flink1.11 set yarn slots failed",
        "content": "-D前缀好使，要设置yarn name用什么参数啊，1.11官网的文档有些都不好使了&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-16 15:03:14，\"flinkcx\" &lt;flinkcx@163.com&gt; 写道：&#010;&gt;是不是应该用-D作为前缀来设置,比如-Dtaskmanager.numberOfTaskSlots=4&#010;&gt;&#010;&gt;&#010;&gt; 原始邮件 &#010;&gt;发件人: Zhou Zach&lt;wander669@163.com&gt;&#010;&gt;收件人: Flink user-zh mailing list&lt;user-zh@flink.apache.org&gt;&#010;&gt;发送时间: 2020年7月16日(周四) 14:51&#010;&gt;主题: flink1.11 set yarn slots failed&#010;&gt;&#010;&gt;&#010;&gt;Hi all, 使用如下命令，设置Number of slots per TaskManager /opt/flink-1.11.0/bin/flink&#010;run-application -t yarn-application \\ -Djobmanager.memory.process.size=1024m \\ -Dtaskmanager.memory.process.size=2048m&#010;\\ -ys 4 \\ 发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#010;of slots per TaskManager？ 另外，有哪些方式可以增Flink UI中的大Available Task&#010;Slots的值，现在每次提交作业都是0&#010;",
        "depth": "1",
        "reply": "<tencent_4339BB19B17BCD02C14A4132@qq.com>"
    },
    {
        "id": "<CA+Lep4bHk39D70SDg=r-sDBFcAgVJB3VXK2mrQ-WPQEuYgxobQ@mail.gmail.com>",
        "from": "Rainie Li &lt;raini...@pinterest.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 07:46:46 GMT",
        "subject": "Flink yarn session exception",
        "content": "大佬们好，我是flink新手，正在用flink 1.9.1&#010;Flink APP cannot run, APP log error,  想求教一下会是什么原因造成的，多谢🙏&#010;&#010;2020-06-16 17:06:21,921 WARN  org.apache.flink.client.cli.CliFrontend&#010;                - Could not load CLI class&#010;org.apache.flink.yarn.cli.FlinkYarnSessionCli.&#010;java.lang.NoClassDefFoundError:&#010;org/apache/hadoop/yarn/exceptions/YarnException&#010;at java.lang.Class.forName0(Native Method)&#010;at java.lang.Class.forName(Class.java:264)&#010;at&#010;org.apache.flink.client.cli.CliFrontend.loadCustomCommandLine(CliFrontend.java:1185)&#010;at&#010;org.apache.flink.client.cli.CliFrontend.loadCustomCommandLines(CliFrontend.java:1145)&#010;at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1070)&#010;Caused by: java.lang.ClassNotFoundException:&#010;org.apache.hadoop.yarn.exceptions.YarnException&#010;at java.net.URLClassLoader.findClass(URLClassLoader.java:382)&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#010;at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;... 5 more&#010;2020-06-16 17:06:21,980 INFO  org.apache.flink.core.fs.FileSystem&#010;                - Hadoop is not in the classpath/dependencies. The extended&#010;set of supported File Systems via Hadoop is not available.&#010;&#010;",
        "depth": "0",
        "reply": "<CA+Lep4bHk39D70SDg=r-sDBFcAgVJB3VXK2mrQ-WPQEuYgxobQ@mail.gmail.com>"
    },
    {
        "id": "<ABADEB88-1EB5-4585-895C-BEBB37789529@gmail.com>",
        "from": "Paul Lam &lt;paullin3...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:12:29 GMT",
        "subject": "Re: Flink yarn session exception",
        "content": "日志里说得比较清楚了，classpath 里没有 Hadoop 的 lib。可以参考这个文档&#010;[1] 来配置你的环境。&#010;&#010;1. https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&gt;&#010;Best,&#010;Paul Lam&#010;&#010;&gt; 2020年7月16日 15:46，Rainie Li &lt;rainieli@pinterest.com.INVALID&gt; 写道：&#010;&gt; &#010;&gt; 大佬们好，我是flink新手，正在用flink 1.9.1&#010;&gt; Flink APP cannot run, APP log error,  想求教一下会是什么原因造成的，多谢🙏&#010;&gt; &#010;&gt; 2020-06-16 17:06:21,921 WARN  org.apache.flink.client.cli.CliFrontend&#010;&gt;                - Could not load CLI class&#010;&gt; org.apache.flink.yarn.cli.FlinkYarnSessionCli.&#010;&gt; java.lang.NoClassDefFoundError:&#010;&gt; org/apache/hadoop/yarn/exceptions/YarnException&#010;&gt; at java.lang.Class.forName0(Native Method)&#010;&gt; at java.lang.Class.forName(Class.java:264)&#010;&gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLine(CliFrontend.java:1185)&#010;&gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLines(CliFrontend.java:1145)&#010;&gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1070)&#010;&gt; Caused by: java.lang.ClassNotFoundException:&#010;&gt; org.apache.hadoop.yarn.exceptions.YarnException&#010;&gt; at java.net.URLClassLoader.findClass(URLClassLoader.java:382)&#010;&gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;&gt; at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#010;&gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;&gt; ... 5 more&#010;&gt; 2020-06-16 17:06:21,980 INFO  org.apache.flink.core.fs.FileSystem&#010;&gt;                - Hadoop is not in the classpath/dependencies. The extended&#010;&gt; set of supported File Systems via Hadoop is not available.&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<CA+Lep4bHk39D70SDg=r-sDBFcAgVJB3VXK2mrQ-WPQEuYgxobQ@mail.gmail.com>"
    },
    {
        "id": "<CA+Lep4a8U=kTs83ketQJfaxhc0Me127=AJERgJwyCK9d-qJmYg@mail.gmail.com>",
        "from": "Rainie Li &lt;raini...@pinterest.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 17:06:14 GMT",
        "subject": "Re: Flink yarn session exception",
        "content": "多谢，我set了这些envs:&#010;&#010;export JAVA_HOME=/usr/lib/jvm/java-8-oracle&#010;export PATH=$JAVA_HOME/bin:$PATH&#010;export HADOOP_HOME=/usr/local/hadoop&#010;export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop&#010;export HADOOP_CLASSPATH=`hadoop classpath`&#010;export FLINK_CONF_DIR=/etc/flink-1.9.1/conf&#010;export FLINK_LOG_DIR=/home/rainieli/&#010;&#010;有什么问题吗？🙏&#010;&#010;&#010;On Thu, Jul 16, 2020 at 1:12 AM Paul Lam &lt;paullin3280@gmail.com&gt; wrote:&#010;&#010;&gt; 日志里说得比较清楚了，classpath 里没有 Hadoop 的 lib。可以参考这个文档&#010;[1] 来配置你的环境。&#010;&gt;&#010;&gt; 1.&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&#010;&gt; &lt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&#010;&gt; &gt;&#010;&gt; Best,&#010;&gt; Paul Lam&#010;&gt;&#010;&gt; &gt; 2020年7月16日 15:46，Rainie Li &lt;rainieli@pinterest.com.INVALID&gt; 写道：&#010;&gt; &gt;&#010;&gt; &gt; 大佬们好，我是flink新手，正在用flink 1.9.1&#010;&gt; &gt; Flink APP cannot run, APP log error,  想求教一下会是什么原因造成的，多谢🙏&#010;&gt; &gt;&#010;&gt; &gt; 2020-06-16 17:06:21,921 WARN  org.apache.flink.client.cli.CliFrontend&#010;&gt; &gt;                - Could not load CLI class&#010;&gt; &gt; org.apache.flink.yarn.cli.FlinkYarnSessionCli.&#010;&gt; &gt; java.lang.NoClassDefFoundError:&#010;&gt; &gt; org/apache/hadoop/yarn/exceptions/YarnException&#010;&gt; &gt; at java.lang.Class.forName0(Native Method)&#010;&gt; &gt; at java.lang.Class.forName(Class.java:264)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLine(CliFrontend.java:1185)&#010;&gt; &gt; at&#010;&gt; &gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLines(CliFrontend.java:1145)&#010;&gt; &gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1070)&#010;&gt; &gt; Caused by: java.lang.ClassNotFoundException:&#010;&gt; &gt; org.apache.hadoop.yarn.exceptions.YarnException&#010;&gt; &gt; at java.net.URLClassLoader.findClass(URLClassLoader.java:382)&#010;&gt; &gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;&gt; &gt; at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#010;&gt; &gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;&gt; &gt; ... 5 more&#010;&gt; &gt; 2020-06-16 17:06:21,980 INFO  org.apache.flink.core.fs.FileSystem&#010;&gt; &gt;                - Hadoop is not in the classpath/dependencies. The&#010;&gt; extended&#010;&gt; &gt; set of supported File Systems via Hadoop is not available.&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CA+Lep4bHk39D70SDg=r-sDBFcAgVJB3VXK2mrQ-WPQEuYgxobQ@mail.gmail.com>"
    },
    {
        "id": "<tencent_CEE39731DDFB6656B85FF3F64D6CAF871A06@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 00:31:59 GMT",
        "subject": "Re: Flink yarn session exception",
        "content": "你可以看看lib里面的包跟官网的要求是不是一样的&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;发自我的iPhone&#013;&#010;&#013;&#010;&#013;&#010;------------------ Original ------------------&#013;&#010;From: Rainie Li &lt;rainieli@pinterest.com.INVALID&amp;gt;&#013;&#010;Date: Fri,Jul 17,2020 1:06 AM&#013;&#010;To: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;Subject: Re: Flink yarn session exception&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;多谢，我set了这些envs:&#013;&#010;&#013;&#010;export JAVA_HOME=/usr/lib/jvm/java-8-oracle&#013;&#010;export PATH=$JAVA_HOME/bin:$PATH&#013;&#010;export HADOOP_HOME=/usr/local/hadoop&#013;&#010;export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop&#013;&#010;export HADOOP_CLASSPATH=`hadoop classpath`&#013;&#010;export FLINK_CONF_DIR=/etc/flink-1.9.1/conf&#013;&#010;export FLINK_LOG_DIR=/home/rainieli/&#013;&#010;&#013;&#010;有什么问题吗？🙏&#013;&#010;&#013;&#010;&#013;&#010;On Thu, Jul 16, 2020 at 1:12 AM Paul Lam &lt;paullin3280@gmail.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; 日志里说得比较清楚了，classpath 里没有 Hadoop 的 lib。可以参考这个文档&#010;[1] 来配置你的环境。&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 1.&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&#013;&#010;&amp;gt; &lt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; Best,&#013;&#010;&amp;gt; Paul Lam&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 2020年7月16日 15:46，Rainie Li &lt;rainieli@pinterest.com.INVALID&amp;gt;&#010;写道：&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 大佬们好，我是flink新手，正在用flink 1.9.1&#013;&#010;&amp;gt; &amp;gt; Flink APP cannot run, APP log error,&amp;nbsp; 想求教一下会是什么原因造成的，多谢🙏&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 2020-06-16 17:06:21,921 WARN&amp;nbsp; org.apache.flink.client.cli.CliFrontend&#013;&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;- Could not load CLI class&#013;&#010;&amp;gt; &amp;gt; org.apache.flink.yarn.cli.FlinkYarnSessionCli.&#013;&#010;&amp;gt; &amp;gt; java.lang.NoClassDefFoundError:&#013;&#010;&amp;gt; &amp;gt; org/apache/hadoop/yarn/exceptions/YarnException&#013;&#010;&amp;gt; &amp;gt; at java.lang.Class.forName0(Native Method)&#013;&#010;&amp;gt; &amp;gt; at java.lang.Class.forName(Class.java:264)&#013;&#010;&amp;gt; &amp;gt; at&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLine(CliFrontend.java:1185)&#013;&#010;&amp;gt; &amp;gt; at&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLines(CliFrontend.java:1145)&#013;&#010;&amp;gt; &amp;gt; at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1070)&#013;&#010;&amp;gt; &amp;gt; Caused by: java.lang.ClassNotFoundException:&#013;&#010;&amp;gt; &amp;gt; org.apache.hadoop.yarn.exceptions.YarnException&#013;&#010;&amp;gt; &amp;gt; at java.net.URLClassLoader.findClass(URLClassLoader.java:382)&#013;&#010;&amp;gt; &amp;gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#013;&#010;&amp;gt; &amp;gt; at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#013;&#010;&amp;gt; &amp;gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#013;&#010;&amp;gt; &amp;gt; ... 5 more&#013;&#010;&amp;gt; &amp;gt; 2020-06-16 17:06:21,980 INFO&amp;nbsp; org.apache.flink.core.fs.FileSystem&#013;&#010;&amp;gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;- Hadoop is not in the classpath/dependencies. The&#013;&#010;&amp;gt; extended&#013;&#010;&amp;gt; &amp;gt; set of supported File Systems via Hadoop is not available.&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;",
        "depth": "1",
        "reply": "<CA+Lep4bHk39D70SDg=r-sDBFcAgVJB3VXK2mrQ-WPQEuYgxobQ@mail.gmail.com>"
    },
    {
        "id": "<CA+Lep4YHBcFeGWUCtkbD=sT+UagWwiqnnSeDBBixt58OfYYwsA@mail.gmail.com>",
        "from": "Rainie Li &lt;raini...@pinterest.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 17 Jul 2020 02:55:15 GMT",
        "subject": "Re: Flink yarn session exception",
        "content": "好搭，谢谢！&#010;&#010;On Thu, Jul 16, 2020 at 5:32 PM 忝忝向仧 &lt;153488125@qq.com&gt; wrote:&#010;&#010;&gt; 你可以看看lib里面的包跟官网的要求是不是一样的&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 发自我的iPhone&#010;&gt;&#010;&gt;&#010;&gt; ------------------ Original ------------------&#010;&gt; From: Rainie Li &lt;rainieli@pinterest.com.INVALID&amp;gt;&#010;&gt; Date: Fri,Jul 17,2020 1:06 AM&#010;&gt; To: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#010;&gt; Subject: Re: Flink yarn session exception&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; 多谢，我set了这些envs:&#010;&gt;&#010;&gt; export JAVA_HOME=/usr/lib/jvm/java-8-oracle&#010;&gt; export PATH=$JAVA_HOME/bin:$PATH&#010;&gt; export HADOOP_HOME=/usr/local/hadoop&#010;&gt; export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop&#010;&gt; export HADOOP_CLASSPATH=`hadoop classpath`&#010;&gt; export FLINK_CONF_DIR=/etc/flink-1.9.1/conf&#010;&gt; export FLINK_LOG_DIR=/home/rainieli/&#010;&gt;&#010;&gt; 有什么问题吗？🙏&#010;&gt;&#010;&gt;&#010;&gt; On Thu, Jul 16, 2020 at 1:12 AM Paul Lam &lt;paullin3280@gmail.com&amp;gt; wrote:&#010;&gt;&#010;&gt; &amp;gt; 日志里说得比较清楚了，classpath 里没有 Hadoop 的 lib。可以参考这个文档&#010;[1] 来配置你的环境。&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; 1.&#010;&gt; &amp;gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&#010;&gt; &amp;gt; &lt;&#010;&gt; &amp;gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html&#010;&gt; &amp;gt; &amp;gt;&#010;&gt; &amp;gt; Best,&#010;&gt; &amp;gt; Paul Lam&#010;&gt; &amp;gt;&#010;&gt; &amp;gt; &amp;gt; 2020年7月16日 15:46，Rainie Li &lt;rainieli@pinterest.com.INVALID&amp;gt;&#010;&gt; 写道：&#010;&gt; &amp;gt; &amp;gt;&#010;&gt; &amp;gt; &amp;gt; 大佬们好，我是flink新手，正在用flink 1.9.1&#010;&gt; &amp;gt; &amp;gt; Flink APP cannot run, APP log error,&amp;nbsp; 想求教一下会是什么原因造成的，多谢🙏&#010;&gt; &amp;gt; &amp;gt;&#010;&gt; &amp;gt; &amp;gt; 2020-06-16 17:06:21,921 WARN&amp;nbsp;&#010;&gt; org.apache.flink.client.cli.CliFrontend&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; - Could not load CLI class&#010;&gt; &amp;gt; &amp;gt; org.apache.flink.yarn.cli.FlinkYarnSessionCli.&#010;&gt; &amp;gt; &amp;gt; java.lang.NoClassDefFoundError:&#010;&gt; &amp;gt; &amp;gt; org/apache/hadoop/yarn/exceptions/YarnException&#010;&gt; &amp;gt; &amp;gt; at java.lang.Class.forName0(Native Method)&#010;&gt; &amp;gt; &amp;gt; at java.lang.Class.forName(Class.java:264)&#010;&gt; &amp;gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLine(CliFrontend.java:1185)&#010;&gt; &amp;gt; &amp;gt; at&#010;&gt; &amp;gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&gt; org.apache.flink.client.cli.CliFrontend.loadCustomCommandLines(CliFrontend.java:1145)&#010;&gt; &amp;gt; &amp;gt; at&#010;&gt; org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1070)&#010;&gt; &amp;gt; &amp;gt; Caused by: java.lang.ClassNotFoundException:&#010;&gt; &amp;gt; &amp;gt; org.apache.hadoop.yarn.exceptions.YarnException&#010;&gt; &amp;gt; &amp;gt; at java.net.URLClassLoader.findClass(URLClassLoader.java:382)&#010;&gt; &amp;gt; &amp;gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#010;&gt; &amp;gt; &amp;gt; at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)&#010;&gt; &amp;gt; &amp;gt; at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#010;&gt; &amp;gt; &amp;gt; ... 5 more&#010;&gt; &amp;gt; &amp;gt; 2020-06-16 17:06:21,980 INFO&amp;nbsp;&#010;&gt; org.apache.flink.core.fs.FileSystem&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&gt; - Hadoop is not in the classpath/dependencies. The&#010;&gt; &amp;gt; extended&#010;&gt; &amp;gt; &amp;gt; set of supported File Systems via Hadoop is not available.&#010;&gt; &amp;gt;&#010;&gt; &amp;gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CA+Lep4bHk39D70SDg=r-sDBFcAgVJB3VXK2mrQ-WPQEuYgxobQ@mail.gmail.com>"
    },
    {
        "id": "<2020071616022061034945@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:02:21 GMT",
        "subject": "FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？",
        "content": "比如：&#013;&#010;&#013;&#010;CREATE TABLE my_table (&#013;&#010;  id BIGINT,&#013;&#010; first_name STRING,&#013;&#010; last_name STRING,&#013;&#010; email STRING&#013;&#010;) WITH (&#013;&#010; 'connector'='kafka',&#013;&#010; 'topic'='user_topic',&#013;&#010; 'properties.bootstrap.servers'='localhost:9092',&#013;&#010; 'scan.startup.mode'='earliest-offset',&#013;&#010; 'format'='debezium-json'&#013;&#010;);&#013;&#010;&#013;&#010;最终解析 debezium-json 应该是  flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/debezium&#010;下面的代码&#013;&#010;但 flinkSQL 是怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<2020071616022061034945@geekplus.com.cn>"
    },
    {
        "id": "<CADQYLGsQ5qo90r0AjGQtozPkJ3mFb7-znL=nbk_HW5ZrduiQ=g@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:38:57 GMT",
        "subject": "Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？",
        "content": "通过Java 的 SPI 机制来找到对应的 format，可以参考 [1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/#how-to-use-connectors&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四 下午4:02写道：&#013;&#010;&#013;&#010;&gt; 比如：&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE my_table (&#013;&#010;&gt;   id BIGINT,&#013;&#010;&gt;  first_name STRING,&#013;&#010;&gt;  last_name STRING,&#013;&#010;&gt;  email STRING&#013;&#010;&gt; ) WITH (&#013;&#010;&gt;  'connector'='kafka',&#013;&#010;&gt;  'topic'='user_topic',&#013;&#010;&gt;  'properties.bootstrap.servers'='localhost:9092',&#013;&#010;&gt;  'scan.startup.mode'='earliest-offset',&#013;&#010;&gt;  'format'='debezium-json'&#013;&#010;&gt; );&#013;&#010;&gt;&#013;&#010;&gt; 最终解析 debezium-json 应该是&#013;&#010;&gt; flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/debezium&#013;&#010;&gt; 下面的代码&#013;&#010;&gt; 但 flinkSQL 是怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<2020071616022061034945@geekplus.com.cn>"
    },
    {
        "id": "<202007161904266394395@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 11:04:27 GMT",
        "subject": "Re: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？",
        "content": "&#013;&#010;我在 flink-formats/flink-json/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory&#010;找到了 SPI 的配置：&#013;&#010;&#013;&#010;org.apache.flink.formats.json.JsonFileSystemFormatFactory&#013;&#010;org.apache.flink.formats.json.JsonFormatFactory&#013;&#010;org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory&#013;&#010;org.apache.flink.formats.json.canal.CanalJsonFormatFactory&#013;&#010;&#013;&#010;还是没有搞清楚 指定 'format'='debezium-json' 怎么就能对应到 DebeziumJsonFormatFactory&#013;&#010;我的理解肯定要有一个地方指明 debezium-json 要对应到 DebeziumJsonFormatFactory，&#010;但是我 grep 代码没找到类似的关系映射配置。&#013;&#010;&#013;&#010;&#013;&#010;谢谢，&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010; &#013;&#010;Sender: godfrey he&#013;&#010;Send Time: 2020-07-16 16:38&#013;&#010;Receiver: user-zh&#013;&#010;Subject: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;通过Java 的 SPI 机制来找到对应的 format，可以参考 [1]&#013;&#010; &#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/#how-to-use-connectors&#013;&#010; &#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010; &#013;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四 下午4:02写道：&#013;&#010; &#013;&#010;&gt; 比如：&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE my_table (&#013;&#010;&gt;   id BIGINT,&#013;&#010;&gt;  first_name STRING,&#013;&#010;&gt;  last_name STRING,&#013;&#010;&gt;  email STRING&#013;&#010;&gt; ) WITH (&#013;&#010;&gt;  'connector'='kafka',&#013;&#010;&gt;  'topic'='user_topic',&#013;&#010;&gt;  'properties.bootstrap.servers'='localhost:9092',&#013;&#010;&gt;  'scan.startup.mode'='earliest-offset',&#013;&#010;&gt;  'format'='debezium-json'&#013;&#010;&gt; );&#013;&#010;&gt;&#013;&#010;&gt; 最终解析 debezium-json 应该是&#013;&#010;&gt; flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/debezium&#013;&#010;&gt; 下面的代码&#013;&#010;&gt; 但 flinkSQL 是怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<2020071616022061034945@geekplus.com.cn>"
    },
    {
        "id": "<CAHOPYeVyeJ8aHib4FBb=b4AnH0p0ryotA-jhkqq9NhhMejORWg@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 11:33:54 GMT",
        "subject": "Re: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？",
        "content": "我的理解 ：  大概就是通过spi机制加载类，然后通过属性去过滤出来需要的class&#010;  类似下面的代码&#010;&#010;private static &lt;T extends TableFactory&gt; T findSingleInternal(&#010;      Class&lt;T&gt; factoryClass,&#010;      Map&lt;String, String&gt; properties,&#010;      Optional&lt;ClassLoader&gt; classLoader) {&#010;&#010;   List&lt;TableFactory&gt; tableFactories = discoverFactories(classLoader);&#010;   List&lt;T&gt; filtered = filter(tableFactories, factoryClass, properties);&#010;&#010;   if (filtered.size() &gt; 1) {&#010;      throw new AmbiguousTableFactoryException(&#010;         filtered,&#010;         factoryClass,&#010;         tableFactories,&#010;         properties);&#010;   } else {&#010;      return filtered.get(0);&#010;   }&#010;}&#010;&#010;private static List&lt;TableFactory&gt;&#010;discoverFactories(Optional&lt;ClassLoader&gt; classLoader) {&#010;   try {&#010;      List&lt;TableFactory&gt; result = new LinkedList&lt;&gt;();&#010;      ClassLoader cl =&#010;classLoader.orElse(Thread.currentThread().getContextClassLoader());&#010;      ServiceLoader&#010;         .load(TableFactory.class, cl)&#010;         .iterator()&#010;         .forEachRemaining(result::add);&#010;      return result;&#010;   } catch (ServiceConfigurationError e) {&#010;      LOG.error(\"Could not load service provider for table factories.\", e);&#010;      throw new TableException(\"Could not load service provider for&#010;table factories.\", e);&#010;   }&#010;&#010;}&#010;&#010;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四 下午7:04写道：&#010;&#010;&gt;&#010;&gt; 我在&#010;&gt; flink-formats/flink-json/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory&#010;&gt; 找到了 SPI 的配置：&#010;&gt;&#010;&gt; org.apache.flink.formats.json.JsonFileSystemFormatFactory&#010;&gt; org.apache.flink.formats.json.JsonFormatFactory&#010;&gt; org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory&#010;&gt; org.apache.flink.formats.json.canal.CanalJsonFormatFactory&#010;&gt;&#010;&gt; 还是没有搞清楚 指定 'format'='debezium-json' 怎么就能对应到 DebeziumJsonFormatFactory&#010;&gt; 我的理解肯定要有一个地方指明 debezium-json 要对应到 DebeziumJsonFormatFactory，&#010;但是我 grep&#010;&gt; 代码没找到类似的关系映射配置。&#010;&gt;&#010;&gt;&#010;&gt; 谢谢，&#010;&gt; 王磊&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; wanglei2@geekplus.com.cn&#010;&gt;&#010;&gt;&#010;&gt; Sender: godfrey he&#010;&gt; Send Time: 2020-07-16 16:38&#010;&gt; Receiver: user-zh&#010;&gt; Subject: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？&#010;&gt; 通过Java 的 SPI 机制来找到对应的 format，可以参考 [1]&#010;&gt;&#010;&gt; [1]&#010;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/#how-to-use-connectors&#010;&gt;&#010;&gt; Best,&#010;&gt; Godfrey&#010;&gt;&#010;&gt; wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四&#010;&gt; 下午4:02写道：&#010;&gt;&#010;&gt; &gt; 比如：&#010;&gt; &gt;&#010;&gt; &gt; CREATE TABLE my_table (&#010;&gt; &gt;   id BIGINT,&#010;&gt; &gt;  first_name STRING,&#010;&gt; &gt;  last_name STRING,&#010;&gt; &gt;  email STRING&#010;&gt; &gt; ) WITH (&#010;&gt; &gt;  'connector'='kafka',&#010;&gt; &gt;  'topic'='user_topic',&#010;&gt; &gt;  'properties.bootstrap.servers'='localhost:9092',&#010;&gt; &gt;  'scan.startup.mode'='earliest-offset',&#010;&gt; &gt;  'format'='debezium-json'&#010;&gt; &gt; );&#010;&gt; &gt;&#010;&gt; &gt; 最终解析 debezium-json 应该是&#010;&gt; &gt;&#010;&gt; flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/debezium&#010;&gt; &gt; 下面的代码&#010;&gt; &gt; 但 flinkSQL 是怎样的机制找到要执行的 Java 代码的呢？&#010;&gt; &gt;&#010;&gt; &gt; 谢谢，&#010;&gt; &gt; 王磊&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; wanglei2@geekplus.com.cn&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best Regards,&#010;Harold Miao&#010;&#010;",
        "depth": "3",
        "reply": "<2020071616022061034945@geekplus.com.cn>"
    },
    {
        "id": "<4A39072B-46F9-45DC-8C1D-D2D9A11AD960@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 11:36:48 GMT",
        "subject": "Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？",
        "content": "Hi,&#013;&#010;&#013;&#010;&gt; 在 2020年7月16日，19:04，wanglei2@geekplus.com.cn 写道：&#013;&#010;&gt; &#013;&#010;&gt; 我的理解肯定要有一个地方指明 debezium-json 要对应到 DebeziumJsonFormatFactory，&#010;但是我 grep 代码没找到类似的关系映射配置。&#013;&#010;&#013;&#010;你DDL中不是写了 ‘format’ = ‘debzium-json’ 吗？就是这里指明的。",
        "depth": "3",
        "reply": "<2020071616022061034945@geekplus.com.cn>"
    },
    {
        "id": "<202007162222008001748@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 14:22:01 GMT",
        "subject": "Re: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？",
        "content": "&#013;&#010; 谢谢，我理解了。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;&#013;&#010;Sender: Harold.Miao&#013;&#010;Send Time: 2020-07-16 19:33&#013;&#010;Receiver: user-zh&#013;&#010;Subject: Re: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;我的理解 ：  大概就是通过spi机制加载类，然后通过属性去过滤出来需要的class&#010;  类似下面的代码&#013;&#010; &#013;&#010;private static &lt;T extends TableFactory&gt; T findSingleInternal(&#013;&#010;      Class&lt;T&gt; factoryClass,&#013;&#010;      Map&lt;String, String&gt; properties,&#013;&#010;      Optional&lt;ClassLoader&gt; classLoader) {&#013;&#010; &#013;&#010;   List&lt;TableFactory&gt; tableFactories = discoverFactories(classLoader);&#013;&#010;   List&lt;T&gt; filtered = filter(tableFactories, factoryClass, properties);&#013;&#010; &#013;&#010;   if (filtered.size() &gt; 1) {&#013;&#010;      throw new AmbiguousTableFactoryException(&#013;&#010;         filtered,&#013;&#010;         factoryClass,&#013;&#010;         tableFactories,&#013;&#010;         properties);&#013;&#010;   } else {&#013;&#010;      return filtered.get(0);&#013;&#010;   }&#013;&#010;}&#013;&#010; &#013;&#010;private static List&lt;TableFactory&gt;&#013;&#010;discoverFactories(Optional&lt;ClassLoader&gt; classLoader) {&#013;&#010;   try {&#013;&#010;      List&lt;TableFactory&gt; result = new LinkedList&lt;&gt;();&#013;&#010;      ClassLoader cl =&#013;&#010;classLoader.orElse(Thread.currentThread().getContextClassLoader());&#013;&#010;      ServiceLoader&#013;&#010;         .load(TableFactory.class, cl)&#013;&#010;         .iterator()&#013;&#010;         .forEachRemaining(result::add);&#013;&#010;      return result;&#013;&#010;   } catch (ServiceConfigurationError e) {&#013;&#010;      LOG.error(\"Could not load service provider for table factories.\", e);&#013;&#010;      throw new TableException(\"Could not load service provider for&#013;&#010;table factories.\", e);&#013;&#010;   }&#013;&#010; &#013;&#010;}&#013;&#010; &#013;&#010; &#013;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四 下午7:04写道：&#013;&#010; &#013;&#010;&gt;&#013;&#010;&gt; 我在&#013;&#010;&gt; flink-formats/flink-json/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory&#013;&#010;&gt; 找到了 SPI 的配置：&#013;&#010;&gt;&#013;&#010;&gt; org.apache.flink.formats.json.JsonFileSystemFormatFactory&#013;&#010;&gt; org.apache.flink.formats.json.JsonFormatFactory&#013;&#010;&gt; org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory&#013;&#010;&gt; org.apache.flink.formats.json.canal.CanalJsonFormatFactory&#013;&#010;&gt;&#013;&#010;&gt; 还是没有搞清楚 指定 'format'='debezium-json' 怎么就能对应到 DebeziumJsonFormatFactory&#013;&#010;&gt; 我的理解肯定要有一个地方指明 debezium-json 要对应到 DebeziumJsonFormatFactory，&#010;但是我 grep&#013;&#010;&gt; 代码没找到类似的关系映射配置。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 谢谢，&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Sender: godfrey he&#013;&#010;&gt; Send Time: 2020-07-16 16:38&#013;&#010;&gt; Receiver: user-zh&#013;&#010;&gt; Subject: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&gt; 通过Java 的 SPI 机制来找到对应的 format，可以参考 [1]&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/#how-to-use-connectors&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Godfrey&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四&#013;&#010;&gt; 下午4:02写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 比如：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; CREATE TABLE my_table (&#013;&#010;&gt; &gt;   id BIGINT,&#013;&#010;&gt; &gt;  first_name STRING,&#013;&#010;&gt; &gt;  last_name STRING,&#013;&#010;&gt; &gt;  email STRING&#013;&#010;&gt; &gt; ) WITH (&#013;&#010;&gt; &gt;  'connector'='kafka',&#013;&#010;&gt; &gt;  'topic'='user_topic',&#013;&#010;&gt; &gt;  'properties.bootstrap.servers'='localhost:9092',&#013;&#010;&gt; &gt;  'scan.startup.mode'='earliest-offset',&#013;&#010;&gt; &gt;  'format'='debezium-json'&#013;&#010;&gt; &gt; );&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 最终解析 debezium-json 应该是&#013;&#010;&gt; &gt;&#013;&#010;&gt; flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/debezium&#013;&#010;&gt; &gt; 下面的代码&#013;&#010;&gt; &gt; 但 flinkSQL 是怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 谢谢，&#013;&#010;&gt; &gt; 王磊&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010; &#013;&#010; &#013;&#010;-- &#013;&#010; &#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "1",
        "reply": "<2020071616022061034945@geekplus.com.cn>"
    },
    {
        "id": "<CAELO9305Bg9jnTD4pvP5m6HC1Qq5DdaB6NkNNr+7zGNY89Ah_Q@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 10:44:09 GMT",
        "subject": "Re: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？",
        "content": "这个对应关系是通过 Factory#factoryIdentifier 来决定的。&#013;&#010;比如 DebeziumJsonFormatFactory#factoryIdentifier() 就是返回了 'debezium-json'&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Thu, 16 Jul 2020 at 22:29, wanglei2@geekplus.com.cn &lt;&#013;&#010;wanglei2@geekplus.com.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;  谢谢，我理解了。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;&gt; Sender: Harold.Miao&#013;&#010;&gt; Send Time: 2020-07-16 19:33&#013;&#010;&gt; Receiver: user-zh&#013;&#010;&gt; Subject: Re: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&gt; 我的理解 ：  大概就是通过spi机制加载类，然后通过属性去过滤出来需要的class&#010;  类似下面的代码&#013;&#010;&gt;&#013;&#010;&gt; private static &lt;T extends TableFactory&gt; T findSingleInternal(&#013;&#010;&gt;       Class&lt;T&gt; factoryClass,&#013;&#010;&gt;       Map&lt;String, String&gt; properties,&#013;&#010;&gt;       Optional&lt;ClassLoader&gt; classLoader) {&#013;&#010;&gt;&#013;&#010;&gt;    List&lt;TableFactory&gt; tableFactories = discoverFactories(classLoader);&#013;&#010;&gt;    List&lt;T&gt; filtered = filter(tableFactories, factoryClass, properties);&#013;&#010;&gt;&#013;&#010;&gt;    if (filtered.size() &gt; 1) {&#013;&#010;&gt;       throw new AmbiguousTableFactoryException(&#013;&#010;&gt;          filtered,&#013;&#010;&gt;          factoryClass,&#013;&#010;&gt;          tableFactories,&#013;&#010;&gt;          properties);&#013;&#010;&gt;    } else {&#013;&#010;&gt;       return filtered.get(0);&#013;&#010;&gt;    }&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt; private static List&lt;TableFactory&gt;&#013;&#010;&gt; discoverFactories(Optional&lt;ClassLoader&gt; classLoader) {&#013;&#010;&gt;    try {&#013;&#010;&gt;       List&lt;TableFactory&gt; result = new LinkedList&lt;&gt;();&#013;&#010;&gt;       ClassLoader cl =&#013;&#010;&gt; classLoader.orElse(Thread.currentThread().getContextClassLoader());&#013;&#010;&gt;       ServiceLoader&#013;&#010;&gt;          .load(TableFactory.class, cl)&#013;&#010;&gt;          .iterator()&#013;&#010;&gt;          .forEachRemaining(result::add);&#013;&#010;&gt;       return result;&#013;&#010;&gt;    } catch (ServiceConfigurationError e) {&#013;&#010;&gt;       LOG.error(\"Could not load service provider for table factories.\", e);&#013;&#010;&gt;       throw new TableException(\"Could not load service provider for&#013;&#010;&gt; table factories.\", e);&#013;&#010;&gt;    }&#013;&#010;&gt;&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四&#013;&#010;&gt; 下午7:04写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 我在&#013;&#010;&gt; &gt;&#013;&#010;&gt; flink-formats/flink-json/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory&#013;&#010;&gt; &gt; 找到了 SPI 的配置：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; org.apache.flink.formats.json.JsonFileSystemFormatFactory&#013;&#010;&gt; &gt; org.apache.flink.formats.json.JsonFormatFactory&#013;&#010;&gt; &gt; org.apache.flink.formats.json.debezium.DebeziumJsonFormatFactory&#013;&#010;&gt; &gt; org.apache.flink.formats.json.canal.CanalJsonFormatFactory&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 还是没有搞清楚 指定 'format'='debezium-json' 怎么就能对应到 DebeziumJsonFormatFactory&#013;&#010;&gt; &gt; 我的理解肯定要有一个地方指明 debezium-json 要对应到 DebeziumJsonFormatFactory，&#010;但是我 grep&#013;&#010;&gt; &gt; 代码没找到类似的关系映射配置。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 谢谢，&#013;&#010;&gt; &gt; 王磊&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Sender: godfrey he&#013;&#010;&gt; &gt; Send Time: 2020-07-16 16:38&#013;&#010;&gt; &gt; Receiver: user-zh&#013;&#010;&gt; &gt; Subject: Re: FlinkSQL 是通过怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&gt; &gt; 通过Java 的 SPI 机制来找到对应的 format，可以参考 [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/#how-to-use-connectors&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月16日周四&#013;&#010;&gt; &gt; 下午4:02写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 比如：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; CREATE TABLE my_table (&#013;&#010;&gt; &gt; &gt;   id BIGINT,&#013;&#010;&gt; &gt; &gt;  first_name STRING,&#013;&#010;&gt; &gt; &gt;  last_name STRING,&#013;&#010;&gt; &gt; &gt;  email STRING&#013;&#010;&gt; &gt; &gt; ) WITH (&#013;&#010;&gt; &gt; &gt;  'connector'='kafka',&#013;&#010;&gt; &gt; &gt;  'topic'='user_topic',&#013;&#010;&gt; &gt; &gt;  'properties.bootstrap.servers'='localhost:9092',&#013;&#010;&gt; &gt; &gt;  'scan.startup.mode'='earliest-offset',&#013;&#010;&gt; &gt; &gt;  'format'='debezium-json'&#013;&#010;&gt; &gt; &gt; );&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 最终解析 debezium-json 应该是&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; flink-formats/flink-json/src/main/java/org/apache/flink/formats/json/debezium&#013;&#010;&gt; &gt; &gt; 下面的代码&#013;&#010;&gt; &gt; &gt; 但 flinkSQL 是怎样的机制找到要执行的 Java 代码的呢？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 谢谢，&#013;&#010;&gt; &gt; &gt; 王磊&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; wanglei2@geekplus.com.cn&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best Regards,&#013;&#010;&gt; Harold Miao&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<2020071616022061034945@geekplus.com.cn>"
    },
    {
        "id": "<1594886710135-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:05:10 GMT",
        "subject": "flink1.11 Application 模式下启动失败问题",
        "content": "Hi all&#010;我把作业提交模式从 yarn-cluster 换成 application 模式，启动失败，报两个错误：&#010;1、java.lang.ClassNotFoundException:&#010;org.apache.hadoop.yarn.api.records.ResourceInformation&#010;2、cannot assign instance of org.apache.commons.collections.map.LinkedMap to&#010;field&#010;org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.pendingOffsetsToCommit&#010;of type org.apache.commons.collections.map.LinkedMap in instance of&#010;com.tydic.tysc.core.flink.cal.v3.core.connector.kafka.source.KafkaTableSource$CustomerFlinkKafkaConsumer&#010;在 yarn-cluster 下正常运行，请各位帮忙看下。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594886710135-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594886790415-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:06:30 GMT",
        "subject": "flink1.11 Application 模式下启动失败问题",
        "content": "Hi all&#010;我把作业提交模式从 yarn-cluster 换成 application 模式，启动失败，报两个错误：&#010;1、java.lang.ClassNotFoundException:&#010;org.apache.hadoop.yarn.api.records.ResourceInformation&#010;2、cannot assign instance of org.apache.commons.collections.map.LinkedMap to&#010;field&#010;org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.pendingOffsetsToCommit&#010;of type org.apache.commons.collections.map.LinkedMap in instance of&#010;com.tydic.tysc.core.flink.cal.v3.core.connector.kafka.source.KafkaTableSource$CustomerFlinkKafkaConsumer&#010;在 yarn-cluster 下正常运行，请各位帮忙看下。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594886710135-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594887889397-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:24:49 GMT",
        "subject": "flink1.11 Application 模式下启动失败问题",
        "content": "Hi all&#010;我把作业提交模式从 yarn-cluster 换成 application 模式，启动失败，报两个错误：&#010;1、java.lang.ClassNotFoundException:&#010;org.apache.hadoop.yarn.api.records.ResourceInformation&#010;2、cannot assign instance of org.apache.commons.collections.map.LinkedMap to&#010;field&#010;org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.pendingOffsetsToCommit&#010;of type org.apache.commons.collections.map.LinkedMap in instance of&#010;com.tydic.tysc.core.flink.cal.v3.core.connector.kafka.source.KafkaTableSource$CustomerFlinkKafkaConsumer&#010;在 yarn-cluster 下正常运行，请各位帮忙看下。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594886710135-0.post@n8.nabble.com>"
    },
    {
        "id": "<1594887966160-0.post@n8.nabble.com>",
        "from": "Hito Zhu &lt;qrshi....@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:26:06 GMT",
        "subject": "flink1.11 Application 模式下启动失败问题",
        "content": "Hi all&#010;我把作业提交模式从 yarn-cluster 换成 application 模式，启动失败，报两个错误：&#010;1、java.lang.ClassNotFoundException:&#010;org.apache.hadoop.yarn.api.records.ResourceInformation&#010;2、cannot assign instance of org.apache.commons.collections.map.LinkedMap to&#010;field&#010;org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.pendingOffsetsToCommit&#010;of type org.apache.commons.collections.map.LinkedMap in instance of&#010;com.tydic.tysc.core.flink.cal.v3.core.connector.kafka.source.KafkaTableSource$CustomerFlinkKafkaConsumer&#010;在 yarn-cluster 下正常运行，请各位帮忙看下。&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "1",
        "reply": "<1594886710135-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAP+gf37bBM_sWhmOFNv3=zvK0Jbu463NZHfiFQsBGg=jo-1Tdw@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:26:08 GMT",
        "subject": "Re: flink1.11 Application 模式下启动失败问题",
        "content": "你的报错是在Client端还是JM端呢，client/JM的日志以及启动命令最好可以发一下吧&#013;&#010;这样方便查问题&#013;&#010;&#013;&#010;&#013;&#010;Application和Perjob模式本质上的区别是用户main运行的位置不一样，所以会导致JM启动的classpath&#013;&#010;也不太一样的&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;Hito Zhu &lt;qrshi.tao@gmail.com&gt; 于2020年7月16日周四 下午4:26写道：&#013;&#010;&#013;&#010;&gt; Hi all&#013;&#010;&gt; 我把作业提交模式从 yarn-cluster 换成 application 模式，启动失败，报两个错误：&#013;&#010;&gt; 1、java.lang.ClassNotFoundException:&#013;&#010;&gt; org.apache.hadoop.yarn.api.records.ResourceInformation&#013;&#010;&gt; 2、cannot assign instance of org.apache.commons.collections.map.LinkedMap to&#013;&#010;&gt; field&#013;&#010;&gt;&#013;&#010;&gt; org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.pendingOffsetsToCommit&#013;&#010;&gt; of type org.apache.commons.collections.map.LinkedMap in instance of&#013;&#010;&gt;&#013;&#010;&gt; com.tydic.tysc.core.flink.cal.v3.core.connector.kafka.source.KafkaTableSource$CustomerFlinkKafkaConsumer&#013;&#010;&gt; 在 yarn-cluster 下正常运行，请各位帮忙看下。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<1594886710135-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_CBF0E1FD17F3DE9D5EF6165C@qq.com>",
        "from": "&quot;xiao cai&quot;&lt;flin...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:15:13 GMT",
        "subject": "Re:回复：flink1.11 set yarn slots failed",
        "content": "可以看这里&#010;&#010;&#010; 原始邮件 &#010;发件人: Zhou Zach&lt;wander669@163.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月16日(周四) 15:28&#010;主题: Re:回复：flink1.11 set yarn slots failed&#010;&#010;&#010;-D前缀好使，要设置yarn name用什么参数啊，1.11官网的文档有些都不好使了&#010;在 2020-07-16 15:03:14，\"flinkcx\" &lt;flinkcx@163.com&gt; 写道： &gt;是不是应该用-D作为前缀来设置,比如-Dtaskmanager.numberOfTaskSlots=4&#010;&gt; &gt; &gt; 原始邮件 &gt;发件人: Zhou Zach&lt;wander669@163.com&gt; &gt;收件人:&#010;Flink user-zh mailing list&lt;user-zh@flink.apache.org&gt; &gt;发送时间: 2020年7月16日(周四) 14:51&#010;&gt;主题: flink1.11 set yarn slots failed &gt; &gt; &gt;Hi all, 使用如下命令，设置Number&#010;of slots per TaskManager /opt/flink-1.11.0/bin/flink run-application -t yarn-application \\&#010;-Djobmanager.memory.process.size=1024m \\ -Dtaskmanager.memory.process.size=2048m \\ -ys 4 \\&#010;发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#010;of slots per TaskManager？ 另外，有哪些方式可以增Flink UI中的大Available Task&#010;Slots的值，现在每次提交作业都是0",
        "depth": "1",
        "reply": "<tencent_CBF0E1FD17F3DE9D5EF6165C@qq.com>"
    },
    {
        "id": "<tencent_B26A6FE8357550A6032DB64C@qq.com>",
        "from": "&quot;xiao cai&quot;&lt;flin...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:16:00 GMT",
        "subject": "Re:回复：flink1.11 set yarn slots failed",
        "content": "可以看这里https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html&#010;&#010; 原始邮件 &#010;发件人: Zhou Zach&lt;wander669@163.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月16日(周四) 15:28&#010;主题: Re:回复：flink1.11 set yarn slots failed&#010;&#010;&#010;-D前缀好使，要设置yarn name用什么参数啊，1.11官网的文档有些都不好使了&#010;在 2020-07-16 15:03:14，\"flinkcx\" &lt;flinkcx@163.com&gt; 写道： &gt;是不是应该用-D作为前缀来设置,比如-Dtaskmanager.numberOfTaskSlots=4&#010;&gt; &gt; &gt; 原始邮件 &gt;发件人: Zhou Zach&lt;wander669@163.com&gt; &gt;收件人:&#010;Flink user-zh mailing list&lt;user-zh@flink.apache.org&gt; &gt;发送时间: 2020年7月16日(周四) 14:51&#010;&gt;主题: flink1.11 set yarn slots failed &gt; &gt; &gt;Hi all, 使用如下命令，设置Number&#010;of slots per TaskManager /opt/flink-1.11.0/bin/flink run-application -t yarn-application \\&#010;-Djobmanager.memory.process.size=1024m \\ -Dtaskmanager.memory.process.size=2048m \\ -ys 4 \\&#010;发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#010;of slots per TaskManager？ 另外，有哪些方式可以增Flink UI中的大Available Task&#010;Slots的值，现在每次提交作业都是0",
        "depth": "1",
        "reply": "<tencent_CBF0E1FD17F3DE9D5EF6165C@qq.com>"
    },
    {
        "id": "<466be14c.6fd8.17356bd20bf.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 16 Jul 2020 08:28:22 GMT",
        "subject": "Re:Re:回复：flink1.11 set yarn slots failed",
        "content": "nice, 可以不用看Command-Line Interface的文档了&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-16 16:16:00，\"xiao cai\" &lt;flinkcx@163.com&gt; 写道：&#010;&gt;可以看这里https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html&#010;&gt;&#010;&gt; 原始邮件 &#010;&gt;发件人: Zhou Zach&lt;wander669@163.com&gt;&#010;&gt;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;&gt;发送时间: 2020年7月16日(周四) 15:28&#010;&gt;主题: Re:回复：flink1.11 set yarn slots failed&#010;&gt;&#010;&gt;&#010;&gt;-D前缀好使，要设置yarn name用什么参数啊，1.11官网的文档有些都不好使了&#010;在 2020-07-16 15:03:14，\"flinkcx\" &lt;flinkcx@163.com&gt; 写道： &gt;是不是应该用-D作为前缀来设置,比如-Dtaskmanager.numberOfTaskSlots=4&#010;&gt; &gt; &gt; 原始邮件 &gt;发件人: Zhou Zach&lt;wander669@163.com&gt; &gt;收件人:&#010;Flink user-zh mailing list&lt;user-zh@flink.apache.org&gt; &gt;发送时间: 2020年7月16日(周四) 14:51&#010;&gt;主题: flink1.11 set yarn slots failed &gt; &gt; &gt;Hi all, 使用如下命令，设置Number&#010;of slots per TaskManager /opt/flink-1.11.0/bin/flink run-application -t yarn-application \\&#010;-Djobmanager.memory.process.size=1024m \\ -Dtaskmanager.memory.process.size=2048m \\ -ys 4 \\&#010;发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#010;of slots per TaskManager？ 另外，有哪些方式可以增Flink UI中的大Available Task&#010;Slots的值，现在每次提交作业都是0&#010;",
        "depth": "2",
        "reply": "<tencent_CBF0E1FD17F3DE9D5EF6165C@qq.com>"
    },
    {
        "id": "<CAP+gf354hG+j_inF=dGg7UQ50th2Qh3_X4A=iHUO8nFMnhxiSA@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 20 Jul 2020 03:23:25 GMT",
        "subject": "Re: Re:回复：flink1.11 set yarn slots failed",
        "content": "从长远看，社区是想逐步统一各个deployment下的参数的（Yarn/K8s），所以CLI&#010;Config options&#013;&#010;会逐渐被废弃，而使用dynamic config options的方式&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;Zhou Zach &lt;wander669@163.com&gt; 于2020年7月16日周四 下午4:28写道：&#013;&#010;&#013;&#010;&gt; nice, 可以不用看Command-Line Interface的文档了&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 在 2020-07-16 16:16:00，\"xiao cai\" &lt;flinkcx@163.com&gt; 写道：&#013;&#010;&gt; &gt;可以看这里&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 原始邮件&#013;&#010;&gt; &gt;发件人: Zhou Zach&lt;wander669@163.com&gt;&#013;&#010;&gt; &gt;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt;发送时间: 2020年7月16日(周四) 15:28&#013;&#010;&gt; &gt;主题: Re:回复：flink1.11 set yarn slots failed&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;-D前缀好使，要设置yarn name用什么参数啊，1.11官网的文档有些都不好使了&#010;在 2020-07-16&#013;&#010;&gt; 15:03:14，\"flinkcx\" &lt;flinkcx@163.com&gt; 写道：&#013;&#010;&gt; &gt;是不是应该用-D作为前缀来设置,比如-Dtaskmanager.numberOfTaskSlots=4&#010;&gt; &gt; &gt; 原始邮件 &gt;发件人: Zhou&#013;&#010;&gt; Zach&lt;wander669@163.com&gt; &gt;收件人: Flink user-zh mailing list&lt;&#013;&#010;&gt; user-zh@flink.apache.org&gt; &gt;发送时间: 2020年7月16日(周四) 14:51 &gt;主题:&#010;flink1.11 set&#013;&#010;&gt; yarn slots failed &gt; &gt; &gt;Hi all, 使用如下命令，设置Number of slots per&#010;TaskManager&#013;&#010;&gt; /opt/flink-1.11.0/bin/flink run-application -t yarn-application \\&#013;&#010;&gt; -Djobmanager.memory.process.size=1024m \\&#013;&#010;&gt; -Dtaskmanager.memory.process.size=2048m \\ -ys 4 \\&#013;&#010;&gt; 发现并不能override/opt/flink-1.11.0/bin/flink/conf/flink-conf.yaml中的默认值，每次要调整只能通过更改flink-conf.yaml的方式才能生效，请问使用run-application的方式，怎样设置Number&#013;&#010;&gt; of slots per TaskManager？ 另外，有哪些方式可以增Flink UI中的大Available&#010;Task&#013;&#010;&gt; Slots的值，现在每次提交作业都是0&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_CBF0E1FD17F3DE9D5EF6165C@qq.com>"
    }
]