[
    {
        "id": "<tencent_4D405BBEECF7FF47D308BD8367AC4F011606@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 11:09:19 GMT",
        "subject": "flink-1.11 ddl kafka-to-hive问题",
        "content": "hive-1.2.1&#013;&#010;chk 已经成功了（去chk目录查看了的确有chk数据，kafka也有数据），但是hive表没有数据，我是哪里缺少了什么吗？&#013;&#010;String hiveSql = \"CREATE  TABLE  stream_tmp.fs_table (\\n\" +&#013;&#010;        \"  host STRING,\\n\" +&#013;&#010;        \"  url STRING,\" +&#013;&#010;        \"  public_date STRING\" +&#013;&#010;        \") partitioned by (public_date string) \" +&#013;&#010;        \"stored as PARQUET \" +&#013;&#010;        \"TBLPROPERTIES (\\n\" +&#013;&#010;        \"  'sink.partition-commit.delay'='0 s',\\n\" +&#013;&#010;        \"  'sink.partition-commit.trigger'='partition-time',\\n\" +&#013;&#010;        \"  'sink.partition-commit.policy.kind'='metastore,success-file'\" +&#013;&#010;        \")\";&#013;&#010;tableEnv.executeSql(hiveSql);&#013;&#010;&#013;&#010;&#013;&#010;tableEnv.executeSql(\"INSERT INTO  stream_tmp.fs_table SELECT host, url, DATE_FORMAT(public_date,&#010;'yyyy-MM-dd') FROM stream_tmp.source_table\");",
        "depth": "0",
        "reply": "<tencent_4D405BBEECF7FF47D308BD8367AC4F011606@qq.com>"
    },
    {
        "id": "<1afc959.ba75.1737161ef00.Coremail.17610775726@163.com>",
        "from": "JasonLee &lt;17610775...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 12:38:30 GMT",
        "subject": "回复：flink-1.11 ddl kafka-to-hive问题",
        "content": "hi&#010;hive表是一直没有数据还是过一段时间就有数据了？&#010;&#010;&#010;| |&#010;JasonLee&#010;|&#010;|&#010;邮箱：17610775726@163.com&#010;|&#010;&#010;Signature is customized by Netease Mail Master&#010;&#010;在2020年07月21日 19:09，kcz 写道：&#010;hive-1.2.1&#010;chk 已经成功了（去chk目录查看了的确有chk数据，kafka也有数据），但是hive表没有数据，我是哪里缺少了什么吗？&#010;String hiveSql = \"CREATE  TABLE  stream_tmp.fs_table (\\n\" +&#010;       \"  host STRING,\\n\" +&#010;       \"  url STRING,\" +&#010;       \"  public_date STRING\" +&#010;       \") partitioned by (public_date string) \" +&#010;       \"stored as PARQUET \" +&#010;       \"TBLPROPERTIES (\\n\" +&#010;       \"  'sink.partition-commit.delay'='0 s',\\n\" +&#010;       \"  'sink.partition-commit.trigger'='partition-time',\\n\" +&#010;       \"  'sink.partition-commit.policy.kind'='metastore,success-file'\" +&#010;       \")\";&#010;tableEnv.executeSql(hiveSql);&#010;&#010;&#010;tableEnv.executeSql(\"INSERT INTO  stream_tmp.fs_table SELECT host, url, DATE_FORMAT(public_date,&#010;'yyyy-MM-dd') FROM stream_tmp.source_table\");",
        "depth": "1",
        "reply": "<tencent_4D405BBEECF7FF47D308BD8367AC4F011606@qq.com>"
    },
    {
        "id": "<CAELO932waOTc1w6=E9hM+TuFZLG24wcugJJ5ma5MAfc6-oR2hw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:37:33 GMT",
        "subject": "Re: flink-1.11 ddl kafka-to-hive问题",
        "content": "rolling 策略配一下？&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/filesystem.html#sink-rolling-policy-rollover-interval&#010;&#010;Best,&#010;Jark&#010;&#010;On Tue, 21 Jul 2020 at 20:38, JasonLee &lt;17610775726@163.com&gt; wrote:&#010;&#010;&gt; hi&#010;&gt; hive表是一直没有数据还是过一段时间就有数据了？&#010;&gt;&#010;&gt;&#010;&gt; | |&#010;&gt; JasonLee&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：17610775726@163.com&#010;&gt; |&#010;&gt;&#010;&gt; Signature is customized by Netease Mail Master&#010;&gt;&#010;&gt; 在2020年07月21日 19:09，kcz 写道：&#010;&gt; hive-1.2.1&#010;&gt; chk 已经成功了（去chk目录查看了的确有chk数据，kafka也有数据），但是hive表没有数据，我是哪里缺少了什么吗？&#010;&gt; String hiveSql = \"CREATE  TABLE  stream_tmp.fs_table (\\n\" +&#010;&gt;        \"  host STRING,\\n\" +&#010;&gt;        \"  url STRING,\" +&#010;&gt;        \"  public_date STRING\" +&#010;&gt;        \") partitioned by (public_date string) \" +&#010;&gt;        \"stored as PARQUET \" +&#010;&gt;        \"TBLPROPERTIES (\\n\" +&#010;&gt;        \"  'sink.partition-commit.delay'='0 s',\\n\" +&#010;&gt;        \"  'sink.partition-commit.trigger'='partition-time',\\n\" +&#010;&gt;        \"  'sink.partition-commit.policy.kind'='metastore,success-file'\" +&#010;&gt;        \")\";&#010;&gt; tableEnv.executeSql(hiveSql);&#010;&gt;&#010;&gt;&#010;&gt; tableEnv.executeSql(\"INSERT INTO  stream_tmp.fs_table SELECT host, url,&#010;&gt; DATE_FORMAT(public_date, 'yyyy-MM-dd') FROM stream_tmp.source_table\");&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_4D405BBEECF7FF47D308BD8367AC4F011606@qq.com>"
    },
    {
        "id": "<tencent_FEF211BE1F3E2C47441C2C623D0B08019E09@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:57:03 GMT",
        "subject": "回复：flink-1.11 ddl kafka-to-hive问题",
        "content": "一直都木有数据 我也不知道哪里不太对 hive有这个表了已经。我测试写ddl&#010;hdfs 是OK的&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: JasonLee &lt;17610775726@163.com&amp;gt;&#013;&#010;发送时间: 2020年7月21日 20:39&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：flink-1.11 ddl kafka-to-hive问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hi&#013;&#010;hive表是一直没有数据还是过一段时间就有数据了？&#013;&#010;&#013;&#010;&#013;&#010;| |&#013;&#010;JasonLee&#013;&#010;|&#013;&#010;|&#013;&#010;邮箱：17610775726@163.com&#013;&#010;|&#013;&#010;&#013;&#010;Signature is customized by Netease Mail Master&#013;&#010;&#013;&#010;在2020年07月21日 19:09，kcz 写道：&#013;&#010;hive-1.2.1&#013;&#010;chk 已经成功了（去chk目录查看了的确有chk数据，kafka也有数据），但是hive表没有数据，我是哪里缺少了什么吗？&#013;&#010;String hiveSql = \"CREATE&amp;nbsp; TABLE&amp;nbsp; stream_tmp.fs_table (\\n\" +&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; host STRING,\\n\" +&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; url STRING,\" +&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; public_date STRING\"&#010;+&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \") partitioned by (public_date&#010;string) \" +&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"stored as PARQUET \" +&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"TBLPROPERTIES (\\n\" +&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; 'sink.partition-commit.delay'='0&#010;s',\\n\" +&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; 'sink.partition-commit.trigger'='partition-time',\\n\"&#010;+&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; 'sink.partition-commit.policy.kind'='metastore,success-file'\"&#010;+&#013;&#010;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \")\";&#013;&#010;tableEnv.executeSql(hiveSql);&#013;&#010;&#013;&#010;&#013;&#010;tableEnv.executeSql(\"INSERT INTO&amp;nbsp; stream_tmp.fs_table SELECT host, url, DATE_FORMAT(public_date,&#010;'yyyy-MM-dd') FROM stream_tmp.source_table\");",
        "depth": "2",
        "reply": "<tencent_4D405BBEECF7FF47D308BD8367AC4F011606@qq.com>"
    },
    {
        "id": "<A9B5B73A-6B5E-4F39-8B1A-BCAE9B6A712D@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 16:02:03 GMT",
        "subject": "Re: flink-1.11 ddl kafka-to-hive问题",
        "content": "HI,&#010;&#010;Hive 表时在flink里建的吗？ 如果是建表时使用了hive dialect吗？可以参考[1]设置下&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_dialect.html#use-hive-dialect&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_dialect.html#use-hive-dialect&gt;&#010;&#010;&gt; 在 2020年7月21日，22:57，kcz &lt;573693104@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 一直都木有数据 我也不知道哪里不太对 hive有这个表了已经。我测试写ddl&#010;hdfs 是OK的&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; ------------------ 原始邮件 ------------------&#010;&gt; 发件人: JasonLee &lt;17610775726@163.com &lt;mailto:17610775726@163.com&gt;&amp;gt;&#010;&gt; 发送时间: 2020年7月21日 20:39&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt;&amp;gt;&#010;&gt; 主题: 回复：flink-1.11 ddl kafka-to-hive问题&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; hi&#010;&gt; hive表是一直没有数据还是过一段时间就有数据了？&#010;&gt; &#010;&gt; &#010;&gt; | |&#010;&gt; JasonLee&#010;&gt; |&#010;&gt; |&#010;&gt; 邮箱：17610775726@163.com&#010;&gt; |&#010;&gt; &#010;&gt; Signature is customized by Netease Mail Master&#010;&gt; &#010;&gt; 在2020年07月21日 19:09，kcz 写道：&#010;&gt; hive-1.2.1&#010;&gt; chk 已经成功了（去chk目录查看了的确有chk数据，kafka也有数据），但是hive表没有数据，我是哪里缺少了什么吗？&#010;&gt; String hiveSql = \"CREATE&amp;nbsp; TABLE&amp;nbsp; stream_tmp.fs_table (\\n\" +&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; host STRING,\\n\"&#010;+&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; url STRING,\"&#010;+&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; public_date&#010;STRING\" +&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \") partitioned by (public_date&#010;string) \" +&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"stored as PARQUET \" +&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"TBLPROPERTIES (\\n\" +&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; 'sink.partition-commit.delay'='0&#010;s',\\n\" +&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; 'sink.partition-commit.trigger'='partition-time',\\n\"&#010;+&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; 'sink.partition-commit.policy.kind'='metastore,success-file'\"&#010;+&#010;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \")\";&#010;&gt; tableEnv.executeSql(hiveSql);&#010;&gt; &#010;&gt; &#010;&gt; tableEnv.executeSql(\"INSERT INTO&amp;nbsp; stream_tmp.fs_table SELECT host, url, DATE_FORMAT(public_date,&#010;'yyyy-MM-dd') FROM stream_tmp.source_table\");&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<tencent_4D405BBEECF7FF47D308BD8367AC4F011606@qq.com>"
    },
    {
        "id": "<CABi+2jTjSC=YwtdjBSSPROtrvXfJyeWeBxZZ1vNF7Y6q2jONiw@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 01:33:24 GMT",
        "subject": "Re: flink-1.11 ddl kafka-to-hive问题",
        "content": "你的Source表是怎么定义的？确定有watermark前进吗？(可以看Flink UI)&#013;&#010;&#013;&#010;'sink.partition-commit.trigger'='partition-time' 去掉试试？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 12:02 AM Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; HI,&#013;&#010;&gt;&#013;&#010;&gt; Hive 表时在flink里建的吗？ 如果是建表时使用了hive dialect吗？可以参考[1]设置下&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_dialect.html#use-hive-dialect&#013;&#010;&gt; &lt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_dialect.html#use-hive-dialect&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月21日，22:57，kcz &lt;573693104@qq.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 一直都木有数据 我也不知道哪里不太对 hive有这个表了已经。我测试写ddl&#010;hdfs 是OK的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ------------------ 原始邮件 ------------------&#013;&#010;&gt; &gt; 发件人: JasonLee &lt;17610775726@163.com &lt;mailto:17610775726@163.com&gt;&amp;gt;&#013;&#010;&gt; &gt; 发送时间: 2020年7月21日 20:39&#013;&#010;&gt; &gt; 收件人: user-zh &lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&#013;&#010;&gt; &gt;&amp;gt;&#013;&#010;&gt; &gt; 主题: 回复：flink-1.11 ddl kafka-to-hive问题&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hi&#013;&#010;&gt; &gt; hive表是一直没有数据还是过一段时间就有数据了？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; | |&#013;&#010;&gt; &gt; JasonLee&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt; 邮箱：17610775726@163.com&#013;&#010;&gt; &gt; |&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Signature is customized by Netease Mail Master&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 在2020年07月21日 19:09，kcz 写道：&#013;&#010;&gt; &gt; hive-1.2.1&#013;&#010;&gt; &gt; chk 已经成功了（去chk目录查看了的确有chk数据，kafka也有数据），但是hive表没有数据，我是哪里缺少了什么吗？&#013;&#010;&gt; &gt; String hiveSql = \"CREATE&amp;nbsp; TABLE&amp;nbsp; stream_tmp.fs_table (\\n\" +&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; host STRING,\\n\"&#010;+&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; url STRING,\"&#010;+&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp; public_date&#010;STRING\" +&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \") partitioned by (public_date&#013;&#010;&gt; string) \" +&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"stored as PARQUET&#010;\" +&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"TBLPROPERTIES (\\n\"&#010;+&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#013;&#010;&gt; 'sink.partition-commit.delay'='0 s',\\n\" +&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#013;&#010;&gt; 'sink.partition-commit.trigger'='partition-time',\\n\" +&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"&amp;nbsp;&#013;&#010;&gt; 'sink.partition-commit.policy.kind'='metastore,success-file'\" +&#013;&#010;&gt; &gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \")\";&#013;&#010;&gt; &gt; tableEnv.executeSql(hiveSql);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; tableEnv.executeSql(\"INSERT INTO&amp;nbsp; stream_tmp.fs_table SELECT host,&#013;&#010;&gt; url, DATE_FORMAT(public_date, 'yyyy-MM-dd') FROM stream_tmp.source_table\");&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "4",
        "reply": "<tencent_4D405BBEECF7FF47D308BD8367AC4F011606@qq.com>"
    },
    {
        "id": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 11:22:33 GMT",
        "subject": "flink1.11 tablefunction",
        "content": "hi&#013;&#010;我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#013;&#010;内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CADQYLGtLN0xiU5-xqr=gjG6q4fKSgk3=p-6y8cTtGj8HU4Z6Ag@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 11:41:03 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "可以，定义清楚 getResultType 和 getParameterTypes, 可以参考[1]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#013;&#010;&#013;&#010;&#013;&#010;Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二 下午7:25写道：&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt;&#013;&#010;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#013;&#010;&gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CAEZk043WzKHbJrKh_hMywXUZxYFsYd0Mjxzu38N2aEwajGJm1g@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:42:34 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "hi,&#010; 我想将一个array&lt;row&gt;打散成多行，但是并没有成功&#010;&#010;@FunctionHint(input =@DataTypeHint(\"ARRAY&lt;ROW&lt;rule_id STRING,rule_name&#010;STRING,rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;\") ,output&#010;= @DataTypeHint(\"ROW&lt;rule_id STRING,rule_name STRING,rule_type_name&#010;STRING,`result` INT,in_path BOOLEAN&gt;\"))&#010;public static class FlatRowFunction extends TableFunction&lt;Row&gt; {&#010;    private static final long serialVersionUID = 1L;&#010;&#010;    public void eval(Row[] rows) {&#010;        for (Row row : rows) {&#010;            collect(row);&#010;        }&#010;    }&#010;}&#010;&#010;异常如下：&#010;&#010;org.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt;From line 1, column 149 to line 1, column 174: No match found for&#010;function signature&#010;flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&#009;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&#009;at com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:60)&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498)&#010;&#009;at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&#009;at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&#009;at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&#009;at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&#009;at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&#009;at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&#009;at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&#009;at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&#009;at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&#009;at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&#009;at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&#009;at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&#009;at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&#009;at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&#009;at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&#009;at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&#009;at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&#009;at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&#009;at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;line 1, column 149 to line 1, column 174: No match found for function&#010;signature flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647)&#010;rule_id, VARCHAR(2147483647) rule_name, VARCHAR(2147483647)&#010;rule_type_name, INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&#009;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&#009;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&#009;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&#009;at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&#009;at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&#009;at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&#009;at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&#009;at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&#009;at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&#009;at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&#009;at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&#009;... 28 more&#010;Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No&#010;match found for function signature&#010;flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&#009;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&#009;... 56 more&#010;&#010;&#010;godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月21日周二 下午7:41写道：&#010;&#010;&gt; 可以，定义清楚 getResultType 和 getParameterTypes, 可以参考[1]&#010;&gt;&#010;&gt; [1]&#010;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#010;&gt;&#010;&gt;&#010;&gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二 下午7:25写道：&#010;&gt;&#010;&gt; &gt; hi&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#010;&gt; &gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CAELO9319SAzGZzte24jSKp5NDpDHqX4bHvNsH2j_1PF1SZAEnw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 03:17:11 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "Hi,&#010;&#010;Row[] 作为 eval 参数，目前还不支持。社区已经有一个 issue 在跟进支持这个功能：&#010;https://issues.apache.org/jira/browse/FLINK-17855&#010;&#010;&#010;Best,&#010;Jark&#010;&#010;On Wed, 22 Jul 2020 at 10:45, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&#010;&gt; hi,&#010;&gt;  我想将一个array&lt;row&gt;打散成多行，但是并没有成功&#010;&gt;&#010;&gt; @FunctionHint(input =@DataTypeHint(\"ARRAY&lt;ROW&lt;rule_id STRING,rule_name&#010;&gt; STRING,rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;\") ,output&#010;&gt; = @DataTypeHint(\"ROW&lt;rule_id STRING,rule_name STRING,rule_type_name&#010;&gt; STRING,`result` INT,in_path BOOLEAN&gt;\"))&#010;&gt; public static class FlatRowFunction extends TableFunction&lt;Row&gt; {&#010;&gt;     private static final long serialVersionUID = 1L;&#010;&gt;&#010;&gt;     public void eval(Row[] rows) {&#010;&gt;         for (Row row : rows) {&#010;&gt;             collect(row);&#010;&gt;         }&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&gt; 异常如下：&#010;&gt;&#010;&gt; org.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt; From line 1, column 149 to line 1, column 174: No match found for&#010;&gt; function signature&#010;&gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt;&#010;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&gt;         at&#010;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:60)&#010;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;         at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;         at&#010;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt;         at&#010;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt;         at&#010;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt;         at&#010;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt;         at&#010;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt;         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt;         at&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt;         at&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt;         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt;         at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt;         at&#010;&gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt;         at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt;         at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt;         at&#010;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt;         at&#010;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt;         at&#010;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt;         at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt; line 1, column 149 to line 1, column 174: No match found for function&#010;&gt; signature flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647)&#010;&gt; rule_id, VARCHAR(2147483647) rule_name, VARCHAR(2147483647)&#010;&gt; rule_type_name, INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; Method)&#010;&gt;         at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;         at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt;         at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;         at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt;         ... 28 more&#010;&gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No&#010;&gt; match found for function signature&#010;&gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; Method)&#010;&gt;         at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;         at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;         at&#010;&gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt;         ... 56 more&#010;&gt;&#010;&gt;&#010;&gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月21日周二 下午7:41写道：&#010;&gt;&#010;&gt; &gt; 可以，定义清楚 getResultType 和 getParameterTypes, 可以参考[1]&#010;&gt; &gt;&#010;&gt; &gt; [1]&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二 下午7:25写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#010;&gt; &gt; &gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_R21p8YP18dJtqcYN=Kdo-u5ovXLTHJa=roZJ9zgc-q1w@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:58:08 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "Hi，&#010;如果只是想打平array，Flink有个内置的方法，可以参考[1]的”Expanding arrays&#010;into a relation“部分&#010;&#010;[1]&#010;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#010;&#010;Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月22日周三 上午11:17写道：&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; Row[] 作为 eval 参数，目前还不支持。社区已经有一个 issue 在跟进支持这个功能：&#010;&gt; https://issues.apache.org/jira/browse/FLINK-17855&#010;&gt;&#010;&gt;&#010;&gt; Best,&#010;&gt; Jark&#010;&gt;&#010;&gt; On Wed, 22 Jul 2020 at 10:45, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; hi,&#010;&gt; &gt;  我想将一个array&lt;row&gt;打散成多行，但是并没有成功&#010;&gt; &gt;&#010;&gt; &gt; @FunctionHint(input =@DataTypeHint(\"ARRAY&lt;ROW&lt;rule_id STRING,rule_name&#010;&gt; &gt; STRING,rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;\") ,output&#010;&gt; &gt; = @DataTypeHint(\"ROW&lt;rule_id STRING,rule_name STRING,rule_type_name&#010;&gt; &gt; STRING,`result` INT,in_path BOOLEAN&gt;\"))&#010;&gt; &gt; public static class FlatRowFunction extends TableFunction&lt;Row&gt; {&#010;&gt; &gt;     private static final long serialVersionUID = 1L;&#010;&gt; &gt;&#010;&gt; &gt;     public void eval(Row[] rows) {&#010;&gt; &gt;         for (Row row : rows) {&#010;&gt; &gt;             collect(row);&#010;&gt; &gt;         }&#010;&gt; &gt;     }&#010;&gt; &gt; }&#010;&gt; &gt;&#010;&gt; &gt; 异常如下：&#010;&gt; &gt;&#010;&gt; &gt; org.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt; &gt; From line 1, column 149 to line 1, column 174: No match found for&#010;&gt; &gt; function signature&#010;&gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt;&#010;&gt; &gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:60)&#010;&gt; &gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt; &gt;         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt; &gt;         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt; &gt;         at&#010;&gt; org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt; &gt;         at&#010;&gt; &gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt; &gt;         at&#010;&gt; org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt; &gt;         at&#010;&gt; org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt; &gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt; &gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt; &gt;         at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt; &gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt; &gt; line 1, column 149 to line 1, column 174: No match found for function&#010;&gt; &gt; signature flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647)&#010;&gt; &gt; rule_id, VARCHAR(2147483647) rule_name, VARCHAR(2147483647)&#010;&gt; &gt; rule_type_name, INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt; Method)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt;         at&#010;&gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt;         at&#010;&gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt; &gt;         at&#010;&gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt; &gt;         at&#010;&gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt; &gt;         at&#010;&gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt; &gt;         at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt;         at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; &gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; &gt;         ... 28 more&#010;&gt; &gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No&#010;&gt; &gt; match found for function signature&#010;&gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt; Method)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt;         at&#010;&gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt;         at&#010;&gt; &gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt;         at&#010;&gt; &gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt; &gt;         ... 56 more&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月21日周二 下午7:41写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; 可以，定义清楚 getResultType 和 getParameterTypes, 可以参考[1]&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; [1]&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二 下午7:25写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; hi&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#010;&gt; &gt; &gt; &gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "4",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CAEZk040=ikNmSZEy8pjqV2cDV5qAHtGnhZgZNcdG+nrong2uvw@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 09:45:10 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "hi 、Benchao Li&#010;我尝试了将数组打散的方式，但是报了一个莫名其妙的错误，可以帮忙看看嘛&#010;&#010;tableEnv.executeSql(\"CREATE TABLE parser_data_test (\\n\" +&#010;        \"  data ROW&lt;flow_task_id BIGINT,flow_id STRING,flow_version&#010;STRING,path STRING,country_id INT,create_time BIGINT,\" +&#010;        \"spent_time DECIMAL(10,2),features&#010;ROW&lt;`user_ic_no_aku_uid.pdl_cdpd`&#010;STRING,`user_ic_no_aku_uid.pdl_current_unpay` INT,\" +&#010;        \"`user_ic_no_aku_uid.current_overdue_collection`&#010;INT&gt;,rule_results ARRAY&lt;ROW&lt;rule_id STRING,rule_name STRING,\" +&#010;        \"rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;&gt;,\\n\" +&#010;        \"  createTime BIGINT,\\n\" +&#010;        \"  tindex INT\\n\" +&#010;        \") WITH (\\n\" +&#010;        \" 'connector' = 'kafka-0.11',\\n\" +&#010;        \" 'topic' = 'parser_data_test',\\n\" +&#010;        \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;        \" 'properties.group.id' = 'testGroup',\\n\" +&#010;        \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;        \" 'format' = 'json',\\n\" +&#010;        \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;        \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;        \")\");&#010;&#010;Table table = tableEnv.sqlQuery(\"select&#010;data.flow_task_id,data.features.`user_ic_no_aku_uid.pdl_current_unpay`,rule_id,tindex&#010;from parser_data_test CROSS JOIN UNNEST(data.rule_results) AS t&#010;(rule_id,rule_name,rule_type_name,`result`,in_path)\");&#010;&#010;table.printSchema();&#010;tableEnv.toAppendStream(table,&#010;Types.ROW(TypeConversions.fromDataTypeToLegacyInfo(table.getSchema().getFieldDataTypes()))).print();&#010;&#010;&#010;异常信息：&#010;&#010;rg.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt;From line 0, column 0 to line 1, column 139: Column 'data.data' not&#010;found in table 'parser_data_test'&#010;&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&#009;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:185)&#010;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:664)&#010;&#009;at com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:63)&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&#009;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&#009;at java.lang.reflect.Method.invoke(Method.java:498)&#010;&#009;at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&#009;at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&#009;at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&#009;at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&#009;at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&#009;at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&#009;at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&#009;at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&#009;at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&#009;at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&#009;at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&#009;at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&#009;at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&#009;at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&#009;at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&#009;at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&#009;at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&#009;at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&#009;at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;line 0, column 0 to line 1, column 139: Column 'data.data' not found&#010;in table 'parser_data_test'&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&#009;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&#009;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&#009;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&#009;at org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5991)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5976)&#010;&#009;at org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5583)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateUnnest(SqlValidatorImpl.java:3271)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3253)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&#009;at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&#009;at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&#009;at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&#009;... 28 more&#010;Caused by: org.apache.calcite.sql.validate.SqlValidatorException:&#010;Column 'data.data' not found in table 'parser_data_test'&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&#009;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&#009;... 51 more&#010;&#010;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月22日周三 下午2:05写道：&#010;&#010;&gt; Hi，&#010;&gt; 如果只是想打平array，Flink有个内置的方法，可以参考[1]的”Expanding&#010;arrays into a relation“部分&#010;&gt;&#010;&gt; [1]&#010;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#010;&gt;&#010;&gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月22日周三 上午11:17写道：&#010;&gt;&#010;&gt; &gt; Hi,&#010;&gt; &gt;&#010;&gt; &gt; Row[] 作为 eval 参数，目前还不支持。社区已经有一个 issue 在跟进支持这个功能：&#010;&gt; &gt; https://issues.apache.org/jira/browse/FLINK-17855&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Jark&#010;&gt; &gt;&#010;&gt; &gt; On Wed, 22 Jul 2020 at 10:45, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; &gt; hi,&#010;&gt; &gt; &gt;  我想将一个array&lt;row&gt;打散成多行，但是并没有成功&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; @FunctionHint(input =@DataTypeHint(\"ARRAY&lt;ROW&lt;rule_id STRING,rule_name&#010;&gt; &gt; &gt; STRING,rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;\") ,output&#010;&gt; &gt; &gt; = @DataTypeHint(\"ROW&lt;rule_id STRING,rule_name STRING,rule_type_name&#010;&gt; &gt; &gt; STRING,`result` INT,in_path BOOLEAN&gt;\"))&#010;&gt; &gt; &gt; public static class FlatRowFunction extends TableFunction&lt;Row&gt; {&#010;&gt; &gt; &gt;     private static final long serialVersionUID = 1L;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;     public void eval(Row[] rows) {&#010;&gt; &gt; &gt;         for (Row row : rows) {&#010;&gt; &gt; &gt;             collect(row);&#010;&gt; &gt; &gt;         }&#010;&gt; &gt; &gt;     }&#010;&gt; &gt; &gt; }&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; 异常如下：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; org.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt; &gt; &gt; From line 1, column 149 to line 1, column 174: No match found for&#010;&gt; &gt; &gt; function signature&#010;&gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:60)&#010;&gt; &gt; &gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; &gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt; &gt; &gt;         at&#010;&gt; org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt; &gt; &gt;         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt; &gt; &gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt; &gt; &gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt; &gt; &gt;         at&#010;&gt; com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt; &gt; &gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt; &gt; &gt; line 1, column 149 to line 1, column 174: No match found for function&#010;&gt; &gt; &gt; signature flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647)&#010;&gt; &gt; &gt; rule_id, VARCHAR(2147483647) rule_name, VARCHAR(2147483647)&#010;&gt; &gt; &gt; rule_type_name, INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt; &gt;         at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt; &gt; Method)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt; &gt; &gt;         at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt; &gt;         at&#010;&gt; org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; &gt; &gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; &gt; &gt;         ... 28 more&#010;&gt; &gt; &gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No&#010;&gt; &gt; &gt; match found for function signature&#010;&gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt; &gt;         at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt; &gt; Method)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt; &gt; &gt;         ... 56 more&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月21日周二 下午7:41写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 可以，定义清楚 getResultType 和 getParameterTypes, 可以参考[1]&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; [1]&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二 下午7:25写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; hi&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#010;&gt; &gt; &gt; &gt; &gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_SU=-WSzS_0VBPekJ_ngf2DTJvu-QCSRsx5f0j=-=gdyA@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 04:44:16 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "我感觉这可能是calcite的bug，CC Danny老师&#010;&#010;Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午5:46写道：&#010;&#010;&gt; hi 、Benchao Li&#010;&gt; 我尝试了将数组打散的方式，但是报了一个莫名其妙的错误，可以帮忙看看嘛&#010;&gt;&#010;&gt; tableEnv.executeSql(\"CREATE TABLE parser_data_test (\\n\" +&#010;&gt;         \"  data ROW&lt;flow_task_id BIGINT,flow_id STRING,flow_version&#010;&gt; STRING,path STRING,country_id INT,create_time BIGINT,\" +&#010;&gt;         \"spent_time DECIMAL(10,2),features&#010;&gt; ROW&lt;`user_ic_no_aku_uid.pdl_cdpd`&#010;&gt; STRING,`user_ic_no_aku_uid.pdl_current_unpay` INT,\" +&#010;&gt;         \"`user_ic_no_aku_uid.current_overdue_collection`&#010;&gt; INT&gt;,rule_results ARRAY&lt;ROW&lt;rule_id STRING,rule_name STRING,\" +&#010;&gt;         \"rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;&gt;,\\n\" +&#010;&gt;         \"  createTime BIGINT,\\n\" +&#010;&gt;         \"  tindex INT\\n\" +&#010;&gt;         \") WITH (\\n\" +&#010;&gt;         \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt;         \" 'topic' = 'parser_data_test',\\n\" +&#010;&gt;         \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;&gt;         \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt;         \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt;         \" 'format' = 'json',\\n\" +&#010;&gt;         \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt;         \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt;         \")\");&#010;&gt;&#010;&gt; Table table = tableEnv.sqlQuery(\"select&#010;&gt;&#010;&gt; data.flow_task_id,data.features.`user_ic_no_aku_uid.pdl_current_unpay`,rule_id,tindex&#010;&gt; from parser_data_test CROSS JOIN UNNEST(data.rule_results) AS t&#010;&gt; (rule_id,rule_name,rule_type_name,`result`,in_path)\");&#010;&gt;&#010;&gt; table.printSchema();&#010;&gt; tableEnv.toAppendStream(table,&#010;&gt;&#010;&gt; Types.ROW(TypeConversions.fromDataTypeToLegacyInfo(table.getSchema().getFieldDataTypes()))).print();&#010;&gt;&#010;&gt;&#010;&gt; 异常信息：&#010;&gt;&#010;&gt; rg.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt; From line 0, column 0 to line 1, column 139: Column 'data.data' not&#010;&gt; found in table 'parser_data_test'&#010;&gt;&#010;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:185)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt;         at&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:664)&#010;&gt;         at&#010;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:63)&#010;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;         at&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;         at&#010;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt;         at&#010;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt;         at&#010;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt;         at&#010;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt;         at&#010;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt;         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt;         at&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt;         at&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt;         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt;         at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt;         at&#010;&gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt;         at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt;         at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt;         at&#010;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt;         at&#010;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt;         at&#010;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt;         at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt; line 0, column 0 to line 1, column 139: Column 'data.data' not found&#010;&gt; in table 'parser_data_test'&#010;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; Method)&#010;&gt;         at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;         at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5991)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5976)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5583)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateUnnest(SqlValidatorImpl.java:3271)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3253)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt;         at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;         at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt;         at&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt;         ... 28 more&#010;&gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException:&#010;&gt; Column 'data.data' not found in table 'parser_data_test'&#010;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; Method)&#010;&gt;         at&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;         at&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;         at&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;         at&#010;&gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt;         ... 51 more&#010;&gt;&#010;&gt;&#010;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月22日周三 下午2:05写道：&#010;&gt;&#010;&gt; &gt; Hi，&#010;&gt; &gt; 如果只是想打平array，Flink有个内置的方法，可以参考[1]的”Expanding&#010;arrays into a relation“部分&#010;&gt; &gt;&#010;&gt; &gt; [1]&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#010;&gt; &gt;&#010;&gt; &gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月22日周三 上午11:17写道：&#010;&gt; &gt;&#010;&gt; &gt; &gt; Hi,&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Row[] 作为 eval 参数，目前还不支持。社区已经有一个 issue&#010;在跟进支持这个功能：&#010;&gt; &gt; &gt; https://issues.apache.org/jira/browse/FLINK-17855&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Jark&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On Wed, 22 Jul 2020 at 10:45, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; hi,&#010;&gt; &gt; &gt; &gt;  我想将一个array&lt;row&gt;打散成多行，但是并没有成功&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; @FunctionHint(input =@DataTypeHint(\"ARRAY&lt;ROW&lt;rule_id&#010;&gt; STRING,rule_name&#010;&gt; &gt; &gt; &gt; STRING,rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;\") ,output&#010;&gt; &gt; &gt; &gt; = @DataTypeHint(\"ROW&lt;rule_id STRING,rule_name STRING,rule_type_name&#010;&gt; &gt; &gt; &gt; STRING,`result` INT,in_path BOOLEAN&gt;\"))&#010;&gt; &gt; &gt; &gt; public static class FlatRowFunction extends TableFunction&lt;Row&gt; {&#010;&gt; &gt; &gt; &gt;     private static final long serialVersionUID = 1L;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;     public void eval(Row[] rows) {&#010;&gt; &gt; &gt; &gt;         for (Row row : rows) {&#010;&gt; &gt; &gt; &gt;             collect(row);&#010;&gt; &gt; &gt; &gt;         }&#010;&gt; &gt; &gt; &gt;     }&#010;&gt; &gt; &gt; &gt; }&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 异常如下：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; org.apache.flink.table.api.ValidationException: SQL validation&#010;&gt; failed.&#010;&gt; &gt; &gt; &gt; From line 1, column 149 to line 1, column 174: No match found for&#010;&gt; &gt; &gt; &gt; function signature&#010;&gt; &gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:60)&#010;&gt; &gt; &gt; &gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;&gt; Method)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt; &gt; &gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt; &gt; &gt; &gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt; &gt; &gt; &gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt; &gt; &gt; &gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt; &gt; &gt; &gt; line 1, column 149 to line 1, column 174: No match found for function&#010;&gt; &gt; &gt; &gt; signature flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647)&#010;&gt; &gt; &gt; &gt; rule_id, VARCHAR(2147483647) rule_name, VARCHAR(2147483647)&#010;&gt; &gt; &gt; &gt; rule_type_name, INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt; &gt; &gt; Method)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt; &gt; &gt; &gt;         at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; &gt; &gt; &gt;         ... 28 more&#010;&gt; &gt; &gt; &gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No&#010;&gt; &gt; &gt; &gt; match found for function signature&#010;&gt; &gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt; &gt; &gt; Method)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt; &gt; &gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt; &gt; &gt; &gt;         ... 56 more&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月21日周二 下午7:41写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 可以，定义清楚 getResultType 和 getParameterTypes, 可以参考[1]&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; [1]&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二&#010;下午7:25写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt; hi&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#010;&gt; &gt; &gt; &gt; &gt; &gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#010;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Benchao Li&#010;&gt; &gt;&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "6",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_Rjz5Hycs7XD9ueDdiEzunaSSca0492z0kEoRDKN=JwfQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 04:52:14 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "现在有一个work around，就是你可以用子查询先把row展开，比如：&#010;select ...&#010;from (&#010;  select data.rule_results as rule_results, ...&#010;) cross join unnest(rule_results) as t(...)&#010;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月23日周四 下午12:44写道：&#010;&#010;&gt; 我感觉这可能是calcite的bug，CC Danny老师&#010;&gt;&#010;&gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午5:46写道：&#010;&gt;&#010;&gt;&gt; hi 、Benchao Li&#010;&gt;&gt; 我尝试了将数组打散的方式，但是报了一个莫名其妙的错误，可以帮忙看看嘛&#010;&gt;&gt;&#010;&gt;&gt; tableEnv.executeSql(\"CREATE TABLE parser_data_test (\\n\" +&#010;&gt;&gt;         \"  data ROW&lt;flow_task_id BIGINT,flow_id STRING,flow_version&#010;&gt;&gt; STRING,path STRING,country_id INT,create_time BIGINT,\" +&#010;&gt;&gt;         \"spent_time DECIMAL(10,2),features&#010;&gt;&gt; ROW&lt;`user_ic_no_aku_uid.pdl_cdpd`&#010;&gt;&gt; STRING,`user_ic_no_aku_uid.pdl_current_unpay` INT,\" +&#010;&gt;&gt;         \"`user_ic_no_aku_uid.current_overdue_collection`&#010;&gt;&gt; INT&gt;,rule_results ARRAY&lt;ROW&lt;rule_id STRING,rule_name STRING,\" +&#010;&gt;&gt;         \"rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;&gt;,\\n\" +&#010;&gt;&gt;         \"  createTime BIGINT,\\n\" +&#010;&gt;&gt;         \"  tindex INT\\n\" +&#010;&gt;&gt;         \") WITH (\\n\" +&#010;&gt;&gt;         \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt;&gt;         \" 'topic' = 'parser_data_test',\\n\" +&#010;&gt;&gt;         \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;&gt;&gt;         \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt;&gt;         \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt;&gt;         \" 'format' = 'json',\\n\" +&#010;&gt;&gt;         \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt;&gt;         \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt;&gt;         \")\");&#010;&gt;&gt;&#010;&gt;&gt; Table table = tableEnv.sqlQuery(\"select&#010;&gt;&gt;&#010;&gt;&gt; data.flow_task_id,data.features.`user_ic_no_aku_uid.pdl_current_unpay`,rule_id,tindex&#010;&gt;&gt; from parser_data_test CROSS JOIN UNNEST(data.rule_results) AS t&#010;&gt;&gt; (rule_id,rule_name,rule_type_name,`result`,in_path)\");&#010;&gt;&gt;&#010;&gt;&gt; table.printSchema();&#010;&gt;&gt; tableEnv.toAppendStream(table,&#010;&gt;&gt;&#010;&gt;&gt; Types.ROW(TypeConversions.fromDataTypeToLegacyInfo(table.getSchema().getFieldDataTypes()))).print();&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; 异常信息：&#010;&gt;&gt;&#010;&gt;&gt; rg.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt;&gt; From line 0, column 0 to line 1, column 139: Column 'data.data' not&#010;&gt;&gt; found in table 'parser_data_test'&#010;&gt;&gt;&#010;&gt;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:185)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:664)&#010;&gt;&gt;         at&#010;&gt;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:63)&#010;&gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt;&gt;         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt;&gt;         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt;&gt;         at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt;&gt;         at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt;&gt;         at&#010;&gt;&gt; org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt;&gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt;&gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt;&gt;         at&#010;&gt;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt;&gt;         at&#010;&gt;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt;&gt;         at&#010;&gt;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt;&gt;         at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt;&gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt;&gt; line 0, column 0 to line 1, column 139: Column 'data.data' not found&#010;&gt;&gt; in table 'parser_data_test'&#010;&gt;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt;&gt; Method)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;&gt;         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5991)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5976)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5583)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateUnnest(SqlValidatorImpl.java:3271)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3253)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;&gt;         at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt;&gt;         ... 28 more&#010;&gt;&gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException:&#010;&gt;&gt; Column 'data.data' not found in table 'parser_data_test'&#010;&gt;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt;&gt; Method)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;&gt;         at&#010;&gt;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;&gt;         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;&gt;         at&#010;&gt;&gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt;&gt;         ... 51 more&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月22日周三 下午2:05写道：&#010;&gt;&gt;&#010;&gt;&gt; &gt; Hi，&#010;&gt;&gt; &gt; 如果只是想打平array，Flink有个内置的方法，可以参考[1]的”Expanding&#010;arrays into a relation“部分&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; [1]&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月22日周三 上午11:17写道：&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; &gt; Hi,&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Row[] 作为 eval 参数，目前还不支持。社区已经有一个 issue&#010;在跟进支持这个功能：&#010;&gt;&gt; &gt; &gt; https://issues.apache.org/jira/browse/FLINK-17855&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; Jark&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; On Wed, 22 Jul 2020 at 10:45, Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;wrote:&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; hi,&#010;&gt;&gt; &gt; &gt; &gt;  我想将一个array&lt;row&gt;打散成多行，但是并没有成功&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; @FunctionHint(input =@DataTypeHint(\"ARRAY&lt;ROW&lt;rule_id&#010;&gt;&gt; STRING,rule_name&#010;&gt;&gt; &gt; &gt; &gt; STRING,rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;\")&#010;&gt;&gt; ,output&#010;&gt;&gt; &gt; &gt; &gt; = @DataTypeHint(\"ROW&lt;rule_id STRING,rule_name STRING,rule_type_name&#010;&gt;&gt; &gt; &gt; &gt; STRING,`result` INT,in_path BOOLEAN&gt;\"))&#010;&gt;&gt; &gt; &gt; &gt; public static class FlatRowFunction extends TableFunction&lt;Row&gt;&#010;{&#010;&gt;&gt; &gt; &gt; &gt;     private static final long serialVersionUID = 1L;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;     public void eval(Row[] rows) {&#010;&gt;&gt; &gt; &gt; &gt;         for (Row row : rows) {&#010;&gt;&gt; &gt; &gt; &gt;             collect(row);&#010;&gt;&gt; &gt; &gt; &gt;         }&#010;&gt;&gt; &gt; &gt; &gt;     }&#010;&gt;&gt; &gt; &gt; &gt; }&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; 异常如下：&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; org.apache.flink.table.api.ValidationException: SQL validation&#010;&gt;&gt; failed.&#010;&gt;&gt; &gt; &gt; &gt; From line 1, column 149 to line 1, column 174: No match found for&#010;&gt;&gt; &gt; &gt; &gt; function signature&#010;&gt;&gt; &gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt;&gt; &gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt;&gt; &gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:60)&#010;&gt;&gt; &gt; &gt; &gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;&gt;&gt; Method)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt;&gt; &gt; &gt; &gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt;&gt; &gt; &gt; &gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt;&gt; &gt; &gt; &gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt;&gt; &gt; &gt; &gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt;&gt; &gt; &gt; &gt; line 1, column 149 to line 1, column 174: No match found for&#010;&gt;&gt; function&#010;&gt;&gt; &gt; &gt; &gt; signature flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647)&#010;&gt;&gt; &gt; &gt; &gt; rule_id, VARCHAR(2147483647) rule_name, VARCHAR(2147483647)&#010;&gt;&gt; &gt; &gt; &gt; rule_type_name, INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt;&gt; &gt; &gt; &gt; Method)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt;&gt; &gt; &gt; &gt;         at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt;&gt; &gt; &gt; &gt;         ... 28 more&#010;&gt;&gt; &gt; &gt; &gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException:&#010;No&#010;&gt;&gt; &gt; &gt; &gt; match found for function signature&#010;&gt;&gt; &gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt;&gt; &gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt;&gt; &gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt;&gt; &gt; &gt; &gt; Method)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt;&gt; &gt; &gt; &gt;         at&#010;&gt;&gt; &gt; &gt; &gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt;&gt; &gt; &gt; &gt;         ... 56 more&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月21日周二 下午7:41写道：&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; 可以，定义清楚 getResultType 和 getParameterTypes, 可以参考[1]&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; [1]&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二&#010;下午7:25写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt; hi&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#010;&gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; --&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Benchao Li&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&gt;&#010;&#010;&#010;-- &#010;&#010;Best,&#010;Benchao Li&#010;&#010;",
        "depth": "7",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<CAEZk043j2byEf8XKeVjcO_wS2zOQiSerUQo1JnjxgMbhWOuB6w@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 07:16:53 GMT",
        "subject": "Re: flink1.11 tablefunction",
        "content": "hi&#010;这貌似确实是一个bug，先用子查询打开后程序就可以运行正常了&#010;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月23日周四 下午12:52写道：&#010;&#010;&gt; 现在有一个work around，就是你可以用子查询先把row展开，比如：&#010;&gt; select ...&#010;&gt; from (&#010;&gt;   select data.rule_results as rule_results, ...&#010;&gt; ) cross join unnest(rule_results) as t(...)&#010;&gt;&#010;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月23日周四 下午12:44写道：&#010;&gt;&#010;&gt; &gt; 我感觉这可能是calcite的bug，CC Danny老师&#010;&gt; &gt;&#010;&gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午5:46写道：&#010;&gt; &gt;&#010;&gt; &gt;&gt; hi 、Benchao Li&#010;&gt; &gt;&gt; 我尝试了将数组打散的方式，但是报了一个莫名其妙的错误，可以帮忙看看嘛&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; tableEnv.executeSql(\"CREATE TABLE parser_data_test (\\n\" +&#010;&gt; &gt;&gt;         \"  data ROW&lt;flow_task_id BIGINT,flow_id STRING,flow_version&#010;&gt; &gt;&gt; STRING,path STRING,country_id INT,create_time BIGINT,\" +&#010;&gt; &gt;&gt;         \"spent_time DECIMAL(10,2),features&#010;&gt; &gt;&gt; ROW&lt;`user_ic_no_aku_uid.pdl_cdpd`&#010;&gt; &gt;&gt; STRING,`user_ic_no_aku_uid.pdl_current_unpay` INT,\" +&#010;&gt; &gt;&gt;         \"`user_ic_no_aku_uid.current_overdue_collection`&#010;&gt; &gt;&gt; INT&gt;,rule_results ARRAY&lt;ROW&lt;rule_id STRING,rule_name STRING,\" +&#010;&gt; &gt;&gt;         \"rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;&gt;,\\n\"&#010;+&#010;&gt; &gt;&gt;         \"  createTime BIGINT,\\n\" +&#010;&gt; &gt;&gt;         \"  tindex INT\\n\" +&#010;&gt; &gt;&gt;         \") WITH (\\n\" +&#010;&gt; &gt;&gt;         \" 'connector' = 'kafka-0.11',\\n\" +&#010;&gt; &gt;&gt;         \" 'topic' = 'parser_data_test',\\n\" +&#010;&gt; &gt;&gt;         \" 'properties.bootstrap.servers' = 'localhost:9092',\\n\" +&#010;&gt; &gt;&gt;         \" 'properties.group.id' = 'testGroup',\\n\" +&#010;&gt; &gt;&gt;         \" 'scan.startup.mode' = 'earliest-offset',\\n\" +&#010;&gt; &gt;&gt;         \" 'format' = 'json',\\n\" +&#010;&gt; &gt;&gt;         \" 'json.fail-on-missing-field' = 'false',\\n\" +&#010;&gt; &gt;&gt;         \" 'json.ignore-parse-errors' = 'true'\\n\" +&#010;&gt; &gt;&gt;         \")\");&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Table table = tableEnv.sqlQuery(\"select&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; data.flow_task_id,data.features.`user_ic_no_aku_uid.pdl_current_unpay`,rule_id,tindex&#010;&gt; &gt;&gt; from parser_data_test CROSS JOIN UNNEST(data.rule_results) AS t&#010;&gt; &gt;&gt; (rule_id,rule_name,rule_type_name,`result`,in_path)\");&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; table.printSchema();&#010;&gt; &gt;&gt; tableEnv.toAppendStream(table,&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; Types.ROW(TypeConversions.fromDataTypeToLegacyInfo(table.getSchema().getFieldDataTypes()))).print();&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; 异常信息：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; rg.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt; &gt;&gt; From line 0, column 0 to line 1, column 139: Column 'data.data' not&#010;&gt; &gt;&gt; found in table 'parser_data_test'&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:185)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:664)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:63)&#010;&gt; &gt;&gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt; &gt;&gt;         at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt; &gt;&gt;         at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt; &gt;&gt;         at&#010;&gt; org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt; &gt;&gt;         at&#010;&gt; org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt; &gt;&gt;         at org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt; &gt;&gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt; &gt;&gt;         at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt; &gt;&gt; Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;&gt; &gt;&gt; line 0, column 0 to line 1, column 139: Column 'data.data' not found&#010;&gt; &gt;&gt; in table 'parser_data_test'&#010;&gt; &gt;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt;&gt; Method)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt;&gt;         at&#010;&gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.DelegatingScope.fullyQualify(DelegatingScope.java:439)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5991)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$Expander.visit(SqlValidatorImpl.java:5976)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.calcite.sql.SqlIdentifier.accept(SqlIdentifier.java:321)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.expand(SqlValidatorImpl.java:5583)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateUnnest(SqlValidatorImpl.java:3271)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3253)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt;&gt;         at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; &gt;&gt;         at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; &gt;&gt;         ... 28 more&#010;&gt; &gt;&gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException:&#010;&gt; &gt;&gt; Column 'data.data' not found in table 'parser_data_test'&#010;&gt; &gt;&gt;         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt;&gt; Method)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt;&gt;         at&#010;&gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt;&gt;         at&#010;&gt; &gt;&gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt; &gt;&gt;         ... 51 more&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月22日周三 下午2:05写道：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; Hi，&#010;&gt; &gt;&gt; &gt; 如果只是想打平array，Flink有个内置的方法，可以参考[1]的”Expanding&#010;arrays into a relation“部分&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; [1]&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/sql/queries.html#joins&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; Jark Wu &lt;imjark@gmail.com&gt; 于2020年7月22日周三 上午11:17写道：&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; Hi,&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; Row[] 作为 eval 参数，目前还不支持。社区已经有一个&#010;issue 在跟进支持这个功能：&#010;&gt; &gt;&gt; &gt; &gt; https://issues.apache.org/jira/browse/FLINK-17855&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; Best,&#010;&gt; &gt;&gt; &gt; &gt; Jark&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; On Wed, 22 Jul 2020 at 10:45, Dream-底限 &lt;zhangyu@akulaku.com&gt;&#010;wrote:&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; hi,&#010;&gt; &gt;&gt; &gt; &gt; &gt;  我想将一个array&lt;row&gt;打散成多行，但是并没有成功&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; @FunctionHint(input =@DataTypeHint(\"ARRAY&lt;ROW&lt;rule_id&#010;&gt; &gt;&gt; STRING,rule_name&#010;&gt; &gt;&gt; &gt; &gt; &gt; STRING,rule_type_name STRING,`result` INT,in_path BOOLEAN&gt;&gt;\")&#010;&gt; &gt;&gt; ,output&#010;&gt; &gt;&gt; &gt; &gt; &gt; = @DataTypeHint(\"ROW&lt;rule_id STRING,rule_name&#010;&gt; STRING,rule_type_name&#010;&gt; &gt;&gt; &gt; &gt; &gt; STRING,`result` INT,in_path BOOLEAN&gt;\"))&#010;&gt; &gt;&gt; &gt; &gt; &gt; public static class FlatRowFunction extends TableFunction&lt;Row&gt;&#010;{&#010;&gt; &gt;&gt; &gt; &gt; &gt;     private static final long serialVersionUID = 1L;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt;     public void eval(Row[] rows) {&#010;&gt; &gt;&gt; &gt; &gt; &gt;         for (Row row : rows) {&#010;&gt; &gt;&gt; &gt; &gt; &gt;             collect(row);&#010;&gt; &gt;&gt; &gt; &gt; &gt;         }&#010;&gt; &gt;&gt; &gt; &gt; &gt;     }&#010;&gt; &gt;&gt; &gt; &gt; &gt; }&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; 异常如下：&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; org.apache.flink.table.api.ValidationException: SQL validation&#010;&gt; &gt;&gt; failed.&#010;&gt; &gt;&gt; &gt; &gt; &gt; From line 1, column 149 to line 1, column 174: No match found&#010;for&#010;&gt; &gt;&gt; &gt; &gt; &gt; function signature&#010;&gt; &gt;&gt; &gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt;&gt; &gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt;&gt; &gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; com.akulaku.data.flink.ParserDataTest.parserDataTest(ParserDataTest.java:60)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native&#010;&gt; &gt;&gt; Method)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at java.lang.reflect.Method.invoke(Method.java:498)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt; org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; org.junit.runners.ParentRunner.run(ParentRunner.java:363)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at org.junit.runner.JUnitCore.run(JUnitCore.java:137)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)&#010;&gt; &gt;&gt; &gt; &gt; &gt; Caused by: org.apache.calcite.runtime.CalciteContextException:&#010;&gt; From&#010;&gt; &gt;&gt; &gt; &gt; &gt; line 1, column 149 to line 1, column 174: No match found for&#010;&gt; &gt;&gt; function&#010;&gt; &gt;&gt; &gt; &gt; &gt; signature flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647)&#010;&gt; &gt;&gt; &gt; &gt; &gt; rule_id, VARCHAR(2147483647) rule_name, VARCHAR(2147483647)&#010;&gt; &gt;&gt; &gt; &gt; &gt; rule_type_name, INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt;&gt; &gt; &gt; &gt; Method)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; $apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         ... 28 more&#010;&gt; &gt;&gt; &gt; &gt; &gt; Caused by: org.apache.calcite.sql.validate.SqlValidatorException:&#010;&gt; No&#010;&gt; &gt;&gt; &gt; &gt; &gt; match found for function signature&#010;&gt; &gt;&gt; &gt; &gt; &gt; flatRow(&lt;RecordType:peek_no_expand(VARCHAR(2147483647) rule_id,&#010;&gt; &gt;&gt; &gt; &gt; &gt; VARCHAR(2147483647) rule_name, VARCHAR(2147483647) rule_type_name,&#010;&gt; &gt;&gt; &gt; &gt; &gt; INTEGER result, BOOLEAN in_path) ARRAY&gt;)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#010;&gt; &gt;&gt; &gt; &gt; &gt; Method)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         at&#010;&gt; &gt;&gt; &gt; &gt; &gt; org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&gt; &gt;&gt; &gt; &gt; &gt;         ... 56 more&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; godfrey he &lt;godfreyhe@gmail.com&gt; 于2020年7月21日周二&#010;下午7:41写道：&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 可以，定义清楚 getResultType 和 getParameterTypes,&#010;可以参考[1]&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; [1]&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/functions/udfs.html#implementation-guide&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月21日周二&#010;下午7:25写道：&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt; hi&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; 我这面在定义一个表函数，通过继承TableFunction完成操作，但是eval方法中的参数类型都是java基本类型（至少看到的demo都是如此），想问一下eval方法中可以传flink&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt; 内部类型吗，比如说我想在eval()方法中传递Row类型要怎么操作，eval(Row&#010;row)&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; --&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; Best,&#010;&gt; &gt;&gt; &gt; Benchao Li&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Benchao Li&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Best,&#010;&gt; Benchao Li&#010;&gt;&#010;&#010;",
        "depth": "8",
        "reply": "<CAEZk040PKkdV99vpu_F7KY64bNqUV0hZBkmt_66QGN0nP35g0g@mail.gmail.com>"
    },
    {
        "id": "<tencent_19730D42001E53BB402271C5@qq.com>",
        "from": "&quot;462329521&quot; &lt;462329...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:28:28 GMT",
        "subject": "Flink sql中可以使用自定义窗口触发器吗",
        "content": "Hi，想问下现在的Flink&amp;nbsp;sql支持使用自定义窗口触发器吗？",
        "depth": "0",
        "reply": "<tencent_19730D42001E53BB402271C5@qq.com>"
    },
    {
        "id": "<CAELO933c5SG6MJauXp+5tc32UR8FvQp0bhJZPuTgNeDGHEKViw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 14:32:42 GMT",
        "subject": "Re: Flink sql中可以使用自定义窗口触发器吗",
        "content": "Hi,&#013;&#010;&#013;&#010;目前是不支持的。不过有个实验性功能可以指定提前输出的策略和迟到处理的策略&#010;[1]，可能可以满足你的需求。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;[1]:&#013;&#010;https://github.com/apache/flink/blob/master/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/WindowEmitStrategy.scala#L175&#013;&#010;&#013;&#010;On Tue, 21 Jul 2020 at 22:28, 462329521 &lt;462329521@qq.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi，想问下现在的Flink&amp;nbsp;sql支持使用自定义窗口触发器吗？&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_19730D42001E53BB402271C5@qq.com>"
    },
    {
        "id": "<2020072123300540512420@163.com>",
        "from": "&quot;dixingxing85@163.com&quot; &lt;dixingxin...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Tue, 21 Jul 2020 15:30:14 GMT",
        "subject": "Flink catalog的几个疑问",
        "content": "Hi Flink社区：&#013;&#010;有几个疑问希望社区小伙伴们帮忙解答一下：&#013;&#010;1.个人感觉Flink很有必要提供一个官方的catalog，用来支持各种connector，比如：kafka，jdbc，hbase等等connector。不知道社区有没有这个打算，目前没有看到对应的flip&#013;&#010;2.社区对hive catalog的定位是什么，后续有可能转正为flink 默认的catalog实现吗？&#010;&#013;&#010;3.hive catalog是不支持大小写敏感的（字段名都是小写），这个后续会带来哪些问题？想征集下大家的意见避免我们以后踩大坑。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Xingxing Di&#013;&#010;",
        "depth": "0",
        "reply": "<2020072123300540512420@163.com>"
    },
    {
        "id": "<CADQYLGueS9nN-67G_jeqo_s-mU7A-oJc_U=X_xeTyNyG6vkJSQ@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:07:46 GMT",
        "subject": "Re: Flink catalog的几个疑问",
        "content": "hi Xingxing,&#013;&#010;&#013;&#010;1. Flink 提供了一套catalog的接口，提提供了几个内置的实现：in-memory catalog,&#010;hive catalog,&#013;&#010;postgres catalog,&#013;&#010;可以根据自己的需求选择。也可以实现自定义的catalog。参考 [1]&#013;&#010;2. hive catalog 主要是对接 hive，方便读取现有的hive catalog的meta信息。当然也可以往hive&#013;&#010;catalog写新的meta。&#013;&#010;是否会转为默认catalog，据我所知，目前没有。&#013;&#010;3. 一般没什么问题。在和其他区分大小写的db对接的时候，可能有问题。&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;dixingxing85@163.com &lt;dixingxing85@163.com&gt; 于2020年7月21日周二 下午11:30写道：&#013;&#010;&#013;&#010;&gt; Hi Flink社区：&#013;&#010;&gt; 有几个疑问希望社区小伙伴们帮忙解答一下：&#013;&#010;&gt;&#013;&#010;&gt; 1.个人感觉Flink很有必要提供一个官方的catalog，用来支持各种connector，比如：kafka，jdbc，hbase等等connector。不知道社区有没有这个打算，目前没有看到对应的flip&#013;&#010;&gt; 2.社区对hive catalog的定位是什么，后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt; 3.hive catalog是不支持大小写敏感的（字段名都是小写），这个后续会带来哪些问题？想征集下大家的意见避免我们以后踩大坑。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Xingxing Di&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<2020072123300540512420@163.com>"
    },
    {
        "id": "<CABi+2jTSUo=jsMbf8zsfi+XKwxEVunN-S8ucD4ZJb5D_GCCU1Q@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:53:23 GMT",
        "subject": "Re: Flink catalog的几个疑问",
        "content": "Hi,&#013;&#010;&#013;&#010;HiveCatalog就是官方唯一的可以保存所有表的持久化Catalog，包括kafka，jdbc，hbase等等connectors。&#013;&#010;&#013;&#010;&gt; 后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&#013;&#010;目前不太可能，你看，Flink连Hadoop的依赖都没有打进来。Hive的依赖更不会默认打进来。&#010;依赖都没有，也不会成为默认的。&#013;&#010;&#013;&#010;&gt; hive catalog是不支持大小写敏感的&#013;&#010;&#013;&#010;是的，就像Godfrey说的，特别是JDBC对接的某些大小写敏感的db，这可能导致字段名对应不了。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 10:39 AM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi Xingxing,&#013;&#010;&gt;&#013;&#010;&gt; 1. Flink 提供了一套catalog的接口，提提供了几个内置的实现：in-memory&#010;catalog, hive catalog,&#013;&#010;&gt; postgres catalog,&#013;&#010;&gt; 可以根据自己的需求选择。也可以实现自定义的catalog。参考 [1]&#013;&#010;&gt; 2. hive catalog 主要是对接 hive，方便读取现有的hive catalog的meta信息。当然也可以往hive&#013;&#010;&gt; catalog写新的meta。&#013;&#010;&gt; 是否会转为默认catalog，据我所知，目前没有。&#013;&#010;&gt; 3. 一般没什么问题。在和其他区分大小写的db对接的时候，可能有问题。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Godfrey&#013;&#010;&gt;&#013;&#010;&gt; dixingxing85@163.com &lt;dixingxing85@163.com&gt; 于2020年7月21日周二 下午11:30写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi Flink社区：&#013;&#010;&gt; &gt; 有几个疑问希望社区小伙伴们帮忙解答一下：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.个人感觉Flink很有必要提供一个官方的catalog，用来支持各种connector，比如：kafka，jdbc，hbase等等connector。不知道社区有没有这个打算，目前没有看到对应的flip&#013;&#010;&gt; &gt; 2.社区对hive catalog的定位是什么，后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt; &gt; 3.hive catalog是不支持大小写敏感的（字段名都是小写），这个后续会带来哪些问题？想征集下大家的意见避免我们以后踩大坑。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Xingxing Di&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "2",
        "reply": "<2020072123300540512420@163.com>"
    },
    {
        "id": "<CAELO9335vZfj9=h-d91C_VncGn6_sKpY7H78LZEG9yJD6d0q8Q@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 03:22:56 GMT",
        "subject": "Re: Flink catalog的几个疑问",
        "content": "非常欢迎贡献开源一个轻量的 catalog 实现 :)&#013;&#010;&#013;&#010;On Wed, 22 Jul 2020 at 10:53, Jingsong Li &lt;jingsonglee0@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; HiveCatalog就是官方唯一的可以保存所有表的持久化Catalog，包括kafka，jdbc，hbase等等connectors。&#013;&#010;&gt;&#013;&#010;&gt; &gt; 后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt;&#013;&#010;&gt; 目前不太可能，你看，Flink连Hadoop的依赖都没有打进来。Hive的依赖更不会默认打进来。&#010;依赖都没有，也不会成为默认的。&#013;&#010;&gt;&#013;&#010;&gt; &gt; hive catalog是不支持大小写敏感的&#013;&#010;&gt;&#013;&#010;&gt; 是的，就像Godfrey说的，特别是JDBC对接的某些大小写敏感的db，这可能导致字段名对应不了。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Wed, Jul 22, 2020 at 10:39 AM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi Xingxing,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 1. Flink 提供了一套catalog的接口，提提供了几个内置的实现：in-memory&#010;catalog, hive catalog,&#013;&#010;&gt; &gt; postgres catalog,&#013;&#010;&gt; &gt; 可以根据自己的需求选择。也可以实现自定义的catalog。参考&#010;[1]&#013;&#010;&gt; &gt; 2. hive catalog 主要是对接 hive，方便读取现有的hive catalog的meta信息。当然也可以往hive&#013;&#010;&gt; &gt; catalog写新的meta。&#013;&#010;&gt; &gt; 是否会转为默认catalog，据我所知，目前没有。&#013;&#010;&gt; &gt; 3. 一般没什么问题。在和其他区分大小写的db对接的时候，可能有问题。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; dixingxing85@163.com &lt;dixingxing85@163.com&gt; 于2020年7月21日周二 下午11:30写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi Flink社区：&#013;&#010;&gt; &gt; &gt; 有几个疑问希望社区小伙伴们帮忙解答一下：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.个人感觉Flink很有必要提供一个官方的catalog，用来支持各种connector，比如：kafka，jdbc，hbase等等connector。不知道社区有没有这个打算，目前没有看到对应的flip&#013;&#010;&gt; &gt; &gt; 2.社区对hive catalog的定位是什么，后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt; &gt; &gt; 3.hive catalog是不支持大小写敏感的（字段名都是小写），这个后续会带来哪些问题？想征集下大家的意见避免我们以后踩大坑。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Xingxing Di&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<2020072123300540512420@163.com>"
    },
    {
        "id": "<d0f09bef2b4840aabd57a087c96709ed@autohome.com.cn>",
        "from": "刘首维 &lt;liushou...@autohome.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:18:25 GMT",
        "subject": "答复: Flink catalog的几个疑问",
        "content": "hi all, 我在想如果社区提供一个unified metastore server是不是会解决这个问题，然后写一个（一系列）catalog和这个metastore对应&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: Jark Wu &lt;imjark@gmail.com&gt;&#013;&#010;发送时间: 2020年7月22日 11:22:56&#013;&#010;收件人: user-zh&#013;&#010;主题: Re: Flink catalog的几个疑问&#013;&#010;&#013;&#010;非常欢迎贡献开源一个轻量的 catalog 实现 :)&#013;&#010;&#013;&#010;On Wed, 22 Jul 2020 at 10:53, Jingsong Li &lt;jingsonglee0@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; HiveCatalog就是官方唯一的可以保存所有表的持久化Catalog，包括kafka，jdbc，hbase等等connectors。&#013;&#010;&gt;&#013;&#010;&gt; &gt; 后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt;&#013;&#010;&gt; 目前不太可能，你看，Flink连Hadoop的依赖都没有打进来。Hive的依赖更不会默认打进来。&#010;依赖都没有，也不会成为默认的。&#013;&#010;&gt;&#013;&#010;&gt; &gt; hive catalog是不支持大小写敏感的&#013;&#010;&gt;&#013;&#010;&gt; 是的，就像Godfrey说的，特别是JDBC对接的某些大小写敏感的db，这可能导致字段名对应不了。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Wed, Jul 22, 2020 at 10:39 AM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi Xingxing,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 1. Flink 提供了一套catalog的接口，提提供了几个内置的实现：in-memory&#010;catalog, hive catalog,&#013;&#010;&gt; &gt; postgres catalog,&#013;&#010;&gt; &gt; 可以根据自己的需求选择。也可以实现自定义的catalog。参考&#010;[1]&#013;&#010;&gt; &gt; 2. hive catalog 主要是对接 hive，方便读取现有的hive catalog的meta信息。当然也可以往hive&#013;&#010;&gt; &gt; catalog写新的meta。&#013;&#010;&gt; &gt; 是否会转为默认catalog，据我所知，目前没有。&#013;&#010;&gt; &gt; 3. 一般没什么问题。在和其他区分大小写的db对接的时候，可能有问题。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; dixingxing85@163.com &lt;dixingxing85@163.com&gt; 于2020年7月21日周二 下午11:30写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi Flink社区：&#013;&#010;&gt; &gt; &gt; 有几个疑问希望社区小伙伴们帮忙解答一下：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.个人感觉Flink很有必要提供一个官方的catalog，用来支持各种connector，比如：kafka，jdbc，hbase等等connector。不知道社区有没有这个打算，目前没有看到对应的flip&#013;&#010;&gt; &gt; &gt; 2.社区对hive catalog的定位是什么，后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt; &gt; &gt; 3.hive catalog是不支持大小写敏感的（字段名都是小写），这个后续会带来哪些问题？想征集下大家的意见避免我们以后踩大坑。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Xingxing Di&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<2020072123300540512420@163.com>"
    },
    {
        "id": "<2020072213171218857078@163.com>",
        "from": "&quot;dixingxing85@163.com&quot; &lt;dixingxin...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:17:12 GMT",
        "subject": "Re: Re: Flink catalog的几个疑问",
        "content": "@Godfrey @Jingsong 感谢回复，很好的解答了我的疑惑！&#013;&#010;背景是这样的，目前我们正打算实现一套支持持久化的catalog，同时基于这个catalog实现一个metaserver，对外暴露REST接口，用来支持日常管理操作，比如：&#013;&#010;1.基于原生DDL管理source，sink，支持多种connector，并将这些元数据持久化到mysql中。&#013;&#010;2.做统一的权限控制&#013;&#010;&#013;&#010;我们面临两种选择：&#013;&#010;1.基于hive catalog建设自己的catalog（或者说直接使用hive catalog）：&#013;&#010;优势：鉴于hive catalog已经相对比较完善，直接使用可以减少开发量。&#013;&#010;劣势：不太明确社区对hive catalog的定位；大小写不敏感带来的麻烦。（大致是之前提到的3个问题）&#013;&#010;&#013;&#010;2.完全自建catalog：&#013;&#010;优势：灵活可控；依然可以利用已有的catalog&#013;&#010;劣势：设计开发成本高，引入大量代码可能需要持续维护（比如后续catalog&#010;api发生变动）；同时如果社区后续提供官方的catalog默认实现，我们会再次面临是否切换的问题。&#013;&#010;&#013;&#010;目前我们是倾向于自建catalog的。&#013;&#010;&#013;&#010;@Jark 默认的catalog应该算是个通用的需求，感觉在批流一体的大势下，是挺重要的一步（目前hive&#010;catalog可能还不够）。另外很多公司都在基于开源Flink做计算平台，如果Flink有默认catalog并提供metaserver，那么无疑是十分友好的。&#013;&#010;我们优先实现内部版本，实现既定目标。有机会的话，我们也希望能回馈社区。&#013;&#010;&#013;&#010;@All 目前我们想的还不够多，考虑可能不全面，还希望大家给些建议。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Xingxing Di&#013;&#010; &#013;&#010;Sender: Jark Wu&#013;&#010;Send Time: 2020-07-22 11:22&#013;&#010;Receiver: user-zh&#013;&#010;Subject: Re: Flink catalog的几个疑问&#013;&#010;非常欢迎贡献开源一个轻量的 catalog 实现 :)&#013;&#010; &#013;&#010;On Wed, 22 Jul 2020 at 10:53, Jingsong Li &lt;jingsonglee0@gmail.com&gt; wrote:&#013;&#010; &#013;&#010;&gt; Hi,&#013;&#010;&gt;&#013;&#010;&gt; HiveCatalog就是官方唯一的可以保存所有表的持久化Catalog，包括kafka，jdbc，hbase等等connectors。&#013;&#010;&gt;&#013;&#010;&gt; &gt; 后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt;&#013;&#010;&gt; 目前不太可能，你看，Flink连Hadoop的依赖都没有打进来。Hive的依赖更不会默认打进来。&#010;依赖都没有，也不会成为默认的。&#013;&#010;&gt;&#013;&#010;&gt; &gt; hive catalog是不支持大小写敏感的&#013;&#010;&gt;&#013;&#010;&gt; 是的，就像Godfrey说的，特别是JDBC对接的某些大小写敏感的db，这可能导致字段名对应不了。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Wed, Jul 22, 2020 at 10:39 AM godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi Xingxing,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 1. Flink 提供了一套catalog的接口，提提供了几个内置的实现：in-memory&#010;catalog, hive catalog,&#013;&#010;&gt; &gt; postgres catalog,&#013;&#010;&gt; &gt; 可以根据自己的需求选择。也可以实现自定义的catalog。参考&#010;[1]&#013;&#010;&gt; &gt; 2. hive catalog 主要是对接 hive，方便读取现有的hive catalog的meta信息。当然也可以往hive&#013;&#010;&gt; &gt; catalog写新的meta。&#013;&#010;&gt; &gt; 是否会转为默认catalog，据我所知，目前没有。&#013;&#010;&gt; &gt; 3. 一般没什么问题。在和其他区分大小写的db对接的时候，可能有问题。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; dixingxing85@163.com &lt;dixingxing85@163.com&gt; 于2020年7月21日周二 下午11:30写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi Flink社区：&#013;&#010;&gt; &gt; &gt; 有几个疑问希望社区小伙伴们帮忙解答一下：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 1.个人感觉Flink很有必要提供一个官方的catalog，用来支持各种connector，比如：kafka，jdbc，hbase等等connector。不知道社区有没有这个打算，目前没有看到对应的flip&#013;&#010;&gt; &gt; &gt; 2.社区对hive catalog的定位是什么，后续有可能转正为flink 默认的catalog实现吗？&#013;&#010;&gt; &gt; &gt; 3.hive catalog是不支持大小写敏感的（字段名都是小写），这个后续会带来哪些问题？想征集下大家的意见避免我们以后踩大坑。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best,&#013;&#010;&gt; &gt; &gt; Xingxing Di&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<2020072123300540512420@163.com>"
    },
    {
        "id": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>",
        "from": "&quot;1129656513@qq.com&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:08:56 GMT",
        "subject": "flinksql1.11中主键声明的问题",
        "content": "hi： &#013;&#010;    我在使用pyflink1.11过程中，使用flinksql维表时声明了主键primary key 但是还是会报错说我没有用声明主键，另外，当我使用inner&#010;join代替left join就不会有这个问题，请问这是什么问题&#013;&#010;    下面我附录了报错信息和代码。谢谢！&#013;&#010;&#013;&#010;报错附录&#013;&#010;Traceback (most recent call last):&#013;&#010;  File \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line 147, in deco&#013;&#010;    return f(*a, **kw)&#013;&#010;  File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value&#013;&#010;    format(target_id, \".\", name), value)&#013;&#010;py4j.protocol.Py4JJavaError: An error occurred while calling o5.execute.&#013;&#010;: org.apache.flink.table.api.TableException: UpsertStreamTableSink requires that Table has&#010;a full primary keys if it is updated.&#013;&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:93)&#013;&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlanInternal(StreamExecLegacySink.scala:48)&#013;&#010;        at org.apache.flink.table.planner.plan.nodes.exec.ExecNode$class.translateToPlan(ExecNode.scala:58)&#013;&#010;        at org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecLegacySink.translateToPlan(StreamExecLegacySink.scala:48)&#013;&#010;        at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:67)&#013;&#010;        at org.apache.flink.table.planner.delegation.StreamPlanner$$anonfun$translateToPlan$1.apply(StreamPlanner.scala:66)&#013;&#010;        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#013;&#010;        at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#013;&#010;        at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#013;&#010;        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#013;&#010;        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#013;&#010;        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#013;&#010;        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#013;&#010;        at org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:66)&#013;&#010;        at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:166)&#013;&#010;        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1248)&#013;&#010;        at org.apache.flink.table.api.internal.TableEnvironmentImpl.translateAndClearBuffer(TableEnvironmentImpl.java:1240)&#013;&#010;        at org.apache.flink.table.api.internal.TableEnvironmentImpl.execute(TableEnvironmentImpl.java:1197)&#013;&#010;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#013;&#010;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#013;&#010;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#013;&#010;        at java.lang.reflect.Method.invoke(Method.java:498)&#013;&#010;        at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#013;&#010;        at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#013;&#010;        at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)&#013;&#010;        at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#013;&#010;        at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)&#013;&#010;        at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)&#013;&#010;        at java.lang.Thread.run(Thread.java:748)&#013;&#010;&#013;&#010;&#013;&#010;During handling of the above exception, another exception occurred:&#013;&#010;&#013;&#010;Traceback (most recent call last):&#013;&#010;  File \"mysql_join.py\", line 90, in &lt;module&gt;&#013;&#010;    from_kafka_to_kafka_demo()&#013;&#010;  File \"mysql_join.py\", line 22, in from_kafka_to_kafka_demo&#013;&#010;    st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;  File \"/usr/local/lib/python3.7/site-packages/pyflink/table/table_environment.py\", line 1057,&#010;in execute&#013;&#010;    return JobExecutionResult(self._j_tenv.execute(job_name))&#013;&#010;  File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1286, in __call__&#013;&#010;    answer, self.gateway_client, self.target_id, self.name)&#013;&#010;  File \"/usr/local/lib/python3.7/site-packages/pyflink/util/exceptions.py\", line 154, in deco&#013;&#010;    raise exception_mapping[exception](s.split(': ', 1)[1], stack_trace)&#013;&#010;pyflink.util.exceptions.TableException: 'UpsertStreamTableSink requires that Table has a full&#010;primary keys if it is updated.'&#013;&#010;&#013;&#010;代码附录&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings,DataTypes,&#010;CsvTableSource, CsvTableSink&#013;&#010;from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;from pyflink.table.window import Tumble &#013;&#010;&#013;&#010;&#013;&#010;def from_kafka_to_kafka_demo():&#013;&#010;&#013;&#010;    # use blink table planner&#013;&#010;    env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;    env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;    env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;    st_env = StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010;&#013;&#010;    # register source and sink&#013;&#010;    register_rides_source(st_env)&#013;&#010;    register_rides_sink(st_env)&#013;&#010;    register_mysql_source(st_env)&#013;&#010;  &#013;&#010;&#013;&#010;    st_env.sql_update(\"insert into flink_result select  cast(t1.id as int) as id,cast(t2.type&#010;as varchar),cast( t1.time1 as bigint) as rowtime from source1 t1 left join dim_mysql t2 on&#010;t1.type=cast(t2.id as varchar) \")&#013;&#010;    st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;    &#013;&#010;&#013;&#010;&#013;&#010;def register_rides_source(st_env):&#013;&#010;    source_ddl = \\&#013;&#010;    \"\"\"&#013;&#010;    create table source1(&#013;&#010;     id int,&#013;&#010;     time1 varchar ,&#013;&#010;     type string&#013;&#010;     ) with (&#013;&#010;    'connector.type' = 'kafka',&#013;&#010;    'connector.topic' = 'tp1',&#013;&#010;    'connector.startup-mode' = 'latest-offset',&#013;&#010;    'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;    'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;    'format.type' = 'json',&#013;&#010;    'connector.version' = 'universal',&#013;&#010;    'update-mode' = 'append'&#013;&#010;     )&#013;&#010;    \"\"\"&#013;&#010;    st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;def register_mysql_source(st_env):&#013;&#010;    source_ddl = \\&#013;&#010;    \"\"\"&#013;&#010;    CREATE TABLE dim_mysql (&#013;&#010;    id int,  -- &#013;&#010;    type varchar -- &#013;&#010;    ) WITH (&#013;&#010;    'connector.type' = 'jdbc',&#013;&#010;    'connector.url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;    'connector.table' = 'flink_test',&#013;&#010;    'connector.driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;    'connector.username' = '***',&#013;&#010;    'connector.password' = '***',&#013;&#010;    'connector.lookup.cache.max-rows' = '5000',&#013;&#010;    'connector.lookup.cache.ttl' = '1min',&#013;&#010;    'connector.lookup.max-retries' = '3'&#013;&#010;    )&#013;&#010;    \"\"\"    &#013;&#010;    st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;def register_rides_sink(st_env):&#013;&#010;    sink_ddl = \\&#013;&#010;    \"\"\"&#013;&#010;    CREATE TABLE flink_result (&#013;&#010;    id int,   &#013;&#010;    type varchar,&#013;&#010;    rtime bigint,&#013;&#010;    primary key(id)  NOT ENFORCED&#013;&#010;    ) WITH (&#013;&#010;     'connector.type' = 'jdbc',&#013;&#010;    'connector.url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;    'connector.table' = 'flink_result',&#013;&#010;    'connector.driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;    'connector.username' = '***',&#013;&#010;    'connector.password' = '***',&#013;&#010;    'connector.write.flush.max-rows' = '5000', &#013;&#010;    'connector.write.flush.interval' = '2s', &#013;&#010;    'connector.write.max-retries' = '3'&#013;&#010;    )&#013;&#010;    \"\"\"&#013;&#010;    st_env.sql_update(sink_ddl)&#013;&#010;&#013;&#010;&#013;&#010;if __name__ == '__main__':&#013;&#010;    from_kafka_to_kafka_demo()&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;初学者&#013;&#010;琴师&#013;&#010;&#013;&#010;",
        "depth": "0",
        "reply": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>"
    },
    {
        "id": "<4271F43B-9B92-4729-A44C-D3A649DB8AFF@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:54:08 GMT",
        "subject": "Re: flinksql1.11中主键声明的问题",
        "content": "Hi,&#010;&#010;你这还是connector的with参数里不是新 connector的写法[1]，会走到老代码，老代码不支持声明PK的。&#010;在老代码里，PK是通过query推导的，你用inner join替换left join后，应该是能够推断出PK了，所以没有报错。&#010;我理解你把connector的with参数更新成新的就解决问题了。&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#connector-options&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#connector-options&gt;&#010;&gt; &#010;&gt; def register_rides_source(st_env):&#010;&gt;    source_ddl = \\&#010;&gt;    \"\"\"&#010;&gt;    create table source1(&#010;&gt;     id int,&#010;&gt;     time1 varchar ,&#010;&gt;     type string&#010;&gt;     ) with (&#010;&gt;    'connector.type' = 'kafka',&#010;&gt;    'connector.topic' = 'tp1',&#010;&gt;    'connector.startup-mode' = 'latest-offset',&#010;&gt;    'connector.properties.bootstrap.servers' = 'localhost:9092',&#010;&gt;    'connector.properties.zookeeper.connect' = 'localhost:2181',&#010;&gt;    'format.type' = 'json',&#010;&gt;    'connector.version' = 'universal',&#010;&gt;    'update-mode' = 'append'&#010;&gt;     )&#010;&gt;    “\"\" &#010;&#010;",
        "depth": "1",
        "reply": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>"
    },
    {
        "id": "<tencent_F9FA9862C2A59D89F6C324F4592878814806@qq.com>",
        "from": "&quot;1129656513@qq.com&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 06:13:01 GMT",
        "subject": "回复: Re: flinksql1.11中主键声明的问题",
        "content": "您好：&#013;&#010;&#013;&#010;       非常感谢您的建议，我已经成功解决了这个问题，但是我又发现了一个新的问题，我这里设置的超时时间是一分钟或者超时行数是5000行，&#013;&#010;我在这期间更新了维表数据，但是我发现已经超过了超时时间，输出结果仍然没有被更新，是我理解的有问题么？&#013;&#010;我尝试了停止输入流数据直到达到超时时间后仍然没有更新维表，除非停止整个程序，否则我的维表数据都不会被更新。&#013;&#010;请问这个问题有解决的办法么？&#013;&#010;&#013;&#010;def register_mysql_source(st_env):&#013;&#010;    source_ddl = \\&#013;&#010;    \"\"\"&#013;&#010;    CREATE TABLE dim_mysql (&#013;&#010;    id int,  -- &#013;&#010;    type varchar -- &#013;&#010;    ) WITH (&#013;&#010;    'connector' = 'jdbc',&#013;&#010;    'url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;    'table-name' = 'flink_test',&#013;&#010;    'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;    'username' = '****',&#013;&#010;    'password' = '****',&#013;&#010;    'lookup.cache.max-rows' = '5000',&#013;&#010;    'lookup.cache.ttl' = '1s',&#013;&#010;    'lookup.max-retries' = '3'&#013;&#010;    )&#013;&#010;    \"\"\"    &#013;&#010;    st_env.sql_update(source_ddl)&#013;&#010;  &#013;&#010;&#013;&#010;&#013;&#010;                                      感谢！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;琴师&#013;&#010;&#013;&#010; &#013;&#010;发件人： Leonard Xu&#013;&#010;发送时间： 2020-07-22 10:54&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: flinksql1.11中主键声明的问题&#013;&#010;Hi,&#013;&#010; &#013;&#010;你这还是connector的with参数里不是新 connector的写法[1]，会走到老代码，老代码不支持声明PK的。&#013;&#010;在老代码里，PK是通过query推导的，你用inner join替换left join后，应该是能够推断出PK了，所以没有报错。&#013;&#010;我理解你把connector的with参数更新成新的就解决问题了。&#013;&#010; &#013;&#010;Best&#013;&#010;Leonard Xu&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#connector-options&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/jdbc.html#connector-options&gt;&#013;&#010;&gt; &#013;&#010;&gt; def register_rides_source(st_env):&#013;&#010;&gt;    source_ddl = \\&#013;&#010;&gt;    \"\"\"&#013;&#010;&gt;    create table source1(&#013;&#010;&gt;     id int,&#013;&#010;&gt;     time1 varchar ,&#013;&#010;&gt;     type string&#013;&#010;&gt;     ) with (&#013;&#010;&gt;    'connector.type' = 'kafka',&#013;&#010;&gt;    'connector.topic' = 'tp1',&#013;&#010;&gt;    'connector.startup-mode' = 'latest-offset',&#013;&#010;&gt;    'connector.properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&gt;    'connector.properties.zookeeper.connect' = 'localhost:2181',&#013;&#010;&gt;    'format.type' = 'json',&#013;&#010;&gt;    'connector.version' = 'universal',&#013;&#010;&gt;    'update-mode' = 'append'&#013;&#010;&gt;     )&#013;&#010;&gt;    “\"\" &#013;&#010;",
        "depth": "2",
        "reply": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>"
    },
    {
        "id": "<EECE27F2-E2CE-4C26-A462-9C64CECCC3F9@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 06:42:18 GMT",
        "subject": "Re: flinksql1.11中主键声明的问题",
        "content": "Hello&#013;&#010;你说的输出结果更新，是指之前关联的维表时老数据，过了一段时间，这个数据变，之前输出的历史也希望更新吗？维表join的实现，只有事实表中才会有retract消息才会更新，才会传递到下游，维表的数据是事实表&#013;&#010;去look up该表时的数据，维表的更新是不会retract之前的历史记录的。&#013;&#010;&#013;&#010;祝好&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;&gt; 在 2020年7月22日，14:13，1129656513@qq.com 写道：&#013;&#010;&gt; &#013;&#010;&gt; 输出结果仍然没有被更新&#013;&#010;&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>"
    },
    {
        "id": "<tencent_1A31ED04663B226D92C9D8331D707991B207@qq.com>",
        "from": "&quot;奇怪的不朽琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 06:50:27 GMT",
        "subject": "回复： flinksql1.11中主键声明的问题",
        "content": "你好：&#013;&#010;&#013;&#010;&#013;&#010;可能是我描述的不清楚，我了解这个机制，我的意思维表更新后，即便已经达到了超时的时间，新的输出结果还是用维表历史缓存数据，&#013;&#010;我感觉上是维表没有刷新缓存，但是我不知道这为什么。&#013;&#010;&#013;&#010;&#013;&#010;谢谢&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月22日(星期三) 下午2:42&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flinksql1.11中主键声明的问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hello&#013;&#010;你说的输出结果更新，是指之前关联的维表时老数据，过了一段时间，这个数据变，之前输出的历史也希望更新吗？维表join的实现，只有事实表中才会有retract消息才会更新，才会传递到下游，维表的数据是事实表&#013;&#010;去look up该表时的数据，维表的更新是不会retract之前的历史记录的。&#013;&#010;&#013;&#010;祝好&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;&amp;gt; 在 2020年7月22日，14:13，1129656513@qq.com 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 输出结果仍然没有被更新",
        "depth": "4",
        "reply": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>"
    },
    {
        "id": "<36882CB1-E5B6-4251-AE6E-0C62595CFB14@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:05:11 GMT",
        "subject": "Re: flinksql1.11中主键声明的问题",
        "content": "Hi,&#013;&#010;&#013;&#010;  我试了下应该是会更新缓存的，你有能复现的例子吗？&#013;&#010;&#013;&#010;祝好&#013;&#010;&gt; 在 2020年7月22日，14:50，奇怪的不朽琴师 &lt;1129656513@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 你好：&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 可能是我描述的不清楚，我了解这个机制，我的意思维表更新后，即便已经达到了超时的时间，新的输出结果还是用维表历史缓存数据，&#013;&#010;&gt; 我感觉上是维表没有刷新缓存，但是我不知道这为什么。&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 谢谢&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:                                                                          &#010;                                             \"user-zh\"                                   &#010;                                                &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月22日(星期三) 下午2:42&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt; &#013;&#010;&gt; 主题:&amp;nbsp;Re: flinksql1.11中主键声明的问题&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; Hello&#013;&#010;&gt; 你说的输出结果更新，是指之前关联的维表时老数据，过了一段时间，这个数据变，之前输出的历史也希望更新吗？维表join的实现，只有事实表中才会有retract消息才会更新，才会传递到下游，维表的数据是事实表&#013;&#010;&gt; 去look up该表时的数据，维表的更新是不会retract之前的历史记录的。&#013;&#010;&gt; &#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &amp;gt; 在 2020年7月22日，14:13，1129656513@qq.com 写道：&#013;&#010;&gt; &amp;gt; &#013;&#010;&gt; &amp;gt; 输出结果仍然没有被更新&#013;&#010;&#013;&#010;",
        "depth": "5",
        "reply": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>"
    },
    {
        "id": "<tencent_A469CF52C019454CC6F413E8EC30B5DF2609@qq.com>",
        "from": "琴师 &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:17:20 GMT",
        "subject": "回复: Re: flinksql1.11中主键声明的问题",
        "content": "你好：&#013;&#010;下面是我的代码，我用的版本是1.11.0，数据库是TIDB，我跑的是demo数据，维表只有两行。&#013;&#010;&#013;&#010;我的输入流如下，每秒新增一条写入到kafka&#013;&#010; topic = 'tp1'&#013;&#010;    for i  in  range(1,10000) :&#013;&#010;        stime=datetime.datetime.now().strftime('%Y%m%d%H%M%S')&#013;&#010;        msg = {}&#013;&#010;        msg['id']= i&#013;&#010;        msg['time1']= stime&#013;&#010;        msg['type']=1&#013;&#010;        print(msg)&#013;&#010;        send_msg(topic, msg)&#013;&#010;        time.sleep(1)&#013;&#010;&#013;&#010;{'id': 1, 'time1': '20200722140624', 'type': 1}&#013;&#010;{'id': 2, 'time1': '20200722140625', 'type': 1}&#013;&#010;{'id': 3, 'time1': '20200722140626', 'type': 1}&#013;&#010;{'id': 4, 'time1': '20200722140627', 'type': 1}&#013;&#010;{'id': 5, 'time1': '20200722140628', 'type': 1}&#013;&#010;{'id': 6, 'time1': '20200722140629', 'type': 1}&#013;&#010;{'id': 7, 'time1': '20200722140631', 'type': 1}&#013;&#010;{'id': 8, 'time1': '20200722140632', 'type': 1}&#013;&#010;&#013;&#010;维表数据如下&#013;&#010;id    type&#013;&#010;2 err&#013;&#010;1 err&#013;&#010;&#013;&#010;我在程序正常期间更新了维表，但是后续输出的结果显示维表还是之前的缓存数据，事实上已经远远大于超时时间了，甚至我停下输入流，直到达到超时时间后再次输入，新的结果还是输出旧的维表数据&#013;&#010;&#013;&#010;&#013;&#010;from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings,DataTypes,&#010;CsvTableSource, CsvTableSink&#013;&#010;from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;from pyflink.table.window import Tumble &#013;&#010;&#013;&#010;&#013;&#010;def from_kafka_to_kafka_demo():&#013;&#010;&#013;&#010;    # use blink table planner&#013;&#010;    env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;    env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;    env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;    st_env = StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010;&#013;&#010;    # register source and sink&#013;&#010;    register_rides_source(st_env)&#013;&#010;    register_rides_sink(st_env)&#013;&#010;    register_mysql_source(st_env)&#013;&#010;  &#013;&#010;&#013;&#010;    st_env.sql_update(\"insert into flink_result select  cast(t1.id as int) as id,cast(t2.type&#010;as varchar),cast( t1.time1 as bigint) as rowtime from source1 t1 left join dim_mysql t2 on&#010;t1.type=cast(t2.id as varchar) \")&#013;&#010;    st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;    &#013;&#010;&#013;&#010;&#013;&#010;def register_rides_source(st_env):&#013;&#010;    source_ddl = \\&#013;&#010;    \"\"\"&#013;&#010;    create table source1(&#013;&#010;     id int,&#013;&#010;     time1 varchar ,&#013;&#010;     type string&#013;&#010;     ) with (&#013;&#010;    'connector' = 'kafka',&#013;&#010;    'topic' = 'tp1',&#013;&#010;    'scan.startup.mode' = 'latest-offset',&#013;&#010;    'properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;    'format' = 'json'&#013;&#010;     )&#013;&#010;    \"\"\"&#013;&#010;    st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;def register_mysql_source(st_env):&#013;&#010;    source_ddl = \\&#013;&#010;    \"\"\"&#013;&#010;    CREATE TABLE dim_mysql (&#013;&#010;    id int,  -- &#013;&#010;    type varchar -- &#013;&#010;    ) WITH (&#013;&#010;    'connector' = 'jdbc',&#013;&#010;    'url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;    'table-name' = 'flink_test',&#013;&#010;    'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;    'username' = '***',&#013;&#010;    'password' = '***',&#013;&#010;    'lookup.cache.max-rows' = '5000',&#013;&#010;    'lookup.cache.ttl' = '1s',&#013;&#010;    'lookup.max-retries' = '3'&#013;&#010;    )&#013;&#010;    \"\"\"    &#013;&#010;    st_env.sql_update(source_ddl)&#013;&#010;&#013;&#010;def register_rides_sink(st_env):&#013;&#010;    sink_ddl = \\&#013;&#010;    \"\"\"&#013;&#010;    CREATE TABLE flink_result (&#013;&#010;    id int,   &#013;&#010;    type varchar,&#013;&#010;    rtime bigint,&#013;&#010;    primary key(id)  NOT ENFORCED&#013;&#010;    ) WITH (&#013;&#010;    'connector' = 'jdbc',&#013;&#010;    'url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;    'table-name' = 'flink_result',&#013;&#010;    'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;    'username' = '***',&#013;&#010;    'password' = '***',&#013;&#010;    'sink.buffer-flush.max-rows' = '5000', &#013;&#010;    'sink.buffer-flush.interval' = '2s', &#013;&#010;    'sink.max-retries' = '3'&#013;&#010;    )&#013;&#010;    \"\"\"&#013;&#010;    st_env.sql_update(sink_ddl)&#013;&#010;&#013;&#010;&#013;&#010;if __name__ == '__main__':&#013;&#010;    from_kafka_to_kafka_demo()&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;初学者&#013;&#010;PyFlink爱好者&#013;&#010;琴师&#013;&#010;&#013;&#010; &#013;&#010;发件人： Leonard Xu&#013;&#010;发送时间： 2020-07-22 15:05&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: flinksql1.11中主键声明的问题&#013;&#010;Hi,&#013;&#010; &#013;&#010;  我试了下应该是会更新缓存的，你有能复现的例子吗？&#013;&#010; &#013;&#010;祝好&#013;&#010;&gt; 在 2020年7月22日，14:50，奇怪的不朽琴师 &lt;1129656513@qq.com&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; 你好：&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 可能是我描述的不清楚，我了解这个机制，我的意思维表更新后，即便已经达到了超时的时间，新的输出结果还是用维表历史缓存数据，&#013;&#010;&gt; 我感觉上是维表没有刷新缓存，但是我不知道这为什么。&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 谢谢&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:                                                                          &#010;                                             \"user-zh\"                                   &#010;                                                &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月22日(星期三) 下午2:42&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt; &#013;&#010;&gt; 主题:&amp;nbsp;Re: flinksql1.11中主键声明的问题&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; Hello&#013;&#010;&gt; 你说的输出结果更新，是指之前关联的维表时老数据，过了一段时间，这个数据变，之前输出的历史也希望更新吗？维表join的实现，只有事实表中才会有retract消息才会更新，才会传递到下游，维表的数据是事实表&#013;&#010;&gt; 去look up该表时的数据，维表的更新是不会retract之前的历史记录的。&#013;&#010;&gt; &#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &amp;gt; 在 2020年7月22日，14:13，1129656513@qq.com 写道：&#013;&#010;&gt; &amp;gt; &#013;&#010;&gt; &amp;gt; 输出结果仍然没有被更新&#013;&#010; &#013;&#010;",
        "depth": "1",
        "reply": "<tencent_DED9356AA34E0A9F31817B8A688FC126A80A@qq.com>"
    },
    {
        "id": "<tencent_565DD5C663BA7AC9A6C91AA9834005DAA30A@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:26:48 GMT",
        "subject": "flink1.11 web ui没有DAG",
        "content": "本地linux下单机版安装的，提交flink代码运行后，正常运行，有日志，但是为啥UI上面却不显示数据接收和发送的条数，求大佬解答",
        "depth": "1",
        "reply": "<tencent_565DD5C663BA7AC9A6C91AA9834005DAA30A@qq.com>"
    },
    {
        "id": "<tencent_A2235022F62C57AAF157AE7AB2C1B298EA08@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 02:43:59 GMT",
        "subject": "flink1.11 web ui没有DAG",
        "content": "本地linux下单机版安装的，提交flink代码运行后，正常运行，有日志，但是为啥UI上面却不显示数据接收和发送的条数，求大佬解答",
        "depth": "1",
        "reply": "<tencent_565DD5C663BA7AC9A6C91AA9834005DAA30A@qq.com>"
    },
    {
        "id": "<CAA8tFvsXBaPM8ZVSxbHLfy_64YnS75uD2UKpkjw+XTy0=ymrOA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:10:45 GMT",
        "subject": "Re: flink1.11 web ui没有DAG",
        "content": "Hi&#013;&#010;    这边说的 UI 上不显示数据接受和发送的条数，能否截图发一下，这样大家能更好的理解这个问题。另外&#010;flink 作业有数据输入和处理吗？&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;小学生 &lt;201782053@qq.com&gt; 于2020年7月22日周三 上午10:47写道：&#013;&#010;&#013;&#010;&gt; 本地linux下单机版安装的，提交flink代码运行后，正常运行，有日志，但是为啥UI上面却不显示数据接收和发送的条数，求大佬解答&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_565DD5C663BA7AC9A6C91AA9834005DAA30A@qq.com>"
    },
    {
        "id": "<CAEZk041DqCADdTb71-EX0kLAh5MDt7N-wQiTOOzcwYuE6Lv1rg@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 03:24:25 GMT",
        "subject": "flink1.11 实现tablefunction报错",
        "content": "hi、&#010;我这面实现了一个tablefunction想打撒数据，但是现在我运行官方demo样式的demo都无法成功，请问下面是什么原因：&#010;&#010;@FunctionHint(output = @DataTypeHint(\"ROW&lt;word STRING&gt;\"))&#010;public static class FlatRowFunction extends TableFunction&lt;Row&gt; {&#010;    private static final long serialVersionUID = 1L;&#010;&#010;    public void eval(String rows) {&#010;        for (String row : rows.split(\"--&gt;\")) {&#010;            collect(Row.of(row));&#010;        }&#010;    }&#010;}&#010;&#010;sql使用：from parser_data_test, LATERAL TABLE(flatRow(data.path))&#010;&#010;异常：&#010;&#010;Exception in thread \"main\"&#010;org.apache.flink.table.api.ValidationException: SQL validation failed.&#010;&gt;From line 1, column 154 to line 1, column 171: No match found for&#010;function signature flatRow(&lt;CHARACTER&gt;)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:146)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:108)&#010;&#009;at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:187)&#010;&#009;at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:78)&#010;&#009;at org.apache.flink.table.api.internal.TableEnvironmentImpl.sqlQuery(TableEnvironmentImpl.java:658)&#010;&#009;at com.akulaku.data.flink.ParserDataTest.main(ParserDataTest.java:60)&#010;Caused by: org.apache.calcite.runtime.CalciteContextException: From&#010;line 1, column 154 to line 1, column 171: No match found for function&#010;signature flatRow(&lt;CHARACTER&gt;)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&#009;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&#009;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:839)&#010;&#009;at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:824)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.newValidationError(SqlValidatorImpl.java:5089)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.handleUnresolvedFunction(SqlValidatorImpl.java:1882)&#010;&#009;at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:305)&#010;&#009;at org.apache.calcite.sql.SqlFunction.deriveType(SqlFunction.java:218)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5858)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl$DeriveTypeVisitor.visit(SqlValidatorImpl.java:5845)&#010;&#009;at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.deriveTypeImpl(SqlValidatorImpl.java:1800)&#010;&#009;at org.apache.calcite.sql.validate.ProcedureNamespace.validateImpl(ProcedureNamespace.java:57)&#010;&#009;at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3256)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3238)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateJoin(SqlValidatorImpl.java:3303)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkCalciteSqlValidator.validateJoin(FlinkCalciteSqlValidator.java:86)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateFrom(SqlValidatorImpl.java:3247)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateSelect(SqlValidatorImpl.java:3510)&#010;&#009;at org.apache.calcite.sql.validate.SelectNamespace.validateImpl(SelectNamespace.java:60)&#010;&#009;at org.apache.calcite.sql.validate.AbstractNamespace.validate(AbstractNamespace.java:84)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateNamespace(SqlValidatorImpl.java:1110)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateQuery(SqlValidatorImpl.java:1084)&#010;&#009;at org.apache.calcite.sql.SqlSelect.validate(SqlSelect.java:232)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validateScopedExpression(SqlValidatorImpl.java:1059)&#010;&#009;at org.apache.calcite.sql.validate.SqlValidatorImpl.validate(SqlValidatorImpl.java:766)&#010;&#009;at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:141)&#010;&#009;... 5 more&#010;Caused by: org.apache.calcite.sql.validate.SqlValidatorException: No&#010;match found for function signature flatRow(&lt;CHARACTER&gt;)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#010;&#009;at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#010;&#009;at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#010;&#009;at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:457)&#010;&#009;at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:550)&#010;&#009;... 33 more&#010;&#010;",
        "depth": "0",
        "reply": "<CAEZk041DqCADdTb71-EX0kLAh5MDt7N-wQiTOOzcwYuE6Lv1rg@mail.gmail.com>"
    },
    {
        "id": "<CAEZk043zQb5fYi5znOCn9EPH2JdQPrszBJwH0EryGYTcxH8TkA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:14:04 GMT",
        "subject": "flink1.11 sql",
        "content": "hi&#013;&#010;flink支持配置hive方言，那么flink可以直接使用hive内自定义的udf、udtf函数吗&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk043zQb5fYi5znOCn9EPH2JdQPrszBJwH0EryGYTcxH8TkA@mail.gmail.com>"
    },
    {
        "id": "<B78FCE2A-AF9F-4DA2-B9D0-5C33176E4D6B@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:16:57 GMT",
        "subject": "Re: flink1.11 sql",
        "content": "Hi&#010;&#010;必须可以呢，参考[1]&#010;&#010;Best,&#010;Leonard Xu&#010;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/hive/hive_functions.html&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/hive/hive_functions.html&gt;&#010;&#010;&#010;&gt; 在 2020年7月22日，12:14，Dream-底限 &lt;zhangyu@akulaku.com&gt; 写道：&#010;&gt; &#010;&gt; hi&#010;&gt; flink支持配置hive方言，那么flink可以直接使用hive内自定义的udf、udtf函数吗&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<CAEZk043zQb5fYi5znOCn9EPH2JdQPrszBJwH0EryGYTcxH8TkA@mail.gmail.com>"
    },
    {
        "id": "<CADH6UNRmHTx=0eemGTYw-w3BXLUYMZmYQRTzPt+pv9Jwo8YfJw@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 02:44:53 GMT",
        "subject": "Re: flink1.11 sql",
        "content": "支持的，也是需要配合HiveCatalog一起使用，你在hive那边创建的函数在flink里就能调用了&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 12:14 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt; flink支持配置hive方言，那么flink可以直接使用hive内自定义的udf、udtf函数吗&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk043zQb5fYi5znOCn9EPH2JdQPrszBJwH0EryGYTcxH8TkA@mail.gmail.com>"
    },
    {
        "id": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>",
        "from": "刘首维 &lt;liushou...@autohome.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:34:36 GMT",
        "subject": "关于1.11Flink SQL 全新API设计的一些问题 ",
        "content": "Hi all,&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;    很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&#013;&#010;    我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;    所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;",
        "depth": "0",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<CABi+2jRbOKDU8-Z7YpyroCzTdJLkUT1jCt2JmkmUkXFJz10EtQ@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:26:00 GMT",
        "subject": "Re: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&#013;&#010;Best&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi all,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;     很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt;&#013;&#010;&gt;     我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;     所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "1",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<33feef38ce134c89a3c39ce68808944c@autohome.com.cn>",
        "from": "刘首维 &lt;liushou...@autohome.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:47:25 GMT",
        "subject": "答复: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi JingSong,&#013;&#010;&#013;&#010;  简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#010;SDK&#013;&#010;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&#013;&#010;&#013;&#010;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&#013;&#010;&#013;&#010;如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#013;&#010;发送时间: 2020年7月22日 13:26:00&#013;&#010;收件人: user-zh&#013;&#010;抄送: imjark@gmail.com&#013;&#010;主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&#013;&#010;可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&#013;&#010;Best&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi all,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;     很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt;&#013;&#010;&gt;     我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;     所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;--&#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "2",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<CABi+2jSqsYe+ir3R0WuCcnjgKE-ajfmxEo-n5jaqnQnRtPKV+w@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:58:51 GMT",
        "subject": "Re: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi 首维，&#013;&#010;&#013;&#010;非常感谢你的信息，我们试图 去掉太灵活的“DataStream”，但是也逐渐发现一些额外的需求，再考虑和评估下你的需求。&#013;&#010;&#013;&#010;CC: @Jark Wu &lt;imjark@gmail.com&gt;&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 1:49 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi JingSong,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#013;&#010;&gt; SDK&#013;&#010;&gt;   下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;   1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt;   2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt;   3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt;   4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; 收件人: user-zh&#013;&#010;&gt; 抄送: imjark@gmail.com&#013;&#010;&gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt;&#013;&#010;&gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi all,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;     很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;     我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; &gt;&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;     所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "3",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<5cf900267581450585ab23efc4dac4c9@autohome.com.cn>",
        "from": "刘首维 &lt;liushou...@autohome.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 06:17:37 GMT",
        "subject": "答复: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi JingSong，&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;    感谢回复，真心期待一个理想的解决方案~&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#013;&#010;发送时间: 2020年7月22日 13:58:51&#013;&#010;收件人: user-zh; Jark Wu&#013;&#010;主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&#013;&#010;Hi 首维，&#013;&#010;&#013;&#010;非常感谢你的信息，我们试图 去掉太灵活的“DataStream”，但是也逐渐发现一些额外的需求，再考虑和评估下你的需求。&#013;&#010;&#013;&#010;CC: @Jark Wu &lt;imjark@gmail.com&gt;&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 1:49 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi JingSong,&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#013;&#010;&gt; SDK&#013;&#010;&gt;   下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;   1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt;   2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt;   3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt;   4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; 收件人: user-zh&#013;&#010;&gt; 抄送: imjark@gmail.com&#013;&#010;&gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt;&#013;&#010;&gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi all,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;     很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;     我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; &gt;&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;     所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;--&#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "3",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<7b33efb4.57e9.17375941551.Coremail.greemqqran@163.com>",
        "from": "&quot;Michael Ran&quot; &lt;greemqq...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:11:45 GMT",
        "subject": "Re:答复: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "这个需求 我们也比较类似：&lt;br/&gt;要获取注册的表信息，自己用stream+table&#010;实现部分逻辑&#010;在 2020-07-22 13:47:25，\"刘首维\" &lt;liushouwei@autohome.com.cn&gt; 写道：&#010;&gt;Hi JingSong,&#013;&#010;&gt;&#013;&#010;&gt;  简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#010;SDK&#013;&#010;&gt;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;________________________________&#013;&#010;&gt;发件人: Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#013;&#010;&gt;发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt;收件人: user-zh&#013;&#010;&gt;抄送: imjark@gmail.com&#013;&#010;&gt;主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt;&#013;&#010;&gt;可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt;&#013;&#010;&gt;Best&#013;&#010;&gt;Jingsong&#013;&#010;&gt;&#013;&#010;&gt;On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi all,&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;     很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;     我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;     所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;--&#013;&#010;&gt;Best, Jingsong Lee&#013;&#010;",
        "depth": "3",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<A156AC06-F89C-48E7-8706-56DBF05334C6@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:27:03 GMT",
        "subject": "Re: 关于1.11Flink SQL 全新API设计的一些问题 ",
        "content": "Hi，首维， Ran&#013;&#010;&#013;&#010;感谢分享， 我理解1.11新的API主要是想把 Table API 和 DataStream API 两套尽量拆分干净,&#010;但看起来平台级的开发工作会依赖DataStream的一些预处理和用户逻辑。&#013;&#010;我觉得这类需求对平台开发是合理，可以收集反馈下的， cc: godfrey&#013;&#010;&#013;&#010;祝好&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;&gt; 在 2020年7月22日，13:47，刘首维 &lt;liushouwei@autohome.com.cn&gt; 写道：&#013;&#010;&gt; &#013;&#010;&gt; Hi JingSong,&#013;&#010;&gt; &#013;&#010;&gt;  简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#010;SDK&#013;&#010;&gt;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; 收件人: user-zh&#013;&#010;&gt; 抄送: imjark@gmail.com&#013;&#010;&gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt; &#013;&#010;&gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt; &#013;&#010;&gt; Best&#013;&#010;&gt; Jingsong&#013;&#010;&gt; &#013;&#010;&gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&gt; &#013;&#010;&gt;&gt; Hi all,&#013;&#010;&gt;&gt; &#013;&#010;&gt;&gt; &#013;&#010;&gt;&gt; &#013;&#010;&gt;&gt;    很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt;&gt; &#013;&#010;&gt;&gt;    我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt;&gt; &#013;&#010;&gt;&gt; &#013;&#010;&gt;&gt; &#013;&#010;&gt;&gt;    所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt;&gt; &#013;&#010;&gt; &#013;&#010;&gt; &#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&#013;&#010;",
        "depth": "3",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<CAELO933mq-FyH5ekcjJ+vm7DAat3Zo6zeE68VPCpkZVuiYaE+w@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:33:45 GMT",
        "subject": "Re: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi，首维，&#013;&#010;&#013;&#010;非常感谢反馈。与 DataStream 解耦是 FLIP-95 的一个非常重要的设计目标，这让&#010;sink/source 对于框架来说不再是黑盒，&#013;&#010;因此将来才可以做诸如 state 兼容升级、消息顺序保障、自动并发设置等等事情。&#013;&#010;&#013;&#010;关于你的一些需求，下面是我的建议和回复：&#013;&#010;&#013;&#010;&gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;这个理论上还属于“数据格式”的职责，所以建议做在 DeserializationSchema&#010;上，目前 DeserializationSchema&#013;&#010;支持一对多的输出。可以参考 DebeziumJsonDeserializationSchema 的实现。&#013;&#010;&#013;&#010;&gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&#013;&#010;&gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;这个社区也有计划在 FLIP-95 之上支持，会提供并发（或者分区）推断的能力。&#013;&#010;&#013;&#010;&gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;这个能在具体一点吗？目前像 SupportsPartitioning 接口，就可以指定数据在交给&#010;sink 之前先做 group by&#013;&#010;partition。我感觉这个可能也可以通过引入类似的接口解决。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Wed, 22 Jul 2020 at 16:27, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi，首维， Ran&#013;&#010;&gt;&#013;&#010;&gt; 感谢分享， 我理解1.11新的API主要是想把 Table API 和 DataStream API 两套尽量拆分干净,&#013;&#010;&gt; 但看起来平台级的开发工作会依赖DataStream的一些预处理和用户逻辑。&#013;&#010;&gt; 我觉得这类需求对平台开发是合理，可以收集反馈下的， cc: godfrey&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月22日，13:47，刘首维 &lt;liushouwei@autohome.com.cn&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hi JingSong,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#013;&#010;&gt; SDK&#013;&#010;&gt; &gt;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt; &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; &gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#013;&#010;&gt; &gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; &gt; 收件人: user-zh&#013;&#010;&gt; &gt; 抄送: imjark@gmail.com&#013;&#010;&gt; &gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best&#013;&#010;&gt; &gt; Jingsong&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi all,&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<264f7f37b2eb4c7bbf48daacc8b8991a@autohome.com.cn>",
        "from": "刘首维 &lt;liushou...@autohome.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 09:21:22 GMT",
        "subject": "答复: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi, Jark&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;   感谢你的建议！&#013;&#010;&#013;&#010;   我们这边充分相信社区在SQL/Table上的苦心孤诣，也愿意跟进Flink在新版本的变化。&#013;&#010;&#013;&#010;   先看我遇到问题本身，你的建议确实可以帮助我解决问题。我想聊一下我对问题之外的一些想法&#013;&#010;&#013;&#010;   ```&#013;&#010;&#013;&#010;         &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;        这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&#013;&#010;  ```&#013;&#010;&#013;&#010; 比如上述这个问题2，我们确实可以把它做到SinkFunction中，但是我个人认为这可能在设计上不够理想的。我个人在设计编排Function/算子的时候习惯于遵循”算子单一职责”的原则，这也是我为什么会拆分出多个process/filter算子编排到SinkFunction前面而非将这些功能耦合到SinkFunction去做。另一方面，没了DataStream，向新的API的迁移成本相对来说变得更高了一些~&#010;又或者，我们现在还有一些特殊原因，算子编排的时候会去修改TaskChain&#010;Strategy，这个时候DataStream的灵活性是必不可少的&#013;&#010;&#013;&#010;考虑到Flink Task都可以拆分成Source -&gt; Transformation -&gt; sink 三个阶段，那么能让用户可以对自己的作业针对（流或批）的运行模式下，可以有效灵活做一些自己的定制策略/优化/逻辑可能是会方便的~&#013;&#010;&#013;&#010;   诚然，DataStream的灵活性确实会是一把双刃剑，但就像@leonard提到的，平台层和应用层的目的和开发重点可能也不太一样，对Flink&#010;API使用侧重点也不同。我个人还是希望可以在享受全新API设计优势同时，&#013;&#010;&#013;&#010;可以继续使用DataStream（Transformation）的灵活性，助力Flink组件在我们组的开落地&#013;&#010;&#013;&#010;&#013;&#010;再次感谢各位的回复！&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: Jark Wu &lt;imjark@gmail.com&gt;&#013;&#010;发送时间: 2020年7月22日 16:33:45&#013;&#010;收件人: user-zh&#013;&#010;抄送: godfrey he; greemqqran@163.com; 刘首维&#013;&#010;主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&#013;&#010;Hi，首维，&#013;&#010;&#013;&#010;非常感谢反馈。与 DataStream 解耦是 FLIP-95 的一个非常重要的设计目标，这让&#010;sink/source 对于框架来说不再是黑盒，&#013;&#010;因此将来才可以做诸如 state 兼容升级、消息顺序保障、自动并发设置等等事情。&#013;&#010;&#013;&#010;关于你的一些需求，下面是我的建议和回复：&#013;&#010;&#013;&#010;&gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;这个理论上还属于“数据格式”的职责，所以建议做在 DeserializationSchema&#010;上，目前 DeserializationSchema 支持一对多的输出。可以参考 DebeziumJsonDeserializationSchema&#010;的实现。&#013;&#010;&#013;&#010;&gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&#013;&#010;&gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;这个社区也有计划在 FLIP-95 之上支持，会提供并发（或者分区）推断的能力。&#013;&#010;&#013;&#010;&gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;这个能在具体一点吗？目前像 SupportsPartitioning 接口，就可以指定数据在交给&#010;sink 之前先做 group by partition。我感觉这个可能也可以通过引入类似的接口解决。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Wed, 22 Jul 2020 at 16:27, Leonard Xu &lt;xbjtdcq@gmail.com&lt;mailto:xbjtdcq@gmail.com&gt;&gt;&#010;wrote:&#013;&#010;Hi，首维， Ran&#013;&#010;&#013;&#010;感谢分享， 我理解1.11新的API主要是想把 Table API 和 DataStream API 两套尽量拆分干净,&#010;但看起来平台级的开发工作会依赖DataStream的一些预处理和用户逻辑。&#013;&#010;我觉得这类需求对平台开发是合理，可以收集反馈下的， cc: godfrey&#013;&#010;&#013;&#010;祝好&#013;&#010;Leonard Xu&#013;&#010;&#013;&#010;&#013;&#010;&gt; 在 2020年7月22日，13:47，刘首维 &lt;liushouwei@autohome.com.cn&lt;mailto:liushouwei@autohome.com.cn&gt;&gt;&#010;写道：&#013;&#010;&gt;&#013;&#010;&gt; Hi JingSong,&#013;&#010;&gt;&#013;&#010;&gt;  简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#010;SDK&#013;&#010;&gt;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&lt;mailto:jingsonglee0@gmail.com&gt;&gt;&#013;&#010;&gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; 收件人: user-zh&#013;&#010;&gt; 抄送: imjark@gmail.com&lt;mailto:imjark@gmail.com&gt;&#013;&#010;&gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt;&#013;&#010;&gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt;&#013;&#010;&gt; Best&#013;&#010;&gt; Jingsong&#013;&#010;&gt;&#013;&#010;&gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&lt;mailto:liushouwei@autohome.com.cn&gt;&gt;&#010;wrote:&#013;&#010;&gt;&#013;&#010;&gt;&gt; Hi all,&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;    很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;    我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;    所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt; Best, Jingsong Lee&#013;&#010;&#013;&#010;",
        "depth": "5",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<CADQYLGuJ7Zd1rWVTTr8n0S3En0xtv4h0aQO_bro8+Sog8Cmgsg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 09:49:27 GMT",
        "subject": "Re: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi，首维&#013;&#010;&#013;&#010;感谢给出非常详细的反馈。这个问题我们之前内部也有一些讨论，但由于缺乏一些真实场景，最后维持了当前的接口。&#013;&#010;我们会根据你提供的场景进行后续讨论。&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;刘首维 &lt;liushouwei@autohome.com.cn&gt; 于2020年7月22日周三 下午5:23写道：&#013;&#010;&#013;&#010;&gt; Hi, Jark&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;    感谢你的建议！&#013;&#010;&gt;&#013;&#010;&gt;    我们这边充分相信社区在SQL/Table上的苦心孤诣，也愿意跟进Flink在新版本的变化。&#013;&#010;&gt;&#013;&#010;&gt;    先看我遇到问题本身，你的建议确实可以帮助我解决问题。我想聊一下我对问题之外的一些想法&#013;&#010;&gt;&#013;&#010;&gt;    ```&#013;&#010;&gt;&#013;&#010;&gt;          &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#013;&#010;&gt; 用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt;         这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&gt;&#013;&#010;&gt;   ```&#013;&#010;&gt;&#013;&#010;&gt;  比如上述这个问题2，我们确实可以把它做到SinkFunction中，但是我个人认为这可能在设计上不够理想的。我个人在设计编排Function/算子的时候习惯于遵循”算子单一职责”的原则，这也是我为什么会拆分出多个process/filter算子编排到SinkFunction前面而非将这些功能耦合到SinkFunction去做。另一方面，没了DataStream，向新的API的迁移成本相对来说变得更高了一些~&#013;&#010;&gt; 又或者，我们现在还有一些特殊原因，算子编排的时候会去修改TaskChain&#010;Strategy，这个时候DataStream的灵活性是必不可少的&#013;&#010;&gt;&#013;&#010;&gt; 考虑到Flink Task都可以拆分成Source -&gt; Transformation -&gt; sink&#013;&#010;&gt; 三个阶段，那么能让用户可以对自己的作业针对（流或批）的运行模式下，可以有效灵活做一些自己的定制策略/优化/逻辑可能是会方便的~&#013;&#010;&gt;&#013;&#010;&gt;    诚然，DataStream的灵活性确实会是一把双刃剑，但就像@leonard提到的，平台层和应用层的目的和开发重点可能也不太一样，对Flink&#013;&#010;&gt; API使用侧重点也不同。我个人还是希望可以在享受全新API设计优势同时，&#013;&#010;&gt;&#013;&#010;&gt; 可以继续使用DataStream（Transformation）的灵活性，助力Flink组件在我们组的开落地&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 再次感谢各位的回复！&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Jark Wu &lt;imjark@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月22日 16:33:45&#013;&#010;&gt; 收件人: user-zh&#013;&#010;&gt; 抄送: godfrey he; greemqqran@163.com; 刘首维&#013;&#010;&gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt;&#013;&#010;&gt; Hi，首维，&#013;&#010;&gt;&#013;&#010;&gt; 非常感谢反馈。与 DataStream 解耦是 FLIP-95 的一个非常重要的设计目标，这让&#010;sink/source 对于框架来说不再是黑盒，&#013;&#010;&gt; 因此将来才可以做诸如 state 兼容升级、消息顺序保障、自动并发设置等等事情。&#013;&#010;&gt;&#013;&#010;&gt; 关于你的一些需求，下面是我的建议和回复：&#013;&#010;&gt;&#013;&#010;&gt; &gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt; 这个理论上还属于“数据格式”的职责，所以建议做在 DeserializationSchema&#010;上，目前 DeserializationSchema&#013;&#010;&gt; 支持一对多的输出。可以参考 DebeziumJsonDeserializationSchema 的实现。&#013;&#010;&gt;&#013;&#010;&gt; &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; 这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&gt;&#013;&#010;&gt; &gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt; 这个社区也有计划在 FLIP-95 之上支持，会提供并发（或者分区）推断的能力。&#013;&#010;&gt;&#013;&#010;&gt; &gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; 这个能在具体一点吗？目前像 SupportsPartitioning 接口，就可以指定数据在交给&#010;sink 之前先做 group by&#013;&#010;&gt; partition。我感觉这个可能也可以通过引入类似的接口解决。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Wed, 22 Jul 2020 at 16:27, Leonard Xu &lt;xbjtdcq@gmail.com&lt;mailto:&#013;&#010;&gt; xbjtdcq@gmail.com&gt;&gt; wrote:&#013;&#010;&gt; Hi，首维， Ran&#013;&#010;&gt;&#013;&#010;&gt; 感谢分享， 我理解1.11新的API主要是想把 Table API 和 DataStream API 两套尽量拆分干净,&#013;&#010;&gt; 但看起来平台级的开发工作会依赖DataStream的一些预处理和用户逻辑。&#013;&#010;&gt; 我觉得这类需求对平台开发是合理，可以收集反馈下的， cc: godfrey&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月22日，13:47，刘首维 &lt;liushouwei@autohome.com.cn&lt;mailto:&#013;&#010;&gt; liushouwei@autohome.com.cn&gt;&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hi JingSong,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#013;&#010;&gt; SDK&#013;&#010;&gt; &gt;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt; &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; &gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&lt;mailto:jingsonglee0@gmail.com&gt;&gt;&#013;&#010;&gt; &gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; &gt; 收件人: user-zh&#013;&#010;&gt; &gt; 抄送: imjark@gmail.com&lt;mailto:imjark@gmail.com&gt;&#013;&#010;&gt; &gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best&#013;&#010;&gt; &gt; Jingsong&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&lt;mailto:&#013;&#010;&gt; liushouwei@autohome.com.cn&gt;&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi all,&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<e7120ccb3ecc46718f6ef5de11a419e9@autohome.com.cn>",
        "from": "刘首维 &lt;liushou...@autohome.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 10:05:14 GMT",
        "subject": "答复: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi, godfrey&#013;&#010;&#013;&#010;&#013;&#010;好的，如果可以的话，有了相关讨论的jira或者mail可以cc一下我吗，谢谢啦&#013;&#010;&#013;&#010;________________________________&#013;&#010;发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;发送时间: 2020年7月22日 17:49:27&#013;&#010;收件人: user-zh&#013;&#010;抄送: Jark Wu; xbjtdcq@gmail.com; jingsonglee0@gmail.com&#013;&#010;主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&#013;&#010;Hi，首维&#013;&#010;&#013;&#010;感谢给出非常详细的反馈。这个问题我们之前内部也有一些讨论，但由于缺乏一些真实场景，最后维持了当前的接口。&#013;&#010;我们会根据你提供的场景进行后续讨论。&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;刘首维 &lt;liushouwei@autohome.com.cn&gt; 于2020年7月22日周三 下午5:23写道：&#013;&#010;&#013;&#010;&gt; Hi, Jark&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;    感谢你的建议！&#013;&#010;&gt;&#013;&#010;&gt;    我们这边充分相信社区在SQL/Table上的苦心孤诣，也愿意跟进Flink在新版本的变化。&#013;&#010;&gt;&#013;&#010;&gt;    先看我遇到问题本身，你的建议确实可以帮助我解决问题。我想聊一下我对问题之外的一些想法&#013;&#010;&gt;&#013;&#010;&gt;    ```&#013;&#010;&gt;&#013;&#010;&gt;          &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#013;&#010;&gt; 用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt;         这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&gt;&#013;&#010;&gt;   ```&#013;&#010;&gt;&#013;&#010;&gt;  比如上述这个问题2，我们确实可以把它做到SinkFunction中，但是我个人认为这可能在设计上不够理想的。我个人在设计编排Function/算子的时候习惯于遵循”算子单一职责”的原则，这也是我为什么会拆分出多个process/filter算子编排到SinkFunction前面而非将这些功能耦合到SinkFunction去做。另一方面，没了DataStream，向新的API的迁移成本相对来说变得更高了一些~&#013;&#010;&gt; 又或者，我们现在还有一些特殊原因，算子编排的时候会去修改TaskChain&#010;Strategy，这个时候DataStream的灵活性是必不可少的&#013;&#010;&gt;&#013;&#010;&gt; 考虑到Flink Task都可以拆分成Source -&gt; Transformation -&gt; sink&#013;&#010;&gt; 三个阶段，那么能让用户可以对自己的作业针对（流或批）的运行模式下，可以有效灵活做一些自己的定制策略/优化/逻辑可能是会方便的~&#013;&#010;&gt;&#013;&#010;&gt;    诚然，DataStream的灵活性确实会是一把双刃剑，但就像@leonard提到的，平台层和应用层的目的和开发重点可能也不太一样，对Flink&#013;&#010;&gt; API使用侧重点也不同。我个人还是希望可以在享受全新API设计优势同时，&#013;&#010;&gt;&#013;&#010;&gt; 可以继续使用DataStream（Transformation）的灵活性，助力Flink组件在我们组的开落地&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 再次感谢各位的回复！&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: Jark Wu &lt;imjark@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月22日 16:33:45&#013;&#010;&gt; 收件人: user-zh&#013;&#010;&gt; 抄送: godfrey he; greemqqran@163.com; 刘首维&#013;&#010;&gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt;&#013;&#010;&gt; Hi，首维，&#013;&#010;&gt;&#013;&#010;&gt; 非常感谢反馈。与 DataStream 解耦是 FLIP-95 的一个非常重要的设计目标，这让&#010;sink/source 对于框架来说不再是黑盒，&#013;&#010;&gt; 因此将来才可以做诸如 state 兼容升级、消息顺序保障、自动并发设置等等事情。&#013;&#010;&gt;&#013;&#010;&gt; 关于你的一些需求，下面是我的建议和回复：&#013;&#010;&gt;&#013;&#010;&gt; &gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt; 这个理论上还属于“数据格式”的职责，所以建议做在 DeserializationSchema&#010;上，目前 DeserializationSchema&#013;&#010;&gt; 支持一对多的输出。可以参考 DebeziumJsonDeserializationSchema 的实现。&#013;&#010;&gt;&#013;&#010;&gt; &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; 这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&gt;&#013;&#010;&gt; &gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt; 这个社区也有计划在 FLIP-95 之上支持，会提供并发（或者分区）推断的能力。&#013;&#010;&gt;&#013;&#010;&gt; &gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; 这个能在具体一点吗？目前像 SupportsPartitioning 接口，就可以指定数据在交给&#010;sink 之前先做 group by&#013;&#010;&gt; partition。我感觉这个可能也可以通过引入类似的接口解决。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Jark&#013;&#010;&gt;&#013;&#010;&gt; On Wed, 22 Jul 2020 at 16:27, Leonard Xu &lt;xbjtdcq@gmail.com&lt;mailto:&#013;&#010;&gt; xbjtdcq@gmail.com&gt;&gt; wrote:&#013;&#010;&gt; Hi，首维， Ran&#013;&#010;&gt;&#013;&#010;&gt; 感谢分享， 我理解1.11新的API主要是想把 Table API 和 DataStream API 两套尽量拆分干净,&#013;&#010;&gt; 但看起来平台级的开发工作会依赖DataStream的一些预处理和用户逻辑。&#013;&#010;&gt; 我觉得这类需求对平台开发是合理，可以收集反馈下的， cc: godfrey&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard Xu&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月22日，13:47，刘首维 &lt;liushouwei@autohome.com.cn&lt;mailto:&#013;&#010;&gt; liushouwei@autohome.com.cn&gt;&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hi JingSong,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#013;&#010;&gt; SDK&#013;&#010;&gt; &gt;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt; &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; &gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&lt;mailto:jingsonglee0@gmail.com&gt;&gt;&#013;&#010;&gt; &gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; &gt; 收件人: user-zh&#013;&#010;&gt; &gt; 抄送: imjark@gmail.com&lt;mailto:imjark@gmail.com&gt;&#013;&#010;&gt; &gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best&#013;&#010;&gt; &gt; Jingsong&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&lt;mailto:&#013;&#010;&gt; liushouwei@autohome.com.cn&gt;&gt; wrote:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; Hi all,&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&gt;    所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best, Jingsong Lee&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "6",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<CAELO933BH-Y5maPUOtB1J6cQ-DHyxzedRE33yE_Xt51ZjJ64_w@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 15:54:21 GMT",
        "subject": "Re: 关于1.11Flink SQL 全新API设计的一些问题",
        "content": "Hi 首维，&#013;&#010;&#013;&#010;我建了一个 issue 来跟进这个问题：https://issues.apache.org/jira/browse/FLINK-18674&#013;&#010;我们可以在这个里面继续讨论需求和评估解决方案。&#013;&#010;&#013;&#010;On Wed, 22 Jul 2020 at 18:07, 刘首维 &lt;liushouwei@autohome.com.cn&gt; wrote:&#013;&#010;&#013;&#010;&gt; Hi, godfrey&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 好的，如果可以的话，有了相关讨论的jira或者mail可以cc一下我吗，谢谢啦&#013;&#010;&gt;&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月22日 17:49:27&#013;&#010;&gt; 收件人: user-zh&#013;&#010;&gt; 抄送: Jark Wu; xbjtdcq@gmail.com; jingsonglee0@gmail.com&#013;&#010;&gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt;&#013;&#010;&gt; Hi，首维&#013;&#010;&gt;&#013;&#010;&gt; 感谢给出非常详细的反馈。这个问题我们之前内部也有一些讨论，但由于缺乏一些真实场景，最后维持了当前的接口。&#013;&#010;&gt; 我们会根据你提供的场景进行后续讨论。&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Godfrey&#013;&#010;&gt;&#013;&#010;&gt; 刘首维 &lt;liushouwei@autohome.com.cn&gt; 于2020年7月22日周三 下午5:23写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi, Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;    感谢你的建议！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;    我们这边充分相信社区在SQL/Table上的苦心孤诣，也愿意跟进Flink在新版本的变化。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;    先看我遇到问题本身，你的建议确实可以帮助我解决问题。我想聊一下我对问题之外的一些想法&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;    ```&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;          &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#013;&#010;&gt; &gt; 用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; &gt;         这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;   ```&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 比如上述这个问题2，我们确实可以把它做到SinkFunction中，但是我个人认为这可能在设计上不够理想的。我个人在设计编排Function/算子的时候习惯于遵循”算子单一职责”的原则，这也是我为什么会拆分出多个process/filter算子编排到SinkFunction前面而非将这些功能耦合到SinkFunction去做。另一方面，没了DataStream，向新的API的迁移成本相对来说变得更高了一些~&#013;&#010;&gt; &gt; 又或者，我们现在还有一些特殊原因，算子编排的时候会去修改TaskChain&#010;Strategy，这个时候DataStream的灵活性是必不可少的&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 考虑到Flink Task都可以拆分成Source -&gt; Transformation -&gt; sink&#013;&#010;&gt; &gt; 三个阶段，那么能让用户可以对自己的作业针对（流或批）的运行模式下，可以有效灵活做一些自己的定制策略/优化/逻辑可能是会方便的~&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 诚然，DataStream的灵活性确实会是一把双刃剑，但就像@leonard提到的，平台层和应用层的目的和开发重点可能也不太一样，对Flink&#013;&#010;&gt; &gt; API使用侧重点也不同。我个人还是希望可以在享受全新API设计优势同时，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 可以继续使用DataStream（Transformation）的灵活性，助力Flink组件在我们组的开落地&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 再次感谢各位的回复！&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; ________________________________&#013;&#010;&gt; &gt; 发件人: Jark Wu &lt;imjark@gmail.com&gt;&#013;&#010;&gt; &gt; 发送时间: 2020年7月22日 16:33:45&#013;&#010;&gt; &gt; 收件人: user-zh&#013;&#010;&gt; &gt; 抄送: godfrey he; greemqqran@163.com; 刘首维&#013;&#010;&gt; &gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Hi，首维，&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 非常感谢反馈。与 DataStream 解耦是 FLIP-95 的一个非常重要的设计目标，这让&#010;sink/source 对于框架来说不再是黑盒，&#013;&#010;&gt; &gt; 因此将来才可以做诸如 state 兼容升级、消息顺序保障、自动并发设置等等事情。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 关于你的一些需求，下面是我的建议和回复：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; &gt;&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt; &gt; 这个理论上还属于“数据格式”的职责，所以建议做在 DeserializationSchema&#010;上，目前 DeserializationSchema&#013;&#010;&gt; &gt; 支持一对多的输出。可以参考 DebeziumJsonDeserializationSchema 的实现。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; &gt; 这个我觉得也可以封装在 SinkFunction 里面。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt; 这个社区也有计划在 FLIP-95 之上支持，会提供并发（或者分区）推断的能力。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt; 这个能在具体一点吗？目前像 SupportsPartitioning 接口，就可以指定数据在交给&#010;sink 之前先做 group by&#013;&#010;&gt; &gt; partition。我感觉这个可能也可以通过引入类似的接口解决。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Jark&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; On Wed, 22 Jul 2020 at 16:27, Leonard Xu &lt;xbjtdcq@gmail.com&lt;mailto:&#013;&#010;&gt; &gt; xbjtdcq@gmail.com&gt;&gt; wrote:&#013;&#010;&gt; &gt; Hi，首维， Ran&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 感谢分享， 我理解1.11新的API主要是想把 Table API 和 DataStream API&#010;两套尽量拆分干净,&#013;&#010;&gt; &gt; 但看起来平台级的开发工作会依赖DataStream的一些预处理和用户逻辑。&#013;&#010;&gt; &gt; 我觉得这类需求对平台开发是合理，可以收集反馈下的， cc:&#010;godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 祝好&#013;&#010;&gt; &gt; Leonard Xu&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; 在 2020年7月22日，13:47，刘首维 &lt;liushouwei@autohome.com.cn&lt;mailto:&#013;&#010;&gt; &gt; liushouwei@autohome.com.cn&gt;&gt; 写道：&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi JingSong,&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 简单介绍一下背景，我们组的一个工作就是用户在页面写Sql,我们负责将Sql处理转换成Flink作业并在我们的平台上运行，这个转换的过程依赖我们的SQL&#013;&#010;&gt; &gt; SDK&#013;&#010;&gt; &gt; &gt;  下面我举几个我们比较常用且感觉用1.11新API不太好实现的例子&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;  1.  我们现在有某种特定的Kafka数据格式，1条Kafka数据&#013;&#010;&gt; &gt;&#013;&#010;&gt; 会对应转换n（n为正整数）条Row数据，我们的做法是在emitDataStream的时候增加了一个process/FlatMap阶段，用于处理这种情况，这样对用户是透明的。&#013;&#010;&gt; &gt; &gt;  2.  我们目前封装了一些自己的Sink，我们会在Sink之前增加一个process/Filter&#010;用来做缓冲池/微批/数据过滤等功能&#013;&#010;&gt; &gt; &gt;  3.  调整或者指定Source/Sink并行度为用户指定值，我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt; &gt;  4.  对于一些特殊Source Sink，他们会和KeyBy操作组合（对用户透明），我们也是在DataStream层面上去做的&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 如果可以的话，能让我在API层面拿到Transformation也是能满足我需求的&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; ________________________________&#013;&#010;&gt; &gt; &gt; 发件人: Jingsong Li &lt;jingsonglee0@gmail.com&lt;mailto:jingsonglee0@gmail.com&#013;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt; 发送时间: 2020年7月22日 13:26:00&#013;&#010;&gt; &gt; &gt; 收件人: user-zh&#013;&#010;&gt; &gt; &gt; 抄送: imjark@gmail.com&lt;mailto:imjark@gmail.com&gt;&#013;&#010;&gt; &gt; &gt; 主题: Re: 关于1.11Flink SQL 全新API设计的一些问题&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 可以分享下你们为啥要拿到DataStream吗？什么场景一定离不开DataStream吗？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; Best&#013;&#010;&gt; &gt; &gt; Jingsong&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; On Wed, Jul 22, 2020 at 12:36 PM 刘首维 &lt;liushouwei@autohome.com.cn&#013;&#010;&gt; &lt;mailto:&#013;&#010;&gt; &gt; liushouwei@autohome.com.cn&gt;&gt; wrote:&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt; Hi all,&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;    很高兴看到Flink 1.11的发布，FLIP95和FLIP105也成功落地~&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;    我们最近在调研基于1.11的SQL/Table API对我们旧有的SQL&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; SDK进行升级。经过初步调研发现，基于`Factory`和`DynamicTable`的API，CatalogTable会被Planner直接变成Transformation，而不是跟之前一样可以拿到DataStream。比如之前的`StreamTableSource`#getDataStream，我可以直接获得DataStream对象的。这个动作/方法对于我们很重要，因为我们封装的SDK中，很多内部操作是在这个方法调用时触发/完成的。&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&gt;    所以，如果基于新的API去开发的话，我该如何获取到DataStream，或者达成类似的效果呢（尽可能不去动执行计划的话）&#013;&#010;&gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; --&#013;&#010;&gt; &gt; &gt; Best, Jingsong Lee&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "7",
        "reply": "<eaa373b855814d0897671e48f6cd2ace@autohome.com.cn>"
    },
    {
        "id": "<237f40e8.1cf0.17374d5039a.Coremail.apache22@163.com>",
        "from": "酷酷的浑蛋 &lt;apach...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 04:43:03 GMT",
        "subject": "flink1.11任务启动",
        "content": "现在启动任务（yarn）怎么指定-ys   和-p  都不管用了？  自动就分配好多core？&#010;默认启动个任务给我启动了100个core？100个container？我擦，啥情况啊，现在指定什么参数才生效啊？我默认配置也没有配置过多少个core啊，默认不是1个吗",
        "depth": "0",
        "reply": "<237f40e8.1cf0.17374d5039a.Coremail.apache22@163.com>"
    },
    {
        "id": "<CAA8tFvvH6mwXf5PR1NhgUN0J3LSRV-5w7ZgrykidLt-t_XbJaA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 05:04:56 GMT",
        "subject": "Re: flink1.11任务启动",
        "content": "Hi&#013;&#010;    你可以把的启动命令贴一下，然后说一下你期望的行为是什么，现在看到的行为是什么。&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;酷酷的浑蛋 &lt;apache22@163.com&gt; 于2020年7月22日周三 下午12:43写道：&#013;&#010;&#013;&#010;&gt; 现在启动任务（yarn）怎么指定-ys   和-p  都不管用了？  自动就分配好多core？&#013;&#010;&gt; 默认启动个任务给我启动了100个core？100个container？我擦，啥情况啊，现在指定什么参数才生效啊？我默认配置也没有配置过多少个core啊，默认不是1个吗&#013;&#010;",
        "depth": "1",
        "reply": "<237f40e8.1cf0.17374d5039a.Coremail.apache22@163.com>"
    },
    {
        "id": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>",
        "from": "Dian Fu &lt;dia...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 06:52:44 GMT",
        "subject": "[ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "The Apache Flink community is very happy to announce the release of Apache Flink 1.11.1, which&#010;is the first bugfix release for the Apache Flink 1.11 series.&#010;&#010;Apache Flink® is an open-source stream processing framework for distributed, high-performing,&#010;always-available, and accurate data streaming applications.&#010;&#010;The release is available for download at:&#010;https://flink.apache.org/downloads.html&#010;&#010;Please check out the release blog post for an overview of the improvements for this bugfix&#010;release:&#010;https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&#010;The full release notes are available in Jira:&#010;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&#010;We would like to thank all contributors of the Apache Flink community who made this release&#010;possible!&#010;&#010;Regards,&#010;Dian&#010;",
        "depth": "0",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<F2C3CAFC-FEF8-4B01-A63B-BCBB287062D3@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:09:25 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Congratulations!&#010;&#010;Thanks Dian Fu for the great work as release manager, and thanks everyone involved!&#010;&#010;Best&#010;Leonard Xu&#010;&#010;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt; 写道：&#010;&gt; &#010;&gt; The Apache Flink community is very happy to announce the release of Apache Flink 1.11.1,&#010;which is the first bugfix release for the Apache Flink 1.11 series.&#010;&gt; &#010;&gt; Apache Flink® is an open-source stream processing framework for distributed, high-performing,&#010;always-available, and accurate data streaming applications.&#010;&gt; &#010;&gt; The release is available for download at:&#010;&gt; https://flink.apache.org/downloads.html&#010;&gt; &#010;&gt; Please check out the release blog post for an overview of the improvements for this bugfix&#010;release:&#010;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt; &#010;&gt; The full release notes are available in Jira:&#010;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt; &#010;&gt; We would like to thank all contributors of the Apache Flink community who made this release&#010;possible!&#010;&gt; &#010;&gt; Regards,&#010;&gt; Dian&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<1487C3E6-598E-4CE6-B967-7BCB28E36DAF@gmail.com>",
        "from": "Wei Zhong &lt;weizhong0...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:14:53 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Congratulations! Thanks Dian for the great work!&#010;&#010;Best,&#010;Wei&#010;&#010;&gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Congratulations!&#010;&gt; &#010;&gt; Thanks Dian Fu for the great work as release manager, and thanks everyone involved!&#010;&gt; &#010;&gt; Best&#010;&gt; Leonard Xu&#010;&gt; &#010;&gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; The Apache Flink community is very happy to announce the release of Apache Flink&#010;1.11.1, which is the first bugfix release for the Apache Flink 1.11 series.&#010;&gt;&gt; &#010;&gt;&gt; Apache Flink® is an open-source stream processing framework for distributed, high-performing,&#010;always-available, and accurate data streaming applications.&#010;&gt;&gt; &#010;&gt;&gt; The release is available for download at:&#010;&gt;&gt; https://flink.apache.org/downloads.html&#010;&gt;&gt; &#010;&gt;&gt; Please check out the release blog post for an overview of the improvements for this&#010;bugfix release:&#010;&gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt;&gt; &#010;&gt;&gt; The full release notes are available in Jira:&#010;&gt;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt;&gt; &#010;&gt;&gt; We would like to thank all contributors of the Apache Flink community who made this&#010;release possible!&#010;&gt;&gt; &#010;&gt;&gt; Regards,&#010;&gt;&gt; Dian&#010;&gt; &#010;&#010;&#010;",
        "depth": "2",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CAFTKPZroNLx=2U_CkavwO9-XkoGki7UWmOwpfSUKn9+QjtxwLw@mail.gmail.com>",
        "from": "Yangze Guo &lt;karma...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:38:08 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Congrats!&#010;&#010;Thanks Dian Fu for being release manager, and everyone involved!&#010;&#010;Best,&#010;Yangze Guo&#010;&#010;On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; Congratulations! Thanks Dian for the great work!&#010;&gt;&#010;&gt; Best,&#010;&gt; Wei&#010;&gt;&#010;&gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &gt;&#010;&gt; &gt; Congratulations!&#010;&gt; &gt;&#010;&gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks everyone involved!&#010;&gt; &gt;&#010;&gt; &gt; Best&#010;&gt; &gt; Leonard Xu&#010;&gt; &gt;&#010;&gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt; 写道：&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; The Apache Flink community is very happy to announce the release of Apache Flink&#010;1.11.1, which is the first bugfix release for the Apache Flink 1.11 series.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Apache Flink® is an open-source stream processing framework for distributed,&#010;high-performing, always-available, and accurate data streaming applications.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; The release is available for download at:&#010;&gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Please check out the release blog post for an overview of the improvements for&#010;this bugfix release:&#010;&gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt; &gt;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; We would like to thank all contributors of the Apache Flink community who made&#010;this release possible!&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Regards,&#010;&gt; &gt;&gt; Dian&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "3",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CAELO931-q9y9Zhxz9E2Qdoz4vr0zY1TJKkNd7teWdE=X3OD3PQ@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:39:08 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Congratulations! Thanks Dian for the great work and to be the release&#010;manager!&#010;&#010;Best,&#010;Jark&#010;&#010;On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&#010;&gt; Congrats!&#010;&gt;&#010;&gt; Thanks Dian Fu for being release manager, and everyone involved!&#010;&gt;&#010;&gt; Best,&#010;&gt; Yangze Guo&#010;&gt;&#010;&gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; Congratulations! Thanks Dian for the great work!&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Wei&#010;&gt; &gt;&#010;&gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Congratulations!&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks&#010;&gt; everyone involved!&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best&#010;&gt; &gt; &gt; Leonard Xu&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt; 写道：&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce the release of&#010;&gt; Apache Flink 1.11.1, which is the first bugfix release for the Apache Flink&#010;&gt; 1.11 series.&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Apache Flink® is an open-source stream processing framework for&#010;&gt; distributed, high-performing, always-available, and accurate data streaming&#010;&gt; applications.&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; The release is available for download at:&#010;&gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Please check out the release blog post for an overview of the&#010;&gt; improvements for this bugfix release:&#010;&gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt; &gt; &gt;&gt;&#010;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache Flink community&#010;&gt; who made this release possible!&#010;&gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&gt; Regards,&#010;&gt; &gt; &gt;&gt; Dian&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "4",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CAGkDawnc=bq1kiYKavotB9QgBjO5QpOFQ_2DYErPjQSw7VKnRQ@mail.gmail.com>",
        "from": "Hequn Cheng &lt;he...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 09:36:15 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Thanks Dian for the great work and thanks to everyone who makes this&#010;release possible!&#010;&#010;Best, Hequn&#010;&#010;On Wed, Jul 22, 2020 at 4:40 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#010;&#010;&gt; Congratulations! Thanks Dian for the great work and to be the release&#010;&gt; manager!&#010;&gt;&#010;&gt; Best,&#010;&gt; Jark&#010;&gt;&#010;&gt; On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; Congrats!&#010;&gt; &gt;&#010;&gt; &gt; Thanks Dian Fu for being release manager, and everyone involved!&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Yangze Guo&#010;&gt; &gt;&#010;&gt; &gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt;&#010;&gt; wrote:&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Congratulations! Thanks Dian for the great work!&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Wei&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Congratulations!&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks&#010;&gt; &gt; everyone involved!&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Best&#010;&gt; &gt; &gt; &gt; Leonard Xu&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt; 写道：&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce the release of&#010;&gt; &gt; Apache Flink 1.11.1, which is the first bugfix release for the Apache&#010;&gt; Flink&#010;&gt; &gt; 1.11 series.&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; Apache Flink® is an open-source stream processing framework for&#010;&gt; &gt; distributed, high-performing, always-available, and accurate data&#010;&gt; streaming&#010;&gt; &gt; applications.&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; The release is available for download at:&#010;&gt; &gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; Please check out the release blog post for an overview of the&#010;&gt; &gt; improvements for this bugfix release:&#010;&gt; &gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache Flink&#010;&gt; community&#010;&gt; &gt; who made this release possible!&#010;&gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt;&gt; Regards,&#010;&gt; &gt; &gt; &gt;&gt; Dian&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CAJNyZN5Aipb1V2ReL0UhzS--=y-X45maLh+4Wjo885EendT+-A@mail.gmail.com>",
        "from": "Till Rohrmann &lt;trohrm...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 11:42:46 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Thanks for being the release manager for the 1.11.1 release, Dian. Thanks a&#010;lot to everyone who contributed to this release.&#010;&#010;Cheers,&#010;Till&#010;&#010;On Wed, Jul 22, 2020 at 11:38 AM Hequn Cheng &lt;hequn@apache.org&gt; wrote:&#010;&#010;&gt; Thanks Dian for the great work and thanks to everyone who makes this&#010;&gt; release possible!&#010;&gt;&#010;&gt; Best, Hequn&#010;&gt;&#010;&gt; On Wed, Jul 22, 2020 at 4:40 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; Congratulations! Thanks Dian for the great work and to be the release&#010;&gt; &gt; manager!&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Jark&#010;&gt; &gt;&#010;&gt; &gt; On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; &gt; Congrats!&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Thanks Dian Fu for being release manager, and everyone involved!&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Yangze Guo&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt;&#010;&gt; &gt; wrote:&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Congratulations! Thanks Dian for the great work!&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; Wei&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#010;写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Congratulations!&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks&#010;&gt; &gt; &gt; everyone involved!&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Best&#010;&gt; &gt; &gt; &gt; &gt; Leonard Xu&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt;&#010;写道：&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce the release&#010;&gt; of&#010;&gt; &gt; &gt; Apache Flink 1.11.1, which is the first bugfix release for the Apache&#010;&gt; &gt; Flink&#010;&gt; &gt; &gt; 1.11 series.&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Apache Flink® is an open-source stream processing framework&#010;for&#010;&gt; &gt; &gt; distributed, high-performing, always-available, and accurate data&#010;&gt; &gt; streaming&#010;&gt; &gt; &gt; applications.&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; The release is available for download at:&#010;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Please check out the release blog post for an overview of the&#010;&gt; &gt; &gt; improvements for this bugfix release:&#010;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache Flink&#010;&gt; &gt; community&#010;&gt; &gt; &gt; who made this release possible!&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Regards,&#010;&gt; &gt; &gt; &gt; &gt;&gt; Dian&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "6",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CAAjxVPODSr61hjQDn_fUHHd3Ddy2-zo_H2zQqN48L9ksTs5W=A@mail.gmail.com>",
        "from": "Konstantin Knauf &lt;kna...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 11:53:48 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Thank you for managing the quick follow up release. I think this was very&#010;important for Table &amp; SQL users.&#010;&#010;On Wed, Jul 22, 2020 at 1:45 PM Till Rohrmann &lt;trohrmann@apache.org&gt; wrote:&#010;&#010;&gt; Thanks for being the release manager for the 1.11.1 release, Dian. Thanks&#010;&gt; a lot to everyone who contributed to this release.&#010;&gt;&#010;&gt; Cheers,&#010;&gt; Till&#010;&gt;&#010;&gt; On Wed, Jul 22, 2020 at 11:38 AM Hequn Cheng &lt;hequn@apache.org&gt; wrote:&#010;&gt;&#010;&gt;&gt; Thanks Dian for the great work and thanks to everyone who makes this&#010;&gt;&gt; release possible!&#010;&gt;&gt;&#010;&gt;&gt; Best, Hequn&#010;&gt;&gt;&#010;&gt;&gt; On Wed, Jul 22, 2020 at 4:40 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt; &gt; Congratulations! Thanks Dian for the great work and to be the release&#010;&gt;&gt; &gt; manager!&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Jark&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; &gt; Congrats!&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Thanks Dian Fu for being release manager, and everyone involved!&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; Yangze Guo&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt;&#010;&gt;&gt; &gt; wrote:&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; Congratulations! Thanks Dian for the great work!&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; &gt; Wei&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#010;写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; Congratulations!&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks&#010;&gt;&gt; &gt; &gt; everyone involved!&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; Best&#010;&gt;&gt; &gt; &gt; &gt; &gt; Leonard Xu&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt;&#010;写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce the&#010;release&#010;&gt;&gt; of&#010;&gt;&gt; &gt; &gt; Apache Flink 1.11.1, which is the first bugfix release for the Apache&#010;&gt;&gt; &gt; Flink&#010;&gt;&gt; &gt; &gt; 1.11 series.&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Apache Flink® is an open-source stream processing framework&#010;for&#010;&gt;&gt; &gt; &gt; distributed, high-performing, always-available, and accurate data&#010;&gt;&gt; &gt; streaming&#010;&gt;&gt; &gt; &gt; applications.&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; The release is available for download at:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Please check out the release blog post for an overview of&#010;the&#010;&gt;&gt; &gt; &gt; improvements for this bugfix release:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache Flink&#010;&gt;&gt; &gt; community&#010;&gt;&gt; &gt; &gt; who made this release possible!&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Regards,&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Dian&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&#010;&#010;-- &#010;&#010;Konstantin Knauf&#010;&#010;https://twitter.com/snntrable&#010;&#010;https://github.com/knaufk&#010;&#010;",
        "depth": "7",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<d7ad17ee-5b68-4480-9de8-1eb3d1b27fa3.wangzhijiang999@aliyun.com>",
        "from": "&quot;Zhijiang&quot; &lt;wangzhijiang...@aliyun.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 02:10:40 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Thanks for being the release manager and the efficient work, Dian!&#010;&#010;Best,&#010;Zhijiang&#010;&#010;&#010;------------------------------------------------------------------&#010;From:Konstantin Knauf &lt;knaufk@apache.org&gt;&#010;Send Time:2020年7月22日(星期三) 19:55&#010;To:Till Rohrmann &lt;trohrmann@apache.org&gt;&#010;Cc:dev &lt;dev@flink.apache.org&gt;; Yangze Guo &lt;karmagyz@gmail.com&gt;; Dian Fu &lt;dianfu@apache.org&gt;;&#010;user &lt;user@flink.apache.org&gt;; user-zh &lt;user-zh@flink.apache.org&gt;&#010;Subject:Re: [ANNOUNCE] Apache Flink 1.11.1 released&#010;&#010;Thank you for managing the quick follow up release. I think this was very important for Table&#010;&amp; SQL users.&#010;On Wed, Jul 22, 2020 at 1:45 PM Till Rohrmann &lt;trohrmann@apache.org&gt; wrote:&#010;&#010;Thanks for being the release manager for the 1.11.1 release, Dian. Thanks a lot to everyone&#010;who contributed to this release.&#010;&#010;Cheers,&#010;Till&#010;On Wed, Jul 22, 2020 at 11:38 AM Hequn Cheng &lt;hequn@apache.org&gt; wrote:&#010;Thanks Dian for the great work and thanks to everyone who makes this&#010; release possible!&#010;&#010; Best, Hequn&#010;&#010; On Wed, Jul 22, 2020 at 4:40 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#010;&#010; &gt; Congratulations! Thanks Dian for the great work and to be the release&#010; &gt; manager!&#010; &gt;&#010; &gt; Best,&#010; &gt; Jark&#010; &gt;&#010; &gt; On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010; &gt;&#010; &gt; &gt; Congrats!&#010; &gt; &gt;&#010; &gt; &gt; Thanks Dian Fu for being release manager, and everyone involved!&#010; &gt; &gt;&#010; &gt; &gt; Best,&#010; &gt; &gt; Yangze Guo&#010; &gt; &gt;&#010; &gt; &gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt;&#010; &gt; wrote:&#010; &gt; &gt; &gt;&#010; &gt; &gt; &gt; Congratulations! Thanks Dian for the great work!&#010; &gt; &gt; &gt;&#010; &gt; &gt; &gt; Best,&#010; &gt; &gt; &gt; Wei&#010; &gt; &gt; &gt;&#010; &gt; &gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt; 写道：&#010; &gt; &gt; &gt; &gt;&#010; &gt; &gt; &gt; &gt; Congratulations!&#010; &gt; &gt; &gt; &gt;&#010; &gt; &gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks&#010; &gt; &gt; everyone involved!&#010; &gt; &gt; &gt; &gt;&#010; &gt; &gt; &gt; &gt; Best&#010; &gt; &gt; &gt; &gt; Leonard Xu&#010; &gt; &gt; &gt; &gt;&#010; &gt; &gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt;&#010;写道：&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce the release&#010;of&#010; &gt; &gt; Apache Flink 1.11.1, which is the first bugfix release for the Apache&#010; &gt; Flink&#010; &gt; &gt; 1.11 series.&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt; &gt; &gt;&gt; Apache Flink(r) is an open-source stream processing framework for&#010; &gt; &gt; distributed, high-performing, always-available, and accurate data&#010; &gt; streaming&#010; &gt; &gt; applications.&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt; &gt; &gt;&gt; The release is available for download at:&#010; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt; &gt; &gt;&gt; Please check out the release blog post for an overview of the&#010; &gt; &gt; improvements for this bugfix release:&#010; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt;&#010; &gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache Flink&#010; &gt; community&#010; &gt; &gt; who made this release possible!&#010; &gt; &gt; &gt; &gt;&gt;&#010; &gt; &gt; &gt; &gt;&gt; Regards,&#010; &gt; &gt; &gt; &gt;&gt; Dian&#010; &gt; &gt; &gt; &gt;&#010; &gt; &gt; &gt;&#010; &gt; &gt;&#010; &gt;&#010;&#010;&#010;-- &#010;Konstantin Knauf &#010;https://twitter.com/snntrable&#010;https://github.com/knaufk &#010;&#010;",
        "depth": "7",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CABi+2jSXPFkTKO30r4VgK3Dqg9Piaw=-h2dNrxBB8sff8pH58Q@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 02:22:12 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Thanks for being the release manager for the 1.11.1 release, Dian.&#010;&#010;Best,&#010;Jingsong&#010;&#010;On Thu, Jul 23, 2020 at 10:12 AM Zhijiang &lt;wangzhijiang999@aliyun.com&gt;&#010;wrote:&#010;&#010;&gt; Thanks for being the release manager and the efficient work, Dian!&#010;&gt;&#010;&gt; Best,&#010;&gt; Zhijiang&#010;&gt;&#010;&gt; ------------------------------------------------------------------&#010;&gt; From:Konstantin Knauf &lt;knaufk@apache.org&gt;&#010;&gt; Send Time:2020年7月22日(星期三) 19:55&#010;&gt; To:Till Rohrmann &lt;trohrmann@apache.org&gt;&#010;&gt; Cc:dev &lt;dev@flink.apache.org&gt;; Yangze Guo &lt;karmagyz@gmail.com&gt;; Dian Fu &lt;&#010;&gt; dianfu@apache.org&gt;; user &lt;user@flink.apache.org&gt;; user-zh &lt;&#010;&gt; user-zh@flink.apache.org&gt;&#010;&gt; Subject:Re: [ANNOUNCE] Apache Flink 1.11.1 released&#010;&gt;&#010;&gt; Thank you for managing the quick follow up release. I think this was very&#010;&gt; important for Table &amp; SQL users.&#010;&gt;&#010;&gt; On Wed, Jul 22, 2020 at 1:45 PM Till Rohrmann &lt;trohrmann@apache.org&gt;&#010;&gt; wrote:&#010;&gt; Thanks for being the release manager for the 1.11.1 release, Dian. Thanks&#010;&gt; a lot to everyone who contributed to this release.&#010;&gt;&#010;&gt; Cheers,&#010;&gt; Till&#010;&gt;&#010;&gt; On Wed, Jul 22, 2020 at 11:38 AM Hequn Cheng &lt;hequn@apache.org&gt; wrote:&#010;&gt; Thanks Dian for the great work and thanks to everyone who makes this&#010;&gt; release possible!&#010;&gt;&#010;&gt; Best, Hequn&#010;&gt;&#010;&gt; On Wed, Jul 22, 2020 at 4:40 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#010;&gt;&#010;&gt; &gt; Congratulations! Thanks Dian for the great work and to be the release&#010;&gt; &gt; manager!&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Jark&#010;&gt; &gt;&#010;&gt; &gt; On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt; &gt; Congrats!&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Thanks Dian Fu for being release manager, and everyone involved!&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; Yangze Guo&#010;&gt; &gt; &gt;&#010;&gt; &gt; &gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt;&#010;&gt; &gt; wrote:&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Congratulations! Thanks Dian for the great work!&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; Best,&#010;&gt; &gt; &gt; &gt; Wei&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#010;写道：&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Congratulations!&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks&#010;&gt; &gt; &gt; everyone involved!&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt; Best&#010;&gt; &gt; &gt; &gt; &gt; Leonard Xu&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt;&#010;写道：&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce the release&#010;&gt; of&#010;&gt; &gt; &gt; Apache Flink 1.11.1, which is the first bugfix release for the Apache&#010;&gt; &gt; Flink&#010;&gt; &gt; &gt; 1.11 series.&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Apache Flink® is an open-source stream processing framework&#010;for&#010;&gt; &gt; &gt; distributed, high-performing, always-available, and accurate data&#010;&gt; &gt; streaming&#010;&gt; &gt; &gt; applications.&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; The release is available for download at:&#010;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Please check out the release blog post for an overview of the&#010;&gt; &gt; &gt; improvements for this bugfix release:&#010;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache Flink&#010;&gt; &gt; community&#010;&gt; &gt; &gt; who made this release possible!&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt; &gt; &gt; &gt;&gt; Regards,&#010;&gt; &gt; &gt; &gt; &gt;&gt; Dian&#010;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt; &gt;&#010;&gt; &gt; &gt;&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt;&#010;&gt; Konstantin Knauf&#010;&gt;&#010;&gt; https://twitter.com/snntrable&#010;&gt;&#010;&gt; https://github.com/knaufk&#010;&gt;&#010;&gt;&#010;&gt;&#010;&#010;-- &#010;Best, Jingsong Lee&#010;&#010;",
        "depth": "8",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CADH6UNQF3DxY6soko-qEA-va0NVAC8xQPc0-p7rVYO-50NKMyg@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 02:46:58 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Thanks Dian for the great work!&#010;&#010;On Thu, Jul 23, 2020 at 10:22 AM Jingsong Li &lt;jingsonglee0@gmail.com&gt; wrote:&#010;&#010;&gt; Thanks for being the release manager for the 1.11.1 release, Dian.&#010;&gt;&#010;&gt; Best,&#010;&gt; Jingsong&#010;&gt;&#010;&gt; On Thu, Jul 23, 2020 at 10:12 AM Zhijiang &lt;wangzhijiang999@aliyun.com&gt;&#010;&gt; wrote:&#010;&gt;&#010;&gt;&gt; Thanks for being the release manager and the efficient work, Dian!&#010;&gt;&gt;&#010;&gt;&gt; Best,&#010;&gt;&gt; Zhijiang&#010;&gt;&gt;&#010;&gt;&gt; ------------------------------------------------------------------&#010;&gt;&gt; From:Konstantin Knauf &lt;knaufk@apache.org&gt;&#010;&gt;&gt; Send Time:2020年7月22日(星期三) 19:55&#010;&gt;&gt; To:Till Rohrmann &lt;trohrmann@apache.org&gt;&#010;&gt;&gt; Cc:dev &lt;dev@flink.apache.org&gt;; Yangze Guo &lt;karmagyz@gmail.com&gt;; Dian&#010;Fu &lt;&#010;&gt;&gt; dianfu@apache.org&gt;; user &lt;user@flink.apache.org&gt;; user-zh &lt;&#010;&gt;&gt; user-zh@flink.apache.org&gt;&#010;&gt;&gt; Subject:Re: [ANNOUNCE] Apache Flink 1.11.1 released&#010;&gt;&gt;&#010;&gt;&gt; Thank you for managing the quick follow up release. I think this was very&#010;&gt;&gt; important for Table &amp; SQL users.&#010;&gt;&gt;&#010;&gt;&gt; On Wed, Jul 22, 2020 at 1:45 PM Till Rohrmann &lt;trohrmann@apache.org&gt;&#010;&gt;&gt; wrote:&#010;&gt;&gt; Thanks for being the release manager for the 1.11.1 release, Dian. Thanks&#010;&gt;&gt; a lot to everyone who contributed to this release.&#010;&gt;&gt;&#010;&gt;&gt; Cheers,&#010;&gt;&gt; Till&#010;&gt;&gt;&#010;&gt;&gt; On Wed, Jul 22, 2020 at 11:38 AM Hequn Cheng &lt;hequn@apache.org&gt; wrote:&#010;&gt;&gt; Thanks Dian for the great work and thanks to everyone who makes this&#010;&gt;&gt; release possible!&#010;&gt;&gt;&#010;&gt;&gt; Best, Hequn&#010;&gt;&gt;&#010;&gt;&gt; On Wed, Jul 22, 2020 at 4:40 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#010;&gt;&gt;&#010;&gt;&gt; &gt; Congratulations! Thanks Dian for the great work and to be the release&#010;&gt;&gt; &gt; manager!&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; Best,&#010;&gt;&gt; &gt; Jark&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt;&gt; &gt;&#010;&gt;&gt; &gt; &gt; Congrats!&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Thanks Dian Fu for being release manager, and everyone involved!&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; Yangze Guo&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt;&#010;&gt;&gt; &gt; wrote:&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; Congratulations! Thanks Dian for the great work!&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; Best,&#010;&gt;&gt; &gt; &gt; &gt; Wei&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#010;写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; Congratulations!&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and thanks&#010;&gt;&gt; &gt; &gt; everyone involved!&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt; Best&#010;&gt;&gt; &gt; &gt; &gt; &gt; Leonard Xu&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt;&#010;写道：&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce the&#010;release&#010;&gt;&gt; of&#010;&gt;&gt; &gt; &gt; Apache Flink 1.11.1, which is the first bugfix release for the Apache&#010;&gt;&gt; &gt; Flink&#010;&gt;&gt; &gt; &gt; 1.11 series.&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Apache Flink® is an open-source stream processing framework&#010;for&#010;&gt;&gt; &gt; &gt; distributed, high-performing, always-available, and accurate data&#010;&gt;&gt; &gt; streaming&#010;&gt;&gt; &gt; &gt; applications.&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; The release is available for download at:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Please check out the release blog post for an overview of&#010;the&#010;&gt;&gt; &gt; &gt; improvements for this bugfix release:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache Flink&#010;&gt;&gt; &gt; community&#010;&gt;&gt; &gt; &gt; who made this release possible!&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Regards,&#010;&gt;&gt; &gt; &gt; &gt; &gt;&gt; Dian&#010;&gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt; &gt;&#010;&gt;&gt; &gt; &gt;&#010;&gt;&gt; &gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; --&#010;&gt;&gt;&#010;&gt;&gt; Konstantin Knauf&#010;&gt;&gt;&#010;&gt;&gt; https://twitter.com/snntrable&#010;&gt;&gt;&#010;&gt;&gt; https://github.com/knaufk&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Best, Jingsong Lee&#010;&gt;&#010;&#010;&#010;-- &#010;Best regards!&#010;Rui Li&#010;&#010;",
        "depth": "9",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<CAA8tFvtZ2Rv-RJ=VyX83Dg_Q-1OtSJH3t7AMohy6WJukyOSz4A@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 05:16:07 GMT",
        "subject": "Re: [ANNOUNCE] Apache Flink 1.11.1 released",
        "content": "Thanks Dian for the great work and thanks to everyone who makes this&#010;release possible!&#010;&#010;Best,&#010;Congxian&#010;&#010;&#010;Rui Li &lt;lirui.fudan@gmail.com&gt; 于2020年7月23日周四 上午10:48写道：&#010;&#010;&gt; Thanks Dian for the great work!&#010;&gt;&#010;&gt; On Thu, Jul 23, 2020 at 10:22 AM Jingsong Li &lt;jingsonglee0@gmail.com&gt;&#010;&gt; wrote:&#010;&gt;&#010;&gt; &gt; Thanks for being the release manager for the 1.11.1 release, Dian.&#010;&gt; &gt;&#010;&gt; &gt; Best,&#010;&gt; &gt; Jingsong&#010;&gt; &gt;&#010;&gt; &gt; On Thu, Jul 23, 2020 at 10:12 AM Zhijiang &lt;wangzhijiang999@aliyun.com&gt;&#010;&gt; &gt; wrote:&#010;&gt; &gt;&#010;&gt; &gt;&gt; Thanks for being the release manager and the efficient work, Dian!&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Best,&#010;&gt; &gt;&gt; Zhijiang&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; ------------------------------------------------------------------&#010;&gt; &gt;&gt; From:Konstantin Knauf &lt;knaufk@apache.org&gt;&#010;&gt; &gt;&gt; Send Time:2020年7月22日(星期三) 19:55&#010;&gt; &gt;&gt; To:Till Rohrmann &lt;trohrmann@apache.org&gt;&#010;&gt; &gt;&gt; Cc:dev &lt;dev@flink.apache.org&gt;; Yangze Guo &lt;karmagyz@gmail.com&gt;;&#010;Dian&#010;&gt; Fu &lt;&#010;&gt; &gt;&gt; dianfu@apache.org&gt;; user &lt;user@flink.apache.org&gt;; user-zh &lt;&#010;&gt; &gt;&gt; user-zh@flink.apache.org&gt;&#010;&gt; &gt;&gt; Subject:Re: [ANNOUNCE] Apache Flink 1.11.1 released&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Thank you for managing the quick follow up release. I think this was&#010;&gt; very&#010;&gt; &gt;&gt; important for Table &amp; SQL users.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; On Wed, Jul 22, 2020 at 1:45 PM Till Rohrmann &lt;trohrmann@apache.org&gt;&#010;&gt; &gt;&gt; wrote:&#010;&gt; &gt;&gt; Thanks for being the release manager for the 1.11.1 release, Dian.&#010;&gt; Thanks&#010;&gt; &gt;&gt; a lot to everyone who contributed to this release.&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Cheers,&#010;&gt; &gt;&gt; Till&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; On Wed, Jul 22, 2020 at 11:38 AM Hequn Cheng &lt;hequn@apache.org&gt; wrote:&#010;&gt; &gt;&gt; Thanks Dian for the great work and thanks to everyone who makes this&#010;&gt; &gt;&gt; release possible!&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Best, Hequn&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; On Wed, Jul 22, 2020 at 4:40 PM Jark Wu &lt;imjark@gmail.com&gt; wrote:&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; Congratulations! Thanks Dian for the great work and to be the release&#010;&gt; &gt;&gt; &gt; manager!&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; Best,&#010;&gt; &gt;&gt; &gt; Jark&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; On Wed, 22 Jul 2020 at 15:45, Yangze Guo &lt;karmagyz@gmail.com&gt; wrote:&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; Congrats!&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; Thanks Dian Fu for being release manager, and everyone involved!&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; Best,&#010;&gt; &gt;&gt; &gt; &gt; Yangze Guo&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; On Wed, Jul 22, 2020 at 3:14 PM Wei Zhong &lt;weizhong0618@gmail.com&gt;&#010;&gt; &gt;&gt; &gt; wrote:&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; Congratulations! Thanks Dian for the great work!&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; Best,&#010;&gt; &gt;&gt; &gt; &gt; &gt; Wei&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; 在 2020年7月22日，15:09，Leonard Xu &lt;xbjtdcq@gmail.com&gt;&#010;写道：&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Congratulations!&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Thanks Dian Fu for the great work as release manager, and&#010;thanks&#010;&gt; &gt;&gt; &gt; &gt; everyone involved!&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Best&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt; Leonard Xu&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; 在 2020年7月22日，14:52，Dian Fu &lt;dianfu@apache.org&gt;&#010;写道：&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; The Apache Flink community is very happy to announce&#010;the&#010;&gt; release&#010;&gt; &gt;&gt; of&#010;&gt; &gt;&gt; &gt; &gt; Apache Flink 1.11.1, which is the first bugfix release for the&#010;&gt; Apache&#010;&gt; &gt;&gt; &gt; Flink&#010;&gt; &gt;&gt; &gt; &gt; 1.11 series.&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; Apache Flink® is an open-source stream processing framework&#010;for&#010;&gt; &gt;&gt; &gt; &gt; distributed, high-performing, always-available, and accurate data&#010;&gt; &gt;&gt; &gt; streaming&#010;&gt; &gt;&gt; &gt; &gt; applications.&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; The release is available for download at:&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/downloads.html&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; Please check out the release blog post for an overview&#010;of the&#010;&gt; &gt;&gt; &gt; &gt; improvements for this bugfix release:&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; https://flink.apache.org/news/2020/07/21/release-1.11.1.html&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; The full release notes are available in Jira:&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12348323&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; We would like to thank all contributors of the Apache&#010;Flink&#010;&gt; &gt;&gt; &gt; community&#010;&gt; &gt;&gt; &gt; &gt; who made this release possible!&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; Regards,&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&gt; Dian&#010;&gt; &gt;&gt; &gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt; &gt;&#010;&gt; &gt;&gt; &gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; --&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; Konstantin Knauf&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; https://twitter.com/snntrable&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt; https://github.com/knaufk&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt; Best, Jingsong Lee&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Best regards!&#010;&gt; Rui Li&#010;&gt;&#010;&#010;",
        "depth": "10",
        "reply": "<8B0FF263-43F7-4261-8A34-68CF52FFB420@apache.org>"
    },
    {
        "id": "<211e3037.49fa.17375729338.Coremail.stevenchen01@163.com>",
        "from": "&quot;steven chen&quot; &lt;stevenche...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:35:09 GMT",
        "subject": "flink 问题排查",
        "content": "hi:&#010;这个flink 版本1.10 全是提交sql 运行，生产环境经常出现这种问题，然后节点就死了，任务又只能从checkpoits&#010;恢复，该如何解决？sql  里mysql 如何释放mysql 这个，求大佬回答？这是生产环境",
        "depth": "0",
        "reply": "<211e3037.49fa.17375729338.Coremail.stevenchen01@163.com>"
    },
    {
        "id": "<tencent_D47BB72BD27814D758D90CD956F55B283607@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:45:21 GMT",
        "subject": "Re: flink1.11 web ui没有DAG",
        "content": "",
        "depth": "0",
        "reply": "<tencent_D47BB72BD27814D758D90CD956F55B283607@qq.com>"
    },
    {
        "id": "<CAA8tFvs_1Eycb8B7oxYD2AFTi0LYt45E1HcT8Hwd20fPvbs-fw@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 05:33:16 GMT",
        "subject": "Re: flink1.11 web ui没有DAG",
        "content": "Hi&#013;&#010;   你的图片我这边显示不出来，能否把图片放到某个图床，然后把链接发过来呢？这样大家能更好的查看图片&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;小学生 &lt;201782053@qq.com&gt; 于2020年7月22日周三 下午3:49写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_D47BB72BD27814D758D90CD956F55B283607@qq.com>"
    },
    {
        "id": "<202007221551443887362@163.com>",
        "from": "&quot;tiantingting5435@163.com&quot; &lt;tiantingting5...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 07:51:44 GMT",
        "subject": "flink stream如何为每条数据生成自增主键",
        "content": "&#013;&#010;flink stream如何为每条数据生成自增主键？？时间戳貌似不行，同一时间戳可能会产生多条数据，无法区分数据的现后顺序。&#013;&#010;&#013;&#010;&#013;&#010;tiantingting5435@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007221551443887362@163.com>"
    },
    {
        "id": "<4c8a761c.58f9.173759850c3.Coremail.greemqqran@163.com>",
        "from": "&quot;Michael Ran&quot; &lt;greemqq...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:16:23 GMT",
        "subject": "Re:flink stream如何为每条数据生成自增主键",
        "content": "id 生成器吧&#010;在 2020-07-22 15:51:44，\"tiantingting5435@163.com\" &lt;tiantingting5435@163.com&gt; 写道：&#010;&gt;&#013;&#010;&gt;flink stream如何为每条数据生成自增主键？？时间戳貌似不行，同一时间戳可能会产生多条数据，无法区分数据的现后顺序。&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;tiantingting5435@163.com&#013;&#010;",
        "depth": "1",
        "reply": "<202007221551443887362@163.com>"
    },
    {
        "id": "<CAA8tFvuMMbjeDSCYZrsR7tvGMinhtiQ7ELq1bFhdLHSfTH-dkQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 05:31:24 GMT",
        "subject": "Re: flink stream如何为每条数据生成自增主键",
        "content": "Hi&#013;&#010;   你是希望每条数据有一个 id，这个 id 是随着数据递增的是啊？或许你可以使用&#010;RichMapFunction[1] 来做这个事情，在每次&#013;&#010;mapFunction 中把自定的 id 加进去，然后这个 id 还可以保存到 state 中，这样就算作业&#010;failover 了，自增 id&#013;&#010;也不会有问题。&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/user_defined_functions.html#rich-functions&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Michael Ran &lt;greemqqran@163.com&gt; 于2020年7月22日周三 下午4:17写道：&#013;&#010;&#013;&#010;&gt; id 生成器吧&#013;&#010;&gt; 在 2020-07-22 15:51:44，\"tiantingting5435@163.com\" &lt;tiantingting5435@163.com&gt;&#013;&#010;&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;flink stream如何为每条数据生成自增主键？？时间戳貌似不行，同一时间戳可能会产生多条数据，无法区分数据的现后顺序。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;tiantingting5435@163.com&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<202007221551443887362@163.com>"
    },
    {
        "id": "<42b48ade.578a.17375929387.Coremail.greemqqran@163.com>",
        "from": "&quot;Michael Ran&quot; &lt;greemqq...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:10:07 GMT",
        "subject": "flink sqlUpdate,如何获取里面的字段，字段类型，with 等3个属性",
        "content": "dear all:&#010;         我用flink 注册一张表：&#010;  CREATE TABLE dim_mysql (&#010;    id int,  -- &#010;    type varchar -- &#010;    ) WITH (&#010;    'connector' = 'jdbc',&#010;    'url' = 'jdbc:mysql://localhost:3390/test',&#010;    'table-name' = 'flink_test',&#010;    'driver' = 'com.mysql.cj.jdbc.Driver',&#010;    'username' = '****',&#010;    'password' = '****',&#010;    'lookup.cache.max-rows' = '5000',&#010;    'lookup.cache.ttl' = '1s',&#010;    'lookup.max-retries' = '3'&#010;    )&#010;有没有通过 tableEnv 去获取，字段[id,type]  类型[INTEGER,VARCHAR] 以及属性，map&lt;String,String&gt;&#010;这种。&#010;我看阿里官方有blink 支持自定义sink:&#010;publicabstractclassCustomSinkBaseimplementsSerializable{&#010;protectedMap&lt;String,String&gt; userParamsMap;// 您在sql with语句中定义的键值对，但所有的键均为小写&#010;protectedSet&lt;String&gt; primaryKeys;// 您定义的主键字段名&#010;protectedList&lt;String&gt; headerFields;// 标记为header的字段列表&#010;protectedRowTypeInfo rowTypeInfo;// 字段类型和名称&#010;核心需求是：获取定义的表的所有属性，自己实现自己的功能，包括&#010;join sink 等各种逻辑",
        "depth": "0",
        "reply": "<42b48ade.578a.17375929387.Coremail.greemqqran@163.com>"
    },
    {
        "id": "<CADQYLGvNr8oqYcyEbCxKFg8Aps6CLhG7obkcMZSgFp55Pu78Fg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 10:22:22 GMT",
        "subject": "Re: flink sqlUpdate,如何获取里面的字段，字段类型，with 等3个属性",
        "content": "tableEnv 中 可以通过&#013;&#010;tableEvn.from(xx).getSchema() 拿到该表的schema信息，但是没法拿到对应的properties。&#013;&#010;如果要拿到properties，可以通过catalog的接口得到 [1]。&#013;&#010;如果要自定义实现source/sink，可以参考 [2]&#013;&#010;&#013;&#010;[1]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/catalogs.html&#013;&#010;[2]&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sourceSinks.html&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Michael Ran &lt;greemqqran@163.com&gt; 于2020年7月22日周三 下午4:10写道：&#013;&#010;&#013;&#010;&gt; dear all:&#013;&#010;&gt;          我用flink 注册一张表：&#013;&#010;&gt;   CREATE TABLE dim_mysql (&#013;&#010;&gt;     id int,  --&#013;&#010;&gt;     type varchar --&#013;&#010;&gt;     ) WITH (&#013;&#010;&gt;     'connector' = 'jdbc',&#013;&#010;&gt;     'url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;&gt;     'table-name' = 'flink_test',&#013;&#010;&gt;     'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&gt;     'username' = '****',&#013;&#010;&gt;     'password' = '****',&#013;&#010;&gt;     'lookup.cache.max-rows' = '5000',&#013;&#010;&gt;     'lookup.cache.ttl' = '1s',&#013;&#010;&gt;     'lookup.max-retries' = '3'&#013;&#010;&gt;     )&#013;&#010;&gt; 有没有通过 tableEnv 去获取，字段[id,type]  类型[INTEGER,VARCHAR]&#013;&#010;&gt; 以及属性，map&lt;String,String&gt; 这种。&#013;&#010;&gt; 我看阿里官方有blink 支持自定义sink:&#013;&#010;&gt; publicabstractclassCustomSinkBaseimplementsSerializable{&#013;&#010;&gt; protectedMap&lt;String,String&gt; userParamsMap;// 您在sql with语句中定义的键值对，但所有的键均为小写&#013;&#010;&gt; protectedSet&lt;String&gt; primaryKeys;// 您定义的主键字段名&#013;&#010;&gt; protectedList&lt;String&gt; headerFields;// 标记为header的字段列表&#013;&#010;&gt; protectedRowTypeInfo rowTypeInfo;// 字段类型和名称&#013;&#010;&gt; 核心需求是：获取定义的表的所有属性，自己实现自己的功能，包括&#010;join sink 等各种逻辑&#013;&#010;",
        "depth": "1",
        "reply": "<42b48ade.578a.17375929387.Coremail.greemqqran@163.com>"
    },
    {
        "id": "<4418e209.772a.17376a24047.Coremail.greemqqran@163.com>",
        "from": "&quot;Michael Ran&quot; &lt;greemqq...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 13:06:51 GMT",
        "subject": "Re:Re: flink sqlUpdate,如何获取里面的字段，字段类型，with 等3个属性",
        "content": "1.tableEvn.from(xx).getSchema() 我确实通过这个拿到了schema，&lt;br/&gt;2.with properties属性很重要&#010;，关系我自定义的一些参数设定。&lt;br/&gt;3.关于  catalog 这个东西，是不是只有1.11&#010;版本才能从catalog  获取  with properties 哦? 1.10 you  有支持吗&#010;在 2020-07-22 18:22:22，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#010;&gt;tableEnv 中 可以通过&#013;&#010;&gt;tableEvn.from(xx).getSchema() 拿到该表的schema信息，但是没法拿到对应的properties。&#013;&#010;&gt;如果要拿到properties，可以通过catalog的接口得到 [1]。&#013;&#010;&gt;如果要自定义实现source/sink，可以参考 [2]&#013;&#010;&gt;&#013;&#010;&gt;[1]&#013;&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/catalogs.html&#013;&#010;&gt;[2]&#013;&#010;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sourceSinks.html&#013;&#010;&gt;&#013;&#010;&gt;Best,&#013;&#010;&gt;Godfrey&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;Michael Ran &lt;greemqqran@163.com&gt; 于2020年7月22日周三 下午4:10写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; dear all:&#013;&#010;&gt;&gt;          我用flink 注册一张表：&#013;&#010;&gt;&gt;   CREATE TABLE dim_mysql (&#013;&#010;&gt;&gt;     id int,  --&#013;&#010;&gt;&gt;     type varchar --&#013;&#010;&gt;&gt;     ) WITH (&#013;&#010;&gt;&gt;     'connector' = 'jdbc',&#013;&#010;&gt;&gt;     'url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;&gt;&gt;     'table-name' = 'flink_test',&#013;&#010;&gt;&gt;     'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&gt;&gt;     'username' = '****',&#013;&#010;&gt;&gt;     'password' = '****',&#013;&#010;&gt;&gt;     'lookup.cache.max-rows' = '5000',&#013;&#010;&gt;&gt;     'lookup.cache.ttl' = '1s',&#013;&#010;&gt;&gt;     'lookup.max-retries' = '3'&#013;&#010;&gt;&gt;     )&#013;&#010;&gt;&gt; 有没有通过 tableEnv 去获取，字段[id,type]  类型[INTEGER,VARCHAR]&#013;&#010;&gt;&gt; 以及属性，map&lt;String,String&gt; 这种。&#013;&#010;&gt;&gt; 我看阿里官方有blink 支持自定义sink:&#013;&#010;&gt;&gt; publicabstractclassCustomSinkBaseimplementsSerializable{&#013;&#010;&gt;&gt; protectedMap&lt;String,String&gt; userParamsMap;// 您在sql with语句中定义的键值对，但所有的键均为小写&#013;&#010;&gt;&gt; protectedSet&lt;String&gt; primaryKeys;// 您定义的主键字段名&#013;&#010;&gt;&gt; protectedList&lt;String&gt; headerFields;// 标记为header的字段列表&#013;&#010;&gt;&gt; protectedRowTypeInfo rowTypeInfo;// 字段类型和名称&#013;&#010;&gt;&gt; 核心需求是：获取定义的表的所有属性，自己实现自己的功能，包括&#010;join sink 等各种逻辑&#013;&#010;",
        "depth": "2",
        "reply": "<42b48ade.578a.17375929387.Coremail.greemqqran@163.com>"
    },
    {
        "id": "<CADQYLGsURfBEKR7Jx_OUkwxdx=rTs2fjNYHAAq5gUHe+ab5Guw@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 12:49:37 GMT",
        "subject": "Re: Re: flink sqlUpdate,如何获取里面的字段，字段类型，with 等3个属性",
        "content": "1.10 也是支持的&#013;&#010;&#013;&#010;Michael Ran &lt;greemqqran@163.com&gt; 于2020年7月22日周三 下午9:07写道：&#013;&#010;&#013;&#010;&gt; 1.tableEvn.from(xx).getSchema() 我确实通过这个拿到了schema，&lt;br/&gt;2.with&#013;&#010;&gt; properties属性很重要 ，关系我自定义的一些参数设定。&lt;br/&gt;3.关于&#010; catalog 这个东西，是不是只有1.11&#013;&#010;&gt; 版本才能从catalog  获取  with properties 哦? 1.10 you  有支持吗&#013;&#010;&gt; 在 2020-07-22 18:22:22，\"godfrey he\" &lt;godfreyhe@gmail.com&gt; 写道：&#013;&#010;&gt; &gt;tableEnv 中 可以通过&#013;&#010;&gt; &gt;tableEvn.from(xx).getSchema() 拿到该表的schema信息，但是没法拿到对应的properties。&#013;&#010;&gt; &gt;如果要拿到properties，可以通过catalog的接口得到 [1]。&#013;&#010;&gt; &gt;如果要自定义实现source/sink，可以参考 [2]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;[1]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/catalogs.html&#013;&#010;&gt; &gt;[2]&#013;&#010;&gt; &gt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sourceSinks.html&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Best,&#013;&#010;&gt; &gt;Godfrey&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;Michael Ran &lt;greemqqran@163.com&gt; 于2020年7月22日周三 下午4:10写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt; dear all:&#013;&#010;&gt; &gt;&gt;          我用flink 注册一张表：&#013;&#010;&gt; &gt;&gt;   CREATE TABLE dim_mysql (&#013;&#010;&gt; &gt;&gt;     id int,  --&#013;&#010;&gt; &gt;&gt;     type varchar --&#013;&#010;&gt; &gt;&gt;     ) WITH (&#013;&#010;&gt; &gt;&gt;     'connector' = 'jdbc',&#013;&#010;&gt; &gt;&gt;     'url' = 'jdbc:mysql://localhost:3390/test',&#013;&#010;&gt; &gt;&gt;     'table-name' = 'flink_test',&#013;&#010;&gt; &gt;&gt;     'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&gt; &gt;&gt;     'username' = '****',&#013;&#010;&gt; &gt;&gt;     'password' = '****',&#013;&#010;&gt; &gt;&gt;     'lookup.cache.max-rows' = '5000',&#013;&#010;&gt; &gt;&gt;     'lookup.cache.ttl' = '1s',&#013;&#010;&gt; &gt;&gt;     'lookup.max-retries' = '3'&#013;&#010;&gt; &gt;&gt;     )&#013;&#010;&gt; &gt;&gt; 有没有通过 tableEnv 去获取，字段[id,type]  类型[INTEGER,VARCHAR]&#013;&#010;&gt; &gt;&gt; 以及属性，map&lt;String,String&gt; 这种。&#013;&#010;&gt; &gt;&gt; 我看阿里官方有blink 支持自定义sink:&#013;&#010;&gt; &gt;&gt; publicabstractclassCustomSinkBaseimplementsSerializable{&#013;&#010;&gt; &gt;&gt; protectedMap&lt;String,String&gt; userParamsMap;// 您在sql&#013;&#010;&gt; with语句中定义的键值对，但所有的键均为小写&#013;&#010;&gt; &gt;&gt; protectedSet&lt;String&gt; primaryKeys;// 您定义的主键字段名&#013;&#010;&gt; &gt;&gt; protectedList&lt;String&gt; headerFields;// 标记为header的字段列表&#013;&#010;&gt; &gt;&gt; protectedRowTypeInfo rowTypeInfo;// 字段类型和名称&#013;&#010;&gt; &gt;&gt; 核心需求是：获取定义的表的所有属性，自己实现自己的功能，包括&#010;join sink 等各种逻辑&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<42b48ade.578a.17375929387.Coremail.greemqqran@163.com>"
    },
    {
        "id": "<CA+Lep4ZPxTQ42YkKTbH=RPEJLqH4bSj888200pbdYVppZhUw5A@mail.gmail.com>",
        "from": "Rainie Li &lt;raini...@pinterest.com.INVALID&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 08:10:33 GMT",
        "subject": "Flink APP restart policy not working",
        "content": "各位大佬好，&#010;本人Flink新手上路，想咨询一下有时候Flink App 设置了restartPolicy 但是还是restart不了，这种情况怎么破？&#010;&#010;*Job’s restartPolicy:*&#010;&#010;*env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1000,&#010;org.apache.flink.api.common.time.Time.seconds(30)));*&#010;&#010;*Job Manager log:*&#010;&#010;2020-07-15 20:26:27,831 INFO&#010;org.apache.flink.runtime.executiongraph.ExecutionGraph        - Job&#010;switched from state RUNNING to FAILING.&#010;&#010;org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException:&#010;Connection unexpectedly closed by remote task manager. This might&#010;indicate that the remote task manager was lost.&#010;&#010;    at org.apache.flink.runtime.io.network.netty.CreditBasedPartitionRequestClientHandler.channelInactive(CreditBasedPartitionRequestClientHandler.java:136)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:390)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:355)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:224)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1429)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:245)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:231)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:947)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:826)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:474)&#010;&#010;    at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:909)&#010;&#010;    at java.lang.Thread.run(Thread.java:748)&#010;&#010;&#010;*yarn node manager log:*&#010;&#010;2020-07-15 20:57:11.927858: I tensorflow/cc/saved_model/reader.cc:31]&#010;Reading SavedModel from&#010;&#010;2020-07-15 20:57:11.928419: I tensorflow/cc/saved_model/reader.cc:54]&#010;Reading meta graph with tags&#010;&#010;2020-07-15 20:57:11.928923: I&#010;tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports&#010;instructions that this TensorFlow binary was not compiled to use:&#010;SSE4.1 SSE4.2 AVX AVX2 FMA&#010;&#010;2020-07-15 20:57:11.935924: I tensorflow/cc/saved_model/loader.cc:162]&#010;Restoring SavedModel bundle.&#010;&#010;2020-07-15 20:57:11.939271: I tensorflow/cc/saved_model/loader.cc:138]&#010;Running MainOp with key saved_model_main_op on SavedModel bundle.&#010;&#010;2020-07-15 20:57:11.944583: I tensorflow/cc/saved_model/loader.cc:259]&#010;SavedModel load for tags; Status: success. Took 16732 microseconds.&#010;&#010;2020-07-15 20:58:13.356004: F&#010;tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot&#010;register 2 metrics with the same name:&#010;/tensorflow/cc/saved_model/load_attempt_count&#010;&#010;&#010;多谢🙏&#010;Rainie&#010;&#010;",
        "depth": "0",
        "reply": "<CA+Lep4ZPxTQ42YkKTbH=RPEJLqH4bSj888200pbdYVppZhUw5A@mail.gmail.com>"
    },
    {
        "id": "<163900c8.61f4.17375c56ff6.Coremail.stevenchen01@163.com>",
        "from": "&quot;steven chen&quot; &lt;stevenche...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 09:05:40 GMT",
        "subject": "flink 问题排查补充",
        "content": "hi:&#010;这个flink 版本1.10 全是提交sql 运行，生产环境经常出现这种问题，然后节点就死了，任务又只能从checkpoits&#010;恢复，该如何解决？sql  里mysql 如何释放mysql 这个，求大佬回答？这是生产环境&#010;&#010;2020-07-22 11:46:40,085 INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator  &#010;  - Discarding checkpoint 43842 of job a3eae3f691bdea687b9979b9e0ac28e2.&#010;&#010;org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 43842&#010;for operator GroupAggregate(groupBy=[item_code], select=[item_code, COUNT(ts) AS pv, COUNT(DISTINCT&#010;channelOrOfflineId) FILTER $f3 AS share_time, COUNT(ts) FILTER $f4 AS ios_pv, COUNT(ts) FILTER&#010;$f5 AS android_pv, COUNT(ts) FILTER $f6 AS other_pv, COUNT(DISTINCT userId) AS uv, COUNT(DISTINCT&#010;userId) FILTER $f4 AS ios_uv, COUNT(DISTINCT userId) FILTER $f5 AS android_uv, COUNT(DISTINCT&#010;userId) FILTER $f6 AS other_uv]) -&gt; SinkConversionToTuple2 -&gt; Sink: JDBCUpsertTableSink(item_code,&#010;pv, share_time, ios_pv, android_pv, other_pv, uv, ios_uv, android_uv, other_uv) (1/1). Failure&#010;reason: Checkpoint was declined.&#010;&#010;        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:434)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.checkpointStreamOperator(StreamTask.java:1420)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask$CheckpointingOperation.executeCheckpointing(StreamTask.java:1354)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.checkpointState(StreamTask.java:991)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$5(StreamTask.java:887)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:860)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:820)&#010;&#010;        at org.apache.flink.streaming.runtime.io.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:86)&#010;&#010;        at org.apache.flink.streaming.runtime.io.CheckpointBarrierAligner.processBarrier(CheckpointBarrierAligner.java:113)&#010;&#010;        at org.apache.flink.streaming.runtime.io.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:155)&#010;&#010;        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:133)&#010;&#010;        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:69)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:311)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:487)&#010;&#010;        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:470)&#010;&#010;        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)&#010;&#010;        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)&#010;&#010;        at java.lang.Thread.run(Thread.java:748)&#010;&#010;Caused by: java.lang.RuntimeException: Writing records to JDBC failed.&#010;&#010;        at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.checkFlushException(JDBCUpsertOutputFormat.java:135)&#010;&#010;        at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.flush(JDBCUpsertOutputFormat.java:155)&#010;&#010;        at org.apache.flink.api.java.io.jdbc.JDBCUpsertSinkFunction.snapshotState(JDBCUpsertSinkFunction.java:56)&#010;&#010;        at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.trySnapshotFunctionState(StreamingFunctionUtils.java:118)&#010;&#010;        at org.apache.flink.streaming.util.functions.StreamingFunctionUtils.snapshotFunctionState(StreamingFunctionUtils.java:99)&#010;&#010;        at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.snapshotState(AbstractUdfStreamOperator.java:90)&#010;&#010;        at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:402)&#010;&#010;        ... 19 more&#010;&#010;Caused by: java.lang.RuntimeException: Writing records to JDBC failed.&#010;&#010;        at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.checkFlushException(JDBCUpsertOutputFormat.java:135)&#010;&#010;        at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.flush(JDBCUpsertOutputFormat.java:155)&#010;&#010;        at org.apache.flink.api.java.io.jdbc.JDBCUpsertOutputFormat.lambda$open$0(JDBCUpsertOutputFormat.java:124)&#010;&#010;        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#010;&#010;        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)&#010;&#010;        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)&#010;&#010;&#010;&#010;&#010;&#010; ",
        "depth": "0",
        "reply": "<163900c8.61f4.17375c56ff6.Coremail.stevenchen01@163.com>"
    },
    {
        "id": "<2020072217340933948314@163.com>",
        "from": "&quot;amenhub@163.com&quot; &lt;amen...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 09:34:13 GMT",
        "subject": "flink 1.11 cdc相关问题",
        "content": "hi everyone,&#013;&#010;&#013;&#010;小白通过debezium将pgsql cdc数据同步至kafka之后，使用我们flink的sql client提交测试任务，但当kafka端cdc&#010;json数据一开始发送，任务即报错，通过web ui log查看界面发现错误日志如下，还请大佬帮忙分析，谢谢！&#013;&#010;&#013;&#010;====================================分割线======================================&#013;&#010;DDL：&#013;&#010;&#013;&#010;    CREATE TABLE pgsql_person_cdc(&#013;&#010;id BIGINT,&#013;&#010;name STRING,&#013;&#010;age STRING,&#013;&#010;sex STRING,&#013;&#010;phone STRING&#013;&#010;    ) WITH (&#013;&#010;'connector' = 'kafka',&#013;&#010;'topic' = 'postgres.public.person',&#013;&#010;'properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;'properties.group.id' = 'pgsql_cdc',&#013;&#010;'format' = 'debezium-json',&#013;&#010;'debezium-json.schema-include' = 'true'&#013;&#010;    )&#013;&#010;    CREATE TABLE sql_out (&#013;&#010;  id BIGINT,&#013;&#010;name STRING,&#013;&#010;age STRING,&#013;&#010;sex STRING,&#013;&#010;phone STRING&#013;&#010;    ) WITH (&#013;&#010;        'connector' = 'print'&#013;&#010;    )&#013;&#010;    INSERT INTO sql_out SELECT * FROM pgsql_person_cdc;&#013;&#010;&#013;&#010;====================================分割线======================================&#013;&#010;错误日志：&#013;&#010;&#013;&#010;java.io.IOException: Corrupt Debezium JSON message '{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"postgres.public.person.Envelope\"},\"payload\":{\"before\":null,\"after\":{\"id\":2,\"name\":\"liushimin\",\"age\":\"24\",\"sex\":\"man\",\"phone\":\"155555555\"},\"source\":{\"version\":\"1.2.0.Final\",\"connector\":\"postgresql\",\"name\":\"postgres\",\"ts_ms\":1595409754151,\"snapshot\":\"false\",\"db\":\"postgres\",\"schema\":\"public\",\"table\":\"person\",\"txId\":569,\"lsn\":23632344,\"xmin\":null},\"op\":\"u\",\"ts_ms\":1595409754270,\"transaction\":null}}'.&#013;&#010;    at org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.deserialize(DebeziumJsonDeserializationSchema.java:136)&#010;~[flink-json-1.11.0.jar:1.11.0]&#013;&#010;    at org.apache.flink.streaming.connectors.kafka.internals.KafkaDeserializationSchemaWrapper.deserialize(KafkaDeserializationSchemaWrapper.java:56)&#010;~[flink-connector-kafka-base_2.11-1.11.0.jar:1.11.0]&#013;&#010;    at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.partitionConsumerRecordsHandler(KafkaFetcher.java:181)&#010;~[flink-connector-kafka_2.11-1.11.0.jar:1.11.0]&#013;&#010;    at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.runFetchLoop(KafkaFetcher.java:141)&#010;~[flink-connector-kafka_2.11-1.11.0.jar:1.11.0]&#013;&#010;    at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.run(FlinkKafkaConsumerBase.java:755)&#010;~[flink-connector-kafka-base_2.11-1.11.0.jar:1.11.0]&#013;&#010;    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) ~[flink-dist_2.11-1.11.0.jar:1.11.0]&#013;&#010;    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) ~[flink-dist_2.11-1.11.0.jar:1.11.0]&#013;&#010;    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)&#010;~[flink-dist_2.11-1.11.0.jar:1.11.0]&#013;&#010;Caused by: java.lang.NullPointerException&#013;&#010;    at org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.deserialize(DebeziumJsonDeserializationSchema.java:120)&#010;~[flink-json-1.11.0.jar:1.11.0]&#013;&#010;    ... 7 more&#013;&#010;2020-07-22 17:22:34,415 INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources&#010;for Source: TableSourceScan(table=[[default_catalog, default_database, pgsql_person_cdc",
        "depth": "0",
        "reply": "<2020072217340933948314@163.com>"
    },
    {
        "id": "<A7EC1D6B-54C8-4CB5-91CF-F36DA78249AF@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 13:06:52 GMT",
        "subject": "Re: flink 1.11 cdc相关问题",
        "content": "Hello,&#010;&#010;代码在为before这条数据设置rowKind时抛了一个NPE，before正常应该是不为null的。&#010;看起来是你的数据问题，一条 update 的changelog， before 为null， 这是不合理的，没有before的数据，是无法处理after的数据的。&#010;如果确认是脏数据，可以开启ignore-parse-errors跳过[1]&#010;&#010;祝好&#010;Leonard&#010;[1]https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&gt;&#010;&#010;{&#010;    \"payload\": {&#010;        \"before\": null,&#010;        \"after\": {&#010;            \"id\": 2,&#010;            \"name\": \"liushimin\",&#010;            \"age\": \"24\",&#010;            \"sex\": \"man\",&#010;            \"phone\": \"155555555\"&#010;        },&#010;        \"source\": {&#010;            \"version\": \"1.2.0.Final\",&#010;            \"connector\": \"postgresql\",&#010;            \"name\": \"postgres\",&#010;            \"ts_ms\": 1595409754151,&#010;            \"snapshot\": \"false\",&#010;            \"db\": \"postgres\",&#010;            \"schema\": \"public\",&#010;            \"table\": \"person\",&#010;            \"txId\": 569,&#010;            \"lsn\": 23632344,&#010;            \"xmin\": null&#010;        },&#010;        \"op\": \"u\",&#010;        \"ts_ms\": 1595409754270,&#010;        \"transaction\": null&#010;    }&#010;}&#010;&#010;&gt; 在 2020年7月22日，17:34，amenhub@163.com 写道：&#010;&gt; &#010;&gt; {\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"postgres.public.person.Envelope\"},\"payload\":{\"before\":null,\"after\":{\"id\":2,\"name\":\"liushimin\",\"age\":\"24\",\"sex\":\"man\",\"phone\":\"155555555\"},\"source\":{\"version\":\"1.2.0.Final\",\"connector\":\"postgresql\",\"name\":\"postgres\",\"ts_ms\":1595409754151,\"snapshot\":\"false\",\"db\":\"postgres\",\"schema\":\"public\",\"table\":\"person\",\"txId\":569,\"lsn\":23632344,\"xmin\":null},\"op\":\"u\",\"ts_ms\":1595409754270,\"transaction\":null}}&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<2020072217340933948314@163.com>"
    },
    {
        "id": "<CAELO931_knxAhBpUjhtLv+DcAO-2bAZvs9hQ7UOh-Mkjp-CthA@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 15:56:41 GMT",
        "subject": "Re: flink 1.11 cdc相关问题",
        "content": "Hi,&#010;&#010;这是个已知问题，目前 debezium 同步不同数据库并没有保证一模一样地数据格式，比如同步&#010;PG 的UPDATE消息时候，before 和&#010;after 字段就不是全的。&#010;这个问题会在后面地版本中解决。&#010;&#010;Best,&#010;Jark&#010;&#010;On Wed, 22 Jul 2020 at 21:07, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#010;&#010;&gt; Hello,&#010;&gt;&#010;&gt; 代码在为before这条数据设置rowKind时抛了一个NPE，before正常应该是不为null的。&#010;&gt; 看起来是你的数据问题，一条 update 的changelog， before 为null，&#010;&gt; 这是不合理的，没有before的数据，是无法处理after的数据的。&#010;&gt; 如果确认是脏数据，可以开启ignore-parse-errors跳过[1]&#010;&gt;&#010;&gt; 祝好&#010;&gt; Leonard&#010;&gt; [1]&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#010;&gt; &lt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#010;&gt; &gt;&#010;&gt;&#010;&gt; {&#010;&gt;     \"payload\": {&#010;&gt;         \"before\": null,&#010;&gt;         \"after\": {&#010;&gt;             \"id\": 2,&#010;&gt;             \"name\": \"liushimin\",&#010;&gt;             \"age\": \"24\",&#010;&gt;             \"sex\": \"man\",&#010;&gt;             \"phone\": \"155555555\"&#010;&gt;         },&#010;&gt;         \"source\": {&#010;&gt;             \"version\": \"1.2.0.Final\",&#010;&gt;             \"connector\": \"postgresql\",&#010;&gt;             \"name\": \"postgres\",&#010;&gt;             \"ts_ms\": 1595409754151,&#010;&gt;             \"snapshot\": \"false\",&#010;&gt;             \"db\": \"postgres\",&#010;&gt;             \"schema\": \"public\",&#010;&gt;             \"table\": \"person\",&#010;&gt;             \"txId\": 569,&#010;&gt;             \"lsn\": 23632344,&#010;&gt;             \"xmin\": null&#010;&gt;         },&#010;&gt;         \"op\": \"u\",&#010;&gt;         \"ts_ms\": 1595409754270,&#010;&gt;         \"transaction\": null&#010;&gt;     }&#010;&gt; }&#010;&gt;&#010;&gt; &gt; 在 2020年7月22日，17:34，amenhub@163.com 写道：&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; {\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"postgres.public.person.Envelope\"},\"payload\":{\"before\":null,\"after\":{\"id\":2,\"name\":\"liushimin\",\"age\":\"24\",\"sex\":\"man\",\"phone\":\"155555555\"},\"source\":{\"version\":\"1.2.0.Final\",\"connector\":\"postgresql\",\"name\":\"postgres\",\"ts_ms\":1595409754151,\"snapshot\":\"false\",\"db\":\"postgres\",\"schema\":\"public\",\"table\":\"person\",\"txId\":569,\"lsn\":23632344,\"xmin\":null},\"op\":\"u\",\"ts_ms\":1595409754270,\"transaction\":null}}&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<2020072217340933948314@163.com>"
    },
    {
        "id": "<2020072309144393177117@163.com>",
        "from": "&quot;amenhub@163.com&quot; &lt;amen...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 01:14:47 GMT",
        "subject": "Re: Re: flink 1.11 cdc相关问题",
        "content": "感谢二位大佬@Leonard, @Jark的解答！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;amenhub@163.com&#013;&#010; &#013;&#010;发件人： Jark Wu&#013;&#010;发送时间： 2020-07-22 23:56&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: flink 1.11 cdc相关问题&#013;&#010;Hi,&#013;&#010; &#013;&#010;这是个已知问题，目前 debezium 同步不同数据库并没有保证一模一样地数据格式，比如同步&#010;PG 的UPDATE消息时候，before 和&#013;&#010;after 字段就不是全的。&#013;&#010;这个问题会在后面地版本中解决。&#013;&#010; &#013;&#010;Best,&#013;&#010;Jark&#013;&#010; &#013;&#010;On Wed, 22 Jul 2020 at 21:07, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#013;&#010; &#013;&#010;&gt; Hello,&#013;&#010;&gt;&#013;&#010;&gt; 代码在为before这条数据设置rowKind时抛了一个NPE，before正常应该是不为null的。&#013;&#010;&gt; 看起来是你的数据问题，一条 update 的changelog， before 为null，&#013;&#010;&gt; 这是不合理的，没有before的数据，是无法处理after的数据的。&#013;&#010;&gt; 如果确认是脏数据，可以开启ignore-parse-errors跳过[1]&#013;&#010;&gt;&#013;&#010;&gt; 祝好&#013;&#010;&gt; Leonard&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#013;&#010;&gt; &lt;&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt; {&#013;&#010;&gt;     \"payload\": {&#013;&#010;&gt;         \"before\": null,&#013;&#010;&gt;         \"after\": {&#013;&#010;&gt;             \"id\": 2,&#013;&#010;&gt;             \"name\": \"liushimin\",&#013;&#010;&gt;             \"age\": \"24\",&#013;&#010;&gt;             \"sex\": \"man\",&#013;&#010;&gt;             \"phone\": \"155555555\"&#013;&#010;&gt;         },&#013;&#010;&gt;         \"source\": {&#013;&#010;&gt;             \"version\": \"1.2.0.Final\",&#013;&#010;&gt;             \"connector\": \"postgresql\",&#013;&#010;&gt;             \"name\": \"postgres\",&#013;&#010;&gt;             \"ts_ms\": 1595409754151,&#013;&#010;&gt;             \"snapshot\": \"false\",&#013;&#010;&gt;             \"db\": \"postgres\",&#013;&#010;&gt;             \"schema\": \"public\",&#013;&#010;&gt;             \"table\": \"person\",&#013;&#010;&gt;             \"txId\": 569,&#013;&#010;&gt;             \"lsn\": 23632344,&#013;&#010;&gt;             \"xmin\": null&#013;&#010;&gt;         },&#013;&#010;&gt;         \"op\": \"u\",&#013;&#010;&gt;         \"ts_ms\": 1595409754270,&#013;&#010;&gt;         \"transaction\": null&#013;&#010;&gt;     }&#013;&#010;&gt; }&#013;&#010;&gt;&#013;&#010;&gt; &gt; 在 2020年7月22日，17:34，amenhub@163.com 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; {\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"postgres.public.person.Envelope\"},\"payload\":{\"before\":null,\"after\":{\"id\":2,\"name\":\"liushimin\",\"age\":\"24\",\"sex\":\"man\",\"phone\":\"155555555\"},\"source\":{\"version\":\"1.2.0.Final\",\"connector\":\"postgresql\",\"name\":\"postgres\",\"ts_ms\":1595409754151,\"snapshot\":\"false\",\"db\":\"postgres\",\"schema\":\"public\",\"table\":\"person\",\"txId\":569,\"lsn\":23632344,\"xmin\":null},\"op\":\"u\",\"ts_ms\":1595409754270,\"transaction\":null}}&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<2020072217340933948314@163.com>"
    },
    {
        "id": "<1BB60551-C6FB-4767-87D1-D058534850A0@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 08:20:05 GMT",
        "subject": "Re: flink 1.11 cdc相关问题",
        "content": "Hi amenhub&#010;&#010;针对这个问题，我建了个issue来跟踪这个问题[1],&#010;另外你可以在你的PG 里面把表的IDENTITY设置为FULL，这样 debezium 同步的UPDATE数据就会有完整的信息，&#010;DB命令是：ALTER TABLE yourTable REPLICA IDENTITY FULL, 可以参考debezium官网文档[2]&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18700 &lt;https://issues.apache.org/jira/browse/FLINK-18700&gt;&#010;[2] https://debezium.io/documentation/reference/1.2/connectors/postgresql.html &lt;https://debezium.io/documentation/reference/1.2/connectors/postgresql.html&gt;&#010;&#010;&gt; 在 2020年7月23日，09:14，amenhub@163.com 写道：&#010;&gt; &#010;&gt; 感谢二位大佬@Leonard, @Jark的解答！&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; amenhub@163.com&#010;&gt; &#010;&gt; 发件人： Jark Wu&#010;&gt; 发送时间： 2020-07-22 23:56&#010;&gt; 收件人： user-zh&#010;&gt; 主题： Re: flink 1.11 cdc相关问题&#010;&gt; Hi,&#010;&gt; &#010;&gt; 这是个已知问题，目前 debezium 同步不同数据库并没有保证一模一样地数据格式，比如同步&#010;PG 的UPDATE消息时候，before 和&#010;&gt; after 字段就不是全的。&#010;&gt; 这个问题会在后面地版本中解决。&#010;&gt; &#010;&gt; Best,&#010;&gt; Jark&#010;&gt; &#010;&gt; On Wed, 22 Jul 2020 at 21:07, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#010;&gt; &#010;&gt;&gt; Hello,&#010;&gt;&gt; &#010;&gt;&gt; 代码在为before这条数据设置rowKind时抛了一个NPE，before正常应该是不为null的。&#010;&gt;&gt; 看起来是你的数据问题，一条 update 的changelog， before 为null，&#010;&gt;&gt; 这是不合理的，没有before的数据，是无法处理after的数据的。&#010;&gt;&gt; 如果确认是脏数据，可以开启ignore-parse-errors跳过[1]&#010;&gt;&gt; &#010;&gt;&gt; 祝好&#010;&gt;&gt; Leonard&#010;&gt;&gt; [1]&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#010;&gt;&gt; &lt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#010;&gt;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; {&#010;&gt;&gt;    \"payload\": {&#010;&gt;&gt;        \"before\": null,&#010;&gt;&gt;        \"after\": {&#010;&gt;&gt;            \"id\": 2,&#010;&gt;&gt;            \"name\": \"liushimin\",&#010;&gt;&gt;            \"age\": \"24\",&#010;&gt;&gt;            \"sex\": \"man\",&#010;&gt;&gt;            \"phone\": \"155555555\"&#010;&gt;&gt;        },&#010;&gt;&gt;        \"source\": {&#010;&gt;&gt;            \"version\": \"1.2.0.Final\",&#010;&gt;&gt;            \"connector\": \"postgresql\",&#010;&gt;&gt;            \"name\": \"postgres\",&#010;&gt;&gt;            \"ts_ms\": 1595409754151,&#010;&gt;&gt;            \"snapshot\": \"false\",&#010;&gt;&gt;            \"db\": \"postgres\",&#010;&gt;&gt;            \"schema\": \"public\",&#010;&gt;&gt;            \"table\": \"person\",&#010;&gt;&gt;            \"txId\": 569,&#010;&gt;&gt;            \"lsn\": 23632344,&#010;&gt;&gt;            \"xmin\": null&#010;&gt;&gt;        },&#010;&gt;&gt;        \"op\": \"u\",&#010;&gt;&gt;        \"ts_ms\": 1595409754270,&#010;&gt;&gt;        \"transaction\": null&#010;&gt;&gt;    }&#010;&gt;&gt; }&#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月22日，17:34，amenhub@163.com 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt; {\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"postgres.public.person.Envelope\"},\"payload\":{\"before\":null,\"after\":{\"id\":2,\"name\":\"liushimin\",\"age\":\"24\",\"sex\":\"man\",\"phone\":\"155555555\"},\"source\":{\"version\":\"1.2.0.Final\",\"connector\":\"postgresql\",\"name\":\"postgres\",\"ts_ms\":1595409754151,\"snapshot\":\"false\",\"db\":\"postgres\",\"schema\":\"public\",\"table\":\"person\",\"txId\":569,\"lsn\":23632344,\"xmin\":null},\"op\":\"u\",\"ts_ms\":1595409754270,\"transaction\":null}}&#010;&gt;&gt; &#010;&gt;&gt; &#010;&#010;&#010;",
        "depth": "4",
        "reply": "<2020072217340933948314@163.com>"
    },
    {
        "id": "<323e52a5.67ea.1738054bc16.Coremail.amenhub@163.com>",
        "from": "&quot;amenhub@163.com&quot; &lt;amen...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 10:18:23 GMT",
        "subject": "回复: Re: flink 1.11 cdc相关问题",
        "content": "多谢！已关注~&#010;&#010;&#010;Best&#010;&#010;&#010;amenhub@163.com&#010; &#010;发件人： Leonard Xu&#010;发送时间： 2020-07-24 16:20&#010;收件人： user-zh&#010;主题： Re: flink 1.11 cdc相关问题&#010;Hi amenhub&#010; &#010;针对这个问题，我建了个issue来跟踪这个问题[1],&#010;另外你可以在你的PG 里面把表的IDENTITY设置为FULL，这样 debezium 同步的UPDATE数据就会有完整的信息，&#010;DB命令是：ALTER TABLE yourTable REPLICA IDENTITY FULL, 可以参考debezium官网文档[2]&#010; &#010;Best&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18700 &lt;https://issues.apache.org/jira/browse/FLINK-18700&gt;&#010;[2] https://debezium.io/documentation/reference/1.2/connectors/postgresql.html &lt;https://debezium.io/documentation/reference/1.2/connectors/postgresql.html&gt;&#010; &#010;&gt; 在 2020年7月23日，09:14，amenhub@163.com 写道：&#010;&gt;&#010;&gt; 感谢二位大佬@Leonard, @Jark的解答！&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; amenhub@163.com&#010;&gt;&#010;&gt; 发件人： Jark Wu&#010;&gt; 发送时间： 2020-07-22 23:56&#010;&gt; 收件人： user-zh&#010;&gt; 主题： Re: flink 1.11 cdc相关问题&#010;&gt; Hi,&#010;&gt;&#010;&gt; 这是个已知问题，目前 debezium 同步不同数据库并没有保证一模一样地数据格式，比如同步&#010;PG 的UPDATE消息时候，before 和&#010;&gt; after 字段就不是全的。&#010;&gt; 这个问题会在后面地版本中解决。&#010;&gt;&#010;&gt; Best,&#010;&gt; Jark&#010;&gt;&#010;&gt; On Wed, 22 Jul 2020 at 21:07, Leonard Xu &lt;xbjtdcq@gmail.com&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hello,&#010;&gt;&gt;&#010;&gt;&gt; 代码在为before这条数据设置rowKind时抛了一个NPE，before正常应该是不为null的。&#010;&gt;&gt; 看起来是你的数据问题，一条 update 的changelog， before 为null，&#010;&gt;&gt; 这是不合理的，没有before的数据，是无法处理after的数据的。&#010;&gt;&gt; 如果确认是脏数据，可以开启ignore-parse-errors跳过[1]&#010;&gt;&gt;&#010;&gt;&gt; 祝好&#010;&gt;&gt; Leonard&#010;&gt;&gt; [1]&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#010;&gt;&gt; &lt;&#010;&gt;&gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/connectors/formats/debezium.html#debezium-json-ignore-parse-errors&#010;&gt;&gt;&gt;&#010;&gt;&gt;&#010;&gt;&gt; {&#010;&gt;&gt;    \"payload\": {&#010;&gt;&gt;        \"before\": null,&#010;&gt;&gt;        \"after\": {&#010;&gt;&gt;            \"id\": 2,&#010;&gt;&gt;            \"name\": \"liushimin\",&#010;&gt;&gt;            \"age\": \"24\",&#010;&gt;&gt;            \"sex\": \"man\",&#010;&gt;&gt;            \"phone\": \"155555555\"&#010;&gt;&gt;        },&#010;&gt;&gt;        \"source\": {&#010;&gt;&gt;            \"version\": \"1.2.0.Final\",&#010;&gt;&gt;            \"connector\": \"postgresql\",&#010;&gt;&gt;            \"name\": \"postgres\",&#010;&gt;&gt;            \"ts_ms\": 1595409754151,&#010;&gt;&gt;            \"snapshot\": \"false\",&#010;&gt;&gt;            \"db\": \"postgres\",&#010;&gt;&gt;            \"schema\": \"public\",&#010;&gt;&gt;            \"table\": \"person\",&#010;&gt;&gt;            \"txId\": 569,&#010;&gt;&gt;            \"lsn\": 23632344,&#010;&gt;&gt;            \"xmin\": null&#010;&gt;&gt;        },&#010;&gt;&gt;        \"op\": \"u\",&#010;&gt;&gt;        \"ts_ms\": 1595409754270,&#010;&gt;&gt;        \"transaction\": null&#010;&gt;&gt;    }&#010;&gt;&gt; }&#010;&gt;&gt;&#010;&gt;&gt;&gt; 在 2020年7月22日，17:34，amenhub@163.com 写道：&#010;&gt;&gt;&gt;&#010;&gt;&gt;&gt;&#010;&gt;&gt; {\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"name\"},{\"type\":\"string\",\"optional\":true,\"field\":\"age\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sex\"},{\"type\":\"string\",\"optional\":true,\"field\":\"phone\"}],\"optional\":true,\"name\":\"postgres.public.person.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"postgres.public.person.Envelope\"},\"payload\":{\"before\":null,\"after\":{\"id\":2,\"name\":\"liushimin\",\"age\":\"24\",\"sex\":\"man\",\"phone\":\"155555555\"},\"source\":{\"version\":\"1.2.0.Final\",\"connector\":\"postgresql\",\"name\":\"postgres\",\"ts_ms\":1595409754151,\"snapshot\":\"false\",\"db\":\"postgres\",\"schema\":\"public\",\"table\":\"person\",\"txId\":569,\"lsn\":23632344,\"xmin\":null},\"op\":\"u\",\"ts_ms\":1595409754270,\"transaction\":null}}&#010;&gt;&gt;&#010;&gt;&gt;&#010; ",
        "depth": "1",
        "reply": "<2020072217340933948314@163.com>"
    },
    {
        "id": "<CAEZk041eq3vyYgD1R3cpGXXAtLcGFWGKUdd8NZvBsjwmg+muiA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 11:21:29 GMT",
        "subject": "flink row 类型",
        "content": "hi、&#013;&#010;我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过&#013;&#010;row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#013;&#010;&#013;&#010;rule_key  转换为rule_key1,rulekey2&#013;&#010;1&#013;&#010;2&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk041eq3vyYgD1R3cpGXXAtLcGFWGKUdd8NZvBsjwmg+muiA@mail.gmail.com>"
    },
    {
        "id": "<CABKuJ_STgnmamcfvDkeKX1Xi7a-McC60JvVoNtp8kgMZOEe2-g@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 04:48:25 GMT",
        "subject": "Re: flink row 类型",
        "content": "这个应该是做不到的。name只是SQL plan过程的东西，在运行时它就没有什么实际意义了。&#013;&#010;你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#013;&#010;&#013;&#010;Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道：&#013;&#010;&#013;&#010;&gt; hi、&#013;&#010;&gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过&#013;&#010;&gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#013;&#010;&gt;&#013;&#010;&gt; rule_key  转换为rule_key1,rulekey2&#013;&#010;&gt; 1&#013;&#010;&gt; 2&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk041eq3vyYgD1R3cpGXXAtLcGFWGKUdd8NZvBsjwmg+muiA@mail.gmail.com>"
    },
    {
        "id": "<CAEZk041Ut5jNq4CUAnAgq-wPmy6Bkwor1Jfe5fjEXYzayb149Q@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 06:08:49 GMT",
        "subject": "Re: flink row 类型",
        "content": "hi&#013;&#010;是的，我们的数据场景比较尴尬，那我想其他方式实现一下&#013;&#010;&#013;&#010;Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月23日周四 下午12:55写道：&#013;&#010;&#013;&#010;&gt; 这个应该是做不到的。name只是SQL plan过程的东西，在运行时它就没有什么实际意义了。&#013;&#010;&gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#013;&#010;&gt;&#013;&#010;&gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hi、&#013;&#010;&gt; &gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过&#013;&#010;&gt; &gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; rule_key  转换为rule_key1,rulekey2&#013;&#010;&gt; &gt; 1&#013;&#010;&gt; &gt; 2&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Benchao Li&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk041eq3vyYgD1R3cpGXXAtLcGFWGKUdd8NZvBsjwmg+muiA@mail.gmail.com>"
    },
    {
        "id": "<CABi+2jSUOHJou=L8xDJpbn9P61SjrrCJZB_5kvqyJqu+jyO18A@mail.gmail.com>",
        "from": "Jingsong Li &lt;jingsongl...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 06:13:24 GMT",
        "subject": "Re: flink row 类型",
        "content": "可以看下Flink 1.11的UDF type inference.&#013;&#010;&#013;&#010;在TypeInference中有input的type，这个type应该是包含字段信息的。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Thu, Jul 23, 2020 at 2:09 PM Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi&#013;&#010;&gt; 是的，我们的数据场景比较尴尬，那我想其他方式实现一下&#013;&#010;&gt;&#013;&#010;&gt; Benchao Li &lt;libenchao@apache.org&gt; 于2020年7月23日周四 下午12:55写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 这个应该是做不到的。name只是SQL plan过程的东西，在运行时它就没有什么实际意义了。&#013;&#010;&gt; &gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; hi、&#013;&#010;&gt; &gt; &gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过&#013;&#010;&gt; &gt; &gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; rule_key  转换为rule_key1,rulekey2&#013;&#010;&gt; &gt; &gt; 1&#013;&#010;&gt; &gt; &gt; 2&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; Benchao Li&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee&#013;&#010;",
        "depth": "3",
        "reply": "<CAEZk041eq3vyYgD1R3cpGXXAtLcGFWGKUdd8NZvBsjwmg+muiA@mail.gmail.com>"
    },
    {
        "id": "<tencent_91461F1123490E39B13805F900E3F9E0DF08@qq.com>",
        "from": "&quot;kcz&quot; &lt;573693...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 11:23:59 GMT",
        "subject": "回复：flink-1.11 ddl kafka-to-hive问题",
        "content": "谢谢大佬们，公众号有demo了，我去对比一下看看&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: Jingsong Li &lt;jingsonglee0@gmail.com&amp;gt;&#013;&#010;发送时间: 2020年7月22日 09:34&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：flink-1.11 ddl kafka-to-hive问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;你的Source表是怎么定义的？确定有watermark前进吗？(可以看Flink UI)&#013;&#010;&#013;&#010;'sink.partition-commit.trigger'='partition-time' 去掉试试？&#013;&#010;&#013;&#010;Best,&#013;&#010;Jingsong&#013;&#010;&#013;&#010;On Wed, Jul 22, 2020 at 12:02 AM Leonard Xu &lt;xbjtdcq@gmail.com&amp;gt; wrote:&#013;&#010;&#013;&#010;&amp;gt; HI,&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Hive 表时在flink里建的吗？ 如果是建表时使用了hive dialect吗？可以参考[1]设置下&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; Best&#013;&#010;&amp;gt; Leonard Xu&#013;&#010;&amp;gt; [1]&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_dialect.html#use-hive-dialect&#013;&#010;&amp;gt; &lt;&#013;&#010;&amp;gt; https://ci.apache.org/projects/flink/flink-docs-master/dev/table/hive/hive_dialect.html#use-hive-dialect&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; &amp;gt; 在 2020年7月21日，22:57，kcz &lt;573693104@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 一直都木有数据 我也不知道哪里不太对 hive有这个表了已经。我测试写ddl&#010;hdfs 是OK的&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt; &amp;gt; 发件人: JasonLee &lt;17610775726@163.com &lt;mailto:17610775726@163.com&amp;gt;&amp;amp;gt;&#013;&#010;&amp;gt; &amp;gt; 发送时间: 2020年7月21日 20:39&#013;&#010;&amp;gt; &amp;gt; 收件人: user-zh &lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&#013;&#010;&amp;gt; &amp;gt;&amp;amp;gt;&#013;&#010;&amp;gt; &amp;gt; 主题: 回复：flink-1.11 ddl kafka-to-hive问题&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; hi&#013;&#010;&amp;gt; &amp;gt; hive表是一直没有数据还是过一段时间就有数据了？&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; | |&#013;&#010;&amp;gt; &amp;gt; JasonLee&#013;&#010;&amp;gt; &amp;gt; |&#013;&#010;&amp;gt; &amp;gt; |&#013;&#010;&amp;gt; &amp;gt; 邮箱：17610775726@163.com&#013;&#010;&amp;gt; &amp;gt; |&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; Signature is customized by Netease Mail Master&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; 在2020年07月21日 19:09，kcz 写道：&#013;&#010;&amp;gt; &amp;gt; hive-1.2.1&#013;&#010;&amp;gt; &amp;gt; chk 已经成功了（去chk目录查看了的确有chk数据，kafka也有数据），但是hive表没有数据，我是哪里缺少了什么吗？&#013;&#010;&amp;gt; &amp;gt; String hiveSql = \"CREATE&amp;amp;nbsp; TABLE&amp;amp;nbsp; stream_tmp.fs_table&#010;(\\n\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"&amp;amp;nbsp; host STRING,\\n\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"&amp;amp;nbsp; url STRING,\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"&amp;amp;nbsp; public_date STRING\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\") partitioned by (public_date&#013;&#010;&amp;gt; string) \" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"stored as PARQUET \" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"TBLPROPERTIES (\\n\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"&amp;amp;nbsp;&#013;&#010;&amp;gt; 'sink.partition-commit.delay'='0 s',\\n\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"&amp;amp;nbsp;&#013;&#010;&amp;gt; 'sink.partition-commit.trigger'='partition-time',\\n\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\"&amp;amp;nbsp;&#013;&#010;&amp;gt; 'sink.partition-commit.policy.kind'='metastore,success-file'\" +&#013;&#010;&amp;gt; &amp;gt; &amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&amp;amp;nbsp;&#010;\")\";&#013;&#010;&amp;gt; &amp;gt; tableEnv.executeSql(hiveSql);&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt;&#013;&#010;&amp;gt; &amp;gt; tableEnv.executeSql(\"INSERT INTO&amp;amp;nbsp; stream_tmp.fs_table SELECT&#010;host,&#013;&#010;&amp;gt; url, DATE_FORMAT(public_date, 'yyyy-MM-dd') FROM stream_tmp.source_table\");&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&#013;&#010;-- &#013;&#010;Best, Jingsong Lee",
        "depth": "0",
        "reply": "<tencent_91461F1123490E39B13805F900E3F9E0DF08@qq.com>"
    },
    {
        "id": "<tencent_1FF36C91503C21140D95C1D8CF6FDB4D6306@qq.com>",
        "from": "&quot;Asahi Lee&quot; &lt;978466...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Wed, 22 Jul 2020 16:07:39 GMT",
        "subject": "flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误",
        "content": "1. 程序&#013;&#010;StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv,&#010;bsSettings);&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; String sourceTableDDL = \"CREATE TABLE fs_table&#010;(\" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&amp;nbsp;&#010;user_id STRING,\" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&amp;nbsp;&#010;order_amount DOUBLE,\" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&amp;nbsp;&#010;dt TIMESTAMP(3),\" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&amp;nbsp;&#010;pt AS PROCTIME() \" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&#010;) WITH (\" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&amp;nbsp;&#010;'connector'='filesystem',\" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&amp;nbsp;&#010;'path'='D:\\\\Program Files\\\\JetBrains\\\\workspace\\\\table-walkthrough\\\\src\\\\main\\\\resources\\\\csv\\\\order.csv',\"&#010;+&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&amp;nbsp;&#010;'format'='csv'\" +&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; \"&#010;)\";&#013;&#010;&#013;&#010;&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; bsTableEnv.executeSql(sourceTableDDL);&#013;&#010;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; bsTableEnv.executeSql(\"select * from fs_table\").print();&#013;&#010;2. csv文件&#013;&#010;order.csv&#013;&#010;zhangsan,12.34,2020-08-03 12:23:50&#013;&#010;lisi,234.67,2020-08-03 12:25:50&#013;&#010;wangwu,57.6,2020-08-03 12:25:50&#013;&#010;zhaoliu,345,2020-08-03 12:28:50&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;3. 错误&#013;&#010;&amp;nbsp;- Source: FileSystemTableSource(user_id, order_amount, dt, pt) -&amp;gt; Calc(select=[user_id,&#010;order_amount, dt, PROCTIME_MATERIALIZE(()) AS pt]) -&amp;gt; SinkConversionToRow (4/6) (9ee0383d676a190b0a62d206039db26c)&#010;switched from RUNNING to FAILED.&#013;&#010;java.io.IOException: Failed to deserialize CSV row.&#013;&#010;&#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:299)&#013;&#010;&#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:210)&#013;&#010;&#009;at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:91)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#013;&#010;&#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#013;&#010;&#009;at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)&#013;&#010;Caused by: java.lang.RuntimeException: Row length mismatch. 4 fields expected but was 3.&#013;&#010;&#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.validateArity(CsvRowDataDeserializationSchema.java:441)&#013;&#010;&#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.lambda$createRowConverter$1ca9c073$1(CsvRowDataDeserializationSchema.java:244)&#013;&#010;&#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:293)&#013;&#010;&#009;... 5 more",
        "depth": "1",
        "reply": "<tencent_1FF36C91503C21140D95C1D8CF6FDB4D6306@qq.com>"
    },
    {
        "id": "<DC68EFBF-BD28-4050-9D6E-967D80AE6583@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 01:55:50 GMT",
        "subject": "Re: flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误",
        "content": "Hi, Asahi&#010;&#010;这是一个已知bug[1]，filesystem connector上处理计算列有点问题，已经有PR了，会在1.11.2和1.12版本上修复&#010;&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18665 &lt;https://issues.apache.org/jira/browse/FLINK-18665&gt;&#010;&#010;&gt; 在 2020年7月23日，00:07，Asahi Lee &lt;978466273@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 1. 程序&#010;&gt; StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv,&#010;bsSettings);&#010;&gt; &#010;&gt; &#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; String sourceTableDDL = \"CREATE TABLE fs_table&#010;(\" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\"&amp;nbsp; user_id STRING,\" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\"&amp;nbsp; order_amount DOUBLE,\" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\"&amp;nbsp; dt TIMESTAMP(3),\" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\"&amp;nbsp; pt AS PROCTIME() \" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\" ) WITH (\" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\"&amp;nbsp; 'connector'='filesystem',\" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\"&amp;nbsp; 'path'='D:\\\\Program Files\\\\JetBrains\\\\workspace\\\\table-walkthrough\\\\src\\\\main\\\\resources\\\\csv\\\\order.csv',\"&#010;+&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\"&amp;nbsp; 'format'='csv'\" +&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&#010;\" )\";&#010;&gt; &#010;&gt; &#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; bsTableEnv.executeSql(sourceTableDDL);&#010;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; bsTableEnv.executeSql(\"select * from fs_table\").print();&#010;&gt; 2. csv文件&#010;&gt; order.csv&#010;&gt; zhangsan,12.34,2020-08-03 12:23:50&#010;&gt; lisi,234.67,2020-08-03 12:25:50&#010;&gt; wangwu,57.6,2020-08-03 12:25:50&#010;&gt; zhaoliu,345,2020-08-03 12:28:50&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 3. 错误&#010;&gt; &amp;nbsp;- Source: FileSystemTableSource(user_id, order_amount, dt, pt) -&amp;gt; Calc(select=[user_id,&#010;order_amount, dt, PROCTIME_MATERIALIZE(()) AS pt]) -&amp;gt; SinkConversionToRow (4/6) (9ee0383d676a190b0a62d206039db26c)&#010;switched from RUNNING to FAILED.&#010;&gt; java.io.IOException: Failed to deserialize CSV row.&#010;&gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:299)&#010;&gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:210)&#010;&gt; &#009;at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:91)&#010;&gt; &#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#010;&gt; &#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#010;&gt; &#009;at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)&#010;&gt; Caused by: java.lang.RuntimeException: Row length mismatch. 4 fields expected but was&#010;3.&#010;&gt; &#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.validateArity(CsvRowDataDeserializationSchema.java:441)&#010;&gt; &#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.lambda$createRowConverter$1ca9c073$1(CsvRowDataDeserializationSchema.java:244)&#010;&gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:293)&#010;&gt; &#009;... 5 more&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<tencent_1FF36C91503C21140D95C1D8CF6FDB4D6306@qq.com>"
    },
    {
        "id": "<tencent_4CA58DAFF179B6414889909286CF40F10705@qq.com>",
        "from": "&quot;Asahi Lee&quot; &lt;978466...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 14:05:15 GMT",
        "subject": "回复： flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误",
        "content": "使用filesystem读取csv作为源，使用流环境，为什么我的程序一执行就停止，而不是等待文件的追加写入，继续计算呢？&#013;&#010;还是filesystem只能用于批操作？&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月23日(星期四) 上午9:55&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;Re: flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi, Asahi&#013;&#010;&#013;&#010;这是一个已知bug[1]，filesystem connector上处理计算列有点问题，已经有PR了，会在1.11.2和1.12版本上修复&#013;&#010;&#013;&#010;&#013;&#010;Best&#013;&#010;Leonard Xu&#013;&#010;[1] https://issues.apache.org/jira/browse/FLINK-18665 &lt;https://issues.apache.org/jira/browse/FLINK-18665&amp;gt;&#013;&#010;&#013;&#010;&amp;gt; 在 2020年7月23日，00:07，Asahi Lee &lt;978466273@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 1. 程序&#013;&#010;&amp;gt; StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; EnvironmentSettings bsSettings&#010;= EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; StreamTableEnvironment&#010;bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; String sourceTableDDL&#010;= \"CREATE TABLE fs_table (\" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; user_id STRING,\" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; order_amount DOUBLE,\" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; dt TIMESTAMP(3),\" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; pt AS PROCTIME() \" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \" ) WITH (\" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'connector'='filesystem',\" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'path'='D:\\\\Program Files\\\\JetBrains\\\\workspace\\\\table-walkthrough\\\\src\\\\main\\\\resources\\\\csv\\\\order.csv',\"&#010;+&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'format'='csv'\" +&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \" )\";&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; bsTableEnv.executeSql(sourceTableDDL);&#013;&#010;&amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; bsTableEnv.executeSql(\"select&#010;* from fs_table\").print();&#013;&#010;&amp;gt; 2. csv文件&#013;&#010;&amp;gt; order.csv&#013;&#010;&amp;gt; zhangsan,12.34,2020-08-03 12:23:50&#013;&#010;&amp;gt; lisi,234.67,2020-08-03 12:25:50&#013;&#010;&amp;gt; wangwu,57.6,2020-08-03 12:25:50&#013;&#010;&amp;gt; zhaoliu,345,2020-08-03 12:28:50&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 3. 错误&#013;&#010;&amp;gt; &amp;amp;nbsp;- Source: FileSystemTableSource(user_id, order_amount, dt, pt) -&amp;amp;gt;&#010;Calc(select=[user_id, order_amount, dt, PROCTIME_MATERIALIZE(()) AS pt]) -&amp;amp;gt; SinkConversionToRow&#010;(4/6) (9ee0383d676a190b0a62d206039db26c) switched from RUNNING to FAILED.&#013;&#010;&amp;gt; java.io.IOException: Failed to deserialize CSV row.&#013;&#010;&amp;gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:299)&#013;&#010;&amp;gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:210)&#013;&#010;&amp;gt; &#009;at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:91)&#013;&#010;&amp;gt; &#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#013;&#010;&amp;gt; &#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#013;&#010;&amp;gt; &#009;at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)&#013;&#010;&amp;gt; Caused by: java.lang.RuntimeException: Row length mismatch. 4 fields expected but&#010;was 3.&#013;&#010;&amp;gt; &#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.validateArity(CsvRowDataDeserializationSchema.java:441)&#013;&#010;&amp;gt; &#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.lambda$createRowConverter$1ca9c073$1(CsvRowDataDeserializationSchema.java:244)&#013;&#010;&amp;gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:293)&#013;&#010;&amp;gt; &#009;... 5 more",
        "depth": "3",
        "reply": "<tencent_1FF36C91503C21140D95C1D8CF6FDB4D6306@qq.com>"
    },
    {
        "id": "<9C6EE85E-8C9D-458C-8D71-AEAC29081511@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 14:27:59 GMT",
        "subject": "Re: flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误",
        "content": "Hi,&#010;&#010;Filesystem connector 支持streaming 写入，streaming 读取 还未支持，所以读取完了就停止。支持streaming&#010;写入从文档上看[1]应该是有计划的&#010;&#010;&#010;[1]https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/connectors/filesystem.html&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/connectors/filesystem.html&gt;&#010;&#010;&#010;&gt; 在 2020年7月23日，22:05，Asahi Lee &lt;978466273@qq.com&gt; 写道：&#010;&gt; &#010;&gt; 使用filesystem读取csv作为源，使用流环境，为什么我的程序一执行就停止，而不是等待文件的追加写入，继续计算呢？&#010;&gt; 还是filesystem只能用于批操作？&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; 发件人:                                                                          &#010;                                             \"user-zh\"                                   &#010;                                                &lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&gt;&amp;gt;;&#010;&gt; 发送时间:&amp;nbsp;2020年7月23日(星期四) 上午9:55&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt;&amp;gt;;&#010;&gt; &#010;&gt; 主题:&amp;nbsp;Re: flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; Hi, Asahi&#010;&gt; &#010;&gt; 这是一个已知bug[1]，filesystem connector上处理计算列有点问题，已经有PR了，会在1.11.2和1.12版本上修复&#010;&gt; &#010;&gt; &#010;&gt; Best&#010;&gt; Leonard Xu&#010;&gt; [1] https://issues.apache.org/jira/browse/FLINK-18665 &lt;https://issues.apache.org/jira/browse/FLINK-18665&gt;&#010;&lt;https://issues.apache.org/jira/browse/FLINK-18665&amp;gt &lt;https://issues.apache.org/jira/browse/FLINK-18665&amp;gt&gt;;&#010;&gt; &#010;&gt; &amp;gt; 在 2020年7月23日，00:07，Asahi Lee &lt;978466273@qq.com &lt;mailto:978466273@qq.com&gt;&amp;gt;&#010;写道：&#010;&gt; &amp;gt; &#010;&gt; &amp;gt; 1. 程序&#010;&gt; &amp;gt; StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; EnvironmentSettings&#010;bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; StreamTableEnvironment&#010;bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt; &amp;gt; &#010;&gt; &amp;gt; &#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; String sourceTableDDL&#010;= \"CREATE TABLE fs_table (\" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; user_id STRING,\" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; order_amount DOUBLE,\" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; dt TIMESTAMP(3),\" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; pt AS PROCTIME() \" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \" ) WITH (\" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'connector'='filesystem',\" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'path'='D:\\\\Program Files\\\\JetBrains\\\\workspace\\\\table-walkthrough\\\\src\\\\main\\\\resources\\\\csv\\\\order.csv',\"&#010;+&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'format'='csv'\" +&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp; &amp;amp;nbsp; \" )\";&#010;&gt; &amp;gt; &#010;&gt; &amp;gt; &#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; bsTableEnv.executeSql(sourceTableDDL);&#010;&gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; bsTableEnv.executeSql(\"select&#010;* from fs_table\").print();&#010;&gt; &amp;gt; 2. csv文件&#010;&gt; &amp;gt; order.csv&#010;&gt; &amp;gt; zhangsan,12.34,2020-08-03 12:23:50&#010;&gt; &amp;gt; lisi,234.67,2020-08-03 12:25:50&#010;&gt; &amp;gt; wangwu,57.6,2020-08-03 12:25:50&#010;&gt; &amp;gt; zhaoliu,345,2020-08-03 12:28:50&#010;&gt; &amp;gt; &#010;&gt; &amp;gt; &#010;&gt; &amp;gt; &#010;&gt; &amp;gt; 3. 错误&#010;&gt; &amp;gt; &amp;amp;nbsp;- Source: FileSystemTableSource(user_id, order_amount, dt, pt)&#010;-&amp;amp;gt; Calc(select=[user_id, order_amount, dt, PROCTIME_MATERIALIZE(()) AS pt]) -&amp;amp;gt;&#010;SinkConversionToRow (4/6) (9ee0383d676a190b0a62d206039db26c) switched from RUNNING to FAILED.&#010;&gt; &amp;gt; java.io.IOException: Failed to deserialize CSV row.&#010;&gt; &amp;gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:299)&#010;&gt; &amp;gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:210)&#010;&gt; &amp;gt; &#009;at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:91)&#010;&gt; &amp;gt; &#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#010;&gt; &amp;gt; &#009;at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#010;&gt; &amp;gt; &#009;at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)&#010;&gt; &amp;gt; Caused by: java.lang.RuntimeException: Row length mismatch. 4 fields expected&#010;but was 3.&#010;&gt; &amp;gt; &#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.validateArity(CsvRowDataDeserializationSchema.java:441)&#010;&gt; &amp;gt; &#009;at org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.lambda$createRowConverter$1ca9c073$1(CsvRowDataDeserializationSchema.java:244)&#010;&gt; &amp;gt; &#009;at org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:293)&#010;&gt; &amp;gt; &#009;... 5 more&#010;&#010;&#010;",
        "depth": "4",
        "reply": "<tencent_1FF36C91503C21140D95C1D8CF6FDB4D6306@qq.com>"
    },
    {
        "id": "<CADQYLGt_8TgKbSjVaOUAF=iSRxp+QKi7YryO66S=16=WPHR9oQ@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 15:04:27 GMT",
        "subject": "Re: flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误",
        "content": "和hive结合下，filesystem是支持流式读取的，可以参考 [1]&#010;&#010;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/hive/hive_streaming.html#streaming-reading&#010;&#010;Leonard Xu &lt;xbjtdcq@gmail.com&gt; 于2020年7月23日周四 下午10:28写道：&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; Filesystem connector 支持streaming 写入，streaming 读取&#010;&gt; 还未支持，所以读取完了就停止。支持streaming 写入从文档上看[1]应该是有计划的&#010;&gt;&#010;&gt;&#010;&gt; [1]&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/connectors/filesystem.html&#010;&gt; &lt;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/connectors/filesystem.html&#010;&gt; &gt;&#010;&gt;&#010;&gt;&#010;&gt; &gt; 在 2020年7月23日，22:05，Asahi Lee &lt;978466273@qq.com&gt; 写道：&#010;&gt; &gt;&#010;&gt; &gt; 使用filesystem读取csv作为源，使用流环境，为什么我的程序一执行就停止，而不是等待文件的追加写入，继续计算呢？&#010;&gt; &gt; 还是filesystem只能用于批操作？&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt; &gt; 发件人:&#010;&gt;                                                     \"user-zh\"&#010;&gt;                                                                       &lt;&#010;&gt; xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&gt;&amp;gt;;&#010;&gt; &gt; 发送时间:&amp;nbsp;2020年7月23日(星期四) 上午9:55&#010;&gt; &gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org &lt;mailto:&#010;&gt; user-zh@flink.apache.org&gt;&amp;gt;;&#010;&gt; &gt;&#010;&gt; &gt; 主题:&amp;nbsp;Re: flink 1.11 ddl sql 添加PROCTIME()列，读取csv错误&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Hi, Asahi&#010;&gt; &gt;&#010;&gt; &gt; 这是一个已知bug[1]，filesystem connector上处理计算列有点问题，已经有PR了，会在1.11.2和1.12版本上修复&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; Best&#010;&gt; &gt; Leonard Xu&#010;&gt; &gt; [1] https://issues.apache.org/jira/browse/FLINK-18665 &lt;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18665&gt; &lt;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18665&amp;gt &lt;&#010;&gt; https://issues.apache.org/jira/browse/FLINK-18665&amp;gt&gt;;&#010;&gt; &gt;&#010;&gt; &gt; &amp;gt; 在 2020年7月23日，00:07，Asahi Lee &lt;978466273@qq.com &lt;mailto:&#010;&gt; 978466273@qq.com&gt;&amp;gt; 写道：&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt; 1. 程序&#010;&gt; &gt; &amp;gt; StreamExecutionEnvironment bsEnv =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; EnvironmentSettings&#010;&gt; bsSettings =&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; StreamTableEnvironment&#010;&gt; bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; String sourceTableDDL&#010;=&#010;&gt; \"CREATE TABLE fs_table (\" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; user_id STRING,\" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; order_amount DOUBLE,\" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; dt TIMESTAMP(3),\" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; pt AS PROCTIME() \" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \" ) WITH (\" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'connector'='filesystem',\" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'path'='D:\\\\Program&#010;&gt; Files\\\\JetBrains\\\\workspace\\\\table-walkthrough\\\\src\\\\main\\\\resources\\\\csv\\\\order.csv',\"&#010;&gt; +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \"&amp;amp;nbsp; 'format'='csv'\" +&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&amp;amp;nbsp;&#010;&gt; &amp;amp;nbsp; &amp;amp;nbsp; \" )\";&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; bsTableEnv.executeSql(sourceTableDDL);&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp; &amp;amp;nbsp;&#010;&gt; bsTableEnv.executeSql(\"select * from fs_table\").print();&#010;&gt; &gt; &amp;gt; 2. csv文件&#010;&gt; &gt; &amp;gt; order.csv&#010;&gt; &gt; &amp;gt; zhangsan,12.34,2020-08-03 12:23:50&#010;&gt; &gt; &amp;gt; lisi,234.67,2020-08-03 12:25:50&#010;&gt; &gt; &amp;gt; wangwu,57.6,2020-08-03 12:25:50&#010;&gt; &gt; &amp;gt; zhaoliu,345,2020-08-03 12:28:50&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt;&#010;&gt; &gt; &amp;gt; 3. 错误&#010;&gt; &gt; &amp;gt; &amp;amp;nbsp;- Source: FileSystemTableSource(user_id, order_amount,&#010;&gt; dt, pt) -&amp;amp;gt; Calc(select=[user_id, order_amount, dt,&#010;&gt; PROCTIME_MATERIALIZE(()) AS pt]) -&amp;amp;gt; SinkConversionToRow (4/6)&#010;&gt; (9ee0383d676a190b0a62d206039db26c) switched from RUNNING to FAILED.&#010;&gt; &gt; &amp;gt; java.io.IOException: Failed to deserialize CSV row.&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:299)&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:210)&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:91)&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100)&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63)&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201)&#010;&gt; &gt; &amp;gt; Caused by: java.lang.RuntimeException: Row length mismatch. 4&#010;&gt; fields expected but was 3.&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.validateArity(CsvRowDataDeserializationSchema.java:441)&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.formats.csv.CsvRowDataDeserializationSchema.lambda$createRowConverter$1ca9c073$1(CsvRowDataDeserializationSchema.java:244)&#010;&gt; &gt; &amp;gt;  at&#010;&gt; org.apache.flink.formats.csv.CsvFileSystemFormatFactory$CsvInputFormat.nextRecord(CsvFileSystemFormatFactory.java:293)&#010;&gt; &gt; &amp;gt;  ... 5 more&#010;&gt;&#010;&gt;&#010;&#010;",
        "depth": "5",
        "reply": "<tencent_1FF36C91503C21140D95C1D8CF6FDB4D6306@qq.com>"
    },
    {
        "id": "<CAGR9zpZq7x3tukrJytAtNS5MtYk1xwXUMqh4vtVniDroJzqhhQ@mail.gmail.com>",
        "from": "Peihui He &lt;peihu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 00:55:45 GMT",
        "subject": "flink sink kafka Error while confirming checkpoint",
        "content": "Hello,&#010;&#010;flink 1.10.1&#010;kafka 2.12-1.1.0&#010;&#010;运行一段时间后会出现一下错误，不知道有遇到过没？&#010;java.lang.RuntimeException: Error while confirming checkpoint&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:935)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$7(StreamTask.java:907)&#010;        at&#010;org.apache.flink.util.function.FunctionUtils.lambda$asCallable$5(FunctionUtils.java:125)&#010;        at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.run(StreamTaskActionExecutor.java:87)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:78)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:261)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:186)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:485)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:469)&#010;        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)&#010;        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)&#010;        at java.lang.Thread.run(Thread.java:748)&#010;Caused by: org.apache.flink.util.FlinkRuntimeException: Committing one of&#010;transactions failed, logging first encountered failure&#010;        at&#010;org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:302)&#010;        at&#010;org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.notifyCheckpointComplete(AbstractUdfStreamOperator.java:130)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointComplete$8(StreamTask.java:919)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:101)&#010;        at&#010;org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:913)&#010;        ... 12 more&#010;Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer&#010;attempted an operation with an old epoch. Either there is a newer producer&#010;with the same transactionalId, or the producer's transaction has been&#010;expired by the broker.&#010;&#010;Best wishes.&#010;&#010;",
        "depth": "0",
        "reply": "<CAGR9zpZq7x3tukrJytAtNS5MtYk1xwXUMqh4vtVniDroJzqhhQ@mail.gmail.com>"
    },
    {
        "id": "<F431CE69-6F97-49FA-9B8D-22613CD5E35A@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 01:30:44 GMT",
        "subject": "Re: flinksql1.11中主键声明的问题",
        "content": "Hi,&#010;&#010;看了下query，你没有使用维表join语法 FOR SYSTEM_TIME AS OF ，这样直接做的regular&#010;join，mysql表是bounded的，第一次读完就不会再读了，所以不会更新。&#010;维表join才会按照你设置的时间去look up 最新的数据，维表是我们常说的temporal&#010;table(时态表)的一种，参考[1] 中的 temporal table join&#010;&#010;&#010;祝好&#010;Leonard Xu&#010;[1]  https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&gt;&#010;&#010;&gt; 在 2020年7月23日，09:06，琴师 &lt;1129656513@qq.com&gt; 写道：&#010;&gt; &#010;&gt; &#010;&gt; HI：&#010;&gt; 我是使用场景是这样的，首先开启计算脚本，开启流输入，此时的数据中维表数据如下，此时输出的计算结果如下&#010;&gt;&gt;&gt; 维表           &#010;&gt;&gt;&gt; id    type   &#010;&gt;&gt;&gt; 2&#009;err&#010;&gt;&gt;&gt; 1&#009;err&#010;&gt;&gt;&gt; 结果&#010;&gt;&gt;&gt; &#010;&gt; 1&#009;err&#009;20200723085754&#010;&gt; 2&#009;err&#009;20200723085755&#010;&gt; 3&#009;err&#009;20200723085756&#010;&gt; 4&#009;err&#009;20200723085757&#010;&gt; &#010;&gt; 然后我更新了数据库维表数据，新的数据库维表数据如下，此时输出的结果并没有随着维表的改变而改变，时间已经超过了缓存刷新时间2s：&#010;&gt;&gt;&gt; 维表           &#010;&gt;&gt;&gt; id    type   &#010;&gt;&gt;&gt; 2&#009;acc&#010;&gt;&gt;&gt; 1&#009;acc&#010;&gt;&gt;&gt; 结果&#010;&gt; &#010;&gt; 94&#009;err&#009;20200723084455&#010;&gt; 95&#009;err&#009;20200723084456&#010;&gt; 96&#009;err&#009;20200723084457&#010;&gt; 97&#009;err&#009;20200723084458&#010;&gt; 98&#009;err&#009;20200723084459&#010;&gt; 99&#009;err&#009;20200723084500&#010;&gt; 100&#009;err&#009;20200723084501&#010;&gt; &#010;&gt; 然后我断开了流输入，间隔时间大于缓存刷新时间，然后重新输入流，但是我的新输出结果仅仅是更新了与流有关的时间字段，与维表相关的字段仍没有得到更新。&#010;&gt; &#010;&gt; 请问我的使用过程哪里不对么，请帮我指出来，万分感谢！&#010;&gt; &#010;&gt; &#010;&gt; 谢谢！&#010;&gt; &#010;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt; &#010;&gt; &#010;&gt; ------------------ 原始邮件 ------------------&#010;&gt; 发件人: \"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt;;&#010;&gt; 发送时间: 2020年7月22日(星期三) 晚上9:39&#010;&gt; 收件人: \"琴师\"&lt;1129656513@qq.com&gt;;&#010;&gt; 主题: Re: flinksql1.11中主键声明的问题&#010;&gt; &#010;&gt; &lt;018A8242@6D818D22.7EE2185F.jpg&gt;&#010;&gt; &#010;&gt; 代码应该没问题的，我源码和本地都复现了下，你检查下你使用方式&#010;&gt; &#010;&gt; 祝好&#010;&gt; &#010;&gt;&gt; 在 2020年7月22日，16:39，Leonard Xu &lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&gt;&gt;&#010;写道：&#010;&gt;&gt; &#010;&gt;&gt; HI,&#010;&gt;&gt; 我看了维表这块的代码，应该没啥问题的，晚点我本地环境复现确认下哈。&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月22日，16:27，琴师 &lt;1129656513@qq.com &lt;mailto:1129656513@qq.com&gt;&gt;&#010;写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; HI&#010;&gt;&gt;&gt; 我附录了我的代码，现在基本上测通了流程，卡在维表刷新这里，不能刷新的话很受打击。HELP！！&#010;&gt;&gt;&gt; 谢谢&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; ------------------ 原始邮件 ------------------&#010;&gt;&gt;&gt; 发件人: \"琴师\" &lt;1129656513@qq.com &lt;mailto:1129656513@qq.com&gt;&gt;;&#010;&gt;&gt;&gt; 发送时间: 2020年7月22日(星期三) 下午3:17&#010;&gt;&gt;&gt; 收件人: \"user-zh\"&lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt;&gt;;&#010;&gt;&gt;&gt; 主题: 回复: Re: flinksql1.11中主键声明的问题&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 你好：&#010;&gt;&gt;&gt; 下面是我的代码，我用的版本是1.11.0，数据库是TIDB，我跑的是demo数据，维表只有两行。&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 我的输入流如下，每秒新增一条写入到kafka&#010;&gt;&gt;&gt;  topic = 'tp1'&#010;&gt;&gt;&gt;     for i  in  range(1,10000) :&#010;&gt;&gt;&gt;         stime=datetime.datetime.now().strftime('%Y%m%d%H%M%S')&#010;&gt;&gt;&gt;         msg = {}&#010;&gt;&gt;&gt;         msg['id']= i&#010;&gt;&gt;&gt;         msg['time1']= stime&#010;&gt;&gt;&gt;         msg['type']=1&#010;&gt;&gt;&gt;         print(msg)&#010;&gt;&gt;&gt;         send_msg(topic, msg)&#010;&gt;&gt;&gt;         time.sleep(1)&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; {'id': 1, 'time1': '20200722140624', 'type': 1}&#010;&gt;&gt;&gt; {'id': 2, 'time1': '20200722140625', 'type': 1}&#010;&gt;&gt;&gt; {'id': 3, 'time1': '20200722140626', 'type': 1}&#010;&gt;&gt;&gt; {'id': 4, 'time1': '20200722140627', 'type': 1}&#010;&gt;&gt;&gt; {'id': 5, 'time1': '20200722140628', 'type': 1}&#010;&gt;&gt;&gt; {'id': 6, 'time1': '20200722140629', 'type': 1}&#010;&gt;&gt;&gt; {'id': 7, 'time1': '20200722140631', 'type': 1}&#010;&gt;&gt;&gt; {'id': 8, 'time1': '20200722140632', 'type': 1}&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 维表数据如下&#010;&gt;&gt;&gt; id    type&#010;&gt;&gt;&gt; 2&#009;err&#010;&gt;&gt;&gt; 1&#009;err&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 我在程序正常期间更新了维表，但是后续输出的结果显示维表还是之前的缓存数据，事实上已经远远大于超时时间了，甚至我停下输入流，直到达到超时时间后再次输入，新的结果还是输出旧的维表数据&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#010;&gt;&gt;&gt; from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings,DataTypes,&#010;CsvTableSource, CsvTableSink&#010;&gt;&gt;&gt; from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#010;&gt;&gt;&gt; from pyflink.table.window import Tumble&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; def from_kafka_to_kafka_demo():&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;     # use blink table planner&#010;&gt;&gt;&gt;     env = StreamExecutionEnvironment.get_execution_environment()&#010;&gt;&gt;&gt;     env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#010;&gt;&gt;&gt;     env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#010;&gt;&gt;&gt;     st_env = StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;     # register source and sink&#010;&gt;&gt;&gt;     register_rides_source(st_env)&#010;&gt;&gt;&gt;     register_rides_sink(st_env)&#010;&gt;&gt;&gt;     register_mysql_source(st_env)&#010;&gt;&gt;&gt;  &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;     st_env.sql_update(\"insert into flink_result select  cast(t1.id &lt;http://t1.id/&gt;&#010;as int) as id,cast(t2.type as varchar),cast( t1.time1 as bigint) as rowtime from source1 t1&#010;left join dim_mysql t2 on t1.type=cast(t2.id &lt;http://t2.id/&gt; as varchar) \")&#010;&gt;&gt;&gt;     st_env.execute(\"2-from_kafka_to_kafka\")&#010;&gt;&gt;&gt;    &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; def register_rides_source(st_env):&#010;&gt;&gt;&gt;     source_ddl = \\&#010;&gt;&gt;&gt;     \"\"\"&#010;&gt;&gt;&gt;     create table source1(&#010;&gt;&gt;&gt;      id int,&#010;&gt;&gt;&gt;      time1 varchar ,&#010;&gt;&gt;&gt;      type string&#010;&gt;&gt;&gt;      ) with (&#010;&gt;&gt;&gt;     'connector' = 'kafka',&#010;&gt;&gt;&gt;     'topic' = 'tp1',&#010;&gt;&gt;&gt;     'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt;&gt;     'properties.bootstrap.servers' = 'localhost:9092',&#010;&gt;&gt;&gt;     'format' = 'json'&#010;&gt;&gt;&gt;      )&#010;&gt;&gt;&gt;     \"\"\"&#010;&gt;&gt;&gt;     st_env.sql_update(source_ddl)&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; def register_mysql_source(st_env):&#010;&gt;&gt;&gt;     source_ddl = \\&#010;&gt;&gt;&gt;     \"\"\"&#010;&gt;&gt;&gt;     CREATE TABLE dim_mysql (&#010;&gt;&gt;&gt;     id int,  --&#010;&gt;&gt;&gt;     type varchar --&#010;&gt;&gt;&gt;     ) WITH (&#010;&gt;&gt;&gt;     'connector' = 'jdbc',&#010;&gt;&gt;&gt;     'url' = 'jdbc:mysql://localhost:3390/test' &lt;&gt;,&#010;&gt;&gt;&gt;     'table-name' = 'flink_test',&#010;&gt;&gt;&gt;     'driver' = 'com.mysql.cj.jdbc.Driver',&#010;&gt;&gt;&gt;     'username' = '***',&#010;&gt;&gt;&gt;     'password' = '***',&#010;&gt;&gt;&gt;     'lookup.cache.max-rows' = '5000',&#010;&gt;&gt;&gt;     'lookup.cache.ttl' = '1s',&#010;&gt;&gt;&gt;     'lookup.max-retries' = '3'&#010;&gt;&gt;&gt;     )&#010;&gt;&gt;&gt;     \"\"\"    &#010;&gt;&gt;&gt;     st_env.sql_update(source_ddl)&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; def register_rides_sink(st_env):&#010;&gt;&gt;&gt;     sink_ddl = \\&#010;&gt;&gt;&gt;     \"\"\"&#010;&gt;&gt;&gt;     CREATE TABLE flink_result (&#010;&gt;&gt;&gt;     id int,  &#010;&gt;&gt;&gt;     type varchar,&#010;&gt;&gt;&gt;     rtime bigint,&#010;&gt;&gt;&gt;     primary key(id)  NOT ENFORCED&#010;&gt;&gt;&gt;     ) WITH (&#010;&gt;&gt;&gt;     'connector' = 'jdbc',&#010;&gt;&gt;&gt;     'url' = 'jdbc:mysql://localhost:3390/test' &lt;&gt;,&#010;&gt;&gt;&gt;     'table-name' = 'flink_result',&#010;&gt;&gt;&gt;     'driver' = 'com.mysql.cj.jdbc.Driver',&#010;&gt;&gt;&gt;     'username' = '***',&#010;&gt;&gt;&gt;     'password' = '***',&#010;&gt;&gt;&gt;     'sink.buffer-flush.max-rows' = '5000',&#010;&gt;&gt;&gt;     'sink.buffer-flush.interval' = '2s',&#010;&gt;&gt;&gt;     'sink.max-retries' = '3'&#010;&gt;&gt;&gt;     )&#010;&gt;&gt;&gt;     \"\"\"&#010;&gt;&gt;&gt;     st_env.sql_update(sink_ddl)&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; if __name__ == '__main__':&#010;&gt;&gt;&gt;     from_kafka_to_kafka_demo()&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 初学者&#010;&gt;&gt;&gt; PyFlink爱好者&#010;&gt;&gt;&gt; 琴师&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;  &#010;&gt;&gt;&gt; 发件人： Leonard Xu &lt;mailto:xbjtdcq@gmail.com&gt;&#010;&gt;&gt;&gt; 发送时间： 2020-07-22 15:05&#010;&gt;&gt;&gt; 收件人： user-zh &lt;mailto:user-zh@flink.apache.org&gt;&#010;&gt;&gt;&gt; 主题： Re: flinksql1.11中主键声明的问题&#010;&gt;&gt;&gt; Hi,&#010;&gt;&gt;&gt;  &#010;&gt;&gt;&gt;   我试了下应该是会更新缓存的，你有能复现的例子吗？&#010;&gt;&gt;&gt;  &#010;&gt;&gt;&gt; 祝好&#010;&gt;&gt;&gt; &gt; 在 2020年7月22日，14:50，奇怪的不朽琴师 &lt;1129656513@qq.com&#010;&lt;mailto:1129656513@qq.com&gt;&gt; 写道：&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 你好：&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 可能是我描述的不清楚，我了解这个机制，我的意思维表更新后，即便已经达到了超时的时间，新的输出结果还是用维表历史缓存数据，&#010;&gt;&gt;&gt; &gt; 我感觉上是维表没有刷新缓存，但是我不知道这为什么。&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 谢谢&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#010;&gt;&gt;&gt; &gt; 发件人:                                                             &#010;                                                          \"user-zh\"                      &#010;                                                             &lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&gt;&amp;gt;;&#010;&gt;&gt;&gt; &gt; 发送时间:&amp;nbsp;2020年7月22日(星期三) 下午2:42&#010;&gt;&gt;&gt; &gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt;&amp;gt;;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 主题:&amp;nbsp;Re: flinksql1.11中主键声明的问题&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; Hello&#010;&gt;&gt;&gt; &gt; 你说的输出结果更新，是指之前关联的维表时老数据，过了一段时间，这个数据变，之前输出的历史也希望更新吗？维表join的实现，只有事实表中才会有retract消息才会更新，才会传递到下游，维表的数据是事实表&#010;&gt;&gt;&gt; &gt; 去look up该表时的数据，维表的更新是不会retract之前的历史记录的。&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; 祝好&#010;&gt;&gt;&gt; &gt; Leonard Xu&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt;&#010;&gt;&gt;&gt; &gt; &amp;gt; 在 2020年7月22日，14:13，1129656513@qq.com &lt;http://qq.com/&gt;&#010;写道：&#010;&gt;&gt;&gt; &gt; &amp;gt;&#010;&gt;&gt;&gt; &gt; &amp;gt; 输出结果仍然没有被更新&#010;&gt;&gt; &#010;&gt; &#010;&#010;&#010;",
        "depth": "1",
        "reply": "<F431CE69-6F97-49FA-9B8D-22613CD5E35A@gmail.com>"
    },
    {
        "id": "<tencent_226152F0EC0ADC5C046A4C3CA1D9AE44BB05@qq.com>",
        "from": "&quot;琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 01:35:28 GMT",
        "subject": "回复： flinksql1.11中主键声明的问题",
        "content": "好的感谢，果然还是理解上有些问题！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月23日(星期四) 上午9:30&#013;&#010;收件人:&amp;nbsp;\"琴师\"&lt;1129656513@qq.com&amp;gt;;&#013;&#010;抄送:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;主题:&amp;nbsp;Re: flinksql1.11中主键声明的问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi,&#013;&#010;&#013;&#010;看了下query，你没有使用维表join语法 FOR SYSTEM_TIME AS OF ，这样直接做的regular&#010;join，mysql表是bounded的，第一次读完就不会再读了，所以不会更新。&#013;&#010;维表join才会按照你设置的时间去look up 最新的数据，维表是我们常说的temporal&#010;table(时态表)的一种，参考[1] 中的 temporal table join&#013;&#010;&#013;&#010;&#013;&#010;祝好&#013;&#010;Leonard Xu&#013;&#010;[1]&amp;nbsp; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&amp;gt;&#013;&#010;&#013;&#010;&amp;gt; 在 2020年7月23日，09:06，琴师 &lt;1129656513@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; HI：&#013;&#010;&amp;gt; 我是使用场景是这样的，首先开启计算脚本，开启流输入，此时的数据中维表数据如下，此时输出的计算结果如下&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 维表&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; id&amp;nbsp;&amp;nbsp;&amp;nbsp; type&amp;nbsp;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 2&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 1&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 结果&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt; 1&#009;err&#009;20200723085754&#013;&#010;&amp;gt; 2&#009;err&#009;20200723085755&#013;&#010;&amp;gt; 3&#009;err&#009;20200723085756&#013;&#010;&amp;gt; 4&#009;err&#009;20200723085757&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 然后我更新了数据库维表数据，新的数据库维表数据如下，此时输出的结果并没有随着维表的改变而改变，时间已经超过了缓存刷新时间2s：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 维表&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; id&amp;nbsp;&amp;nbsp;&amp;nbsp; type&amp;nbsp;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 2&#009;acc&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 1&#009;acc&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 结果&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 94&#009;err&#009;20200723084455&#013;&#010;&amp;gt; 95&#009;err&#009;20200723084456&#013;&#010;&amp;gt; 96&#009;err&#009;20200723084457&#013;&#010;&amp;gt; 97&#009;err&#009;20200723084458&#013;&#010;&amp;gt; 98&#009;err&#009;20200723084459&#013;&#010;&amp;gt; 99&#009;err&#009;20200723084500&#013;&#010;&amp;gt; 100&#009;err&#009;20200723084501&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 然后我断开了流输入，间隔时间大于缓存刷新时间，然后重新输入流，但是我的新输出结果仅仅是更新了与流有关的时间字段，与维表相关的字段仍没有得到更新。&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 请问我的使用过程哪里不对么，请帮我指出来，万分感谢！&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 谢谢！&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt; 发件人: \"Leonard Xu\" &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;&amp;gt; 发送时间: 2020年7月22日(星期三) 晚上9:39&#013;&#010;&amp;gt; 收件人: \"琴师\"&lt;1129656513@qq.com&amp;gt;;&#013;&#010;&amp;gt; 主题: Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &lt;018A8242@6D818D22.7EE2185F.jpg&amp;gt;&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 代码应该没问题的，我源码和本地都复现了下，你检查下你使用方式&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 祝好&#013;&#010;&amp;gt; &#013;&#010;&amp;gt;&amp;gt; 在 2020年7月22日，16:39，Leonard Xu &lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&amp;gt;&amp;gt;&#010;写道：&#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt; HI,&#013;&#010;&amp;gt;&amp;gt; 我看了维表这块的代码，应该没啥问题的，晚点我本地环境复现确认下哈。&#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 在 2020年7月22日，16:27，琴师 &lt;1129656513@qq.com &lt;mailto:1129656513@qq.com&amp;gt;&amp;gt;&#010;写道：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; HI&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 我附录了我的代码，现在基本上测通了流程，卡在维表刷新这里，不能刷新的话很受打击。HELP！！&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 谢谢&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发件人: \"琴师\" &lt;1129656513@qq.com &lt;mailto:1129656513@qq.com&amp;gt;&amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发送时间: 2020年7月22日(星期三) 下午3:17&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 收件人: \"user-zh\"&lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&amp;gt;&amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 主题: 回复: Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 你好：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 下面是我的代码，我用的版本是1.11.0，数据库是TIDB，我跑的是demo数据，维表只有两行。&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 我的输入流如下，每秒新增一条写入到kafka&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; topic = 'tp1'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; for i&amp;nbsp; in&amp;nbsp;&#010;range(1,10000) :&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;stime=datetime.datetime.now().strftime('%Y%m%d%H%M%S')&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;msg = {}&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;msg['id']= i&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;msg['time1']= stime&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;msg['type']=1&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;print(msg)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;send_msg(topic, msg)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;time.sleep(1)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 1, 'time1': '20200722140624', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 2, 'time1': '20200722140625', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 3, 'time1': '20200722140626', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 4, 'time1': '20200722140627', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 5, 'time1': '20200722140628', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 6, 'time1': '20200722140629', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 7, 'time1': '20200722140631', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 8, 'time1': '20200722140632', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 维表数据如下&#013;&#010;&amp;gt;&amp;gt;&amp;gt; id&amp;nbsp;&amp;nbsp;&amp;nbsp; type&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 2&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 1&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 我在程序正常期间更新了维表，但是后续输出的结果显示维表还是之前的缓存数据，事实上已经远远大于超时时间了，甚至我停下输入流，直到达到超时时间后再次输入，新的结果还是输出旧的维表数据&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings,DataTypes,&#010;CsvTableSource, CsvTableSink&#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.table.window import Tumble&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; # use blink table planner&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; st_env = StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; # register source and sink&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; register_mysql_source(st_env)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; st_env.sql_update(\"insert&#010;into flink_result select&amp;nbsp; cast(t1.id &lt;http://t1.id/&amp;gt; as int) as id,cast(t2.type&#010;as varchar),cast( t1.time1 as bigint) as rowtime from source1 t1 left join dim_mysql t2 on&#010;t1.type=cast(t2.id &lt;http://t2.id/&amp;gt; as varchar) \")&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; create table source1(&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; id int,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; time1 varchar ,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; type string&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ) with (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'connector' = 'kafka',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'topic' = 'tp1',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'scan.startup.mode' = 'latest-offset',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'properties.bootstrap.servers'&#010;= 'localhost:9092',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'format' = 'json'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; )&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def register_mysql_source(st_env):&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; CREATE TABLE dim_mysql (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; id int,&amp;nbsp; --&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; type varchar --&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ) WITH (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'url' = 'jdbc:mysql://localhost:3390/test'&#010;&lt;&amp;gt;,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'table-name' = 'flink_test',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'username' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'password' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'lookup.cache.max-rows' =&#010;'5000',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'lookup.cache.ttl' = '1s',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'lookup.max-retries' = '3'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; )&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; sink_ddl = \\&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; CREATE TABLE flink_result&#010;(&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; id int,&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; type varchar,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; rtime bigint,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; primary key(id)&amp;nbsp;&#010;NOT ENFORCED&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ) WITH (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'url' = 'jdbc:mysql://localhost:3390/test'&#010;&lt;&amp;gt;,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'table-name' = 'flink_result',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'username' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'password' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'sink.buffer-flush.max-rows'&#010;= '5000',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'sink.buffer-flush.interval'&#010;= '2s',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 'sink.max-retries' = '3'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; )&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 初学者&#013;&#010;&amp;gt;&amp;gt;&amp;gt; PyFlink爱好者&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 琴师&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发件人： Leonard Xu &lt;mailto:xbjtdcq@gmail.com&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发送时间： 2020-07-22 15:05&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 收件人： user-zh &lt;mailto:user-zh@flink.apache.org&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 主题： Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt;&amp;gt;&amp;gt; Hi,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; 我试了下应该是会更新缓存的，你有能复现的例子吗？&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 祝好&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 在 2020年7月22日，14:50，奇怪的不朽琴师 &lt;1129656513@qq.com&#010;&lt;mailto:1129656513@qq.com&amp;gt;&amp;gt; 写道：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 你好：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 可能是我描述的不清楚，我了解这个机制，我的意思维表更新后，即便已经达到了超时的时间，新的输出结果还是用维表历史缓存数据，&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 我感觉上是维表没有刷新缓存，但是我不知道这为什么。&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 谢谢&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 发件人:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&amp;gt;&amp;amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月22日(星期三) 下午2:42&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#010;&lt;mailto:user-zh@flink.apache.org&amp;gt;&amp;amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 主题:&amp;amp;nbsp;Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; Hello&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 你说的输出结果更新，是指之前关联的维表时老数据，过了一段时间，这个数据变，之前输出的历史也希望更新吗？维表join的实现，只有事实表中才会有retract消息才会更新，才会传递到下游，维表的数据是事实表&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 去look up该表时的数据，维表的更新是不会retract之前的历史记录的。&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 祝好&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; Leonard Xu&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; &amp;amp;gt; 在 2020年7月22日，14:13，1129656513@qq.com&#010;&lt;http://qq.com/&amp;gt; 写道：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; &amp;amp;gt; 输出结果仍然没有被更新&#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;",
        "depth": "2",
        "reply": "<F431CE69-6F97-49FA-9B8D-22613CD5E35A@gmail.com>"
    },
    {
        "id": "<tencent_62B34FE29E09801B8B388D812784D8896B0A@qq.com>",
        "from": "&quot;琴师&quot; &lt;1129656...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 02:57:24 GMT",
        "subject": "回复： flinksql1.11中主键声明的问题",
        "content": "HI：&#013;&#010;&#013;&#010;&#013;&#010;听取了大佬的建议，已实现所需功能，非常感谢！&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"琴师\"                                         &#010;                                          &lt;1129656513@qq.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月23日(星期四) 上午9:35&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;抄送:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;主题:&amp;nbsp;回复： flinksql1.11中主键声明的问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;好的感谢，果然还是理解上有些问题！&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月23日(星期四) 上午9:30&#013;&#010;收件人:&amp;nbsp;\"琴师\"&lt;1129656513@qq.com&amp;gt;;&#013;&#010;抄送:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;主题:&amp;nbsp;Re: flinksql1.11中主键声明的问题&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Hi,&#013;&#010;&#013;&#010;看了下query，你没有使用维表join语法 FOR SYSTEM_TIME AS OF ，这样直接做的regular&#010;join，mysql表是bounded的，第一次读完就不会再读了，所以不会更新。&#013;&#010;维表join才会按照你设置的时间去look up 最新的数据，维表是我们常说的temporal&#010;table(时态表)的一种，参考[1] 中的 temporal table join&#013;&#010;&#013;&#010;&#013;&#010;祝好&#013;&#010;Leonard Xu&#013;&#010;[1]&amp;nbsp; https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&amp;gt;&#013;&#010;&#013;&#010;&amp;gt; 在 2020年7月23日，09:06，琴师 &lt;1129656513@qq.com&amp;gt; 写道：&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; HI：&#013;&#010;&amp;gt; 我是使用场景是这样的，首先开启计算脚本，开启流输入，此时的数据中维表数据如下，此时输出的计算结果如下&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 维表&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; id&amp;nbsp; &amp;nbsp; type &amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 2&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 1&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 结果&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt; 1&#009;err&#009;20200723085754&#013;&#010;&amp;gt; 2&#009;err&#009;20200723085755&#013;&#010;&amp;gt; 3&#009;err&#009;20200723085756&#013;&#010;&amp;gt; 4&#009;err&#009;20200723085757&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 然后我更新了数据库维表数据，新的数据库维表数据如下，此时输出的结果并没有随着维表的改变而改变，时间已经超过了缓存刷新时间2s：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 维表&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; id&amp;nbsp; &amp;nbsp; type &amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 2&#009;acc&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 1&#009;acc&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 结果&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 94&#009;err&#009;20200723084455&#013;&#010;&amp;gt; 95&#009;err&#009;20200723084456&#013;&#010;&amp;gt; 96&#009;err&#009;20200723084457&#013;&#010;&amp;gt; 97&#009;err&#009;20200723084458&#013;&#010;&amp;gt; 98&#009;err&#009;20200723084459&#013;&#010;&amp;gt; 99&#009;err&#009;20200723084500&#013;&#010;&amp;gt; 100&#009;err&#009;20200723084501&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 然后我断开了流输入，间隔时间大于缓存刷新时间，然后重新输入流，但是我的新输出结果仅仅是更新了与流有关的时间字段，与维表相关的字段仍没有得到更新。&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 请问我的使用过程哪里不对么，请帮我指出来，万分感谢！&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 谢谢！&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &#013;&#010;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt; 发件人: \"Leonard Xu\" &lt;xbjtdcq@gmail.com&amp;gt;;&#013;&#010;&amp;gt; 发送时间: 2020年7月22日(星期三) 晚上9:39&#013;&#010;&amp;gt; 收件人: \"琴师\"&lt;1129656513@qq.com&amp;gt;;&#013;&#010;&amp;gt; 主题: Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; &lt;018A8242@6D818D22.7EE2185F.jpg&amp;gt;&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 代码应该没问题的，我源码和本地都复现了下，你检查下你使用方式&#013;&#010;&amp;gt; &#013;&#010;&amp;gt; 祝好&#013;&#010;&amp;gt; &#013;&#010;&amp;gt;&amp;gt; 在 2020年7月22日，16:39，Leonard Xu &lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&amp;gt;&amp;gt;&#010;写道：&#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt; HI,&#013;&#010;&amp;gt;&amp;gt; 我看了维表这块的代码，应该没啥问题的，晚点我本地环境复现确认下哈。&#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 在 2020年7月22日，16:27，琴师 &lt;1129656513@qq.com &lt;mailto:1129656513@qq.com&amp;gt;&amp;gt;&#010;写道：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; HI&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 我附录了我的代码，现在基本上测通了流程，卡在维表刷新这里，不能刷新的话很受打击。HELP！！&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 谢谢&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; ------------------ 原始邮件 ------------------&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发件人: \"琴师\" &lt;1129656513@qq.com &lt;mailto:1129656513@qq.com&amp;gt;&amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发送时间: 2020年7月22日(星期三) 下午3:17&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 收件人: \"user-zh\"&lt;user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&amp;gt;&amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 主题: 回复: Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 你好：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 下面是我的代码，我用的版本是1.11.0，数据库是TIDB，我跑的是demo数据，维表只有两行。&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 我的输入流如下，每秒新增一条写入到kafka&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; topic = 'tp1'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; for i&amp;nbsp; in&amp;nbsp; range(1,10000)&#010;:&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;stime=datetime.datetime.now().strftime('%Y%m%d%H%M%S')&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;msg = {}&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;msg['id']= i&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;msg['time1']= stime&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;msg['type']=1&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;print(msg)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;send_msg(topic, msg)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp;&#010;time.sleep(1)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 1, 'time1': '20200722140624', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 2, 'time1': '20200722140625', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 3, 'time1': '20200722140626', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 4, 'time1': '20200722140627', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 5, 'time1': '20200722140628', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 6, 'time1': '20200722140629', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 7, 'time1': '20200722140631', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; {'id': 8, 'time1': '20200722140632', 'type': 1}&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 维表数据如下&#013;&#010;&amp;gt;&amp;gt;&amp;gt; id&amp;nbsp; &amp;nbsp; type&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 2&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 1&#009;err&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 我在程序正常期间更新了维表，但是后续输出的结果显示维表还是之前的缓存数据，事实上已经远远大于超时时间了，甚至我停下输入流，直到达到超时时间后再次输入，新的结果还是输出旧的维表数据&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic&#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.table import StreamTableEnvironment, DataTypes, EnvironmentSettings,DataTypes,&#010;CsvTableSource, CsvTableSink&#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.table.descriptors import Schema, Kafka, Json, Rowtime&#013;&#010;&amp;gt;&amp;gt;&amp;gt; from pyflink.table.window import Tumble&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def from_kafka_to_kafka_demo():&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; # use blink table planner&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; env = StreamExecutionEnvironment.get_execution_environment()&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; env.set_stream_time_characteristic(TimeCharacteristic.EventTime)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; env_settings = EnvironmentSettings.Builder().use_blink_planner().build()&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; st_env = StreamTableEnvironment.create(stream_execution_environment=env,environment_settings=env_settings)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; # register source and sink&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; register_rides_source(st_env)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; register_rides_sink(st_env)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; register_mysql_source(st_env)&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; st_env.sql_update(\"insert into flink_result&#010;select&amp;nbsp; cast(t1.id &lt;http://t1.id/&amp;gt; as int) as id,cast(t2.type as varchar),cast(&#010;t1.time1 as bigint) as rowtime from source1 t1 left join dim_mysql t2 on t1.type=cast(t2.id&#010;&lt;http://t2.id/&amp;gt; as varchar) \")&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; st_env.execute(\"2-from_kafka_to_kafka\")&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def register_rides_source(st_env):&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; create table source1(&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; id int,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; time1 varchar ,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; type string&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) with (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector' = 'kafka',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'topic' = 'tp1',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'scan.startup.mode' = 'latest-offset',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'properties.bootstrap.servers' = 'localhost:9092',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'format' = 'json'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def register_mysql_source(st_env):&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; source_ddl = \\&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE dim_mysql (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; id int,&amp;nbsp; --&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; type varchar --&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'url' = 'jdbc:mysql://localhost:3390/test'&#010;&lt;&amp;gt;,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'table-name' = 'flink_test',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'username' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'password' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'lookup.cache.max-rows' = '5000',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'lookup.cache.ttl' = '1s',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'lookup.max-retries' = '3'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&amp;nbsp; &amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; st_env.sql_update(source_ddl)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; def register_rides_sink(st_env):&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; sink_ddl = \\&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; CREATE TABLE flink_result (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; id int,&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; type varchar,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; rtime bigint,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; primary key(id)&amp;nbsp; NOT ENFORCED&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; ) WITH (&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'connector' = 'jdbc',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'url' = 'jdbc:mysql://localhost:3390/test'&#010;&lt;&amp;gt;,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'table-name' = 'flink_result',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'driver' = 'com.mysql.cj.jdbc.Driver',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'username' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'password' = '***',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'sink.buffer-flush.max-rows' = '5000',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'sink.buffer-flush.interval' = '2s',&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; 'sink.max-retries' = '3'&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; )&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; \"\"\"&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; st_env.sql_update(sink_ddl)&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; if __name__ == '__main__':&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp;&amp;nbsp; &amp;nbsp; from_kafka_to_kafka_demo()&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 初学者&#013;&#010;&amp;gt;&amp;gt;&amp;gt; PyFlink爱好者&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 琴师&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发件人： Leonard Xu &lt;mailto:xbjtdcq@gmail.com&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 发送时间： 2020-07-22 15:05&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 收件人： user-zh &lt;mailto:user-zh@flink.apache.org&amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; 主题： Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt;&amp;gt;&amp;gt; Hi,&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;nbsp; 我试了下应该是会更新缓存的，你有能复现的例子吗？&#013;&#010;&amp;gt;&amp;gt;&amp;gt;&amp;nbsp; &#013;&#010;&amp;gt;&amp;gt;&amp;gt; 祝好&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 在 2020年7月22日，14:50，奇怪的不朽琴师 &lt;1129656513@qq.com&#010;&lt;mailto:1129656513@qq.com&amp;gt;&amp;gt; 写道：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 你好：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 可能是我描述的不清楚，我了解这个机制，我的意思维表更新后，即便已经达到了超时的时间，新的输出结果还是用维表历史缓存数据，&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 我感觉上是维表没有刷新缓存，但是我不知道这为什么。&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 谢谢&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 发件人:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&amp;nbsp; \"user-zh\"&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&amp;nbsp; &lt;xbjtdcq@gmail.com &lt;mailto:xbjtdcq@gmail.com&amp;gt;&amp;amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 发送时间:&amp;amp;nbsp;2020年7月22日(星期三) 下午2:42&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&#010;&lt;mailto:user-zh@flink.apache.org&amp;gt;&amp;amp;gt;;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 主题:&amp;amp;nbsp;Re: flinksql1.11中主键声明的问题&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; Hello&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 你说的输出结果更新，是指之前关联的维表时老数据，过了一段时间，这个数据变，之前输出的历史也希望更新吗？维表join的实现，只有事实表中才会有retract消息才会更新，才会传递到下游，维表的数据是事实表&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 去look up该表时的数据，维表的更新是不会retract之前的历史记录的。&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; 祝好&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; Leonard Xu&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; &amp;amp;gt; 在 2020年7月22日，14:13，1129656513@qq.com&#010;&lt;http://qq.com/&amp;gt; 写道：&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; &amp;amp;gt;&#013;&#010;&amp;gt;&amp;gt;&amp;gt; &amp;gt; &amp;amp;gt; 输出结果仍然没有被更新&#013;&#010;&amp;gt;&amp;gt; &#013;&#010;&amp;gt;",
        "depth": "3",
        "reply": "<F431CE69-6F97-49FA-9B8D-22613CD5E35A@gmail.com>"
    },
    {
        "id": "<tencent_AE77031A80FBBEC7CBA0B22C@qq.com>",
        "from": "&quot;xiao cai&quot;&lt;flin...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 06:17:36 GMT",
        "subject": "Re: flink row 类型",
        "content": "可以考虑把字段索引值保存下来再获取&#010;&#010;&#010; 原始邮件 &#010;发件人: Dream-底限&lt;zhangyu@akulaku.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月23日(周四) 14:08&#010;主题: Re: flink row 类型&#010;&#010;&#010;hi 是的，我们的数据场景比较尴尬，那我想其他方式实现一下 Benchao&#010;Li &lt;libenchao@apache.org&gt; 于2020年7月23日周四 下午12:55写道： &gt; 这个应该是做不到的。name只是SQL&#010;plan过程的东西，在运行时它就没有什么实际意义了。 &gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#010;&gt; &gt; Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道：&#010;&gt; &gt; &gt; hi、 &gt; &gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过&#010;&gt; &gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#010;&gt; &gt; &gt; &gt; rule_key 转换为rule_key1,rulekey2 &gt; &gt; 1 &gt; &gt; 2 &gt; &gt;&#010;&gt; &gt; &gt; -- &gt; &gt; Best, &gt; Benchao Li &gt;",
        "depth": "1",
        "reply": "<tencent_AE77031A80FBBEC7CBA0B22C@qq.com>"
    },
    {
        "id": "<CAEZk0434Z1qi+W5vwfTqpgYEwXodja2xeLRN05VxnhvgWhZ-TQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 06:56:14 GMT",
        "subject": "Re: flink row 类型",
        "content": "hi、Jingsong Li&#013;&#010;我查看了对应的api，并运行了demo测试，通过CallContext我可以拿到对应的字段类型，但是无法拿到对应的字段名称&#013;&#010;&#013;&#010;&gt;&gt;在TypeInference中有input的type，这个type应该是包含字段信息的。&#013;&#010;&#013;&#010;xiao cai &lt;flinkcx@163.com&gt; 于2020年7月23日周四 下午2:19写道：&#013;&#010;&#013;&#010;&gt; 可以考虑把字段索引值保存下来再获取&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;  原始邮件&#013;&#010;&gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&gt;&#013;&#010;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 发送时间: 2020年7月23日(周四) 14:08&#013;&#010;&gt; 主题: Re: flink row 类型&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi 是的，我们的数据场景比较尴尬，那我想其他方式实现一下 Benchao&#010;Li &lt;libenchao@apache.org&gt;&#013;&#010;&gt; 于2020年7月23日周四 下午12:55写道： &gt; 这个应该是做不到的。name只是SQL&#010;plan过程的东西，在运行时它就没有什么实际意义了。 &gt;&#013;&#010;&gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#010;&gt; &gt; Dream-底限 &lt;&#013;&#010;&gt; zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道： &gt; &gt; &gt;&#010;hi、 &gt; &gt;&#013;&#010;&gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过 &gt; &gt;&#013;&#010;&gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt; rule_key 转换为rule_key1,rulekey2 &gt; &gt; 1 &gt; &gt; 2 &gt; &gt; &gt; &gt; &gt;&#010;-- &gt; &gt; Best, &gt; Benchao&#013;&#010;&gt; Li &gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_AE77031A80FBBEC7CBA0B22C@qq.com>"
    },
    {
        "id": "<CAEZk042xkQvbdsQD=SaqTGTLfG9w7JZs9NuZNFy86ppLR2iOGQ@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 06:57:28 GMT",
        "subject": "Re: flink row 类型",
        "content": "hi、xiao cai&#013;&#010;&#013;&#010;可以说一下思路吗，我没太懂&#013;&#010;》》可以考虑把字段索引值保存下来再获取&#013;&#010;&#013;&#010;Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月23日周四 下午2:56写道：&#013;&#010;&#013;&#010;&gt; hi、Jingsong Li&#013;&#010;&gt; 我查看了对应的api，并运行了demo测试，通过CallContext我可以拿到对应的字段类型，但是无法拿到对应的字段名称&#013;&#010;&gt;&#013;&#010;&gt; &gt;&gt;在TypeInference中有input的type，这个type应该是包含字段信息的。&#013;&#010;&gt;&#013;&#010;&gt; xiao cai &lt;flinkcx@163.com&gt; 于2020年7月23日周四 下午2:19写道：&#013;&#010;&gt;&#013;&#010;&gt;&gt; 可以考虑把字段索引值保存下来再获取&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;  原始邮件&#013;&#010;&gt;&gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&gt;&#013;&#010;&gt;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt;&gt; 发送时间: 2020年7月23日(周四) 14:08&#013;&#010;&gt;&gt; 主题: Re: flink row 类型&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt;&#013;&#010;&gt;&gt; hi 是的，我们的数据场景比较尴尬，那我想其他方式实现一下&#010;Benchao Li &lt;libenchao@apache.org&gt;&#013;&#010;&gt;&gt; 于2020年7月23日周四 下午12:55写道： &gt; 这个应该是做不到的。name只是SQL&#010;plan过程的东西，在运行时它就没有什么实际意义了。 &gt;&#013;&#010;&gt;&gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#010;&gt; &gt; Dream-底限 &lt;&#013;&#010;&gt;&gt; zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道： &gt; &gt; &gt;&#010;hi、 &gt; &gt;&#013;&#010;&gt;&gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过 &gt;&#010;&gt;&#013;&#010;&gt;&gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#010;&gt; &gt; &gt; &gt;&#013;&#010;&gt;&gt; rule_key 转换为rule_key1,rulekey2 &gt; &gt; 1 &gt; &gt; 2 &gt; &gt; &gt; &gt;&#010;&gt; -- &gt; &gt; Best, &gt; Benchao&#013;&#010;&gt;&gt; Li &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_AE77031A80FBBEC7CBA0B22C@qq.com>"
    },
    {
        "id": "<tencent_E74651563EEE13754F7A86B0@qq.com>",
        "from": "&quot;xiao cai&quot;&lt;flin...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 07:39:49 GMT",
        "subject": "Re: flink row 类型",
        "content": "Hi ，Dream&#010;&#010;&#010;比如你最终拿到的是Row(10)，10表示有10个字段，这些字段的顺序是固定的，那么你可以把每个字段在row里的索引的映射关系保存下来，如下&#010;map&lt;fieldName, fieldIndex&gt; ，然后 row.getField(map.get(fieldName))获取你需要的值&#010;&#010;&#010; 原始邮件 &#010;发件人: Dream-底限&lt;zhangyu@akulaku.com&gt;&#010;收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;发送时间: 2020年7月23日(周四) 14:57&#010;主题: Re: flink row 类型&#010;&#010;&#010;hi、xiao cai 可以说一下思路吗，我没太懂 》》可以考虑把字段索引值保存下来再获取&#010;Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月23日周四 下午2:56写道： &gt;&#010;hi、Jingsong Li &gt; 我查看了对应的api，并运行了demo测试，通过CallContext我可以拿到对应的字段类型，但是无法拿到对应的字段名称&#010;&gt; &gt; &gt;&gt;在TypeInference中有input的type，这个type应该是包含字段信息的。&#010;&gt; &gt; xiao cai &lt;flinkcx@163.com&gt; 于2020年7月23日周四 下午2:19写道： &gt;&#010;&gt;&gt; 可以考虑把字段索引值保存下来再获取 &gt;&gt; &gt;&gt; &gt;&gt; 原始邮件&#010;&gt;&gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&gt; &gt;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#010;&gt;&gt; 发送时间: 2020年7月23日(周四) 14:08 &gt;&gt; 主题: Re: flink row 类型&#010;&gt;&gt; &gt;&gt; &gt;&gt; hi 是的，我们的数据场景比较尴尬，那我想其他方式实现一下&#010;Benchao Li &lt;libenchao@apache.org&gt; &gt;&gt; 于2020年7月23日周四 下午12:55写道：&#010;&gt; 这个应该是做不到的。name只是SQL plan过程的东西，在运行时它就没有什么实际意义了。&#010;&gt; &gt;&gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#010;&gt; &gt; Dream-底限 &lt; &gt;&gt; zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道：&#010;&gt; &gt; &gt; hi、 &gt; &gt; &gt;&gt; 我这面定义row数据，类型为ROW&lt;rule_key&#010;STRING&gt;，可以通过 &gt; &gt; &gt;&gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#010;&gt; &gt; &gt; &gt; &gt;&gt; rule_key 转换为rule_key1,rulekey2 &gt; &gt; 1 &gt; &gt; 2&#010;&gt; &gt; &gt; &gt; &gt; -- &gt; &gt; Best, &gt; Benchao &gt;&gt; Li &gt; &gt; &gt;",
        "depth": "1",
        "reply": "<tencent_AE77031A80FBBEC7CBA0B22C@qq.com>"
    },
    {
        "id": "<CAEZk042zThCuX5q=Pk2Hjh4MZqNGd29eGChEJD8Bn1sAiFxKPA@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 07:46:35 GMT",
        "subject": "Re: flink row 类型",
        "content": "hi xiao cai&#013;&#010;我懂你的意思了，这确实是一种解决方式，不过这种方式有一个弊端就是每个这种功能都要开发对应的方法，我还是比较倾向于一个方法适用于一类场景，如果做不到只能每次有需求都重新开发了&#013;&#010;&#013;&#010;xiao cai &lt;flinkcx@163.com&gt; 于2020年7月23日周四 下午3:40写道：&#013;&#010;&#013;&#010;&gt; Hi ，Dream&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 比如你最终拿到的是Row(10)，10表示有10个字段，这些字段的顺序是固定的，那么你可以把每个字段在row里的索引的映射关系保存下来，如下&#013;&#010;&gt; map&lt;fieldName, fieldIndex&gt; ，然后 row.getField(map.get(fieldName))获取你需要的值&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;  原始邮件&#013;&#010;&gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&gt;&#013;&#010;&gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 发送时间: 2020年7月23日(周四) 14:57&#013;&#010;&gt; 主题: Re: flink row 类型&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi、xiao cai 可以说一下思路吗，我没太懂 》》可以考虑把字段索引值保存下来再获取&#010;Dream-底限 &lt;&#013;&#010;&gt; zhangyu@akulaku.com&gt; 于2020年7月23日周四 下午2:56写道： &gt; hi、Jingsong&#010;Li &gt;&#013;&#010;&gt; 我查看了对应的api，并运行了demo测试，通过CallContext我可以拿到对应的字段类型，但是无法拿到对应的字段名称&#010;&gt; &gt;&#013;&#010;&gt; &gt;&gt;在TypeInference中有input的type，这个type应该是包含字段信息的。&#010;&gt; &gt; xiao cai &lt;&#013;&#010;&gt; flinkcx@163.com&gt; 于2020年7月23日周四 下午2:19写道： &gt; &gt;&gt; 可以考虑把字段索引值保存下来再获取&#010;&gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; 原始邮件 &gt;&gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&gt; &gt;&gt; 收件人:&#010;user-zh&lt;&#013;&#010;&gt; user-zh@flink.apache.org&gt; &gt;&gt; 发送时间: 2020年7月23日(周四) 14:08&#010;&gt;&gt; 主题: Re: flink&#013;&#010;&gt; row 类型 &gt;&gt; &gt;&gt; &gt;&gt; hi 是的，我们的数据场景比较尴尬，那我想其他方式实现一下&#010;Benchao Li &lt;&#013;&#010;&gt; libenchao@apache.org&gt; &gt;&gt; 于2020年7月23日周四 下午12:55写道： &gt;&#010;这个应该是做不到的。name只是SQL&#013;&#010;&gt; plan过程的东西，在运行时它就没有什么实际意义了。 &gt; &gt;&gt;&#013;&#010;&gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#010;&gt; &gt; Dream-底限 &lt; &gt;&gt;&#013;&#010;&gt; zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道： &gt; &gt; &gt;&#010;hi、 &gt; &gt; &gt;&gt;&#013;&#010;&gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过 &gt; &gt;&#010;&gt;&gt;&#013;&#010;&gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; rule_key 转换为rule_key1,rulekey2 &gt; &gt; 1 &gt; &gt; 2 &gt; &gt; &gt; &gt; &gt;&#010;-- &gt; &gt; Best, &gt; Benchao&#013;&#010;&gt; &gt;&gt; Li &gt; &gt; &gt;&#013;&#010;",
        "depth": "2",
        "reply": "<tencent_AE77031A80FBBEC7CBA0B22C@qq.com>"
    },
    {
        "id": "<CAELO9331S4APMCE+5t2pKQ=Fvai0DPgxZuKAwQMc8y7MmHFRxw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 04:38:17 GMT",
        "subject": "Re: flink row 类型",
        "content": "你可以看看是不是可以把这个字段声明成 MAP，这样就可以 map['rule_key1']&#010;的方式通过字段名去获取了。&#013;&#010;&#013;&#010;Best,&#013;&#010;Jark&#013;&#010;&#013;&#010;On Thu, 23 Jul 2020 at 15:47, Dream-底限 &lt;zhangyu@akulaku.com&gt; wrote:&#013;&#010;&#013;&#010;&gt; hi xiao cai&#013;&#010;&gt;&#013;&#010;&gt; 我懂你的意思了，这确实是一种解决方式，不过这种方式有一个弊端就是每个这种功能都要开发对应的方法，我还是比较倾向于一个方法适用于一类场景，如果做不到只能每次有需求都重新开发了&#013;&#010;&gt;&#013;&#010;&gt; xiao cai &lt;flinkcx@163.com&gt; 于2020年7月23日周四 下午3:40写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi ，Dream&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 比如你最终拿到的是Row(10)，10表示有10个字段，这些字段的顺序是固定的，那么你可以把每个字段在row里的索引的映射关系保存下来，如下&#013;&#010;&gt; &gt; map&lt;fieldName, fieldIndex&gt; ，然后 row.getField(map.get(fieldName))获取你需要的值&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;  原始邮件&#013;&#010;&gt; &gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&gt;&#013;&#010;&gt; &gt; 收件人: user-zh&lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; &gt; 发送时间: 2020年7月23日(周四) 14:57&#013;&#010;&gt; &gt; 主题: Re: flink row 类型&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; hi、xiao cai 可以说一下思路吗，我没太懂 》》可以考虑把字段索引值保存下来再获取&#010;Dream-底限 &lt;&#013;&#010;&gt; &gt; zhangyu@akulaku.com&gt; 于2020年7月23日周四 下午2:56写道： &gt; hi、Jingsong&#010;Li &gt;&#013;&#010;&gt; &gt; 我查看了对应的api，并运行了demo测试，通过CallContext我可以拿到对应的字段类型，但是无法拿到对应的字段名称&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt;&gt;在TypeInference中有input的type，这个type应该是包含字段信息的。&#010;&gt; &gt; xiao cai &lt;&#013;&#010;&gt; &gt; flinkcx@163.com&gt; 于2020年7月23日周四 下午2:19写道： &gt; &gt;&gt;&#010;可以考虑把字段索引值保存下来再获取 &gt;&gt; &gt;&gt; &gt;&gt;&#013;&#010;&gt; &gt; 原始邮件 &gt;&gt; 发件人: Dream-底限&lt;zhangyu@akulaku.com&gt; &gt;&gt;&#010;收件人: user-zh&lt;&#013;&#010;&gt; &gt; user-zh@flink.apache.org&gt; &gt;&gt; 发送时间: 2020年7月23日(周四) 14:08&#010;&gt;&gt; 主题: Re: flink&#013;&#010;&gt; &gt; row 类型 &gt;&gt; &gt;&gt; &gt;&gt; hi 是的，我们的数据场景比较尴尬，那我想其他方式实现一下&#010;Benchao Li &lt;&#013;&#010;&gt; &gt; libenchao@apache.org&gt; &gt;&gt; 于2020年7月23日周四 下午12:55写道：&#010;&gt; 这个应该是做不到的。name只是SQL&#013;&#010;&gt; &gt; plan过程的东西，在运行时它就没有什么实际意义了。 &gt; &gt;&gt;&#013;&#010;&gt; &gt; 你是想在udf里面获取row里面每个字段的名字是吧？如果是的话，我理解现在应该是做不到的。&#010;&gt; &gt; Dream-底限 &lt; &gt;&gt;&#013;&#010;&gt; &gt; zhangyu@akulaku.com&gt; 于2020年7月22日周三 下午7:22写道： &gt; &gt;&#010;&gt; hi、 &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; 我这面定义row数据，类型为ROW&lt;rule_key STRING&gt;，可以通过 &gt;&#010;&gt; &gt;&gt;&#013;&#010;&gt; &gt; row.getField(i)获取到对应的值，但是我想获取对应的row_name名称要怎么操作，貌似没有报漏获取名称的接口&#010;&gt; &gt; &gt; &gt; &gt;&gt;&#013;&#010;&gt; &gt; rule_key 转换为rule_key1,rulekey2 &gt; &gt; 1 &gt; &gt; 2 &gt; &gt; &gt; &gt;&#010;&gt; -- &gt; &gt; Best, &gt;&#013;&#010;&gt; Benchao&#013;&#010;&gt; &gt; &gt;&gt; Li &gt; &gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "3",
        "reply": "<tencent_AE77031A80FBBEC7CBA0B22C@qq.com>"
    },
    {
        "id": "<CAGvqHsevOgnCcpmQpv5AU9LYqNO2b_JVNT5mx=hu6upC-npeOQ@mail.gmail.com>",
        "from": "Sivaprasanna &lt;sivaprasanna...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 06:18:18 GMT",
        "subject": "Re: Is it possible to do state migration with checkpoints?",
        "content": "+user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&#010;A follow up question. I tried taking a savepoint but the job failed&#010;immediately. It happens everytime I take a savepoint. The job is running on&#010;a Yarn cluster so it fails with \"container running out of memory\". The&#010;state size averages around 1.2G but also peaks to ~4.5 GB sometimes (please&#010;refer to the screenshot below). The job is running with 2GB task manager&#010;heap &amp; 2GB task manager managed memory. I increased the managed memory to&#010;6GB assuming the failure has something to do with RocksDB but it failed&#010;even with 6GB managed memory. I guess I am missing on some configurations.&#010;Can you folks please help me with this?&#010;&#010;[image: Screenshot 2020-07-23 at 10.34.29 AM.png]&#010;&#010;On Wed, Jul 22, 2020 at 7:32 PM Sivaprasanna &lt;sivaprasanna246@gmail.com&gt;&#010;wrote:&#010;&#010;&gt; Hi,&#010;&gt;&#010;&gt; We are trying out state schema migration for one of our stateful&#010;&gt; pipelines. We use few Avro type states. Changes made to the job:&#010;&gt;     1. Updated the schema for one of the states (added a new 'boolean'&#010;&gt; field with default value).&#010;&gt;     2. Modified the code by removing a couple of ValueStates.&#010;&gt;&#010;&gt; To push these changes, I stopped the live job and resubmitted the new jar&#010;&gt; with the latest *checkpoint* path. However, the job failed with the&#010;&gt; following error:&#010;&gt;&#010;&gt; java.lang.RuntimeException: Error while getting state&#010;&gt;     at&#010;&gt; org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:62)&#010;&gt;     at&#010;&gt; org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:144)&#010;&gt;     ...&#010;&gt;     ...&#010;&gt; Caused by: org.apache.flink.util.StateMigrationException: The new state&#010;&gt; serializer cannot be incompatible.&#010;&gt;     at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.updateRestoredStateMetaInfo(RocksDBKeyedStateBackend.java:543)&#010;&gt;&#010;&gt;     at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:491)&#010;&gt;&#010;&gt;     at&#010;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:652)&#010;&gt;&#010;&gt; I was going through the state schema evolution doc. The document mentions&#010;&gt; that we need to take a *savepoint* and restart the job with the savepoint&#010;&gt; path. We are using RocksDB backend with incremental checkpoint enabled. Can&#010;&gt; we not use the latest checkpoint available when we are dealing with state&#010;&gt; schema changes?&#010;&gt;&#010;&gt; Complete stacktrace is attached with this mail.&#010;&gt;&#010;&gt; -&#010;&gt; Sivaprasanna&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<CAGvqHsevOgnCcpmQpv5AU9LYqNO2b_JVNT5mx=hu6upC-npeOQ@mail.gmail.com>"
    },
    {
        "id": "<CAGvqHsfWE7Zpu49xMCHYBh=adP9iFSJ3EzFSkY2xOEEZPSqRyw@mail.gmail.com>",
        "from": "Sivaprasanna &lt;sivaprasanna...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 12:03:27 GMT",
        "subject": "Re: Is it possible to do state migration with checkpoints?",
        "content": "Adding dev@ to get some traction. Any help would be greatly appreciated.&#010;&#010;Thanks.&#010;&#010;On Thu, Jul 23, 2020 at 11:48 AM Sivaprasanna &lt;sivaprasanna246@gmail.com&gt;&#010;wrote:&#010;&#010;&gt; +user-zh@flink.apache.org &lt;user-zh@flink.apache.org&gt;&#010;&gt;&#010;&gt; A follow up question. I tried taking a savepoint but the job failed&#010;&gt; immediately. It happens everytime I take a savepoint. The job is running on&#010;&gt; a Yarn cluster so it fails with \"container running out of memory\". The&#010;&gt; state size averages around 1.2G but also peaks to ~4.5 GB sometimes (please&#010;&gt; refer to the screenshot below). The job is running with 2GB task manager&#010;&gt; heap &amp; 2GB task manager managed memory. I increased the managed memory to&#010;&gt; 6GB assuming the failure has something to do with RocksDB but it failed&#010;&gt; even with 6GB managed memory. I guess I am missing on some configurations.&#010;&gt; Can you folks please help me with this?&#010;&gt;&#010;&gt; [image: Screenshot 2020-07-23 at 10.34.29 AM.png]&#010;&gt;&#010;&gt; On Wed, Jul 22, 2020 at 7:32 PM Sivaprasanna &lt;sivaprasanna246@gmail.com&gt;&#010;&gt; wrote:&#010;&gt;&#010;&gt;&gt; Hi,&#010;&gt;&gt;&#010;&gt;&gt; We are trying out state schema migration for one of our stateful&#010;&gt;&gt; pipelines. We use few Avro type states. Changes made to the job:&#010;&gt;&gt;     1. Updated the schema for one of the states (added a new 'boolean'&#010;&gt;&gt; field with default value).&#010;&gt;&gt;     2. Modified the code by removing a couple of ValueStates.&#010;&gt;&gt;&#010;&gt;&gt; To push these changes, I stopped the live job and resubmitted the new jar&#010;&gt;&gt; with the latest *checkpoint* path. However, the job failed with the&#010;&gt;&gt; following error:&#010;&gt;&gt;&#010;&gt;&gt; java.lang.RuntimeException: Error while getting state&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.runtime.state.DefaultKeyedStateStore.getState(DefaultKeyedStateStore.java:62)&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.streaming.api.operators.StreamingRuntimeContext.getState(StreamingRuntimeContext.java:144)&#010;&gt;&gt;     ...&#010;&gt;&gt;     ...&#010;&gt;&gt; Caused by: org.apache.flink.util.StateMigrationException: The new state&#010;&gt;&gt; serializer cannot be incompatible.&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.updateRestoredStateMetaInfo(RocksDBKeyedStateBackend.java:543)&#010;&gt;&gt;&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.tryRegisterKvStateInformation(RocksDBKeyedStateBackend.java:491)&#010;&gt;&gt;&#010;&gt;&gt;     at&#010;&gt;&gt; org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend.createInternalState(RocksDBKeyedStateBackend.java:652)&#010;&gt;&gt;&#010;&gt;&gt; I was going through the state schema evolution doc. The document mentions&#010;&gt;&gt; that we need to take a *savepoint* and restart the job with the savepoint&#010;&gt;&gt; path. We are using RocksDB backend with incremental checkpoint enabled. Can&#010;&gt;&gt; we not use the latest checkpoint available when we are dealing with state&#010;&gt;&gt; schema changes?&#010;&gt;&gt;&#010;&gt;&gt; Complete stacktrace is attached with this mail.&#010;&gt;&gt;&#010;&gt;&gt; -&#010;&gt;&gt; Sivaprasanna&#010;&gt;&gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<CAGvqHsevOgnCcpmQpv5AU9LYqNO2b_JVNT5mx=hu6upC-npeOQ@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LCb4gxZ4MzGE5j=hj6S=NE5RyVtGYD9FyERApNa4Obz4Q@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 07:50:32 GMT",
        "subject": "application模式提交操作hive的任务相关疑问",
        "content": "大家好：&#013;&#010;我现在有一个flink的程序要读写hive的数据，在程序中构造HiveCatalog的时候需要有一个hiveConfDir，如果我使用的是新的application模式去提交任务，这个任务的解析应该是放到了master端，这个时候hadoop集群上没有hive的相关配置，那么这个hiveConfDir该怎么配置呢？&#013;&#010;&#013;&#010;谢谢&#013;&#010;",
        "depth": "0",
        "reply": "<CAM2Y1LCb4gxZ4MzGE5j=hj6S=NE5RyVtGYD9FyERApNa4Obz4Q@mail.gmail.com>"
    },
    {
        "id": "<CADH6UNR+=P0Q8jwUvB0TPb-6MerX5+FeOVTP9nQzMhFPTxqc+g@mail.gmail.com>",
        "from": "Rui Li &lt;lirui.fu...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 11:18:25 GMT",
        "subject": "Re: application模式提交操作hive的任务相关疑问",
        "content": "有一种做法是把hive-site.xml打到作业jar包里，然后程序运行的时候再拷出来放到一个本地目录...我们也可以考虑以后为HiveCatalog添加一个接受HiveConf参数的构造器，这样对API的模式应该会更灵活&#013;&#010;&#013;&#010;On Thu, Jul 23, 2020 at 3:51 PM Jun Zhang &lt;zhangjunemail100@gmail.com&gt;&#013;&#010;wrote:&#013;&#010;&#013;&#010;&gt; 大家好：&#013;&#010;&gt;&#013;&#010;&gt; 我现在有一个flink的程序要读写hive的数据，在程序中构造HiveCatalog的时候需要有一个hiveConfDir，如果我使用的是新的application模式去提交任务，这个任务的解析应该是放到了master端，这个时候hadoop集群上没有hive的相关配置，那么这个hiveConfDir该怎么配置呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li&#013;&#010;",
        "depth": "1",
        "reply": "<CAM2Y1LCb4gxZ4MzGE5j=hj6S=NE5RyVtGYD9FyERApNa4Obz4Q@mail.gmail.com>"
    },
    {
        "id": "<tencent_B9F671235D9BE6D6BE386100566972AF9908@qq.com>",
        "from": "&quot;Jun Zhang&quot; &lt;825875...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 11:35:20 GMT",
        "subject": "回复：application模式提交操作hive的任务相关疑问",
        "content": "我现在是改了源码，是把hivecatalog里面接收HiveConf参数的protected类型的构造方法改成了public类型，然后自己在代码里构造了HiveConf对象，传了一些必要的参数，比如metastore地址等。&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;Best&amp;nbsp;&amp;nbsp;Jun&#013;&#010;&#013;&#010;&#013;&#010;------------------ 原始邮件 ------------------&#013;&#010;发件人: Rui Li &lt;lirui.fudan@gmail.com&amp;gt;&#013;&#010;发送时间: 2020年7月23日 19:25&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&amp;gt;&#013;&#010;主题: 回复：application模式提交操作hive的任务相关疑问&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;有一种做法是把hive-site.xml打到作业jar包里，然后程序运行的时候再拷出来放到一个本地目录...我们也可以考虑以后为HiveCatalog添加一个接受HiveConf参数的构造器，这样对API的模式应该会更灵活&#013;&#010;&#013;&#010;On Thu, Jul 23, 2020 at 3:51 PM Jun Zhang &lt;zhangjunemail100@gmail.com&amp;gt;&#013;&#010;wrote:&#013;&#010;&#013;&#010;&amp;gt; 大家好：&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我现在有一个flink的程序要读写hive的数据，在程序中构造HiveCatalog的时候需要有一个hiveConfDir，如果我使用的是新的application模式去提交任务，这个任务的解析应该是放到了master端，这个时候hadoop集群上没有hive的相关配置，那么这个hiveConfDir该怎么配置呢？&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 谢谢&#013;&#010;&amp;gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best regards!&#013;&#010;Rui Li",
        "depth": "2",
        "reply": "<CAM2Y1LCb4gxZ4MzGE5j=hj6S=NE5RyVtGYD9FyERApNa4Obz4Q@mail.gmail.com>"
    },
    {
        "id": "<CAP+gf35SFoPzCdhk0WY6tnEXDqMRu5wxY6h9bqC4K5kED+3u8w@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 02:08:50 GMT",
        "subject": "Re: application模式提交操作hive的任务相关疑问",
        "content": "可以使用-Dyarn.ship-directories=/path/of/hiveConfDir把hive的配置ship到JobManager端，hiveConfDir默认会在&#013;&#010;当前目录下，同时这个目录也会自动加入到classpath，不太清楚这样是否可以让hive正常加载到&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月23日周四 下午3:51写道：&#013;&#010;&#013;&#010;&gt; 大家好：&#013;&#010;&gt;&#013;&#010;&gt; 我现在有一个flink的程序要读写hive的数据，在程序中构造HiveCatalog的时候需要有一个hiveConfDir，如果我使用的是新的application模式去提交任务，这个任务的解析应该是放到了master端，这个时候hadoop集群上没有hive的相关配置，那么这个hiveConfDir该怎么配置呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAM2Y1LCb4gxZ4MzGE5j=hj6S=NE5RyVtGYD9FyERApNa4Obz4Q@mail.gmail.com>"
    },
    {
        "id": "<CAM2Y1LDwTJzQ99BvYJOa4L76q1FXD_2K1vnBZ8RWE5503G5t2Q@mail.gmail.com>",
        "from": "Jun Zhang &lt;zhangjunemail...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 02:16:20 GMT",
        "subject": "Re: application模式提交操作hive的任务相关疑问",
        "content": "hi,Yang Wang:&#013;&#010;*谢谢你的建议，稍后我测试一下。*&#013;&#010;&#013;&#010;Yang Wang &lt;danrtsey.wy@gmail.com&gt; 于2020年7月24日周五 上午10:09写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 可以使用-Dyarn.ship-directories=/path/of/hiveConfDir把hive的配置ship到JobManager端，hiveConfDir默认会在&#013;&#010;&gt; 当前目录下，同时这个目录也会自动加入到classpath，不太清楚这样是否可以让hive正常加载到&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Yang&#013;&#010;&gt;&#013;&#010;&gt; Jun Zhang &lt;zhangjunemail100@gmail.com&gt; 于2020年7月23日周四 下午3:51写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 大家好：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 我现在有一个flink的程序要读写hive的数据，在程序中构造HiveCatalog的时候需要有一个hiveConfDir，如果我使用的是新的application模式去提交任务，这个任务的解析应该是放到了master端，这个时候hadoop集群上没有hive的相关配置，那么这个hiveConfDir该怎么配置呢？&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 谢谢&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CAM2Y1LCb4gxZ4MzGE5j=hj6S=NE5RyVtGYD9FyERApNa4Obz4Q@mail.gmail.com>"
    },
    {
        "id": "<202007231600168953321@geekplus.com.cn>",
        "from": "&quot;wanglei2@geekplus.com.cn&quot; &lt;wangl...@geekplus.com.cn&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 08:00:17 GMT",
        "subject": "flink-1.11 在 windows 下怎样启动",
        "content": "&#013;&#010;我看 flink-1.11 发布包 bin 目录没有 windows 启动所需的 .bat 文件了。&#013;&#010;那在 windows 下怎样启动呢？&#013;&#010;&#013;&#010;谢谢&#013;&#010;王磊&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &#013;&#010;",
        "depth": "0",
        "reply": "<202007231600168953321@geekplus.com.cn>"
    },
    {
        "id": "<CAP+gf35gWZaO2hteP6r6WNqa7h97xot0FeoZk_myR=vShoK6Mw@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 05:46:40 GMT",
        "subject": "Re: flink-1.11 在 windows 下怎样启动",
        "content": "社区已经不再维护windows版本的脚本了，建议你可以使用docker的方式[1]来运行&#013;&#010;这样会更方便一些&#013;&#010;&#013;&#010;&#013;&#010;[1].&#013;&#010;https://ci.apache.org/projects/flink/flink-docs-master/ops/deployment/docker.html&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;wanglei2@geekplus.com.cn &lt;wanglei2@geekplus.com.cn&gt; 于2020年7月23日周四 下午4:07写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt; 我看 flink-1.11 发布包 bin 目录没有 windows 启动所需的 .bat 文件了。&#013;&#010;&gt; 那在 windows 下怎样启动呢？&#013;&#010;&gt;&#013;&#010;&gt; 谢谢&#013;&#010;&gt; 王磊&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; wanglei2@geekplus.com.cn&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<202007231600168953321@geekplus.com.cn>"
    },
    {
        "id": "<1595492052718-0.post@n8.nabble.com>",
        "from": "曹武 &lt;14701319...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 08:14:12 GMT",
        "subject": "flink 1.11 ddl 写mysql的问题",
        "content": "我使用fink 1.11.1 做cdc,发现一秒钟只能写100条左右数据到mysql,请问有优化方案,或者是其他的批量写入的方案建议嘛&#010;代码如下:&#010;        String sourceDdl =\" CREATE TABLE debezium_source \" +&#010;                \"( \" +&#010;                \"id STRING NOT NULL, name STRING, description STRING, weight&#010;Double\" +&#010;                \") \" +&#010;                \"WITH (\" +&#010;                \" 'connector' = 'kafka-0.11',\" +&#010;                \" 'topic' = 'test0717',\" +&#010;                \" 'properties.bootstrap.servers' = ' 172.22.20.206:9092', \"&#010;+&#010;                \"'scan.startup.mode' =&#010;'group-offsets','properties.group.id'='test',\" +&#010;                \"'format' = 'debezium-json',\" +&#010;                \"'debezium-json.schema-include'='false',\" +&#010;                \"'debezium-json.ignore-parse-errors'='true')\";&#010;        tEnv.executeSql(sourceDdl);&#010;        System.out.println(\"init source ddl successful ==&gt;\" + sourceDdl);&#010;        String sinkDdl = \" CREATE TABLE sink \" +&#010;                \"( \" +&#010;                \"id STRING NOT NULL,\" +&#010;                \" name STRING, \" +&#010;                \"description STRING,\" +&#010;                \" weight Double,\" +&#010;                \" PRIMARY KEY (id) NOT ENFORCED \" +&#010;                \")\" +&#010;                \" WITH \" +&#010;                \"( \" +&#010;                \"'connector' = 'jdbc', \" +&#010;                \"'url' =&#010;'jdbc:mysql://127.0.0.1:3306/test?autoReconnect=true', \" +&#010;                \"'table-name' = 'table-out', \" +&#010;                \"'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;                \"'sink.buffer-flush.interval'='1s',\" +&#010;                \"'sink.buffer-flush.max-rows'='1000',\" +&#010;                \"'username'='DataPip', \" +&#010;                \"'password'='DataPip')\";&#010;        tEnv.executeSql(sinkDdl);&#010;        System.out.println(\"init sink ddl successful ==&gt;\" + sinkDdl);&#010;&#010;         String dml = \"INSERT INTO sink SELECT  id,name ,description, &#010;weight FROM debezium_source\";&#010;        System.out.println(\"execute dml  ==&gt;\" + dml);&#010;        tEnv.executeSql(dml);&#010;        tEnv.executeSql(\"CREATE TABLE print_table WITH ('connector' =&#010;'print')\" +&#010;                \"LIKE debezium_source (EXCLUDING ALL)\");&#010;        tEnv.executeSql(\"INSERT INTO print_table SELECT  id,name&#010;,description,  weight FROM debezium_source\");&#010;&#010;&#010;&#010;--&#010;Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&#010;",
        "depth": "0",
        "reply": "<1595492052718-0.post@n8.nabble.com>"
    },
    {
        "id": "<CADQYLGtkP4bSFFwgrM4a2ucStYiiJVR5ZGXVb0ARWmtERiZQOQ@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 12:24:02 GMT",
        "subject": "Re: flink 1.11 ddl 写mysql的问题",
        "content": "你观察到有sink写不过来导致反压吗？&#010;或者你调大flush interval试试，让每个buffer攒更多的数据&#010;&#010;曹武 &lt;14701319164@163.com&gt; 于2020年7月23日周四 下午4:48写道：&#010;&#010;&gt; 我使用fink 1.11.1 做cdc,发现一秒钟只能写100条左右数据到mysql,请问有优化方案,或者是其他的批量写入的方案建议嘛&#010;&gt; 代码如下:&#010;&gt;         String sourceDdl =\" CREATE TABLE debezium_source \" +&#010;&gt;                 \"( \" +&#010;&gt;                 \"id STRING NOT NULL, name STRING, description STRING,&#010;&gt; weight&#010;&gt; Double\" +&#010;&gt;                 \") \" +&#010;&gt;                 \"WITH (\" +&#010;&gt;                 \" 'connector' = 'kafka-0.11',\" +&#010;&gt;                 \" 'topic' = 'test0717',\" +&#010;&gt;                 \" 'properties.bootstrap.servers' = ' 172.22.20.206:9092',&#010;&gt; \"&#010;&gt; +&#010;&gt;                 \"'scan.startup.mode' =&#010;&gt; 'group-offsets','properties.group.id'='test',\" +&#010;&gt;                 \"'format' = 'debezium-json',\" +&#010;&gt;                 \"'debezium-json.schema-include'='false',\" +&#010;&gt;                 \"'debezium-json.ignore-parse-errors'='true')\";&#010;&gt;         tEnv.executeSql(sourceDdl);&#010;&gt;         System.out.println(\"init source ddl successful ==&gt;\" + sourceDdl);&#010;&gt;         String sinkDdl = \" CREATE TABLE sink \" +&#010;&gt;                 \"( \" +&#010;&gt;                 \"id STRING NOT NULL,\" +&#010;&gt;                 \" name STRING, \" +&#010;&gt;                 \"description STRING,\" +&#010;&gt;                 \" weight Double,\" +&#010;&gt;                 \" PRIMARY KEY (id) NOT ENFORCED \" +&#010;&gt;                 \")\" +&#010;&gt;                 \" WITH \" +&#010;&gt;                 \"( \" +&#010;&gt;                 \"'connector' = 'jdbc', \" +&#010;&gt;                 \"'url' =&#010;&gt; 'jdbc:mysql://127.0.0.1:3306/test?autoReconnect=true', \" +&#010;&gt;                 \"'table-name' = 'table-out', \" +&#010;&gt;                 \"'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;&gt;                 \"'sink.buffer-flush.interval'='1s',\" +&#010;&gt;                 \"'sink.buffer-flush.max-rows'='1000',\" +&#010;&gt;                 \"'username'='DataPip', \" +&#010;&gt;                 \"'password'='DataPip')\";&#010;&gt;         tEnv.executeSql(sinkDdl);&#010;&gt;         System.out.println(\"init sink ddl successful ==&gt;\" + sinkDdl);&#010;&gt;&#010;&gt;          String dml = \"INSERT INTO sink SELECT  id,name ,description,&#010;&gt; weight FROM debezium_source\";&#010;&gt;         System.out.println(\"execute dml  ==&gt;\" + dml);&#010;&gt;         tEnv.executeSql(dml);&#010;&gt;         tEnv.executeSql(\"CREATE TABLE print_table WITH ('connector' =&#010;&gt; 'print')\" +&#010;&gt;                 \"LIKE debezium_source (EXCLUDING ALL)\");&#010;&gt;         tEnv.executeSql(\"INSERT INTO print_table SELECT  id,name&#010;&gt; ,description,  weight FROM debezium_source\");&#010;&gt;&#010;&gt;&#010;&gt;&#010;&gt; --&#010;&gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<1595492052718-0.post@n8.nabble.com>"
    },
    {
        "id": "<CAELO930TiT89PGeVBnxswcUtHbFsCcXdE2XwcmzUaip3kj06dw@mail.gmail.com>",
        "from": "Jark Wu &lt;imj...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 04:42:03 GMT",
        "subject": "Re: flink 1.11 ddl 写mysql的问题",
        "content": "kafka 数据源生产数据的速率是多少呢？ 会不会数据源就是每秒100条数据呢。。。？&#010;Btw, 查看反压状态是一个比较好的排查方式。&#010;&#010;On Thu, 23 Jul 2020 at 20:25, godfrey he &lt;godfreyhe@gmail.com&gt; wrote:&#010;&#010;&gt; 你观察到有sink写不过来导致反压吗？&#010;&gt; 或者你调大flush interval试试，让每个buffer攒更多的数据&#010;&gt;&#010;&gt; 曹武 &lt;14701319164@163.com&gt; 于2020年7月23日周四 下午4:48写道：&#010;&gt;&#010;&gt; &gt; 我使用fink 1.11.1 做cdc,发现一秒钟只能写100条左右数据到mysql,请问有优化方案,或者是其他的批量写入的方案建议嘛&#010;&gt; &gt; 代码如下:&#010;&gt; &gt;         String sourceDdl =\" CREATE TABLE debezium_source \" +&#010;&gt; &gt;                 \"( \" +&#010;&gt; &gt;                 \"id STRING NOT NULL, name STRING, description STRING,&#010;&gt; &gt; weight&#010;&gt; &gt; Double\" +&#010;&gt; &gt;                 \") \" +&#010;&gt; &gt;                 \"WITH (\" +&#010;&gt; &gt;                 \" 'connector' = 'kafka-0.11',\" +&#010;&gt; &gt;                 \" 'topic' = 'test0717',\" +&#010;&gt; &gt;                 \" 'properties.bootstrap.servers' = ' 172.22.20.206:9092&#010;&gt; ',&#010;&gt; &gt; \"&#010;&gt; &gt; +&#010;&gt; &gt;                 \"'scan.startup.mode' =&#010;&gt; &gt; 'group-offsets','properties.group.id'='test',\" +&#010;&gt; &gt;                 \"'format' = 'debezium-json',\" +&#010;&gt; &gt;                 \"'debezium-json.schema-include'='false',\" +&#010;&gt; &gt;                 \"'debezium-json.ignore-parse-errors'='true')\";&#010;&gt; &gt;         tEnv.executeSql(sourceDdl);&#010;&gt; &gt;         System.out.println(\"init source ddl successful ==&gt;\" + sourceDdl);&#010;&gt; &gt;         String sinkDdl = \" CREATE TABLE sink \" +&#010;&gt; &gt;                 \"( \" +&#010;&gt; &gt;                 \"id STRING NOT NULL,\" +&#010;&gt; &gt;                 \" name STRING, \" +&#010;&gt; &gt;                 \"description STRING,\" +&#010;&gt; &gt;                 \" weight Double,\" +&#010;&gt; &gt;                 \" PRIMARY KEY (id) NOT ENFORCED \" +&#010;&gt; &gt;                 \")\" +&#010;&gt; &gt;                 \" WITH \" +&#010;&gt; &gt;                 \"( \" +&#010;&gt; &gt;                 \"'connector' = 'jdbc', \" +&#010;&gt; &gt;                 \"'url' =&#010;&gt; &gt; 'jdbc:mysql://127.0.0.1:3306/test?autoReconnect=true', \" +&#010;&gt; &gt;                 \"'table-name' = 'table-out', \" +&#010;&gt; &gt;                 \"'driver'= 'com.mysql.cj.jdbc.Driver',\" +&#010;&gt; &gt;                 \"'sink.buffer-flush.interval'='1s',\" +&#010;&gt; &gt;                 \"'sink.buffer-flush.max-rows'='1000',\" +&#010;&gt; &gt;                 \"'username'='DataPip', \" +&#010;&gt; &gt;                 \"'password'='DataPip')\";&#010;&gt; &gt;         tEnv.executeSql(sinkDdl);&#010;&gt; &gt;         System.out.println(\"init sink ddl successful ==&gt;\" + sinkDdl);&#010;&gt; &gt;&#010;&gt; &gt;          String dml = \"INSERT INTO sink SELECT  id,name ,description,&#010;&gt; &gt; weight FROM debezium_source\";&#010;&gt; &gt;         System.out.println(\"execute dml  ==&gt;\" + dml);&#010;&gt; &gt;         tEnv.executeSql(dml);&#010;&gt; &gt;         tEnv.executeSql(\"CREATE TABLE print_table WITH ('connector' =&#010;&gt; &gt; 'print')\" +&#010;&gt; &gt;                 \"LIKE debezium_source (EXCLUDING ALL)\");&#010;&gt; &gt;         tEnv.executeSql(\"INSERT INTO print_table SELECT  id,name&#010;&gt; &gt; ,description,  weight FROM debezium_source\");&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt;&#010;&gt; &gt; --&#010;&gt; &gt; Sent from: http://apache-flink.147419.n8.nabble.com/&#010;&gt; &gt;&#010;&gt;&#010;&#010;",
        "depth": "2",
        "reply": "<1595492052718-0.post@n8.nabble.com>"
    },
    {
        "id": "<tencent_6FF6459D026ACF9EEDCA385EC2F55B764305@qq.com>",
        "from": "&quot;小学生&quot; &lt;201782...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 08:58:23 GMT",
        "subject": "flink读取kafka流式数据的速率是多少",
        "content": "想问下各位大佬，有没有了解flink机制的，就是flink连接kafka，每次去取数据的时候，大概的取数速率是多大。",
        "depth": "0",
        "reply": "<tencent_6FF6459D026ACF9EEDCA385EC2F55B764305@qq.com>"
    },
    {
        "id": "<CAG=rS8hXW8Pf2TWbihD7V4YH7Yqpdu=7-ht8AfLkZpYjk+bSXw@mail.gmail.com>",
        "from": "zz zhang &lt;oliver7b...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 09:01:18 GMT",
        "subject": "metrics influxdb reporter 不支持https及jar放置路径问题",
        "content": "hello，目前Flink1.11.1&#013;&#010;发布的org.apache.flink.metrics.influxdb.InfluxdbReporter默认是上报是http协议，并不支持https协议，源码参考[2]&#013;&#010;&#013;&#010;另外，文档[1]标注的需要将 /opt/flink-metrics-influxdb-1.11.0.jar复制到目录plugins/influxdb，经过测试应该是要复制到目录plugins/metrics-influx&#013;&#010;&#013;&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/metrics.html#influxdb-orgapacheflinkmetricsinfluxdbinfluxdbreporter&#013;&#010;[2] https://github.com/apache/flink/blob/release-1.11/flink-metrics/flink-metrics-influxdb/src/main/java/org/apache/flink/metrics/influxdb/InfluxdbReporter.java#L84&#013;&#010;-- &#013;&#010;Best,&#013;&#010;zz zhang&#013;&#010;",
        "depth": "0",
        "reply": "<CAG=rS8hXW8Pf2TWbihD7V4YH7Yqpdu=7-ht8AfLkZpYjk+bSXw@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvt0dbi7E4S9ADcw7-0FitZ70Q4YPaq3PQ=Y-AQOjm8A8g@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 11:39:49 GMT",
        "subject": "Re: metrics influxdb reporter 不支持https及jar放置路径问题",
        "content": "Hi&#013;&#010;   感谢你的反馈，你可以创建两个 issue 来分别跟进反馈的这两个问题。如果你有兴趣的话，也可以尝试对这两个问题进行共享的&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;zz zhang &lt;oliver7b106@gmail.com&gt; 于2020年7月23日周四 下午5:02写道：&#013;&#010;&#013;&#010;&gt; hello，目前Flink1.11.1&#013;&#010;&gt;&#013;&#010;&gt; 发布的org.apache.flink.metrics.influxdb.InfluxdbReporter默认是上报是http协议，并不支持https协议，源码参考[2]&#013;&#010;&gt;&#013;&#010;&gt; 另外，文档[1]标注的需要将&#013;&#010;&gt; /opt/flink-metrics-influxdb-1.11.0.jar复制到目录plugins/influxdb，经过测试应该是要复制到目录plugins/metrics-influx&#013;&#010;&gt;&#013;&#010;&gt; [1]&#013;&#010;&gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/metrics.html#influxdb-orgapacheflinkmetricsinfluxdbinfluxdbreporter&#013;&#010;&gt; [2]&#013;&#010;&gt; https://github.com/apache/flink/blob/release-1.11/flink-metrics/flink-metrics-influxdb/src/main/java/org/apache/flink/metrics/influxdb/InfluxdbReporter.java#L84&#013;&#010;&gt; --&#013;&#010;&gt; Best,&#013;&#010;&gt; zz zhang&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAG=rS8hXW8Pf2TWbihD7V4YH7Yqpdu=7-ht8AfLkZpYjk+bSXw@mail.gmail.com>"
    },
    {
        "id": "<CAG=rS8ioyFhFeQ9p1wvw5qYocqh+cT2MQ+5OG7ZRD_kL0wwrXQ@mail.gmail.com>",
        "from": "zz zhang &lt;oliver7b...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 02:42:02 GMT",
        "subject": "Re: metrics influxdb reporter 不支持https及jar放置路径问题",
        "content": "问题1&#013;&#010;找到了https://issues.apache.org/jira/browse/FLINK-12336，且已经合并到master分支了，看issues中修复版本是1.12.0&#013;&#010;问题2 我又在Flink1.11.1下详细测试了，flink-metrics-influxdb-1.11.0.jar在plugins/influxdb和plugins/metrics-influx均可以上报mertics，当然我也尝试过目录plugins/metrics-influx123也是可以上报&#013;&#010;&#013;&#010;Congxian Qiu &lt;qcx978132955@gmail.com&gt; 于2020年7月25日周六 下午7:40写道：&#013;&#010;&gt;&#013;&#010;&gt; Hi&#013;&#010;&gt;    感谢你的反馈，你可以创建两个 issue 来分别跟进反馈的这两个问题。如果你有兴趣的话，也可以尝试对这两个问题进行共享的&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Congxian&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; zz zhang &lt;oliver7b106@gmail.com&gt; 于2020年7月23日周四 下午5:02写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; hello，目前Flink1.11.1&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 发布的org.apache.flink.metrics.influxdb.InfluxdbReporter默认是上报是http协议，并不支持https协议，源码参考[2]&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 另外，文档[1]标注的需要将&#013;&#010;&gt; &gt; /opt/flink-metrics-influxdb-1.11.0.jar复制到目录plugins/influxdb，经过测试应该是要复制到目录plugins/metrics-influx&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; [1]&#013;&#010;&gt; &gt; https://ci.apache.org/projects/flink/flink-docs-release-1.11/monitoring/metrics.html#influxdb-orgapacheflinkmetricsinfluxdbinfluxdbreporter&#013;&#010;&gt; &gt; [2]&#013;&#010;&gt; &gt; https://github.com/apache/flink/blob/release-1.11/flink-metrics/flink-metrics-influxdb/src/main/java/org/apache/flink/metrics/influxdb/InfluxdbReporter.java#L84&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt; Best,&#013;&#010;&gt; &gt; zz zhang&#013;&#010;&gt; &gt;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best,&#013;&#010;zz zhang&#013;&#010;",
        "depth": "2",
        "reply": "<CAG=rS8hXW8Pf2TWbihD7V4YH7Yqpdu=7-ht8AfLkZpYjk+bSXw@mail.gmail.com>"
    },
    {
        "id": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 10:13:30 GMT",
        "subject": "flink sql 1.11 kafka source with子句使用新参数，下游消费不到数据",
        "content": "Hi all,&#010;&#010;根据文档https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html#start-reading-position，&#010;使用新参数创建kafka_table，下游消费不到数据，使用老参数下游可以消费到数据，是不是新参数的方式有坑啊&#010;&#010;&#010;老参数:&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE kafka_table (&#010;        |    uid BIGINT,&#010;        |    sex VARCHAR,&#010;        |    age INT,&#010;        |    created_time TIMESTAMP(3),&#010;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;        |) WITH (&#010;        |&#010;        |     'connector.type' = 'kafka',&#010;        |    'connector.version' = 'universal',&#010;        |    'connector.topic' = 'user',&#010;        |    'connector.startup-mode' = 'latest-offset',&#010;        |    'connector.properties.zookeeper.connect' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;        |    'connector.properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;        |    'connector.properties.group.id' = 'user_flink',&#010;        |    'format.type' = 'json',&#010;        |    'format.derive-schema' = 'true'&#010;        |&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;新参数：&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE kafka_table (&#010;        |&#010;        |    uid BIGINT,&#010;        |    sex VARCHAR,&#010;        |    age INT,&#010;        |    created_time TIMESTAMP(3),&#010;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;        |) WITH (&#010;        |    'connector' = 'kafka',&#010;        |     'topic' = 'user',&#010;        |    'properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;        |    'properties.group.id' = 'user_flink',&#010;        |    'scan.startup.mode' = 'latest-offset',&#010;        |    'format' = 'json',&#010;        |    'json.fail-on-missing-field' = 'false',&#010;        |    'json.ignore-parse-errors' = 'true'&#010;        |)&#010;        |\"\"\".stripMargin)",
        "depth": "0",
        "reply": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>"
    },
    {
        "id": "<38447BCA-5FD0-41FB-A2E6-DED6AFF8E97F@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 12:10:43 GMT",
        "subject": "Re: flink sql 1.11 kafka source with子句使用新参数，下游消费不到数据",
        "content": "Hi&#010;&#010;你说的下游消费不到数据，这个下游是指当前作业消费不到数据吗？&#010;&#010;正常应该不会的，可以提供个可复现代码吗？ &#010;&#010;祝好&#010;Leonard Xu&#010;&#010;&#010;&gt; 在 2020年7月23日，18:13，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi all,&#010;&gt; &#010;&gt; 根据文档https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html#start-reading-position，&#010;&gt; 使用新参数创建kafka_table，下游消费不到数据，使用老参数下游可以消费到数据，是不是新参数的方式有坑啊&#010;&gt; &#010;&gt; &#010;&gt; 老参数:&#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE kafka_table (&#010;&gt;        |    uid BIGINT,&#010;&gt;        |    sex VARCHAR,&#010;&gt;        |    age INT,&#010;&gt;        |    created_time TIMESTAMP(3),&#010;&gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;        |) WITH (&#010;&gt;        |&#010;&gt;        |     'connector.type' = 'kafka',&#010;&gt;        |    'connector.version' = 'universal',&#010;&gt;        |    'connector.topic' = 'user',&#010;&gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt;        |    'connector.properties.zookeeper.connect' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;        |    'connector.properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt;        |    'format.type' = 'json',&#010;&gt;        |    'format.derive-schema' = 'true'&#010;&gt;        |&#010;&gt;        |)&#010;&gt;        |\"\"\".stripMargin)&#010;&gt; &#010;&gt; 新参数：&#010;&gt; &#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE kafka_table (&#010;&gt;        |&#010;&gt;        |    uid BIGINT,&#010;&gt;        |    sex VARCHAR,&#010;&gt;        |    age INT,&#010;&gt;        |    created_time TIMESTAMP(3),&#010;&gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;        |) WITH (&#010;&gt;        |    'connector' = 'kafka',&#010;&gt;        |     'topic' = 'user',&#010;&gt;        |    'properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;        |    'properties.group.id' = 'user_flink',&#010;&gt;        |    'scan.startup.mode' = 'latest-offset',&#010;&gt;        |    'format' = 'json',&#010;&gt;        |    'json.fail-on-missing-field' = 'false',&#010;&gt;        |    'json.ignore-parse-errors' = 'true'&#010;&gt;        |)&#010;&gt;        |\"\"\".stripMargin)&#010;&#010;&#010;",
        "depth": "1",
        "reply": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>"
    },
    {
        "id": "<646b3860.a313.1737bbdb425.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 12:54:56 GMT",
        "subject": "Re:Re: flink sql 1.11 kafka source with子句使用新参数，下游消费不到数据",
        "content": "当前作业有个sink connector消费不到数据，我找到原因了，根本原因是kafka中时间字段的问题，只是with子句新旧参数对相同的字段数据表现了不同的行为，kafka中的消息格式：&#010;&#010;&#010;{\"uid\":46,\"sex\":\"female\",\"age\":11,\"created_time\":\"2020-07-23T19:53:15.509Z\"}&#010;奇怪的是，在kafka_table DDL中，created_time 定义为TIMESTAMP(3)，with使用老参数是可以成功运行的，with使用新参数，在IDEA中运行没有任何异常，提交到yarn上，会报异常：&#010;java.lang.RuntimeException: RowTime field should not be null, please convert it to a non-nulllong&#010;value.&#010;    at org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.processElement(WatermarkAssignerOperator.java:115)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&#010;&#010;在本地用如下函数测试，结果确实是NULL&#010;TO_TIMESTAMP('2020-07-23T19:53:15.509Z')&#010;kafka producuer将created_time字段设置为整型，或者 “2020-07-23 20:36:55.565”，with使用新参数是没有问题的。调了一下午，调到怀疑人生，还好发现问题&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;&#010;在 2020-07-23 20:10:43，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;Hi&#010;&gt;&#010;&gt;你说的下游消费不到数据，这个下游是指当前作业消费不到数据吗？&#010;&gt;&#010;&gt;正常应该不会的，可以提供个可复现代码吗？ &#010;&gt;&#010;&gt;祝好&#010;&gt;Leonard Xu&#010;&gt;&#010;&gt;&#010;&gt;&gt; 在 2020年7月23日，18:13，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt; &#010;&gt;&gt; Hi all,&#010;&gt;&gt; &#010;&gt;&gt; 根据文档https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html#start-reading-position，&#010;&gt;&gt; 使用新参数创建kafka_table，下游消费不到数据，使用老参数下游可以消费到数据，是不是新参数的方式有坑啊&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt; 老参数:&#010;&gt;&gt;    streamTableEnv.executeSql(&#010;&gt;&gt;      \"\"\"&#010;&gt;&gt;        |&#010;&gt;&gt;        |CREATE TABLE kafka_table (&#010;&gt;&gt;        |    uid BIGINT,&#010;&gt;&gt;        |    sex VARCHAR,&#010;&gt;&gt;        |    age INT,&#010;&gt;&gt;        |    created_time TIMESTAMP(3),&#010;&gt;&gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;&gt;        |) WITH (&#010;&gt;&gt;        |&#010;&gt;&gt;        |     'connector.type' = 'kafka',&#010;&gt;&gt;        |    'connector.version' = 'universal',&#010;&gt;&gt;        |    'connector.topic' = 'user',&#010;&gt;&gt;        |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt;        |    'connector.properties.zookeeper.connect' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt;        |    'connector.properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt;        |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt;        |    'format.type' = 'json',&#010;&gt;&gt;        |    'format.derive-schema' = 'true'&#010;&gt;&gt;        |&#010;&gt;&gt;        |)&#010;&gt;&gt;        |\"\"\".stripMargin)&#010;&gt;&gt; &#010;&gt;&gt; 新参数：&#010;&gt;&gt; &#010;&gt;&gt;    streamTableEnv.executeSql(&#010;&gt;&gt;      \"\"\"&#010;&gt;&gt;        |&#010;&gt;&gt;        |CREATE TABLE kafka_table (&#010;&gt;&gt;        |&#010;&gt;&gt;        |    uid BIGINT,&#010;&gt;&gt;        |    sex VARCHAR,&#010;&gt;&gt;        |    age INT,&#010;&gt;&gt;        |    created_time TIMESTAMP(3),&#010;&gt;&gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;&gt;        |) WITH (&#010;&gt;&gt;        |    'connector' = 'kafka',&#010;&gt;&gt;        |     'topic' = 'user',&#010;&gt;&gt;        |    'properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt;        |    'properties.group.id' = 'user_flink',&#010;&gt;&gt;        |    'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt;        |    'format' = 'json',&#010;&gt;&gt;        |    'json.fail-on-missing-field' = 'false',&#010;&gt;&gt;        |    'json.ignore-parse-errors' = 'true'&#010;&gt;&gt;        |)&#010;&gt;&gt;        |\"\"\".stripMargin)&#010;",
        "depth": "2",
        "reply": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>"
    },
    {
        "id": "<5CAC99C8-21D8-4CE1-AA90-F76B5A8D2CED@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 13:23:28 GMT",
        "subject": "Re: flink sql 1.11 kafka source with子句使用新参数，下游消费不到数据",
        "content": "Hi&#010;&#010;这是1.11里的一个 json format t的不兼容改动[1]，目的是支持更多的 timestamp&#010;format 的解析，你可以把json-timestamp-format-standard &lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&gt;设置成&#010;“ISO-8601”，应该就不用改动了。&#010;&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/formats/json.html#json-timestamp-format-standard&gt;&#010;&#010;&gt; 在 2020年7月23日，20:54，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; 当前作业有个sink connector消费不到数据，我找到原因了，根本原因是kafka中时间字段的问题，只是with子句新旧参数对相同的字段数据表现了不同的行为，kafka中的消息格式：&#010;&gt; &#010;&gt; &#010;&gt; {\"uid\":46,\"sex\":\"female\",\"age\":11,\"created_time\":\"2020-07-23T19:53:15.509Z\"}&#010;&gt; 奇怪的是，在kafka_table DDL中，created_time 定义为TIMESTAMP(3)，with使用老参数是可以成功运行的，with使用新参数，在IDEA中运行没有任何异常，提交到yarn上，会报异常：&#010;&gt; java.lang.RuntimeException: RowTime field should not be null, please convert it to a&#010;non-nulllong value.&#010;&gt;    at org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.processElement(WatermarkAssignerOperator.java:115)&#010;~[flink-table-blink_2.11-1.11.0.jar:1.11.0]&#010;&gt; &#010;&gt; &#010;&gt; 在本地用如下函数测试，结果确实是NULL&#010;&gt; TO_TIMESTAMP('2020-07-23T19:53:15.509Z')&#010;&gt; kafka producuer将created_time字段设置为整型，或者 “2020-07-23 20:36:55.565”，with使用新参数是没有问题的。调了一下午，调到怀疑人生，还好发现问题&#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; &#010;&gt; 在 2020-07-23 20:10:43，\"Leonard Xu\" &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt;&gt; Hi&#010;&gt;&gt; &#010;&gt;&gt; 你说的下游消费不到数据，这个下游是指当前作业消费不到数据吗？&#010;&gt;&gt; &#010;&gt;&gt; 正常应该不会的，可以提供个可复现代码吗？ &#010;&gt;&gt; &#010;&gt;&gt; 祝好&#010;&gt;&gt; Leonard Xu&#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt;&gt;&gt; 在 2020年7月23日，18:13，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; Hi all,&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 根据文档https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html#start-reading-position，&#010;&gt;&gt;&gt; 使用新参数创建kafka_table，下游消费不到数据，使用老参数下游可以消费到数据，是不是新参数的方式有坑啊&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 老参数:&#010;&gt;&gt;&gt;   streamTableEnv.executeSql(&#010;&gt;&gt;&gt;     \"\"\"&#010;&gt;&gt;&gt;       |&#010;&gt;&gt;&gt;       |CREATE TABLE kafka_table (&#010;&gt;&gt;&gt;       |    uid BIGINT,&#010;&gt;&gt;&gt;       |    sex VARCHAR,&#010;&gt;&gt;&gt;       |    age INT,&#010;&gt;&gt;&gt;       |    created_time TIMESTAMP(3),&#010;&gt;&gt;&gt;       |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;&gt;&gt;       |) WITH (&#010;&gt;&gt;&gt;       |&#010;&gt;&gt;&gt;       |     'connector.type' = 'kafka',&#010;&gt;&gt;&gt;       |    'connector.version' = 'universal',&#010;&gt;&gt;&gt;       |    'connector.topic' = 'user',&#010;&gt;&gt;&gt;       |    'connector.startup-mode' = 'latest-offset',&#010;&gt;&gt;&gt;       |    'connector.properties.zookeeper.connect' = 'cdh1:2181,cdh2:2181,cdh3:2181',&#010;&gt;&gt;&gt;       |    'connector.properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt;&gt;       |    'connector.properties.group.id' = 'user_flink',&#010;&gt;&gt;&gt;       |    'format.type' = 'json',&#010;&gt;&gt;&gt;       |    'format.derive-schema' = 'true'&#010;&gt;&gt;&gt;       |&#010;&gt;&gt;&gt;       |)&#010;&gt;&gt;&gt;       |\"\"\".stripMargin)&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt; 新参数：&#010;&gt;&gt;&gt; &#010;&gt;&gt;&gt;   streamTableEnv.executeSql(&#010;&gt;&gt;&gt;     \"\"\"&#010;&gt;&gt;&gt;       |&#010;&gt;&gt;&gt;       |CREATE TABLE kafka_table (&#010;&gt;&gt;&gt;       |&#010;&gt;&gt;&gt;       |    uid BIGINT,&#010;&gt;&gt;&gt;       |    sex VARCHAR,&#010;&gt;&gt;&gt;       |    age INT,&#010;&gt;&gt;&gt;       |    created_time TIMESTAMP(3),&#010;&gt;&gt;&gt;       |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;&gt;&gt;       |) WITH (&#010;&gt;&gt;&gt;       |    'connector' = 'kafka',&#010;&gt;&gt;&gt;       |     'topic' = 'user',&#010;&gt;&gt;&gt;       |    'properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;&gt;&gt;       |    'properties.group.id' = 'user_flink',&#010;&gt;&gt;&gt;       |    'scan.startup.mode' = 'latest-offset',&#010;&gt;&gt;&gt;       |    'format' = 'json',&#010;&gt;&gt;&gt;       |    'json.fail-on-missing-field' = 'false',&#010;&gt;&gt;&gt;       |    'json.ignore-parse-errors' = 'true'&#010;&gt;&gt;&gt;       |)&#010;&gt;&gt;&gt;       |\"\"\".stripMargin)&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>"
    },
    {
        "id": "<6eb8fa6c.3780.1737eb0fec2.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 02:39:55 GMT",
        "subject": "Re:Re: flink sql 1.11 kafka source with子句使用新参数，下游消费不到数据",
        "content": "Hi,&#010;&#010;&#010;按照提示修改了，还是报错的：&#010;&#010;&#010;Query：&#010;&#010;&#010;       val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;        streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;        streamExecutionEnv.setStateBackend(new RocksDBStateBackend(\"hdfs://nameservice1/flink/checkpoints\"))&#010;&#010;        val blinkEnvSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;        val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv, blinkEnvSettings)&#010;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_MODE,CheckpointingMode.EXACTLY_ONCE)&#010;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,Duration.ofSeconds(20))&#010;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_TIMEOUT,Duration.ofSeconds(900))&#010;&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE kafka_table (&#010;        |    uid BIGINT,&#010;        |    sex VARCHAR,&#010;        |    age INT,&#010;        |    created_time TIMESTAMP(3),&#010;        |    procTime AS PROCTIME(),&#010;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;        |) WITH (&#010;        |    'connector' = 'kafka',&#010;        |    'topic' = 'user',&#010;        |    'properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;        |    'properties.group.id' = 'user_flink',&#010;        |    'scan.startup.mode' = 'latest-offset',&#010;        |    'format' = 'json',&#010;        |    'json.fail-on-missing-field' = 'false',&#010;        |    'json.ignore-parse-errors' = 'true',&#010;        |    'json.timestamp-format.standard' = 'ISO-8601'&#010;        |)&#010;        |\"\"\".stripMargin)&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |&#010;        |CREATE TABLE print_table&#010;        |(&#010;        |    uid BIGINT,&#010;        |    sex VARCHAR,&#010;        |    age INT,&#010;        |    created_time TIMESTAMP(3)&#010;        |)&#010;        |WITH ('connector' = 'print')&#010;        |&#010;        |&#010;        |\"\"\".stripMargin)&#010;&#010;    streamTableEnv.executeSql(&#010;      \"\"\"&#010;        |insert into print_table&#010;        |SELECT&#010;        |   uid,sex,age,created_time&#010;        |FROM  kafka_table&#010;        |&#010;        |\"\"\".stripMargin)&#010;&#010;&#010;堆栈：&#010;&#010;&#010;2020-07-2410:33:32,852INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser&#010;[] - Kafka startTimeMs: 1595558012852&#010;2020-07-2410:33:32,853INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.KafkaConsumer&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] Subscribed to partition(s):&#010;user-0&#010;2020-07-2410:33:32,853INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.KafkaConsumer&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] Seeking to offset 36627for&#010;partition user-0&#010;2020-07-2410:33:32,860INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.Metadata&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] ClusterID: cAT_xBISQNWghT9kR5UuIw&#010;2020-07-2410:33:32,871WARN  org.apache.flink.runtime.taskmanager.Task                    []&#010;- Source: TableSourceScan(table=[[default_catalog, default_database, kafka_table",
        "depth": "4",
        "reply": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>"
    },
    {
        "id": "<529D27FE-48C5-47BD-AA3C-E0A3E2AB57A7@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 03:48:08 GMT",
        "subject": "Re: flink sql 1.11 kafka source with子句使用新参数，下游消费不到数据",
        "content": "Hi&#010;&#010;\"2020-07-23T19:53:15.509Z” 是 RFC-3339 格式，这个格式是带zone的时间格式，对应的数据类型是&#010;timestamp with local zone，这个应该在1.12里支持了[1]&#010;1.10版本虽然是支持 RFC-3339 格式，但默认解析时区是有问题的，所以在1.11和1.12逐步中纠正了。&#010;&#010;在1.11版本中，如果json数据是RFC-3339格式，你可以把这个字段当成string读出来，在计算列中用个UDF自己解析到需要的timestamp。&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18296 &lt;https://issues.apache.org/jira/browse/FLINK-18296&gt;&#010;&#010;&gt; 在 2020年7月24日，10:39，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt; &#010;&gt; Hi,&#010;&gt; &#010;&gt; &#010;&gt; 按照提示修改了，还是报错的：&#010;&gt; &#010;&gt; &#010;&gt; Query：&#010;&gt; &#010;&gt; &#010;&gt;       val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;        streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;        streamExecutionEnv.setStateBackend(new RocksDBStateBackend(\"hdfs://nameservice1/flink/checkpoints\"))&#010;&gt; &#010;&gt;        val blinkEnvSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;        val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv, blinkEnvSettings)&#010;&gt;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_MODE,CheckpointingMode.EXACTLY_ONCE)&#010;&gt;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,Duration.ofSeconds(20))&#010;&gt;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_TIMEOUT,Duration.ofSeconds(900))&#010;&gt; &#010;&gt; &#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE kafka_table (&#010;&gt;        |    uid BIGINT,&#010;&gt;        |    sex VARCHAR,&#010;&gt;        |    age INT,&#010;&gt;        |    created_time TIMESTAMP(3),&#010;&gt;        |    procTime AS PROCTIME(),&#010;&gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;        |) WITH (&#010;&gt;        |    'connector' = 'kafka',&#010;&gt;        |    'topic' = 'user',&#010;&gt;        |    'properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;        |    'properties.group.id' = 'user_flink',&#010;&gt;        |    'scan.startup.mode' = 'latest-offset',&#010;&gt;        |    'format' = 'json',&#010;&gt;        |    'json.fail-on-missing-field' = 'false',&#010;&gt;        |    'json.ignore-parse-errors' = 'true',&#010;&gt;        |    'json.timestamp-format.standard' = 'ISO-8601'&#010;&gt;        |)&#010;&gt;        |\"\"\".stripMargin)&#010;&gt; &#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE print_table&#010;&gt;        |(&#010;&gt;        |    uid BIGINT,&#010;&gt;        |    sex VARCHAR,&#010;&gt;        |    age INT,&#010;&gt;        |    created_time TIMESTAMP(3)&#010;&gt;        |)&#010;&gt;        |WITH ('connector' = 'print')&#010;&gt;        |&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin)&#010;&gt; &#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |insert into print_table&#010;&gt;        |SELECT&#010;&gt;        |   uid,sex,age,created_time&#010;&gt;        |FROM  kafka_table&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin)&#010;&gt; &#010;&gt; &#010;&gt; 堆栈：&#010;&gt; &#010;&gt; &#010;&gt; 2020-07-2410:33:32,852INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser&#010;[] - Kafka startTimeMs: 1595558012852&#010;&gt; 2020-07-2410:33:32,853INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.KafkaConsumer&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] Subscribed to partition(s):&#010;user-0&#010;&gt; 2020-07-2410:33:32,853INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.KafkaConsumer&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] Seeking to offset 36627for&#010;partition user-0&#010;&gt; 2020-07-2410:33:32,860INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.Metadata&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] ClusterID: cAT_xBISQNWghT9kR5UuIw&#010;&gt; 2020-07-2410:33:32,871WARN  org.apache.flink.runtime.taskmanager.Task               &#010;    [] - Source: TableSourceScan(table=[[default_catalog, default_database, kafka_table",
        "depth": "5",
        "reply": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>"
    },
    {
        "id": "<34ffb1e2.ab2b.1737f042d4e.Coremail.wander669@163.com>",
        "from": "&quot;Zhou Zach&quot; &lt;wander...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 04:10:46 GMT",
        "subject": "回复：flink sql 1.11 kafka source with子句使用新参数，下游消费不到数据",
        "content": "Hi，&#010;感谢详细答疑！&#010;&#010;&#010;&#010;| |&#010;Zhou Zach&#010;|&#010;|&#010;邮箱：wander669@163.com&#010;|&#010;&#010;签名由 网易邮箱大师 定制&#010;&#010;在2020年07月24日 11:48，Leonard Xu 写道：&#010;Hi&#010;&#010;\"2020-07-23T19:53:15.509Z” 是 RFC-3339 格式，这个格式是带zone的时间格式，对应的数据类型是&#010;timestamp with local zone，这个应该在1.12里支持了[1]&#010;1.10版本虽然是支持 RFC-3339 格式，但默认解析时区是有问题的，所以在1.11和1.12逐步中纠正了。&#010;&#010;在1.11版本中，如果json数据是RFC-3339格式，你可以把这个字段当成string读出来，在计算列中用个UDF自己解析到需要的timestamp。&#010;&#010;Best&#010;Leonard Xu&#010;[1] https://issues.apache.org/jira/browse/FLINK-18296 &lt;https://issues.apache.org/jira/browse/FLINK-18296&amp;gt;&#010;&#010;&gt; 在 2020年7月24日，10:39，Zhou Zach &lt;wander669@163.com&gt; 写道：&#010;&gt;&#010;&gt; Hi,&#010;&gt;&#010;&gt;&#010;&gt; 按照提示修改了，还是报错的：&#010;&gt;&#010;&gt;&#010;&gt; Query：&#010;&gt;&#010;&gt;&#010;&gt;       val streamExecutionEnv = StreamExecutionEnvironment.getExecutionEnvironment&#010;&gt;        streamExecutionEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&#010;&gt;        streamExecutionEnv.setStateBackend(new RocksDBStateBackend(\"hdfs://nameservice1/flink/checkpoints\"))&#010;&gt;&#010;&gt;        val blinkEnvSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()&#010;&gt;        val streamTableEnv = StreamTableEnvironment.create(streamExecutionEnv, blinkEnvSettings)&#010;&gt;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_MODE,CheckpointingMode.EXACTLY_ONCE)&#010;&gt;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,Duration.ofSeconds(20))&#010;&gt;        streamTableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_TIMEOUT,Duration.ofSeconds(900))&#010;&gt;&#010;&gt;&#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE kafka_table (&#010;&gt;        |    uid BIGINT,&#010;&gt;        |    sex VARCHAR,&#010;&gt;        |    age INT,&#010;&gt;        |    created_time TIMESTAMP(3),&#010;&gt;        |    procTime AS PROCTIME(),&#010;&gt;        |    WATERMARK FOR created_time as created_time - INTERVAL '3' SECOND&#010;&gt;        |) WITH (&#010;&gt;        |    'connector' = 'kafka',&#010;&gt;        |    'topic' = 'user',&#010;&gt;        |    'properties.bootstrap.servers' = 'cdh1:9092,cdh2:9092,cdh3:9092',&#010;&gt;        |    'properties.group.id' = 'user_flink',&#010;&gt;        |    'scan.startup.mode' = 'latest-offset',&#010;&gt;        |    'format' = 'json',&#010;&gt;        |    'json.fail-on-missing-field' = 'false',&#010;&gt;        |    'json.ignore-parse-errors' = 'true',&#010;&gt;        |    'json.timestamp-format.standard' = 'ISO-8601'&#010;&gt;        |)&#010;&gt;        |\"\"\".stripMargin)&#010;&gt;&#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |&#010;&gt;        |CREATE TABLE print_table&#010;&gt;        |(&#010;&gt;        |    uid BIGINT,&#010;&gt;        |    sex VARCHAR,&#010;&gt;        |    age INT,&#010;&gt;        |    created_time TIMESTAMP(3)&#010;&gt;        |)&#010;&gt;        |WITH ('connector' = 'print')&#010;&gt;        |&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin)&#010;&gt;&#010;&gt;    streamTableEnv.executeSql(&#010;&gt;      \"\"\"&#010;&gt;        |insert into print_table&#010;&gt;        |SELECT&#010;&gt;        |   uid,sex,age,created_time&#010;&gt;        |FROM  kafka_table&#010;&gt;        |&#010;&gt;        |\"\"\".stripMargin)&#010;&gt;&#010;&gt;&#010;&gt; 堆栈：&#010;&gt;&#010;&gt;&#010;&gt; 2020-07-2410:33:32,852INFO  org.apache.flink.kafka.shaded.org.apache.kafka.common.utils.AppInfoParser&#010;[] - Kafka startTimeMs: 1595558012852&#010;&gt; 2020-07-2410:33:32,853INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.KafkaConsumer&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] Subscribed to partition(s):&#010;user-0&#010;&gt; 2020-07-2410:33:32,853INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.consumer.KafkaConsumer&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] Seeking to offset 36627for&#010;partition user-0&#010;&gt; 2020-07-2410:33:32,860INFO  org.apache.flink.kafka.shaded.org.apache.kafka.clients.Metadata&#010;[] - [Consumer clientId=consumer-user_flink-12, groupId=user_flink] ClusterID: cAT_xBISQNWghT9kR5UuIw&#010;&gt; 2020-07-2410:33:32,871WARN  org.apache.flink.runtime.taskmanager.Task               &#010;    [] - Source: TableSourceScan(table=[[default_catalog, default_database, kafka_table",
        "depth": "6",
        "reply": "<7a2f6b2b.92f0.1737b29e684.Coremail.wander669@163.com>"
    },
    {
        "id": "<BYAPR01MB429462170B08CEB632BD3800D4760@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 11:21:54 GMT",
        "subject": "flink 1.11 executeSql查询kafka表print没有输出",
        "content": "Hi, all:&#013;&#010;      本人当前使用flink版本1.11.0，但是在执行executeSql后，print时没有在console打印出结果（查看kafka是一直有数据产生的）,&#010;sql如下:&#013;&#010;&#013;&#010;&#013;&#010;StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);&#013;&#010;&#013;&#010;Catalog catalog = new HiveCatalog(\"x\", \"default\", \"D:\\\\conf\", \"1.1.0\");&#013;&#010;tEnv.registerCatalog(\"x\", catalog);&#013;&#010;&#013;&#010;TableResult execute = tEnv.executeSql(\"select * from x.ods.ods_binlog_test_trip_create_t_order_1\");&#013;&#010;&#013;&#010;execute.print();&#013;&#010;&#013;&#010;建表语句如下：&#013;&#010;&#013;&#010;CREATE TABLE x.ods.ods_binlog_test_trip_create_t_order_1 (&#013;&#010;  _type STRING,&#013;&#010;  order_no STRING,&#013;&#010;  order_time STRING,&#013;&#010;  dt as TO_TIMESTAMP(order_time),&#013;&#010;  proctime as PROCTIME(),&#013;&#010;  WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#013;&#010;) WITH (&#013;&#010;  'connector.type' = 'kafka',&#013;&#010;  'connector.properties.bootstrap.servers' = '***',&#013;&#010;  'connector.properties.zookeeper.connect' = '****',&#013;&#010;  'connector.version' = 'universal',&#013;&#010;  'format.type' = 'json',&#013;&#010;  'connector.properties.group.id' = 'testGroup',&#013;&#010;  'connector.startup-mode' = 'group-offsets',&#013;&#010;  'connector.topic' = 'test'&#013;&#010;)&#013;&#010;",
        "depth": "0",
        "reply": "<BYAPR01MB429462170B08CEB632BD3800D4760@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CADQYLGtu_1eikA3OYFmuXtQiW0RXCjCHN=J4BurQUEG_EEaZkg@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 11:24:17 GMT",
        "subject": "Re: flink 1.11 executeSql查询kafka表print没有输出",
        "content": "1.11的 TableResult.collect() 和 TableResult.print() 方法在流模式下，&#010;都是exactly once语义，需要配置checkpoint才能得到结果。&#010;&#010;Best，&#010;Godfrey&#010;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月23日周四 下午7:22写道：&#010;&#010;&gt; Hi, all:&#010;&gt;&#010;&gt; 本人当前使用flink版本1.11.0，但是在执行executeSql后，print时没有在console打印出结果（查看kafka是一直有数据产生的）,&#010;&gt; sql如下:&#010;&gt;&#010;&gt;&#010;&gt; StreamExecutionEnvironment env =&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#010;&gt; EnvironmentSettings settings =&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#010;&gt; StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);&#010;&gt;&#010;&gt; Catalog catalog = new HiveCatalog(\"x\", \"default\", \"D:\\\\conf\", \"1.1.0\");&#010;&gt; tEnv.registerCatalog(\"x\", catalog);&#010;&gt;&#010;&gt; TableResult execute = tEnv.executeSql(\"select * from&#010;&gt; x.ods.ods_binlog_test_trip_create_t_order_1\");&#010;&gt;&#010;&gt; execute.print();&#010;&gt;&#010;&gt; 建表语句如下：&#010;&gt;&#010;&gt; CREATE TABLE x.ods.ods_binlog_test_trip_create_t_order_1 (&#010;&gt;   _type STRING,&#010;&gt;   order_no STRING,&#010;&gt;   order_time STRING,&#010;&gt;   dt as TO_TIMESTAMP(order_time),&#010;&gt;   proctime as PROCTIME(),&#010;&gt;   WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#010;&gt; ) WITH (&#010;&gt;   'connector.type' = 'kafka',&#010;&gt;   'connector.properties.bootstrap.servers' = '***',&#010;&gt;   'connector.properties.zookeeper.connect' = '****',&#010;&gt;   'connector.version' = 'universal',&#010;&gt;   'format.type' = 'json',&#010;&gt;   'connector.properties.group.id' = 'testGroup',&#010;&gt;   'connector.startup-mode' = 'group-offsets',&#010;&gt;   'connector.topic' = 'test'&#010;&gt; )&#010;&gt;&#010;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB429462170B08CEB632BD3800D4760@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<BYAPR01MB42949B091D6B0B00184BEECAD4760@BYAPR01MB4294.prod.exchangelabs.com>",
        "from": "&quot;wind.fly.vip@outlook.com&quot; &lt;wind.fly....@outlook.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 11:33:59 GMT",
        "subject": "回复: flink 1.11 executeSql查询kafka表print没有输出",
        "content": "Hi,Godfrey：&#013;&#010;     加了checkpoint后确实可以了，能具体讲一下原理吗？print是在完成快照的时候顺便把结果输出了吗？或者有没有相关文档？&#013;&#010;&#013;&#010;Best,&#013;&#010;Junbao Zhang&#013;&#010;________________________________&#013;&#010;发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;发送时间: 2020年7月23日 19:24&#013;&#010;收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;主题: Re: flink 1.11 executeSql查询kafka表print没有输出&#013;&#010;&#013;&#010;1.11的 TableResult.collect() 和 TableResult.print() 方法在流模式下，&#013;&#010;都是exactly once语义，需要配置checkpoint才能得到结果。&#013;&#010;&#013;&#010;Best，&#013;&#010;Godfrey&#013;&#010;&#013;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月23日周四 下午7:22写道：&#013;&#010;&#013;&#010;&gt; Hi, all:&#013;&#010;&gt;&#013;&#010;&gt; 本人当前使用flink版本1.11.0，但是在执行executeSql后，print时没有在console打印出结果（查看kafka是一直有数据产生的）,&#013;&#010;&gt; sql如下:&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; StreamExecutionEnvironment env =&#013;&#010;&gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; EnvironmentSettings settings =&#013;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt; StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, settings);&#013;&#010;&gt;&#013;&#010;&gt; Catalog catalog = new HiveCatalog(\"x\", \"default\", \"D:\\\\conf\", \"1.1.0\");&#013;&#010;&gt; tEnv.registerCatalog(\"x\", catalog);&#013;&#010;&gt;&#013;&#010;&gt; TableResult execute = tEnv.executeSql(\"select * from&#013;&#010;&gt; x.ods.ods_binlog_test_trip_create_t_order_1\");&#013;&#010;&gt;&#013;&#010;&gt; execute.print();&#013;&#010;&gt;&#013;&#010;&gt; 建表语句如下：&#013;&#010;&gt;&#013;&#010;&gt; CREATE TABLE x.ods.ods_binlog_test_trip_create_t_order_1 (&#013;&#010;&gt;   _type STRING,&#013;&#010;&gt;   order_no STRING,&#013;&#010;&gt;   order_time STRING,&#013;&#010;&gt;   dt as TO_TIMESTAMP(order_time),&#013;&#010;&gt;   proctime as PROCTIME(),&#013;&#010;&gt;   WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#013;&#010;&gt; ) WITH (&#013;&#010;&gt;   'connector.type' = 'kafka',&#013;&#010;&gt;   'connector.properties.bootstrap.servers' = '***',&#013;&#010;&gt;   'connector.properties.zookeeper.connect' = '****',&#013;&#010;&gt;   'connector.version' = 'universal',&#013;&#010;&gt;   'format.type' = 'json',&#013;&#010;&gt;   'connector.properties.group.id' = 'testGroup',&#013;&#010;&gt;   'connector.startup-mode' = 'group-offsets',&#013;&#010;&gt;   'connector.topic' = 'test'&#013;&#010;&gt; )&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<BYAPR01MB429462170B08CEB632BD3800D4760@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CADQYLGsD8fsE7UtXpZfRxHyJY6QYN6CBP9CSGnMGdFQfgGMo=A@mail.gmail.com>",
        "from": "godfrey he &lt;godfre...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 12:05:31 GMT",
        "subject": "Re: flink 1.11 executeSql查询kafka表print没有输出",
        "content": "client端会不断的pull sink产生的数据，但是只有等checkpoint完成后，其对应的数据才能&#010;collect() 和 print()&#013;&#010;返回。&#013;&#010;这是为了保证exactly once语义。&#013;&#010;在1.12里，同时支持了at least once 和 exactly once 语义。默认情况下是 at&#010;least once，collect()&#013;&#010;和 print() 的结果可能有重复。&#013;&#010;如果有兴趣可以参考pr：https://github.com/apache/flink/pull/12867&#013;&#010;&lt;https://github.com/apache/flink/pull/12867#event-3578490750&gt;&#013;&#010;&#013;&#010;Best,&#013;&#010;Godfrey&#013;&#010;&#013;&#010;wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月23日周四 下午7:34写道：&#013;&#010;&#013;&#010;&gt; Hi,Godfrey：&#013;&#010;&gt;      加了checkpoint后确实可以了，能具体讲一下原理吗？print是在完成快照的时候顺便把结果输出了吗？或者有没有相关文档？&#013;&#010;&gt;&#013;&#010;&gt; Best,&#013;&#010;&gt; Junbao Zhang&#013;&#010;&gt; ________________________________&#013;&#010;&gt; 发件人: godfrey he &lt;godfreyhe@gmail.com&gt;&#013;&#010;&gt; 发送时间: 2020年7月23日 19:24&#013;&#010;&gt; 收件人: user-zh &lt;user-zh@flink.apache.org&gt;&#013;&#010;&gt; 主题: Re: flink 1.11 executeSql查询kafka表print没有输出&#013;&#010;&gt;&#013;&#010;&gt; 1.11的 TableResult.collect() 和 TableResult.print() 方法在流模式下，&#013;&#010;&gt; 都是exactly once语义，需要配置checkpoint才能得到结果。&#013;&#010;&gt;&#013;&#010;&gt; Best，&#013;&#010;&gt; Godfrey&#013;&#010;&gt;&#013;&#010;&gt; wind.fly.vip@outlook.com &lt;wind.fly.vip@outlook.com&gt; 于2020年7月23日周四&#013;&#010;&gt; 下午7:22写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi, all:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; 本人当前使用flink版本1.11.0，但是在执行executeSql后，print时没有在console打印出结果（查看kafka是一直有数据产生的）,&#013;&#010;&gt; &gt; sql如下:&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; StreamExecutionEnvironment env =&#013;&#010;&gt; &gt; StreamExecutionEnvironment.getExecutionEnvironment();&#013;&#010;&gt; &gt; EnvironmentSettings settings =&#013;&#010;&gt; &gt;&#013;&#010;&gt; EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();&#013;&#010;&gt; &gt; StreamTableEnvironment tEnv = StreamTableEnvironment.create(env,&#013;&#010;&gt; settings);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Catalog catalog = new HiveCatalog(\"x\", \"default\", \"D:\\\\conf\", \"1.1.0\");&#013;&#010;&gt; &gt; tEnv.registerCatalog(\"x\", catalog);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; TableResult execute = tEnv.executeSql(\"select * from&#013;&#010;&gt; &gt; x.ods.ods_binlog_test_trip_create_t_order_1\");&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; execute.print();&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 建表语句如下：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; CREATE TABLE x.ods.ods_binlog_test_trip_create_t_order_1 (&#013;&#010;&gt; &gt;   _type STRING,&#013;&#010;&gt; &gt;   order_no STRING,&#013;&#010;&gt; &gt;   order_time STRING,&#013;&#010;&gt; &gt;   dt as TO_TIMESTAMP(order_time),&#013;&#010;&gt; &gt;   proctime as PROCTIME(),&#013;&#010;&gt; &gt;   WATERMARK FOR dt AS dt - INTERVAL '5' SECOND&#013;&#010;&gt; &gt; ) WITH (&#013;&#010;&gt; &gt;   'connector.type' = 'kafka',&#013;&#010;&gt; &gt;   'connector.properties.bootstrap.servers' = '***',&#013;&#010;&gt; &gt;   'connector.properties.zookeeper.connect' = '****',&#013;&#010;&gt; &gt;   'connector.version' = 'universal',&#013;&#010;&gt; &gt;   'format.type' = 'json',&#013;&#010;&gt; &gt;   'connector.properties.group.id' = 'testGroup',&#013;&#010;&gt; &gt;   'connector.startup-mode' = 'group-offsets',&#013;&#010;&gt; &gt;   'connector.topic' = 'test'&#013;&#010;&gt; &gt; )&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<BYAPR01MB429462170B08CEB632BD3800D4760@BYAPR01MB4294.prod.exchangelabs.com>"
    },
    {
        "id": "<CACYTR1Ax-PfpuE9QJZF5q3USChaSqW8ZAj6h89dGY9JNiofrSQ@mail.gmail.com>",
        "from": "Fisher Xiang &lt;fisherxia...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 14:14:45 GMT",
        "subject": "自定义metrics reporter 如何不通过flink conf来注册并生效",
        "content": "Hi all,&#013;&#010;&#013;&#010;请问实现了 MetricReporter, CharacterFilter，Scheduled, Reporter 的自定义&#013;&#010;customerReporter 如何*能在 代码env里面注册并实现metric上报*，要求不在flink&#010;conf.xml 文件里面配置&#013;&#010;该customerReporter的信息？&#013;&#010;&#013;&#010;需求：在自定义的source 和sink等算子里面计算处理成功，失败的数据条数并通过自定义reporter上报，并且该reporter需要是通用型的即&#013;&#010;*适用于多个flink&#013;&#010;任务*从而避开重复造轮子。&#013;&#010;&#013;&#010;thx&#013;&#010;&#013;&#010;BR&#013;&#010;Fisher&#013;&#010;",
        "depth": "0",
        "reply": "<CACYTR1Ax-PfpuE9QJZF5q3USChaSqW8ZAj6h89dGY9JNiofrSQ@mail.gmail.com>"
    },
    {
        "id": "<CAA8tFvtYbTqTM-TjhGecZoWbbgmBvRM9ntc=Ej27LRzCokLsQQ@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 11:34:55 GMT",
        "subject": "Re: 自定义metrics reporter 如何不通过flink conf来注册并生效",
        "content": "Hi  Fisher&#013;&#010;   尝试理解一下你的需求，你自己实现了一个 report，也希望在 source&#010;和 sink 中计算一些 metric，希望把 source 和&#013;&#010;sink 的这些 metric 通过自定义的 report 上报到你指定的地方。然后不希望在&#010;env 里面配置 report 的信息，是这样吗？&#013;&#010;能否解释下为什么不希望在 flink-conf 中进行配置，而是希望在 env 中进行配置吗&#013;&#010;&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;Fisher Xiang &lt;fisherxiang6@gmail.com&gt; 于2020年7月23日周四 下午10:16写道：&#013;&#010;&#013;&#010;&gt; Hi all,&#013;&#010;&gt;&#013;&#010;&gt; 请问实现了 MetricReporter, CharacterFilter，Scheduled, Reporter 的自定义&#013;&#010;&gt; customerReporter 如何*能在 代码env里面注册并实现metric上报*，要求不在flink&#010;conf.xml 文件里面配置&#013;&#010;&gt; 该customerReporter的信息？&#013;&#010;&gt;&#013;&#010;&gt; 需求：在自定义的source 和sink等算子里面计算处理成功，失败的数据条数并通过自定义reporter上报，并且该reporter需要是通用型的即&#013;&#010;&gt; *适用于多个flink&#013;&#010;&gt; 任务*从而避开重复造轮子。&#013;&#010;&gt;&#013;&#010;&gt; thx&#013;&#010;&gt;&#013;&#010;&gt; BR&#013;&#010;&gt; Fisher&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CACYTR1Ax-PfpuE9QJZF5q3USChaSqW8ZAj6h89dGY9JNiofrSQ@mail.gmail.com>"
    },
    {
        "id": "<tencent_C39B02E143CD4937A4BD056AAC5F64740008@qq.com>",
        "from": "&quot;忝忝向仧&quot; &lt;153488...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Thu, 23 Jul 2020 14:50:47 GMT",
        "subject": "Flink的broadcast",
        "content": "Hi,all:&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;请教下，flink在生产里面一般多大的数据适合放入broadcastStream里面发到各个下游算子，有没一个生产级别的衡量指标.&#013;&#010;感觉这个功能跟hive里面的map join很类似.&#013;&#010;谢谢.",
        "depth": "0",
        "reply": "<tencent_C39B02E143CD4937A4BD056AAC5F64740008@qq.com>"
    },
    {
        "id": "<CAA8tFvvH7Mo23wyMUgZRFzBNQRHhAA7sMJ+fUro5+0jJHYSYyA@mail.gmail.com>",
        "from": "Congxian Qiu &lt;qcx978132...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Sat, 25 Jul 2020 11:25:31 GMT",
        "subject": "Re: Flink的broadcast",
        "content": "Hi&#013;&#010;    BroadcastStream 的数据后面会保存在 broadcast state 中，这个需要你考虑&#010;broadcast state&#013;&#010;消耗的内存大小（现在 broadcast state 都保存在 内存中），如果太大的话会消耗太多内存的。&#013;&#010;Best,&#013;&#010;Congxian&#013;&#010;&#013;&#010;&#013;&#010;忝忝向仧 &lt;153488125@qq.com&gt; 于2020年7月23日周四 下午10:51写道：&#013;&#010;&#013;&#010;&gt; Hi,all:&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 请教下，flink在生产里面一般多大的数据适合放入broadcastStream里面发到各个下游算子，有没一个生产级别的衡量指标.&#013;&#010;&gt; 感觉这个功能跟hive里面的map join很类似.&#013;&#010;&gt; 谢谢.&#013;&#010;",
        "depth": "1",
        "reply": "<tencent_C39B02E143CD4937A4BD056AAC5F64740008@qq.com>"
    },
    {
        "id": "<CAEZk040JahZY01xVmfJZdM8NcK51SgeHVjMxBtWrGAgCepYYGw@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 02:49:43 GMT",
        "subject": "flink1.11日志上报",
        "content": "hi、&#013;&#010;我这面想实现一个日志上报的功能，就是flink任务启动后，让flink主动将当前任务日志打到外部存储系统，想问一下flink有对应的接口吗，具体要实现哪一个类哪&#013;&#010;",
        "depth": "0",
        "reply": "<CAEZk040JahZY01xVmfJZdM8NcK51SgeHVjMxBtWrGAgCepYYGw@mail.gmail.com>"
    },
    {
        "id": "<tencent_463C705F84CEFC89CD42DBA9AF3D77448E05@qq.com>",
        "from": "&quot;Cayden chen&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 02:52:59 GMT",
        "subject": "回复：flink1.11日志上报",
        "content": "我们的获取逻辑是通过自定义 logback的appder( flink 默认的应该是log4j，对应配置在安装目录的conf下面)，appder通过解析当前系统路径(因为flink每个taskmanager会自己定义一个带有applicationId的路径，然后里面会放各种jar包，包括我自定义的appder)，获取之后通过MDC.put(),给日志加一列appId，在appder里面把日志上报到外部的日志系统&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;发件人:                                                                               &#010;                                        \"user-zh\"                                        &#010;                                           &lt;zhangyu@akulaku.com&amp;gt;;&#013;&#010;发送时间:&amp;nbsp;2020年7月24日(星期五) 上午10:49&#013;&#010;收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&#013;&#010;主题:&amp;nbsp;flink1.11日志上报&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;hi、&#013;&#010;我这面想实现一个日志上报的功能，就是flink任务启动后，让flink主动将当前任务日志打到外部存储系统，想问一下flink有对应的接口吗，具体要实现哪一个类哪",
        "depth": "1",
        "reply": "<CAEZk040JahZY01xVmfJZdM8NcK51SgeHVjMxBtWrGAgCepYYGw@mail.gmail.com>"
    },
    {
        "id": "<CAEZk0427oHpxsONFqMRHnxB1n3QtKZo0s6KG3hOktdEa0kiwHw@mail.gmail.com>",
        "from": "Dream-底限 &lt;zhan...@akulaku.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 03:12:44 GMT",
        "subject": "Re: flink1.11日志上报",
        "content": "hi Cayden chen、&#013;&#010;也就是说你们日志上报的实现方式是实现自定义appder来实现是吧，这确实是一个不错的方式；&#013;&#010;我先前看spark可以实现对应的listener用来实现日志上报，查看了一下flink&#013;&#010;api貌似也有对应listen，具体是实现哪一个还不知道，现在我们还处在一个功能整理阶段&#013;&#010;&#013;&#010;&#013;&#010;Cayden chen &lt;1193216154@qq.com&gt; 于2020年7月24日周五 上午10:53写道：&#013;&#010;&#013;&#010;&gt; 我们的获取逻辑是通过自定义 logback的appder( flink&#013;&#010;&gt; 默认的应该是log4j，对应配置在安装目录的conf下面)，appder通过解析当前系统路径(因为flink每个taskmanager会自己定义一个带有applicationId的路径，然后里面会放各种jar包，包括我自定义的appder)，获取之后通过MDC.put(),给日志加一列appId，在appder里面把日志上报到外部的日志系统&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; ------------------&amp;nbsp;原始邮件&amp;nbsp;------------------&#013;&#010;&gt; 发件人:&#013;&#010;&gt;                                                   \"user-zh\"&#013;&#010;&gt;                                                                     &lt;&#013;&#010;&gt; zhangyu@akulaku.com&amp;gt;;&#013;&#010;&gt; 发送时间:&amp;nbsp;2020年7月24日(星期五) 上午10:49&#013;&#010;&gt; 收件人:&amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;&gt;&#013;&#010;&gt; 主题:&amp;nbsp;flink1.11日志上报&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; hi、&#013;&#010;&gt;&#013;&#010;&gt; 我这面想实现一个日志上报的功能，就是flink任务启动后，让flink主动将当前任务日志打到外部存储系统，想问一下flink有对应的接口吗，具体要实现哪一个类哪&#013;&#010;",
        "depth": "2",
        "reply": "<CAEZk040JahZY01xVmfJZdM8NcK51SgeHVjMxBtWrGAgCepYYGw@mail.gmail.com>"
    },
    {
        "id": "<CAJkeMpgFkcN+Nkm5qSB6JSLZWx07unYfJtRi1dndFN++ZMqe-g@mail.gmail.com>",
        "from": "zilong xiao &lt;acidzz...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 03:51:02 GMT",
        "subject": "Re: flink1.11日志上报",
        "content": "这个可以用配置文件实现，利用kafka&#013;&#010;appender将日志打到kafka中，然后自己去消费kafka处理即可，1.11中支持log4j2了，建议使用log4j2&#013;&#010;&#013;&#010;Dream-底限 &lt;zhangyu@akulaku.com&gt; 于2020年7月24日周五 上午10:50写道：&#013;&#010;&#013;&#010;&gt; hi、&#013;&#010;&gt;&#013;&#010;&gt; 我这面想实现一个日志上报的功能，就是flink任务启动后，让flink主动将当前任务日志打到外部存储系统，想问一下flink有对应的接口吗，具体要实现哪一个类哪&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<CAEZk040JahZY01xVmfJZdM8NcK51SgeHVjMxBtWrGAgCepYYGw@mail.gmail.com>"
    },
    {
        "id": "<CABz5Tvm-SO1CrTAgctX8-iJfW1im17KTMFPr-zG0cwnGY4+mkw@mail.gmail.com>",
        "from": "杨荣 &lt;samyang31...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 02:52:47 GMT",
        "subject": "关于 sql-client",
        "content": "Hi all,&#013;&#010;&#013;&#010;请问：&#013;&#010;1. 在 Embedded mode 下，支持 ClusterClient 进行 job&#013;&#010;提交作业，进行分布式计算吗？在文档中没看到，跟着文档走，只启起了&#010;Local 在本地作业，无法运用到生产环境。&#013;&#010;&#013;&#010;2. GateWay mode 预计在那个版本 release？&#013;&#010;",
        "depth": "0",
        "reply": "<CABz5Tvm-SO1CrTAgctX8-iJfW1im17KTMFPr-zG0cwnGY4+mkw@mail.gmail.com>"
    },
    {
        "id": "<CAHOPYeU87VRnxqiELz05cNNB7oJ4nFskPycmJN0V7B1n=mNrOA@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 03:41:50 GMT",
        "subject": "Re: 关于 sql-client",
        "content": "1 应该是可以的   主要是你要在flink-conf.yaml里面配置正确的 jobmanager.rpc.address&#013;&#010;源码里面有加载主配置文件的逻辑&#013;&#010;&#013;&#010;public LocalExecutor(URL defaultEnv, List&lt;URL&gt; jars, List&lt;URL&gt; libraries) {&#013;&#010;   // discover configuration&#013;&#010;   final String flinkConfigDir;&#013;&#010;   try {&#013;&#010;      // find the configuration directory&#013;&#010;      flinkConfigDir = CliFrontend.getConfigurationDirectoryFromEnv();&#013;&#010;&#013;&#010;      // load the global configuration&#013;&#010;      this.flinkConfig = GlobalConfiguration.loadConfiguration(flinkConfigDir);&#013;&#010;&#013;&#010;      // initialize default file system&#013;&#010;      FileSystem.initialize(flinkConfig,&#013;&#010;PluginUtils.createPluginManagerFromRootFolder(flinkConfig));&#013;&#010;&#013;&#010;      // load command lines for deployment&#013;&#010;      this.commandLines =&#013;&#010;CliFrontend.loadCustomCommandLines(flinkConfig, flinkConfigDir);&#013;&#010;      this.commandLineOptions = collectCommandLineOptions(commandLines);&#013;&#010;   } catch (Exception e) {&#013;&#010;      throw new SqlClientException(\"Could not load Flink configuration.\", e);&#013;&#010;   }&#013;&#010;&#013;&#010;&#013;&#010;2  因为等不及官方的  我们自己wrapper实现了一个&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 上午10:53写道：&#013;&#010;&#013;&#010;&gt; Hi all,&#013;&#010;&gt;&#013;&#010;&gt; 请问：&#013;&#010;&gt; 1. 在 Embedded mode 下，支持 ClusterClient 进行 job&#013;&#010;&gt; 提交作业，进行分布式计算吗？在文档中没看到，跟着文档走，只启起了&#010;Local 在本地作业，无法运用到生产环境。&#013;&#010;&gt;&#013;&#010;&gt; 2. GateWay mode 预计在那个版本 release？&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "1",
        "reply": "<CABz5Tvm-SO1CrTAgctX8-iJfW1im17KTMFPr-zG0cwnGY4+mkw@mail.gmail.com>"
    },
    {
        "id": "<CABz5TvnRAzfSLw5ewh7C+Cb2qh+g_e3mtsBsnzPBezaLsM8Y0A@mail.gmail.com>",
        "from": "杨荣 &lt;samyang31...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 07:18:46 GMT",
        "subject": "Re: 关于 sql-client",
        "content": "你们可以 pr 到官方啊。我觉得这个功能很刚需啊，并且 basic 版的 1.5&#010;就 release 了，不知道为什么相关 gate way 或者&#013;&#010;submit with sql file 的 feature 到现在都还没实现呢。&#013;&#010;&#013;&#010;Harold.Miao &lt;miaohonghit@gmail.com&gt; 于2020年7月24日周五 上午11:42写道：&#013;&#010;&#013;&#010;&gt; 1 应该是可以的   主要是你要在flink-conf.yaml里面配置正确的 jobmanager.rpc.address&#013;&#010;&gt; 源码里面有加载主配置文件的逻辑&#013;&#010;&gt;&#013;&#010;&gt; public LocalExecutor(URL defaultEnv, List&lt;URL&gt; jars, List&lt;URL&gt; libraries)&#010;{&#013;&#010;&gt;    // discover configuration&#013;&#010;&gt;    final String flinkConfigDir;&#013;&#010;&gt;    try {&#013;&#010;&gt;       // find the configuration directory&#013;&#010;&gt;       flinkConfigDir = CliFrontend.getConfigurationDirectoryFromEnv();&#013;&#010;&gt;&#013;&#010;&gt;       // load the global configuration&#013;&#010;&gt;       this.flinkConfig =&#013;&#010;&gt; GlobalConfiguration.loadConfiguration(flinkConfigDir);&#013;&#010;&gt;&#013;&#010;&gt;       // initialize default file system&#013;&#010;&gt;       FileSystem.initialize(flinkConfig,&#013;&#010;&gt; PluginUtils.createPluginManagerFromRootFolder(flinkConfig));&#013;&#010;&gt;&#013;&#010;&gt;       // load command lines for deployment&#013;&#010;&gt;       this.commandLines =&#013;&#010;&gt; CliFrontend.loadCustomCommandLines(flinkConfig, flinkConfigDir);&#013;&#010;&gt;       this.commandLineOptions = collectCommandLineOptions(commandLines);&#013;&#010;&gt;    } catch (Exception e) {&#013;&#010;&gt;       throw new SqlClientException(\"Could not load Flink configuration.\",&#013;&#010;&gt; e);&#013;&#010;&gt;    }&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 2  因为等不及官方的  我们自己wrapper实现了一个&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 上午10:53写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; Hi all,&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 请问：&#013;&#010;&gt; &gt; 1. 在 Embedded mode 下，支持 ClusterClient 进行 job&#013;&#010;&gt; &gt; 提交作业，进行分布式计算吗？在文档中没看到，跟着文档走，只启起了&#010;Local 在本地作业，无法运用到生产环境。&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 2. GateWay mode 预计在那个版本 release？&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; --&#013;&#010;&gt;&#013;&#010;&gt; Best Regards,&#013;&#010;&gt; Harold Miao&#013;&#010;&gt;&#013;&#010;",
        "depth": "2",
        "reply": "<CABz5Tvm-SO1CrTAgctX8-iJfW1im17KTMFPr-zG0cwnGY4+mkw@mail.gmail.com>"
    },
    {
        "id": "<CAADy7x6r+gZXmard3ihKZZVG7S0fgBmY8PQFaTceSx7Xwz+0rw@mail.gmail.com>",
        "from": "Jeff Zhang &lt;zjf...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 07:46:47 GMT",
        "subject": "Re: 关于 sql-client",
        "content": "可以用zeppelin来提交flink sql作业，可以加入钉钉群讨论：32803524&#013;&#010;&#013;&#010;杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 下午3:19写道：&#013;&#010;&#013;&#010;&gt; 你们可以 pr 到官方啊。我觉得这个功能很刚需啊，并且 basic 版的&#010;1.5 就 release 了，不知道为什么相关 gate way 或者&#013;&#010;&gt; submit with sql file 的 feature 到现在都还没实现呢。&#013;&#010;&gt;&#013;&#010;&gt; Harold.Miao &lt;miaohonghit@gmail.com&gt; 于2020年7月24日周五 上午11:42写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 1 应该是可以的   主要是你要在flink-conf.yaml里面配置正确的 jobmanager.rpc.address&#013;&#010;&gt; &gt; 源码里面有加载主配置文件的逻辑&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; public LocalExecutor(URL defaultEnv, List&lt;URL&gt; jars, List&lt;URL&gt;&#013;&#010;&gt; libraries) {&#013;&#010;&gt; &gt;    // discover configuration&#013;&#010;&gt; &gt;    final String flinkConfigDir;&#013;&#010;&gt; &gt;    try {&#013;&#010;&gt; &gt;       // find the configuration directory&#013;&#010;&gt; &gt;       flinkConfigDir = CliFrontend.getConfigurationDirectoryFromEnv();&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // load the global configuration&#013;&#010;&gt; &gt;       this.flinkConfig =&#013;&#010;&gt; &gt; GlobalConfiguration.loadConfiguration(flinkConfigDir);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // initialize default file system&#013;&#010;&gt; &gt;       FileSystem.initialize(flinkConfig,&#013;&#010;&gt; &gt; PluginUtils.createPluginManagerFromRootFolder(flinkConfig));&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // load command lines for deployment&#013;&#010;&gt; &gt;       this.commandLines =&#013;&#010;&gt; &gt; CliFrontend.loadCustomCommandLines(flinkConfig, flinkConfigDir);&#013;&#010;&gt; &gt;       this.commandLineOptions = collectCommandLineOptions(commandLines);&#013;&#010;&gt; &gt;    } catch (Exception e) {&#013;&#010;&gt; &gt;       throw new SqlClientException(\"Could not load Flink configuration.\",&#013;&#010;&gt; &gt; e);&#013;&#010;&gt; &gt;    }&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 2  因为等不及官方的  我们自己wrapper实现了一个&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 上午10:53写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi all,&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 请问：&#013;&#010;&gt; &gt; &gt; 1. 在 Embedded mode 下，支持 ClusterClient 进行 job&#013;&#010;&gt; &gt; &gt; 提交作业，进行分布式计算吗？在文档中没看到，跟着文档走，只启起了&#010;Local 在本地作业，无法运用到生产环境。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 2. GateWay mode 预计在那个版本 release？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best Regards,&#013;&#010;&gt; &gt; Harold Miao&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;Best Regards&#013;&#010;&#013;&#010;Jeff Zhang&#013;&#010;",
        "depth": "3",
        "reply": "<CABz5Tvm-SO1CrTAgctX8-iJfW1im17KTMFPr-zG0cwnGY4+mkw@mail.gmail.com>"
    },
    {
        "id": "<CAHOPYeWLhKezGQeSxzPu7yRON_hDcJYY0me59fbJMtrzhRB3vg@mail.gmail.com>",
        "from": "&quot;Harold.Miao&quot; &lt;miaohong...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 10:15:42 GMT",
        "subject": "Re: 关于 sql-client",
        "content": "这个呢  https://github.com/ververica/flink-sql-gateway&#013;&#010;&#013;&#010;杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 下午3:19写道：&#013;&#010;&#013;&#010;&gt; 你们可以 pr 到官方啊。我觉得这个功能很刚需啊，并且 basic 版的&#010;1.5 就 release 了，不知道为什么相关 gate way 或者&#013;&#010;&gt; submit with sql file 的 feature 到现在都还没实现呢。&#013;&#010;&gt;&#013;&#010;&gt; Harold.Miao &lt;miaohonghit@gmail.com&gt; 于2020年7月24日周五 上午11:42写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 1 应该是可以的   主要是你要在flink-conf.yaml里面配置正确的 jobmanager.rpc.address&#013;&#010;&gt; &gt; 源码里面有加载主配置文件的逻辑&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; public LocalExecutor(URL defaultEnv, List&lt;URL&gt; jars, List&lt;URL&gt;&#013;&#010;&gt; libraries) {&#013;&#010;&gt; &gt;    // discover configuration&#013;&#010;&gt; &gt;    final String flinkConfigDir;&#013;&#010;&gt; &gt;    try {&#013;&#010;&gt; &gt;       // find the configuration directory&#013;&#010;&gt; &gt;       flinkConfigDir = CliFrontend.getConfigurationDirectoryFromEnv();&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // load the global configuration&#013;&#010;&gt; &gt;       this.flinkConfig =&#013;&#010;&gt; &gt; GlobalConfiguration.loadConfiguration(flinkConfigDir);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // initialize default file system&#013;&#010;&gt; &gt;       FileSystem.initialize(flinkConfig,&#013;&#010;&gt; &gt; PluginUtils.createPluginManagerFromRootFolder(flinkConfig));&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // load command lines for deployment&#013;&#010;&gt; &gt;       this.commandLines =&#013;&#010;&gt; &gt; CliFrontend.loadCustomCommandLines(flinkConfig, flinkConfigDir);&#013;&#010;&gt; &gt;       this.commandLineOptions = collectCommandLineOptions(commandLines);&#013;&#010;&gt; &gt;    } catch (Exception e) {&#013;&#010;&gt; &gt;       throw new SqlClientException(\"Could not load Flink configuration.\",&#013;&#010;&gt; &gt; e);&#013;&#010;&gt; &gt;    }&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 2  因为等不及官方的  我们自己wrapper实现了一个&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 上午10:53写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi all,&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 请问：&#013;&#010;&gt; &gt; &gt; 1. 在 Embedded mode 下，支持 ClusterClient 进行 job&#013;&#010;&gt; &gt; &gt; 提交作业，进行分布式计算吗？在文档中没看到，跟着文档走，只启起了&#010;Local 在本地作业，无法运用到生产环境。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 2. GateWay mode 预计在那个版本 release？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best Regards,&#013;&#010;&gt; &gt; Harold Miao&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010;&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best Regards,&#013;&#010;Harold Miao&#013;&#010;",
        "depth": "3",
        "reply": "<CABz5Tvm-SO1CrTAgctX8-iJfW1im17KTMFPr-zG0cwnGY4+mkw@mail.gmail.com>"
    },
    {
        "id": "<tencent_9C6B5E4B87CC9A14B6D3C7CA9269B2FA4407@qq.com>",
        "from": "&quot;chengyanan1008@foxmail.com&quot; &lt;chengyanan1...@foxmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 08:39:37 GMT",
        "subject": "Re: Re: 关于 sql-client",
        "content": "zeppelin 可以网页上提交各种作业，也是很不错的&#013;&#010;另外 submit with SQL file 可以参考大佬写的 https://github.com/wuchong/flink-sql-submit，&#013;&#010;然后在大佬的基础上，我自己稍微简化了一下,https://github.com/Chengyanan1008/flink-sql-submit-client&#013;&#010;直接在服务器上执行./sql-submit.sh -f &lt;sql-file&gt; 就可以执行SQL 文件了&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;chengyanan1008@foxmail.com&#013;&#010; &#013;&#010;发件人： Jeff Zhang&#013;&#010;发送时间： 2020-07-24 15:46&#013;&#010;收件人： user-zh&#013;&#010;主题： Re: 关于 sql-client&#013;&#010;可以用zeppelin来提交flink sql作业，可以加入钉钉群讨论：32803524&#013;&#010; &#013;&#010;杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 下午3:19写道：&#013;&#010; &#013;&#010;&gt; 你们可以 pr 到官方啊。我觉得这个功能很刚需啊，并且 basic 版的&#010;1.5 就 release 了，不知道为什么相关 gate way 或者&#013;&#010;&gt; submit with sql file 的 feature 到现在都还没实现呢。&#013;&#010;&gt;&#013;&#010;&gt; Harold.Miao &lt;miaohonghit@gmail.com&gt; 于2020年7月24日周五 上午11:42写道：&#013;&#010;&gt;&#013;&#010;&gt; &gt; 1 应该是可以的   主要是你要在flink-conf.yaml里面配置正确的 jobmanager.rpc.address&#013;&#010;&gt; &gt; 源码里面有加载主配置文件的逻辑&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; public LocalExecutor(URL defaultEnv, List&lt;URL&gt; jars, List&lt;URL&gt;&#013;&#010;&gt; libraries) {&#013;&#010;&gt; &gt;    // discover configuration&#013;&#010;&gt; &gt;    final String flinkConfigDir;&#013;&#010;&gt; &gt;    try {&#013;&#010;&gt; &gt;       // find the configuration directory&#013;&#010;&gt; &gt;       flinkConfigDir = CliFrontend.getConfigurationDirectoryFromEnv();&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // load the global configuration&#013;&#010;&gt; &gt;       this.flinkConfig =&#013;&#010;&gt; &gt; GlobalConfiguration.loadConfiguration(flinkConfigDir);&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // initialize default file system&#013;&#010;&gt; &gt;       FileSystem.initialize(flinkConfig,&#013;&#010;&gt; &gt; PluginUtils.createPluginManagerFromRootFolder(flinkConfig));&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;       // load command lines for deployment&#013;&#010;&gt; &gt;       this.commandLines =&#013;&#010;&gt; &gt; CliFrontend.loadCustomCommandLines(flinkConfig, flinkConfigDir);&#013;&#010;&gt; &gt;       this.commandLineOptions = collectCommandLineOptions(commandLines);&#013;&#010;&gt; &gt;    } catch (Exception e) {&#013;&#010;&gt; &gt;       throw new SqlClientException(\"Could not load Flink configuration.\",&#013;&#010;&gt; &gt; e);&#013;&#010;&gt; &gt;    }&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 2  因为等不及官方的  我们自己wrapper实现了一个&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; 杨荣 &lt;samyang3125c@gmail.com&gt; 于2020年7月24日周五 上午10:53写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; &gt; Hi all,&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 请问：&#013;&#010;&gt; &gt; &gt; 1. 在 Embedded mode 下，支持 ClusterClient 进行 job&#013;&#010;&gt; &gt; &gt; 提交作业，进行分布式计算吗？在文档中没看到，跟着文档走，只启起了&#010;Local 在本地作业，无法运用到生产环境。&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt; &gt; 2. GateWay mode 预计在那个版本 release？&#013;&#010;&gt; &gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; --&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt; Best Regards,&#013;&#010;&gt; &gt; Harold Miao&#013;&#010;&gt; &gt;&#013;&#010;&gt;&#013;&#010; &#013;&#010; &#013;&#010;-- &#013;&#010;Best Regards&#013;&#010; &#013;&#010;Jeff Zhang&#013;&#010;",
        "depth": "1",
        "reply": "<CABz5Tvm-SO1CrTAgctX8-iJfW1im17KTMFPr-zG0cwnGY4+mkw@mail.gmail.com>"
    },
    {
        "id": "<tencent_969056E4CF3C998D565BB54F28D75A0E3705@qq.com>",
        "from": "&quot;Cayden chen&quot; &lt;1193216...@qq.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 03:41:38 GMT",
        "subject": "回复：flink1.11日志上报",
        "content": "应该没有api,官网推荐的也是log&amp;nbsp; appder这种方式。用这种方式采集的日志是比较全的&#013;&#010;&#013;&#010;&#013;&#010;&#013;&#010;---原始邮件---&#013;&#010;发件人: \"Dream-底限\"&lt;zhangyu@akulaku.com&amp;gt;&#013;&#010;发送时间: 2020年7月24日(周五) 中午11:13&#013;&#010;收件人: \"user-zh\"&lt;user-zh@flink.apache.org&amp;gt;;&#013;&#010;主题: Re: flink1.11日志上报&#013;&#010;&#013;&#010;&#013;&#010;hi Cayden chen、&#013;&#010;也就是说你们日志上报的实现方式是实现自定义appder来实现是吧，这确实是一个不错的方式；&#013;&#010;我先前看spark可以实现对应的listener用来实现日志上报，查看了一下flink&#013;&#010;api貌似也有对应listen，具体是实现哪一个还不知道，现在我们还处在一个功能整理阶段&#013;&#010;&#013;&#010;&#013;&#010;Cayden chen &lt;1193216154@qq.com&amp;gt; 于2020年7月24日周五 上午10:53写道：&#013;&#010;&#013;&#010;&amp;gt; 我们的获取逻辑是通过自定义 logback的appder( flink&#013;&#010;&amp;gt; 默认的应该是log4j，对应配置在安装目录的conf下面)，appder通过解析当前系统路径(因为flink每个taskmanager会自己定义一个带有applicationId的路径，然后里面会放各种jar包，包括我自定义的appder)，获取之后通过MDC.put(),给日志加一列appId，在appder里面把日志上报到外部的日志系统&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; ------------------&amp;amp;nbsp;原始邮件&amp;amp;nbsp;------------------&#013;&#010;&amp;gt; 发件人:&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;\"user-zh\"&#013;&#010;&amp;gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&#010;&lt;&#013;&#010;&amp;gt; zhangyu@akulaku.com&amp;amp;gt;;&#013;&#010;&amp;gt; 发送时间:&amp;amp;nbsp;2020年7月24日(星期五) 上午10:49&#013;&#010;&amp;gt; 收件人:&amp;amp;nbsp;\"user-zh\"&lt;user-zh@flink.apache.org&amp;amp;gt;;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 主题:&amp;amp;nbsp;flink1.11日志上报&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; hi、&#013;&#010;&amp;gt;&#013;&#010;&amp;gt; 我这面想实现一个日志上报的功能，就是flink任务启动后，让flink主动将当前任务日志打到外部存储系统，想问一下flink有对应的接口吗，具体要实现哪一个类哪",
        "depth": "0",
        "reply": "<tencent_969056E4CF3C998D565BB54F28D75A0E3705@qq.com>"
    },
    {
        "id": "<CABKuJ_R-L9Ots7jkYogrm1wgT+Keu0fedxitVWOfPx4C8v9euQ@mail.gmail.com>",
        "from": "Benchao Li &lt;libenc...@apache.org&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 04:25:53 GMT",
        "subject": "Flink Weekly | 每周社区动态更新 - 2020/07/24",
        "content": "大家好，本文为 Flink Weekly 的第二十三期，由蒋晓峰、李本超共同整理及&#013;&#010;Review。本期主要内容包括：近期社区开发进展、邮件问题答疑、Flink&#010;最新社区动态及技术文章推荐等。&#013;&#010;&#013;&#010;&#013;&#010;# Flink 开发进展&#013;&#010;&#013;&#010;Flink 社区近期开发最新动态将从 Release、DEV、FLIP、Discuss、Others 五部分跟大家分享。&#013;&#010;&#013;&#010;## 1.RELEASE&#013;&#010;&#013;&#010;1.11.1 版本的投票已经通过，即将发布。该版本涵盖了比较多重要的 Bugfix，建议尝试&#010;1.11.0 版本的用户都直接切换到这个版本。&#013;&#010;[1]&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/VOTE-Release-1-11-1-release-candidate-1-tp43200.html&#013;&#010;&#013;&#010;## 2.DEV&#013;&#010;&#013;&#010;Chenqin 发起了支持 Thrift Format 的讨论，目前看该特性还是比较受欢迎的，而且确实有些场景是需要的。之前也有一个相关的&#013;&#010;PR[3]，社区希望可以基于这个 PR 来继续推进一下这个工作。&#013;&#010;[2]&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/thrift-support-tp43257.html&#013;&#010;[3] https://github.com/apache/flink/pull/8067&#013;&#010;&#013;&#010;## 3.FLIP&#013;&#010;&#013;&#010;[FLIP-128] 伍翀发起 Refactor Descriptor API to register connectors in Table API&#013;&#010;的提案，改进 Table API 中的“Connect API”，即用户用来在环境中描述/注册表的&#010;API。&#013;&#010;自 1.5.0 起 Flink 引入 Descriptor API 来配置和无效化 TableSources/TableSinks，即&#013;&#010;TableEnvironment#connect API。当前的 Descriptor API 有诸多问题包括社区关注最新版本中的新&#010;SQL DDL&#013;&#010;功能。SQL DDL 经过精心设计具有许多丰富的功能，但是 Descriptor API&#013;&#010;缺少许多关键功能例如计算列、主键、分区键等；当前连接器必须实现相应的描述符（例如&#010;new Kafka()）才能使用 “connect”&#013;&#010;API，希望在没有相应描述符的情况下注册连接器，简化连接器的开发并且替代&#010;registerTableSource/Sink；Descriptor&#013;&#010;API 和 SQL DDL 的基础实现不同，维护两个不同的代码路径非常昂贵。&#013;&#010;提案建议删除现有方法 TableEnvironment#connect（在 1.11 中弃用）和一些相关的接口/类，为表&#010;API 引入一组新的&#013;&#010;Descriptor API ：TableEnvironment#createTemporaryTable()。&#013;&#010;更多信息参考：&#013;&#010;[4]&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-FLIP-129-Refactor-Descriptor-API-to-register-connector-in-Table-API-tp42995.html&#013;&#010;[5]&#013;&#010;https://cwiki.apache.org/confluence/display/FLINK/FLIP-129%3A+Refactor+Descriptor+API+to+register+connectors+in+Table+API&#013;&#010;&#013;&#010;[FLIP-129] 陈水强发起支持 Python DataStream API (Stateless part) 的提案，与 Python&#013;&#010;Table API 类似方式引入 Python DataStream API，建议最初只支持无状态的用户定义功能。&#013;&#010;当前 PyFlink 支持 SQL 和 Table API 为熟悉 Python&#013;&#010;编程语言的用户提供便利，但是用户可能要求进行更复杂的处理操作譬如可能需要访问状态和计时器等。引入&#010;DataStream API 支持 Python&#013;&#010;DataStream 从外部存储（连接器）读取/写入数据，支持配置任务和作业配置（获取/设置资源、并行性、链接策略等），支持无状态数据转换，包括&#013;&#010;map、flatmap、keyby 等。&#013;&#010;更多信息参考：&#013;&#010;[6]&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-FLIP-130-Support-for-Python-DataStream-API-Stateless-Part-td43035.html&#013;&#010;[7]&#013;&#010;https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158866298&#013;&#010;&#013;&#010;## 4.Discuss&#013;&#010;&#013;&#010;[releases] 伍翀发起在 Flink 1.11.0 之后不久发布 Flink 1.11.1的讨论，以提供完整的&#010;CDC 功能和解决大多数在&#013;&#010;Table API / SQL 生态系统发现的重要错误问题，提议不要等待太久收集/修复错误否则再次延迟功能交付，建议于下周一创建第一个&#013;&#010;RC，以梳理清楚 Thomas 之前报告的性能下降。&#013;&#010;更多信息参考：&#013;&#010;[8]&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/DISCUSS-Releasing-Flink-1-11-1-soon-td43065.html&#013;&#010;&#013;&#010;## 5.Others&#013;&#010;&#013;&#010;[announce] Stephan Ewen 宣布 Piotr Nowojski 成为 Apache Flink PMC，Piotr&#013;&#010;擅长数据处理运行时和网络堆栈领域，以及邮件列表或者版本发布管理者的工作。恭喜&#010;Piotr Nowojski 老师！&#013;&#010;更多信息参考：&#013;&#010;[9]&#013;&#010;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/ANNOUNCE-New-PMC-member-Piotr-Nowojski-td42966.html&#013;&#010;&#013;&#010;[FLINK-18419] [1.11.0] 用户无法使用 Jar 文件的代码创建一个 Catalog，例如通过&#010;sql-client 上传的 Jar&#013;&#010;包，将在 1.11.1 版本修复。&#013;&#010;[10]https://issues.apache.org/jira/browse/FLINK-18419&#013;&#010;&#013;&#010;[FLINK-18583] [1.11.0] ElasticSearch Sink 索引中设置不正确的 _id 字段，将在&#010;1.11.1 版本修复。&#013;&#010;[11]https://issues.apache.org/jira/browse/FLINK-18583&#013;&#010;&#013;&#010;[FLINK-18583] [1.11.1] InfluxDB metrics reporter 不能作为插件使用，将在 1.11.1&#010;版本修复。&#013;&#010;[12]https://issues.apache.org/jira/browse/FLINK-18573&#013;&#010;&#013;&#010;[FLINK-18434 [1.10.0] 在使用 JDBC Catalog 时候无法 Select 字段，将在 1.11.1 版本修复。&#013;&#010;[13]https://issues.apache.org/jira/browse/FLINK-18434&#013;&#010;&#013;&#010;[FLINK-18461] [1.11.0] 当前不支持往 upsert sink 中写 ChangeLog 流（例如，Elastic&#010;或者&#013;&#010;JDBC），将在 1.11.1 版本修复。&#013;&#010;[14]https://issues.apache.org/jira/browse/FLINK-18461&#013;&#010;&#013;&#010;&#013;&#010;# 邮件问题答疑&#013;&#010;&#013;&#010;[SQL] Dongwon Kim 提问 Flink SQL 是否支持 PARTITION BY 的时候用嵌套字段。&#013;&#010;现在应该是不支持的。但是可以用计算列来绕过去这个问题，当前的&#010;Filesystem Connector 对于计算列的支持有个&#013;&#010;Bug[FLINK-18665]，详情了解：&#013;&#010;[15]&#013;&#010;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/How-to-use-a-nested-column-for-CREATE-TABLE-PARTITIONED-BY-td36796.html&#013;&#010;&#013;&#010;[SQL] Kelly Smith 提问如何把 Filesystem 当做维表使用。&#013;&#010;当前有几个问题，首先普通的 Join 语法不允许数据中有时间属性，这个接下来可以在&#010;Planner 层做一个优化&#013;&#010;[FLINK-18651]，自动在这种情况下将时间属性物化为普通的时间字段；其次当前的&#010;Filesystem&#013;&#010;还不支持维表访问，这个未来也可以支持[FLINK-17397]。&#013;&#010;[16]&#013;&#010;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Flink-SQL-Join-Lookup-Table-td36775.html&#013;&#010;&#013;&#010;[SQL] sunfulin 提问如何在 1.11 中使用 DML Sink 和 toDataStream 之后 addSink 保持在同一个&#010;job&#013;&#010;中。&#013;&#010;Godfrey 进行了回答，在 1.11 中重构了 TableEnvironment 的接口，目前没有办法可以做到统一这两个方式到同一个&#010;job&#013;&#010;中，只能形成两个不同的 job。&#013;&#010;[17]http://apache-flink.147419.n8.nabble.com/flink-1-11-td5064.html&#013;&#010;&#013;&#010;[SQL] 曹武 提问使用 debezium-json 做数据同步时会有 Delete 数据被丢弃的问题。&#013;&#010;这里有两个问题，一个是如果用了聚合算子，它的 Delete 数据如果没有对应的&#010;Insert 数据，应该会被当做脏数据丢弃；第二个问题是一个已知的&#013;&#010;Bug[FLINK-18461]，这个会在 1.11.1 中修复。&#013;&#010;[18]&#013;&#010;http://apache-flink.147419.n8.nabble.com/flink-1-11-checkpoint-td5059.html&#013;&#010;&#013;&#010;[SQL] claylink 提问如何用 Flink SQL 解析最外层为 Array 的数据，并且添加计算列。&#013;&#010;李本超进行了回答，当前还不能做到这一点，不过 FLINK-18590 正在跟进解决这个问题。&#013;&#010;[19]http://apache-flink.147419.n8.nabble.com/sql-josn-td5120.html&#013;&#010;&#013;&#010;[SQL] wanglei2 提问 Flink SQL 是如何加载到某个具体的 connector/format 的。&#013;&#010;godfrey、Leonard、云邪等进行了回答。首先是通过 SPI 机制找到所有实现了特定接口的工厂类，然后通过每个&#013;&#010;connector/format 工厂给出来的参数来筛选符合条件的，最终定位到具体的实现代码。&#013;&#010;[20]http://apache-flink.147419.n8.nabble.com/FlinkSQL-Java-td5024.html&#013;&#010;&#013;&#010;[SQL] Dream-底限 提问如何在用 Flink SQL 解析 json 失败的时候，把这些脏数据发送到某个特定的&#010;topic。&#013;&#010;云邪回答了该问题，这种需求比较特殊，从 Flink 侧来直接支持不是很容易，因为要抽象一层存储对接到各种存储系统上，而不是单纯的假设一定是&#013;&#010;Kafka。当前建议是可以由 Flink 来输出这个脏数据到日志，然后用户自己定义一些&#010;Log Appender 来收集这些数据。&#013;&#010;[21]http://apache-flink.147419.n8.nabble.com/flink-kafka-json-td5209.html&#013;&#010;&#013;&#010;[sql-client] wldd 提问：Flink 1.11.0 读取 MySQL 数据 DECIMAL 类型强转成 DECIMAL(38,18)&#013;&#010;问题：通过 SQL Client 读取 MySQL 数据时 DECIMAL 类型强转成 DECIMAL(38,18)，Streaming&#010;模式下出现：&#013;&#010;For final plan, using rel#3045:LogicalProject.NONE.any.None:&#013;&#010;0.[NONE].[NONE](input=HepRelVertex#3044,exprs=[CAST($0):DECIMAL(38, 18)])&#013;&#010;徐榜江回答：SQL Client 读取 MySQL 相当于一个 Connector 只支持 DECIMAL(38,18)&#010;的，所有&#013;&#010;DECIMAL(p,s) 都会转到这个类型，因为 SQL Client 用的是 Legacy 数据类型。Stream&#010;模式有这个问题，Batch&#013;&#010;没有，原因是：&#013;&#010;CollectStreamTableSink 实现的是 TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt;&#013;&#010;getOutputType()&#013;&#010;CollectBatchTableSink 实现的是 DataType getConsumedDataType()&#013;&#010;社区有 Issue [22] 彻底解决这个问题，贺小令提 Pull Request 把此两个 CollectSink&#010;都去掉，使用&#013;&#010;TableResult#collect() 来收集数据。&#013;&#010;更多信息参考：&#013;&#010;[22]https://issues.apache.org/jira/browse/FLINK-18550&#013;&#010;[23]&#013;&#010;http://apache-flink.147419.n8.nabble.com/flink1-11-0-mysql-decimal-decimal-38-18-td4925.html&#013;&#010;&#013;&#010;[SQL] Peihui He 提问：Flink 1.10 SQL Kafka Format JSON 定制 Schema 时, 字段数据能否定义为&#013;&#010;JSON Object？&#013;&#010;李本超回答：社区有 Issue[24] 正在解决此问题，指定任意一个 JsonNode&#010;为 VARCHAR 类型。&#013;&#010;此 Feature不能解决所有问题，比如有一个字段内容不太确定而且也不需要额外处理，主要是想保留这个字段，下游输出&#010;JSON 时仍然还是此字段。沿着用&#013;&#010;FLINK-18002 的思路输出到下游的时候，把这部分数据整体作为 JSON&#013;&#010;字符串，从结果上来看未能完全做到原封不动的输出到下游。有两个思路解决此问题：&#013;&#010;用 RAW 类型，此时需要 JSON Node 类型对于 Flink 来讲都是可以序列化的。&#013;&#010;用 BINARY 类型，因为现在已经有对 BINARY 类型的处理，所以需要额外加一个选项来指定对于&#010;BINARY 类型的处理模式，把任意&#013;&#010;JsonNode 转成 JSON 字符串表达形式，再转成 byte[] 进行中间的传输和处理；序列化时再直接通过此&#010; byte[] 数据构造&#013;&#010;JsonNode，保证跟原来的 JsonNode 一模一样。&#013;&#010;更多信息参考：&#013;&#010;[24]https://issues.apache.org/jira/browse/FLINK-18002&#013;&#010;[25]&#013;&#010;http://apache-flink.147419.n8.nabble.com/flink-1-10-sql-kafka-format-json-schema-json-object-td4665.html&#013;&#010;&#013;&#010;[Sink] xueaohui 问：Flink 多 Sink 数据一致性保证即通过把作业加入多个 Sink，这种场景当&#010;HBase&#013;&#010;写入失败的时候不影响 Kafka 的写入。期望 HBase 写入失败，Kafka 也不发送消息，如何保证&#010;HBase 和 Kafka 的写入为原子性呢？&#013;&#010;高赟回答：多个 Sink 的情况下数据保证写入仍然发生在数据写入后的&#010;Checkpoint 完成，如果写入 HBase 失败的时候触发&#013;&#010;Failover 的话，按二阶段提交的逻辑这种情况下 Kafka 的事务被 Abort 掉，数据不会真正写入。多个&#010;Sink 如果按照&#013;&#010;TwoPhaseCommitSinkFunction 来做的话是能够实现多 Sink 一致性的，只要有一个&#010;Sink 出错整个作业都会&#013;&#010;Failover，其它 Sink 当前的事务也会跟着 Abort 掉，然后整个作业回退到上一次&#010;Checkpoint 开始执行。&#013;&#010;社区正在加 Exactly-Once JDBC Sink 实现[26]。如果要实现两阶段提交的 Sink&#010;的话，总是需要有能跨 Session 的&#013;&#010;Transaction 机制，即在作业挂了之后下次起来的时候此事务能够 Abort 掉或者继续提交（取决于是否已经&#010;Snapshot），例如 JDBC&#013;&#010;必须要用 XA 事务，用单纯的 JDBC 事务应该是有问题的，因为即使在 Snapshot&#010;的时候 Precommit&#013;&#010;过，如果作业挂掉连接中断此事务仍然会被 Abort 掉。&#013;&#010;更多信息参考：&#013;&#010;[26]https://issues.apache.org/jira/browse/FLINK-15578&#013;&#010;[27]http://apache-flink.147419.n8.nabble.com/Flink-Sink-td4052.html&#013;&#010;&#013;&#010;&#013;&#010;# 活动 / 博客文章 / 其他&#013;&#010;&#013;&#010;2020 Apache Flink 首场线下 Meetup 正式启动，阵容十分可观，详情了解：&#013;&#010;[28]https://www.huodongxing.com/go/7554937279200&#013;&#010;&#013;&#010;任务部署的原理以及新引入的 Application Mode 介绍&#013;&#010;[29]https://flink.apache.org/news/2020/07/14/application-mode.html&#013;&#010;&#013;&#010;字节跳动基于 Flink 的 MQ-Hive 实时数据集成&#013;&#010;[30]https://mp.weixin.qq.com/s/SDkgYqBZrejObpJ_2bpURw&#013;&#010;&#013;&#010;网易云音乐实时数仓建设实践&#013;&#010;[31]https://mp.weixin.qq.com/s/n4RUxDu3PuGBNl6QXNlp4Q&#013;&#010;&#013;&#010;Alexander Fedulov 介绍有关 Apache Flink 的介绍性视频系列以及 YouTube 上的流处理。&#013;&#010;更多信息参考：&#013;&#010;[32]&#013;&#010;https://www.ververica.com/blog/presenting-our-streaming-concepts-introduction-to-flink-video-series&#013;&#010;[33]https://www.youtube.com/watch?v=ZU1r7uEAO7o&#013;&#010;&#013;&#010;-- &#013;&#010;&#013;&#010;Best,&#013;&#010;Benchao Li&#013;&#010;",
        "depth": "0",
        "reply": "<CABKuJ_R-L9Ots7jkYogrm1wgT+Keu0fedxitVWOfPx4C8v9euQ@mail.gmail.com>"
    },
    {
        "id": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>",
        "from": "liunaihua521 &lt;liunaihua...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 06:14:23 GMT",
        "subject": "flink sql 读取mysql",
        "content": "hi!&#010;    版本:flink  1.10&#010;            mysql 5.7.24&#010;&#010;&#010;    需求场景是:&#010;            使用flink SQL,读取kafka数据,和mysql的维表数据,之后进行join操作,如何配置mysql&#010;connector,才能使维表数据修改后,flink能读取到更新后的数据进行join操作?&#010;&#010;&#010;    现在本地测试时,维表的DDL是:&#010;            &#010;    但是去mysql修改了数据后,join操作还是旧数据.&#010;&#010;&#010;望大神们指点方向,提前谢谢了.&#010;        &#010;&#010;",
        "depth": "0",
        "reply": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>"
    },
    {
        "id": "<E6EC5CE5-00D8-4883-BF48-A6A2CCD27957@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 06:25:02 GMT",
        "subject": "Re: flink sql 读取mysql",
        "content": "Hello&#010;图挂了，可以贴下DDL吗？另外你没有使用维表join语法 FOR SYSTEM_TIME AS&#010;OF[1] &#010;&#010;祝好&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&gt;&#010;&#010;&#010;&gt; 在 2020年7月24日，14:14，liunaihua521 &lt;liunaihua521@163.com&gt; 写道：&#010;&gt; &#010;&gt; hi!&#010;&gt;     版本:flink  1.10&#010;&gt;             mysql 5.7.24&#010;&gt; &#010;&gt;     需求场景是:&#010;&gt;             使用flink SQL,读取kafka数据,和mysql的维表数据,之后进行join操作,如何配置mysql&#010;connector,才能使维表数据修改后,flink能读取到更新后的数据进行join操作?&#010;&gt; &#010;&gt;     现在本地测试时,维表的DDL是:&#010;&gt;             &#010;&gt;     但是去mysql修改了数据后,join操作还是旧数据.&#010;&gt; &#010;&gt; 望大神们指点方向,提前谢谢了.&#010;&gt;         &#010;&gt; &#010;&gt; &#010;&#010;&#010;",
        "depth": "1",
        "reply": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>"
    },
    {
        "id": "<528F3BF6-967E-4767-9C9C-BA0289E649DD@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 06:29:01 GMT",
        "subject": "Re: flink sql 读取mysql",
        "content": "&#010;另外社区中文邮件交流直接发邮件到user-zh@flink.apache.org &lt;mailto:user-zh@flink.apache.org&gt;就可以了，不用发user-zh-faq@flink.apache.org&#010;&lt;mailto:user-zh-faq@flink.apache.org&gt; 这个地址。&#010;&#010;&#010;&gt; 在 2020年7月24日，14:25，Leonard Xu &lt;xbjtdcq@gmail.com&gt; 写道：&#010;&gt; &#010;&gt; Hello&#010;&gt; 图挂了，可以贴下DDL吗？另外你没有使用维表join语法 FOR SYSTEM_TIME&#010;AS OF[1] &#010;&gt; &#010;&gt; 祝好&#010;&gt; Leonard Xu&#010;&gt; [1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&gt;&#010;&#010;&gt; &#010;&gt;&gt; 在 2020年7月24日，14:14，liunaihua521 &lt;liunaihua521@163.com &lt;mailto:liunaihua521@163.com&gt;&gt;&#010;写道：&#010;&gt;&gt; &#010;&gt;&gt; hi!&#010;&gt;&gt;     版本:flink  1.10&#010;&gt;&gt;             mysql 5.7.24&#010;&gt;&gt; &#010;&gt;&gt;     需求场景是:&#010;&gt;&gt;             使用flink SQL,读取kafka数据,和mysql的维表数据,之后进行join操作,如何配置mysql&#010;connector,才能使维表数据修改后,flink能读取到更新后的数据进行join操作?&#010;&gt;&gt; &#010;&gt;&gt;     现在本地测试时,维表的DDL是:&#010;&gt;&gt;             &#010;&gt;&gt;     但是去mysql修改了数据后,join操作还是旧数据.&#010;&gt;&gt; &#010;&gt;&gt; 望大神们指点方向,提前谢谢了.&#010;&gt;&gt;         &#010;&gt;&gt; &#010;&gt;&gt; &#010;&gt; &#010;&#010;&#010;",
        "depth": "2",
        "reply": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>"
    },
    {
        "id": "<45c46929.cfb2.1737fe88d5d.Coremail.liunaihua521@163.com>",
        "from": "liunaihua521 &lt;liunaihua...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 08:20:13 GMT",
        "subject": "回复： flink sql 读取mysql",
        "content": "hi!&#010;    您好,我明白您的意思了,并且看了下网上的资料,改完后如下&#010;&#010;&#010;DDL:&#010;CREATE TABLE user_behavior (&#010;    user_id BIGINT,&#010;    item_id BIGINT,&#010;    category_id BIGINT,&#010;    behavior STRING,&#010;    ts TIMESTAMP(3),&#010;    proctime as PROCTIME()&#010;) WITH (&#010;    'connector.type' = 'kafka',  -- kafka connector&#010;    'connector.version' = 'universal',  -- universal 支持 0.11 以上的版本&#010;    'connector.topic' = 'user_behavior',  -- kafka topic&#010;    'connector.startup-mode' = 'earliest-offset',  -- 从起始 offset 开始读取&#010;    'connector.properties.zookeeper.connect' = '',  -- zk 地址&#010;    'connector.properties.bootstrap.servers' = '',  -- broker 地址&#010;    'format.type' = 'json'  -- 数据源格式为 json&#010;);&#010;&#010;&#010;&#010;&#010;CREATE TABLE category_info (&#010;parent_id BIGINT, -- 商品大类&#010;    category_id BIGINT  -- 商品详细类目&#010;) WITH (&#010;    'connector.type' = 'jdbc',&#010;    'connector.url' = 'jdbc:mysql://:3306/flinkdemo',&#010;    'connector.table' = 'category_info',&#010;    'connector.driver' = 'com.mysql.jdbc.Driver',&#010;    'connector.username' = '',&#010;    'connector.password' = '',&#010;    'connector.lookup.cache.max-rows' = '5000',&#010;    'connector.lookup.cache.ttl' = '10min'&#010;);&#010;&#010;&#010;SQL:&#010;&#010;&#010;SELECT U.user_id, U.item_id, U.behavior, C.parent_id, C.category_id&#010;FROM user_behavior AS U LEFT JOIN category_info FOR SYSTEM_TIME AS OF U.proctime AS C&#010;ON U.category_id = C.category_id;&#010;&#010;&#010;但是执行SQL报错了(由于代码在办公环境粘不出来,就手打如下部分):&#010;org.apache.flink.table.api.SqlParserExcption:Sql parse failed.Encountered \"timestamp,\"at line&#010;Was expecting one of:&#010;\"CURSOR\"...&#010;\"EXISTS\"...&#010;\"NOT\"...&#010;\"ROW\"...&#010;\"(\"...&#010;&#010;&#010;一直调试不好,望指教&#010;&#010;&#010;&#010;&#010;在2020年7月24日 14:25，Leonard Xu&lt;xbjtdcq@gmail.com&gt; 写道：&#010;Hello&#010;图挂了，可以贴下DDL吗？另外你没有使用维表join语法 FOR SYSTEM_TIME AS&#010;OF[1]&#010;&#010;祝好&#010;Leonard Xu&#010;[1] https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&#010;&lt;https://ci.apache.org/projects/flink/flink-docs-release-1.11/zh/dev/table/sql/queries.html#joins&gt;&#010;&#010;在 2020年7月24日，14:14，liunaihua521 &lt;liunaihua521@163.com&gt; 写道：&#010;&#010;hi!&#010;版本:flink  1.10&#010;mysql 5.7.24&#010;&#010;需求场景是:&#010;使用flink SQL,读取kafka数据,和mysql的维表数据,之后进行join操作,如何配置mysql&#010;connector,才能使维表数据修改后,flink能读取到更新后的数据进行join操作?&#010;&#010;现在本地测试时,维表的DDL是:&#010;&#010;但是去mysql修改了数据后,join操作还是旧数据.&#010;&#010;望大神们指点方向,提前谢谢了.&#010;&#010;&#010;&#010;&#010;",
        "depth": "2",
        "reply": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>"
    },
    {
        "id": "<AC355D7C-FBD8-4B49-96D4-0D2821C6201A@gmail.com>",
        "from": "Leonard Xu &lt;xbjt...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 08:33:53 GMT",
        "subject": "Re: flink sql 读取mysql",
        "content": "Hello&#010;&#010;这个报错一般是sql格式错误，比如中英文逗号等，你可以检查下你的SQL语句&#010;&#010;祝好&#010;Leonard Xu&#010;&gt; 在 2020年7月24日，16:20，liunaihua521 &lt;liunaihua521@163.com&gt; 写道：&#010;&gt; &#010;&gt; org.apache.flink.table.api.SqlParserExcption:Sql parse failed.Encountered \"timestamp,\"at&#010;line&#010;&gt; Was expecting one of:&#010;&gt; \"CURSOR\"...&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>"
    },
    {
        "id": "<9CAE6C07-49F9-4965-A724-6474D4A815D7@163.com>",
        "from": "admin &lt;17626017...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 11:31:14 GMT",
        "subject": "Re: flink sql 读取mysql",
        "content": " 'connector.properties.zookeeper.connect' = '',  -- zk 地址&#010;   'connector.properties.bootstrap.servers' = '',  -- broker 地址&#010;&#010;'connector.username' = '',&#010;   'connector.password' = ‘',&#010;这几行有问题吧&#010;&#010;&gt; 2020年7月24日 下午4:20，liunaihua521 &lt;liunaihua521@163.com&gt; 写道：&#010;&gt; &#010;&gt;  'connector.properties.zookeeper.connect' = '',  -- zk 地址&#010;&gt;    'connector.properties.bootstrap.servers' = '',  -- broker 地址&#010;&#010;&#010;",
        "depth": "3",
        "reply": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>"
    },
    {
        "id": "<CAHjjfOTAiueV5DTWweK9_UJO0O6gFd4BCXF=Jd_jzLApcaTBvQ@mail.gmail.com>",
        "from": "Caizhi Weng &lt;tsreape...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 03:28:13 GMT",
        "subject": "Re: flink sql 读取mysql",
        "content": "Hi,&#013;&#010;&#013;&#010;关于数据修改后还是读到旧数据的问题，可能是因为配置了 cache。我看到超时时间配置的是&#010;'connector.lookup.cache.ttl' =&#013;&#010;'10min'，也就是说数据修改后最长要 10 分钟 Flink 才会读到修改后的数据。&#013;&#010;&#013;&#010;admin &lt;17626017841@163.com&gt; 于2020年7月24日周五 下午7:32写道：&#013;&#010;&#013;&#010;&gt;  'connector.properties.zookeeper.connect' = '',  -- zk 地址&#013;&#010;&gt;    'connector.properties.bootstrap.servers' = '',  -- broker 地址&#013;&#010;&gt;&#013;&#010;&gt; 'connector.username' = '',&#013;&#010;&gt;    'connector.password' = ‘',&#013;&#010;&gt; 这几行有问题吧&#013;&#010;&gt;&#013;&#010;&gt; &gt; 2020年7月24日 下午4:20，liunaihua521 &lt;liunaihua521@163.com&gt; 写道：&#013;&#010;&gt; &gt;&#013;&#010;&gt; &gt;  'connector.properties.zookeeper.connect' = '',  -- zk 地址&#013;&#010;&gt; &gt;    'connector.properties.bootstrap.servers' = '',  -- broker 地址&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;",
        "depth": "4",
        "reply": "<47035ee6.c0fd.1737f7558af.Coremail.liunaihua521@163.com>"
    },
    {
        "id": "<202007241615412931501@163.com>",
        "from": "&quot;guaishushu1103@163.com&quot; &lt;guaishushu1...@163.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Fri, 24 Jul 2020 08:15:53 GMT",
        "subject": "Flink CPU利用率低",
        "content": "&#013;&#010;&#013;&#010;想问下大佬们 Flink的cpu利用率这么低吗 0.012？&#013;&#010;&#013;&#010;&#013;&#010;guaishushu1103@163.com&#013;&#010;",
        "depth": "0",
        "reply": "<202007241615412931501@163.com>"
    },
    {
        "id": "<CAP+gf34p0W4qhOT0aSW49=xDQ664m23woP4CQjDrmzf3V3M5Lw@mail.gmail.com>",
        "from": "Yang Wang &lt;danrtsey...@gmail.com&gt;",
        "to": "user-zh@flink.apache.org",
        "date": "Mon, 27 Jul 2020 03:42:09 GMT",
        "subject": "Re: Flink CPU利用率低",
        "content": "Flink CPU利用率的高低主要还是取决于你的任务中的业务逻辑，框架本身的CPU占用是很低的&#013;&#010;&#013;&#010;试想一下，如果你的任务是计算非常简单(或则就是sleep)，那整个TM的CPU利用率就很低了，约等于框架占用的&#013;&#010;如果是一个计算很密集的(或者就是死循环)，那TM CPU利用率就是取决于你的slot数量了，2个slot就是200%的CPU利用率&#013;&#010;&#013;&#010;&#013;&#010;Best,&#013;&#010;Yang&#013;&#010;&#013;&#010;guaishushu1103@163.com &lt;guaishushu1103@163.com&gt; 于2020年7月24日周五 下午4:16写道：&#013;&#010;&#013;&#010;&gt;&#013;&#010;&gt;&#013;&#010;&gt; 想问下大佬们 Flink的cpu利用率这么低吗 0.012？&#013;&#010;&gt; ------------------------------&#013;&#010;&gt; guaishushu1103@163.com&#013;&#010;&gt;&#013;&#010;",
        "depth": "1",
        "reply": "<202007241615412931501@163.com>"
    }
]