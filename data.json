{
    "2020-08-15 10:44:01": {
        "From": "halfcokey@gmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "通过Flink web ui上传jar包时无响应",
        "Content": "通过Flink web ui上传jar包时链接被重置，不清楚是什么原因，请问各位大佬遇到过么？\r\n[image: pic_2020-08-15_10-39-37.png]\r\n[image: pic_2020-08-15_10-40-09.png]\r\n",
        "Attach": []
    },
    "2020-08-15 01:44:01": {
        "From": "weifeng@nequal.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Flink参数配置设置不生效",
        "Content": "各位大佬好：\r\n\r\n    在flink-conf.yaml中设置参数execution.attached: false\r\n\r\n   但是yarn logs查看此参数设置并没有生效，\r\n\r\n   2020-08-15 09:40:13,489 INFO  org.apache.flink.configuration.GlobalConfiguration            - Loading configuration property: execution.attached, true\r\n\r\n   而且根据官网说明此参数默认应该是false才对，已确认在代码中并没有对此参数进行设置，请问这是什么情况呀？\r\n",
        "Attach": []
    },
    "2020-08-15 00:05:02": {
        "From": "flinkcx@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "答复: HBase Sink报错：UpsertStreamTableSink requires that Table has a full primary keys",
        "Content": "Hi Jark:\r\n\t感谢回答，我发现是我join的时候，是想将hbase作为维表使用的，但是我遗漏了for system_time as of语句，添加后就不会再报这个错了。\r\n另外有个问题想请教：1.11中新版hbase connector只是指with中指定version为1.4所创建的表吗，我发现使用1.4.3的版本，也是可以正常使用的。是不是说明pk在1.4和1.4.3两个版本上都是生效的？\r\n再次感谢。\r\n\r\n\r\nBest\r\nXiao Cai\r\n\r\n发送自 Windows 10 版邮件应用\r\n\r\n发件人: Jark Wu\r\n发送时间: 2020年8月14日 23:23\r\n收件人: user-zh\r\n主题: Re: HBase Sink报错：UpsertStreamTableSink requires that Table has a full primary keys\r\n\r\n PK 的问题在1.11 已经解决了，你可以用下1.11 提供的新版 hbase connector，可以在 DDL 上指定 PK，所以 query\r\n推导不出 PK 也不会报错了。\r\n see more:\r\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/hbase.html\r\n\r\n\r\nBest,\r\nJark\r\n\r\n\r\nOn Thu, 13 Aug 2020 at 14:27, xiao cai <flinkcx@163.com> wrote:\r\n\r\n> Hi All:\r\n> 使用flink-sql写入hbase sink时报错：\r\n> UpsertStreamTableSink requires that Table has a full primary keys if it is\r\n> updated.\r\n>\r\n>\r\n> 我共创建了4张表，1张kafka source表，3张hbase 维表，1张hbase sink表\r\n> kafka source表与hbase 维表left join后的结果insert到hbase sink表中：\r\n> sql如下：\r\n> create table user_click_source(\r\n> `id` bigint,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint,\r\n> `catalog_id` int,\r\n> `device_id` int,\r\n> `user_id` int,\r\n> `proc_time` timestamp(3)\r\n> PRIMARY KEY (id) NOT ENFORCED\r\n> )with(\r\n> 'connector.type' = 'kafka',\r\n> ……\r\n> )\r\n> ;\r\n> create table dim_user(\r\n> `rowkey` varchar,\r\n> cf ROW<\r\n> `id` int,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint\r\n> >,\r\n> ts bigint\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n>\r\n>\r\n> create table dim_device(\r\n> `rowkey` varchar,\r\n> cf ROW<\r\n> `id` int,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint\r\n> >\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n>\r\n>\r\n> create table dim_catalog(\r\n> `rowkey` varchar,\r\n> cf ROW<\r\n> `id` int,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint\r\n> >\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n> create table hbase_full_user_click_case1_sink(\r\n> `rowkey` bigint,\r\n> cf ROW<\r\n> `click_id` bigint,\r\n> `click_name` varchar,\r\n> `click_partition` int,\r\n> `click_event_time` bigint,\r\n> `click_write_time` bigint,\r\n> `click_snapshot_time` bigint,\r\n> `click_max_snapshot_time` bigint,\r\n> `catalog_id` int,\r\n> `catalog_name` varchar,\r\n> `catalog_partition` int,\r\n> `catalog_event_time` bigint,\r\n> `catalog_write_time` bigint,\r\n> `catalog_snapshot_time` bigint,\r\n> `catalog_max_snapshot_time` bigint,\r\n> `device_id` int,\r\n> `device_name` varchar,\r\n> `device_partition` int,\r\n> `device_event_time` bigint,\r\n> `device_write_time` bigint,\r\n> `device_snapshot_time` bigint,\r\n> `device_max_snapshot_time` bigint,\r\n> `user_id` int,\r\n> `user_name` varchar,\r\n> `user_partition` int,\r\n> `user_event_time` bigint,\r\n> `user_write_time` bigint,\r\n> `user_snapshot_time` bigint,\r\n> `user_max_snapshot_time` bigint\r\n> >,\r\n> PRIMARY KEY (rowkey) NOT ENFORCED\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n> insert into hbase_full_user_click_case1_sink\r\n> select\r\n> `click_id`,\r\n> ROW(\r\n> `click_id`,\r\n> `click_name`,\r\n> `click_partition`,\r\n> `click_event_time`,\r\n> `click_write_time`,\r\n> `click_snapshot_time`,\r\n> `click_max_snapshot_time`,\r\n> `catalog_id`,\r\n> `catalog_name`,\r\n> `catalog_partition`,\r\n> `catalog_event_time`,\r\n> `catalog_write_time`,\r\n> `catalog_snapshot_time`,\r\n> `catalog_max_snapshot_time`,\r\n> `device_id`,\r\n> `device_name`,\r\n> `device_partition`,\r\n> `device_event_time`,\r\n> `device_write_time`,\r\n> `device_snapshot_time`,\r\n> `device_max_snapshot_time`,\r\n> `user_id`,\r\n> `user_name`,\r\n> `user_partition`,\r\n> `user_event_time`,\r\n> `user_write_time`,\r\n> `user_snapshot_time`,\r\n> `user_max_snapshot_time`\r\n> )\r\n> from (select\r\n> click.id as `click_id`,\r\n> click.name as `click_name`,\r\n> click.kafka_partition as `click_partition`,\r\n> click.event_time as `click_event_time`,\r\n> click.write_time as `click_write_time`,\r\n> click.snapshot_time as `click_snapshot_time`,\r\n> click.max_snapshot_time as `click_max_snapshot_time`,\r\n> cat.cf.id as `catalog_id`,\r\n> cat.cf.name as `catalog_name`,\r\n> cat.cf.kafka_partition as `catalog_partition`,\r\n> cat.cf.event_time as `catalog_event_time`,\r\n> cat.cf.write_time as `catalog_write_time`,\r\n> cat.cf.snapshot_time as `catalog_snapshot_time`,\r\n> cat.cf.max_snapshot_time as `catalog_max_snapshot_time`,\r\n> dev.cf.id as `device_id`,\r\n> dev.cf.name as `device_name`,\r\n> dev.cf.kafka_partition as `device_partition`,\r\n> dev.cf.event_time as `device_event_time`,\r\n> dev.cf.write_time as `device_write_time`,\r\n> dev.cf.snapshot_time as `device_snapshot_time`,\r\n> dev.cf.max_snapshot_time as `device_max_snapshot_time`,\r\n> u.cf.id as `user_id`,\r\n> u.cf.name as `user_name`,\r\n> u.cf.kafka_partition as `user_partition`,\r\n> u.cf.event_time as `user_event_time`,\r\n> u.cf.write_time as `user_write_time`,\r\n> u.cf.snapshot_time as `user_snapshot_time`,\r\n> u.cf.max_snapshot_time as `user_max_snapshot_time`\r\n>\r\n>\r\n> from (select\r\n> id,\r\n> `name`,\r\n> `kafka_partition`,\r\n> `event_time`,\r\n> `write_time`,\r\n> `snapshot_time`,\r\n> `max_snapshot_time`,\r\n> cast(catalog_id as varchar) as catalog_key,\r\n> cast(device_id as varchar) as device_key,\r\n> cast(user_id as varchar) as user_key,\r\n> `catalog_id`,\r\n> `device_id`,\r\n> `user_id`,\r\n> `proc_time`,\r\n> `event_time`,\r\n> FROM user_click_source\r\n> GROUP BY TUMBLE(event_time, INTERVAL '1' SECOND),\r\n> `id`,\r\n> `name`,\r\n> `kafka_partition`,\r\n> `event_time`,\r\n> `write_time`,\r\n> `snapshot_time`,\r\n> `max_snapshot_time`,\r\n> `catalog_id`,\r\n> `device_id`,\r\n> `user_id`,\r\n> `proc_time`) click\r\n>\r\n>\r\n> left join dim_catalog cat on click.catalog_key = cat.rowkey\r\n> left join dim_device dev on click.device_key = dev.rowkey\r\n> left join dim_user u on click.user_key = u.rowkey and click.event_time =\r\n> u.ts\r\n> ) t\r\n\r\n\r\n",
        "Attach": []
    },
    "2020-08-14 23:21:02": {
        "From": "imjark@gmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: HBase Sink报错：UpsertStreamTableSink requires that Table has a full primary keys",
        "Content": " PK 的问题在1.11 已经解决了，你可以用下1.11 提供的新版 hbase connector，可以在 DDL 上指定 PK，所以 query\r\n推导不出 PK 也不会报错了。\r\n see more:\r\nhttps://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/hbase.html\r\n\r\n\r\nBest,\r\nJark\r\n\r\n\r\nOn Thu, 13 Aug 2020 at 14:27, xiao cai <flinkcx@163.com> wrote:\r\n\r\n> Hi All:\r\n> 使用flink-sql写入hbase sink时报错：\r\n> UpsertStreamTableSink requires that Table has a full primary keys if it is\r\n> updated.\r\n>\r\n>\r\n> 我共创建了4张表，1张kafka source表，3张hbase 维表，1张hbase sink表\r\n> kafka source表与hbase 维表left join后的结果insert到hbase sink表中：\r\n> sql如下：\r\n> create table user_click_source(\r\n> `id` bigint,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint,\r\n> `catalog_id` int,\r\n> `device_id` int,\r\n> `user_id` int,\r\n> `proc_time` timestamp(3)\r\n> PRIMARY KEY (id) NOT ENFORCED\r\n> )with(\r\n> 'connector.type' = 'kafka',\r\n> ……\r\n> )\r\n> ;\r\n> create table dim_user(\r\n> `rowkey` varchar,\r\n> cf ROW<\r\n> `id` int,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint\r\n> >,\r\n> ts bigint\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n>\r\n>\r\n> create table dim_device(\r\n> `rowkey` varchar,\r\n> cf ROW<\r\n> `id` int,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint\r\n> >\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n>\r\n>\r\n> create table dim_catalog(\r\n> `rowkey` varchar,\r\n> cf ROW<\r\n> `id` int,\r\n> `name` varchar,\r\n> `kafka_partition` int,\r\n> `event_time` bigint,\r\n> `write_time` bigint,\r\n> `snapshot_time` bigint,\r\n> `max_snapshot_time` bigint\r\n> >\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n> create table hbase_full_user_click_case1_sink(\r\n> `rowkey` bigint,\r\n> cf ROW<\r\n> `click_id` bigint,\r\n> `click_name` varchar,\r\n> `click_partition` int,\r\n> `click_event_time` bigint,\r\n> `click_write_time` bigint,\r\n> `click_snapshot_time` bigint,\r\n> `click_max_snapshot_time` bigint,\r\n> `catalog_id` int,\r\n> `catalog_name` varchar,\r\n> `catalog_partition` int,\r\n> `catalog_event_time` bigint,\r\n> `catalog_write_time` bigint,\r\n> `catalog_snapshot_time` bigint,\r\n> `catalog_max_snapshot_time` bigint,\r\n> `device_id` int,\r\n> `device_name` varchar,\r\n> `device_partition` int,\r\n> `device_event_time` bigint,\r\n> `device_write_time` bigint,\r\n> `device_snapshot_time` bigint,\r\n> `device_max_snapshot_time` bigint,\r\n> `user_id` int,\r\n> `user_name` varchar,\r\n> `user_partition` int,\r\n> `user_event_time` bigint,\r\n> `user_write_time` bigint,\r\n> `user_snapshot_time` bigint,\r\n> `user_max_snapshot_time` bigint\r\n> >,\r\n> PRIMARY KEY (rowkey) NOT ENFORCED\r\n> )with(\r\n> 'connector.type'='hbase',\r\n> ……\r\n> )\r\n> ;\r\n> insert into hbase_full_user_click_case1_sink\r\n> select\r\n> `click_id`,\r\n> ROW(\r\n> `click_id`,\r\n> `click_name`,\r\n> `click_partition`,\r\n> `click_event_time`,\r\n> `click_write_time`,\r\n> `click_snapshot_time`,\r\n> `click_max_snapshot_time`,\r\n> `catalog_id`,\r\n> `catalog_name`,\r\n> `catalog_partition`,\r\n> `catalog_event_time`,\r\n> `catalog_write_time`,\r\n> `catalog_snapshot_time`,\r\n> `catalog_max_snapshot_time`,\r\n> `device_id`,\r\n> `device_name`,\r\n> `device_partition`,\r\n> `device_event_time`,\r\n> `device_write_time`,\r\n> `device_snapshot_time`,\r\n> `device_max_snapshot_time`,\r\n> `user_id`,\r\n> `user_name`,\r\n> `user_partition`,\r\n> `user_event_time`,\r\n> `user_write_time`,\r\n> `user_snapshot_time`,\r\n> `user_max_snapshot_time`\r\n> )\r\n> from (select\r\n> click.id as `click_id`,\r\n> click.name as `click_name`,\r\n> click.kafka_partition as `click_partition`,\r\n> click.event_time as `click_event_time`,\r\n> click.write_time as `click_write_time`,\r\n> click.snapshot_time as `click_snapshot_time`,\r\n> click.max_snapshot_time as `click_max_snapshot_time`,\r\n> cat.cf.id as `catalog_id`,\r\n> cat.cf.name as `catalog_name`,\r\n> cat.cf.kafka_partition as `catalog_partition`,\r\n> cat.cf.event_time as `catalog_event_time`,\r\n> cat.cf.write_time as `catalog_write_time`,\r\n> cat.cf.snapshot_time as `catalog_snapshot_time`,\r\n> cat.cf.max_snapshot_time as `catalog_max_snapshot_time`,\r\n> dev.cf.id as `device_id`,\r\n> dev.cf.name as `device_name`,\r\n> dev.cf.kafka_partition as `device_partition`,\r\n> dev.cf.event_time as `device_event_time`,\r\n> dev.cf.write_time as `device_write_time`,\r\n> dev.cf.snapshot_time as `device_snapshot_time`,\r\n> dev.cf.max_snapshot_time as `device_max_snapshot_time`,\r\n> u.cf.id as `user_id`,\r\n> u.cf.name as `user_name`,\r\n> u.cf.kafka_partition as `user_partition`,\r\n> u.cf.event_time as `user_event_time`,\r\n> u.cf.write_time as `user_write_time`,\r\n> u.cf.snapshot_time as `user_snapshot_time`,\r\n> u.cf.max_snapshot_time as `user_max_snapshot_time`\r\n>\r\n>\r\n> from (select\r\n> id,\r\n> `name`,\r\n> `kafka_partition`,\r\n> `event_time`,\r\n> `write_time`,\r\n> `snapshot_time`,\r\n> `max_snapshot_time`,\r\n> cast(catalog_id as varchar) as catalog_key,\r\n> cast(device_id as varchar) as device_key,\r\n> cast(user_id as varchar) as user_key,\r\n> `catalog_id`,\r\n> `device_id`,\r\n> `user_id`,\r\n> `proc_time`,\r\n> `event_time`,\r\n> FROM user_click_source\r\n> GROUP BY TUMBLE(event_time, INTERVAL '1' SECOND),\r\n> `id`,\r\n> `name`,\r\n> `kafka_partition`,\r\n> `event_time`,\r\n> `write_time`,\r\n> `snapshot_time`,\r\n> `max_snapshot_time`,\r\n> `catalog_id`,\r\n> `device_id`,\r\n> `user_id`,\r\n> `proc_time`) click\r\n>\r\n>\r\n> left join dim_catalog cat on click.catalog_key = cat.rowkey\r\n> left join dim_device dev on click.device_key = dev.rowkey\r\n> left join dim_user u on click.user_key = u.rowkey and click.event_time =\r\n> u.ts\r\n> ) t\r\n",
        "Attach": []
    },
    "2020-08-14 07:02:00": {
        "From": "17610775726@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: 请教大佬一个在flink调用kafka数据源时'scan.startup.mode'参数的使用问题",
        "Content": "hi\r\n\r\n参数是这么写的没错 'scan.startup.mode' = 'earliest-offset' 你确定你是用的新的groupid吗\n我这里测试是可以的从头开始消费的 不知道是不是你测试的方法不对\n\n\n\r\n--\r\nSent from: http://apache-flink.147419.n8.nabble.com/",
        "Attach": []
    },
    "2020-08-14 06:22:01": {
        "From": "17610775726@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: Flink任务写入kafka 开启了EOS，statebackend为rocksdb，任务反压source端日志量堆积从cp恢复失败",
        "Content": "hi\r\n\r\n没有日志不太好定位失败的原因 但是没有设置uid的话 是有可能重启失败的 建议还是都设置uid最好\n\n\n\r\n--\r\nSent from: http://apache-flink.147419.n8.nabble.com/",
        "Attach": []
    },
    "2020-08-14 19:21:04": {
        "From": "kandy1203@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re:Re: Re: Re: Re:Re: flink 1.11 StreamingFileWriter及sql-client问题",
        "Content": "@Jingsong  orc格式，都看过了，还是没有commit。感觉你们可以测一下这个场景\n\n在 2020-08-12 16:04:13，\"Jingsong Li\" <jingsonglee0@gmail.com> 写道：\n>另外问一下，是什么格式？csv还是parquet。\n>有等到10分钟(rollover-interval)过后和下一次checkpoint后再看吗？\n>\n>On Wed, Aug 12, 2020 at 2:45 PM kandy.wang <kandy1203@163.com> wrote:\n>\n>>\n>>\n>>\n>>\n>>\n>>\n>> 有的。就是写了一半，做了一个checkpoint ，然后程序 做一个savepoint cancel掉，\n>> 重启的时候，从最新的savepoint恢复，但是重启的时候已经属于新分区了。\n>> 就是感觉停止之前正在写的那个分区，没有触发commit\n>>\n>>\n>>\n>>\n>> 在 2020-08-12 14:26:53，\"Jingsong Li\" <jingsonglee0@gmail.com> 写道：\n>> >那你之前的分区除了in-progress文件，有已完成的文件吗？\n>> >\n>> >On Wed, Aug 12, 2020 at 1:57 PM kandy.wang <kandy1203@163.com> wrote:\n>> >\n>> >>\n>> >>\n>> >>\n>> >> source就是kafka\n>> >>\n>> json格式，是exactly-once，按照process-time处理就已经写完了呢。起来的时候，process-time已经属于新的分区了，很正常。但以前的老分区状态还没提交呢。\n>> >>\n>> >>\n>> >>\n>> >>\n>> >>\n>> >>\n>> >> in-progress还在，就证明了这个分区的数据还没写完，理论上源头数据需要回退消费，那为什么你重启后作业不会再写这个分区了呢？\n>> >>\n>> >>\n>> >>\n>> >> in-progress还在，就证明了这个分区的数据还没写完，理论上源头数据需要回退消费，那为什么你重启后作业不会再写这个分区了呢？\n>> >>\n>> >> 在 2020-08-12 13:28:01，\"Jingsong Li\" <jingsonglee0@gmail.com> 写道：\n>> >> >你的source是exactly-once的source吗？\n>> >> >\n>> >> >in-progress还在，就证明了这个分区的数据还没写完，理论上源头数据需要回退消费，那为什么你重启后作业不会再写这个分区了呢？\n>> >> >\n>> >> >On Wed, Aug 12, 2020 at 12:51 PM kandy.wang <kandy1203@163.com> wrote:\n>> >> >\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >>\n>> >> >> >@ Jingsong\n>> >> >>\n>> >> >> >导致的影响是停止前的那个分区，分区没有提交， 当程序起来之后，写的分区和之前分区不是同一个分区，没有_SUCCESS文件标记。\n>> >> >> 用presto查询查不了\n>> >> >> 举例：12：35分钟应当写的是 12：35 00秒 -12：39分 59秒 之间的数据，\n>> >> >>  'sink.partition-commit.trigger'='process-time',\n>> >> >>   'sink.partition-commit.delay'='0 min',\n>> >> >>\n>>  'sink.partition-commit.policy.kind'='metastore,success-file,custom',\n>> >> >>   'sink.rolling-policy.check-interval'='30s',\n>> >> >>   'sink.rolling-policy.rollover-interval'='10min',\n>> >> >>   'sink.rolling-policy.file-size'='128MB'\n>> >> >>    如果是12：39分 05秒左右做一次savepoint，然后\n>> >> >> 12：41分程序重启后，发现之前的12：35分区不再写入，里面的in-progress文件还在，但是分区没有提交，没有往hive add\n>> >> >> partition，就导致有数据，但是确查不 了。\n>> >> >>\n>> >>\n>> 按照你说的，in-progress文件对没影响，但是影响了分区提交。就没地方触发之前12：35分区提交逻辑了。相当于丢了一个分区。这种情况我试了一下，手动add\n>> >> >> partition 也能查了。\n>> >> >> >\n>> >> >> >\n>> >> >> >\n>> >> >> >在 2020-08-12 12:11:53，\"Jingsong Li\" <jingsonglee0@gmail.com> 写道：\n>> >> >> >>in-progress文件带来了什么具体问题吗？它们是多余的文件，对流程没有影响\n>> >> >> >>\n>> >> >> >>On Wed, Aug 12, 2020 at 11:05 AM Jark Wu <imjark@gmail.com> wrote:\n>> >> >> >>\n>> >> >> >>> 与我所知，(2) & (3) 有希望能在 1.12 中支持。\n>> >> >> >>>\n>> >> >> >>> On Tue, 11 Aug 2020 at 21:15, kandy.wang <kandy1203@163.com>\n>> wrote:\n>> >> >> >>>\n>> >> >> >>> > 1.StreamingFileWriter\n>> >> 测试下来目前发现，sql方式提交任务，不能从checkpoint、savepoint恢复。\n>> >> >> >>> >    举例：5min产生一个分区，数据按照process_time来落，hm= 2100 的分区， 在\n>> >> >> >>> > 21：04分左右的时候做一次checkpoint 或savepoint，重启任务的时候，hm\n>> >> >> >>> > =2100分区的数据还存在很多的in-progress文件。\n>> >> >> >>> >\n>> 另外，目前在hdfs目录下没看到pending文件，想了解一下这文件状态是如何转换的，跟之前的bucketsink好像实现不太一样。\n>> >> >> >>> >\n>> >> >> >>> >\n>> >> >> >>> > 2. sql-client不支持 checkpoint savepoint恢复的问题，何时可以支持\n>> >> >> >>> >\n>> >> >> >>> >\n>> >> >> >>> > 3.sql-client 提交任务，不支持StatementSet批量提交，何时可以支持\n>> >> >> >>>\n>> >> >> >>\n>> >> >> >>\n>> >> >> >>--\n>> >> >> >>Best, Jingsong Lee\n>> >> >>\n>> >> >\n>> >> >\n>> >> >--\n>> >> >Best, Jingsong Lee\n>> >>\n>> >\n>> >\n>> >--\n>> >Best, Jingsong Lee\n>>\n>\n>\n>-- \n>Best, Jingsong Lee\n",
        "Attach": []
    },
    "2020-08-14 19:13:02": {
        "From": "hinobleyd@gmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "FlinkSQL tableEnv 依赖问题",
        "Content": "代码如下：\r\n// tEnv;\r\ntEnv.sqlUpdate(\"create table dr1(  \" +\r\n        \"  cid STRING,  \" +\r\n        \"  server_time BIGINT,  \" +\r\n        \"  d MAP<STRING, STRING>,  \" +\r\n        \"  process_time AS PROCTIME(),  \" +\r\n        \"  event_time AS TO_TIMESTAMP(FROM_UNIXTIME(server_time / 1000)),  \" +\r\n        \"  WATERMARK FOR event_time AS event_time - INTERVAL '60' SECOND  \" +\r\n        \") WITH (  \" +\r\n        \"  'update-mode' = 'append',  \" +\r\n        \"  'connector.type' = 'kafka',  \" +\r\n        \"  'connector.version' = 'universal',  \" +\r\n        \"  'connector.topic' = 'antibot_dr1',  \" +\r\n        \"  'connector.startup-mode' = 'latest-offset',  \" +\r\n        \"  'connector.properties.zookeeper.connect' =\r\n'yq01-sw-xxx03.yq01:8681',  \" +\r\n        \"  'connector.properties.bootstrap.servers' =\r\n'yq01-sw-xxx03.yq01:8192',  \" +\r\n        \"  'format.type' = 'json'  \" +\r\n        \")\");\r\nTable t1 = tEnv.sqlQuery(\"select * from dr1\");\r\n\r\n我打包会把flink-json打包进去，最终结果包是test.jar。\r\n\r\ntest.jar是个fat jar，相关依赖都有了。\r\n\r\n然后我执行：flink run -c test.SQLWC1 --detached  test.jar 报错：\r\n\r\nCaused by: org.apache.flink.table.api.NoMatchingTableFactoryException:\r\nCould not find a suitable table factory for\r\n'org.apache.flink.table.factories.DeserializationSchemaFactory' in\r\nthe classpath.\r\n\r\n可是我flink-json.jar都打包进去了，居然还是报错。。。\r\n\r\n解决方式，必须是执行 flink run -c test.SQLWC1 --detached  test.jar 这个命令的机器\r\n\r\n上的flink的环境中有flink-json这个包。但实际上这个机器只作为提交，实际执行任务的集群是另一个机器。\r\n\r\n搞不懂，FlinkSQL找依赖的过程到底啥情况，我fat jar打包进去的flink-json不会被考虑吗？\r\n",
        "Attach": []
    },
    "2020-08-14 03:51:04": {
        "From": "zhbmeng@126.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "flink1.9.1用采用-d（分离模式提交）作业报错，但是不加-d是可以正常跑的",
        "Content": "请问大家：\r\n我采用如下命令提交：\r\nflink run \\\r\n-m yarn-cluster \\\r\n-yn 3 \\\r\n-ys 3 \\\r\n-yjm 2048m \\\r\n-ytm 2048m \\\r\n-ynm flink_test \\\r\n-d \\\r\n-c net.realtime.app.FlinkTest ./hotmall-flink.jar\r\n就会失败，报错信息如下：\r\n[AMRM Callback Handler Thread] ERROR\r\norg.apache.flink.yarn.YarnResourceManager - Fatal error occurred in\r\nResourceManager.\r\njava.lang.NoSuchMethodError:\r\norg.apache.hadoop.yarn.api.protocolrecords.AllocateRequest.newInstance(IFLjava/util/List;Ljava/util/List;Ljava/util/List;Lorg/apache/hadoop/yarn/api/records/ResourceBlacklistRequest;)Lorg/apache/hadoop/yarn/api/protocolrecords/AllocateRequest;\r\n\tat\r\norg.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:279)\r\n\tat\r\norg.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:273)\r\n[AMRM Callback Handler Thread] ERROR\r\norg.apache.flink.runtime.entrypoint.ClusterEntrypoint - Fatal error occurred\r\nin the cluster entrypoint.\r\njava.lang.NoSuchMethodError:\r\norg.apache.hadoop.yarn.api.protocolrecords.AllocateRequest.newInstance(IFLjava/util/List;Ljava/util/List;Ljava/util/List;Lorg/apache/hadoop/yarn/api/records/ResourceBlacklistRequest;)Lorg/apache/hadoop/yarn/api/protocolrecords/AllocateRequest;\r\n\tat\r\norg.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:279)\r\n\tat\r\norg.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:273)\r\n[flink-akka.actor.default-dispatcher-2] INFO\r\norg.apache.flink.yarn.YarnResourceManager - ResourceManager\r\nakka.tcp://flink@emr-worker-8.cluster-174460:33650/user/resourcemanager was\r\ngranted leadership with fencing token 00000000000000000000000000000000\r\n[BlobServer shutdown hook] INFO org.apache.flink.runtime.blob.BlobServer -\r\nStopped BLOB server at 0.0.0.0:36247\r\n<http://apache-flink.147419.n8.nabble.com/file/t802/%E6%8D%95%E8%8E%B71111.png> \r\n但是我在提交命令时，不加-d，就可以正常提交运行；更奇怪的是，我运行另一个任务，加了-d参数，可以正常提交。\r\n我这个提交失败的任务开始是用如下命令运行的：\r\nnohup flink run \\\r\n-m yarn-cluster \\\r\n-yn 3 \\\r\n-ys 3 \\\r\n-yjm 2048m \\\r\n-ytm 2048m \\\r\n-ynm flink_test \\\r\n-c net.realtime.app.FlinkTest ./hotmall-flink.jar > /logs/flink.log 2>&1 &\r\n > /logs/nohup.out 2>&1 &\r\n\r\n在这个任务挂掉之后，再用-d的方式重启就会出现我开始说的问题，很奇怪，有大佬知道为什么么？\r\n\r\n\r\n\r\n-----\r\nBest Wishes\r\n--\r\nSent from: http://apache-flink.147419.n8.nabble.com/",
        "Attach": []
    },
    "2020-08-14 10:37:01": {
        "From": "wind.fly.vip@outlook.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "回复: flink 1.11 发布sql任务到yarn session报java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ByteStringer",
        "Content": "终于找到了问题，原因是flink-dist-*.jar包中打入了高版本的protobuf-java(3.7.1)，高版本的protobuf-java中LiteralByteString是ByteString的私有内部类：\r\n\r\nprivate static class LiteralByteString extends ByteString.LeafByteString {\r\n  private static final long serialVersionUID = 1L;\r\n\r\n  protected final byte[] bytes;\r\n\r\n  /**\r\n   * Creates a {@code LiteralByteString} backed by the given array, without copying.\r\n   *\r\n   * @param bytes array to wrap\r\n   */\r\n  LiteralByteString(byte[] bytes) {\r\n    if (bytes == null) {\r\n      throw new NullPointerException();\r\n    }\r\n    this.bytes = bytes;\r\n  }\r\n\r\n而HBase Connector(1.4.3) 读取数据过程中初始化org.apache.hadoop.hbase.util.ByteStringer时调用了new LiteralByteString()，这样就无法找到该类，从而报了java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ByteStringer。\r\n\r\n解决方法：flink打包时去掉了protobuf-java(3.7.1)依赖，提交时将protobuf-java:2.5.0作为依赖即可。\r\n________________________________\r\n发件人: wind.fly.vip@outlook.com <wind.fly.vip@outlook.com>\r\n发送时间: 2020年8月13日 10:09\r\n收件人: user-zh@flink.apache.org <user-zh@flink.apache.org>\r\n主题: flink 1.11 发布sql任务到yarn session报java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ByteStringer\r\n\r\nHi, all:\r\n         本人试图将flink-sql-gateway(https://github.com/ververica/flink-sql-gateway)升级到1.11支持版本，将flink sql（用到hbase connector）提交到yarn session后运行时报:\r\n        org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ByteStringer\r\nat org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:248)\r\nat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:221)\r\nat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:388)\r\nat org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:362)\r\nat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:142)\r\nat org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.util.ByteStringer\r\nat org.apache.hadoop.hbase.protobuf.RequestConverter.buildRegionSpecifier(RequestConverter.java:1053)\r\nat org.apache.hadoop.hbase.protobuf.RequestConverter.buildScanRequest(RequestConverter.java:496)\r\nat org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:402)\r\nat org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:274)\r\nat org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)\r\nat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:219)\r\n... 7 more\r\n        经过搜索怀疑可能是因为hbase-protobuf依赖的protobuf-java版本不对，但我怎么查看运行时jm、tm对应的classpath是什么样的，依赖了什么样的jar，希望给出分析思路或方法，谢谢。\r\n",
        "Attach": []
    },
    "2020-08-14 18:04:04": {
        "From": "18579099920@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re:Re: Re:Re: 用hive streaming写 orc文件的问题",
        "Content": "根据我在IDE上面的测试，如果是写入parquet表的话，不添加您发的这段代码，程序依然在运行，并且每间隔checkpoint-interval的时间\n会打印parquet相关的日志，但是如果是写入orc表的话，则没有任何日志输出，程序依然在运行。另外我通过sql client提交相同的任务，\nparquet表依然没有任何问题，而orc表任务无限重启。并报错。\n\njava.io.FileNotFoundException: File does not exist: hdfs://nspt-cs/hive/warehouse/hive_user_orc/ts_dt=2020-08-14/ts_hour=17/ts_minute=55/.part-650c3d36-328a-4d8d-8bdd-c170109edfba-0-0.inprogress.398158d9-eaf7-4863-855e-238c7069e298\n    at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309) ~[flink-shaded-hadoop-2-uber-2.7.5-10.0.jar:2.7.5-10.0]\n    at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301) ~[flink-shaded-hadoop-2-uber-2.7.5-10.0.jar:2.7.5-10.0]\n    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[flink-shaded-hadoop-2-uber-2.7.5-10.0.jar:2.7.5-10.0]\n    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317) ~[flink-shaded-hadoop-2-uber-2.7.5-10.0.jar:2.7.5-10.0]\n    at org.apache.flink.connectors.hive.write.HiveBulkWriterFactory$1.getSize(HiveBulkWriterFactory.java:54) ~[flink-connector-hive_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.formats.hadoop.bulk.HadoopPathBasedPartFileWriter.getSize(HadoopPathBasedPartFileWriter.java:84) ~[flink-connector-hive_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.table.filesystem.FileSystemTableSink$TableRollingPolicy.shouldRollOnEvent(FileSystemTableSink.java:451) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.table.filesystem.FileSystemTableSink$TableRollingPolicy.shouldRollOnEvent(FileSystemTableSink.java:421) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.functions.sink.filesystem.Bucket.write(Bucket.java:193) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.functions.sink.filesystem.Buckets.onElement(Buckets.java:282) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSinkHelper.onElement(StreamingFileSinkHelper.java:104) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.table.filesystem.stream.StreamingFileWriter.processElement(StreamingFileWriter.java:118) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at StreamExecCalc$21.processElement(Unknown Source) ~[?:?]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperator.processElement(WatermarkAssignerOperator.java:123) ~[flink-table-blink_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at StreamExecCalc$4.processElement(Unknown Source) ~[?:?]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.pushToOperator(OperatorChain.java:717) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:692) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.OperatorChain$CopyingChainingOutput.collect(OperatorChain.java:672) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:52) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:30) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:305) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:394) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.functions.source.datagen.DataGeneratorSource.run(DataGeneratorSource.java:82) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:100) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:63) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:201) ~[flink-dist_2.11-1.11.1.jar:1.11.1]\n这个文件是存在的，并且无法关闭，然后又会起新的文件，然后无法关闭，一直重复。\n在使用sql client的过程中，并行度好像只能需要读取的文件数有关。我有一张分区表，进行查询，需要58个并行度，而我的集群只有10个，导致无法查询到数据，我应该\n如果能解决这个问题呢\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n在 2020-08-13 15:40:54，\"Rui Li\" <lirui.fudan@gmail.com> 写道：\n>如果是IDE里执行的话，tableEnv.executeSql是马上返回的，然后就退出了，可以用类似这种写法等作业结束：\n>\n>val tableResult = tEnv.executeSql(insert)\n>// wait to finish\n>tableResult.getJobClient.get\n>  .getJobExecutionResult(Thread.currentThread.getContextClassLoader)\n>  .get\n>\n>> 为什么hive streaming 生成orc文件需要导入flink-orc_2.11jar包，而parquet不需要？\n>\n>这里其实是缺少orc的依赖，按说只有table.exec.hive.fallback-mapred-writer设置为false的时候才会发生，我后面修复一下\n>\n>> sql client 我想要设置checkpoint生成间隔我应该在哪里设置？\n>\n>可以在flink-conf.yaml里设置execution.checkpointing.interval\n>\n>\n>On Thu, Aug 13, 2020 at 10:23 AM flink小猪 <18579099920@163.com> wrote:\n>\n>> 添加不了附件，我就直接贴代码了\n>>\n>> import java.time.Duration\n>>\n>>\n>> import org.apache.flink.streaming.api.{CheckpointingMode,\n>> TimeCharacteristic}\n>> import\n>> org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions\n>> import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\n>> import org.apache.flink.table.api.{EnvironmentSettings, SqlDialect,\n>> TableResult}\n>> import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment\n>> import org.apache.flink.table.catalog.hive.HiveCatalog\n>>\n>>\n>>\n>>\n>> /**\n>>   * author dinghh\n>>   * time 2020-08-11 17:03\n>>   */\n>> object WriteHiveStreaming {\n>>     def main(args: Array[String]): Unit = {\n>>\n>>\n>>         val streamEnv = StreamExecutionEnvironment.getExecutionEnvironment\n>>         streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)\n>>         streamEnv.setParallelism(3)\n>>\n>>\n>>         val tableEnvSettings = EnvironmentSettings.newInstance()\n>>                 .useBlinkPlanner()\n>>                 .inStreamingMode()\n>>                 .build()\n>>         val tableEnv = StreamTableEnvironment.create(streamEnv,\n>> tableEnvSettings)\n>>\n>> tableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_MODE,\n>> CheckpointingMode.EXACTLY_ONCE)\n>>\n>> tableEnv.getConfig.getConfiguration.set(ExecutionCheckpointingOptions.CHECKPOINTING_INTERVAL,\n>> Duration.ofSeconds(20))\n>>\n>>\n>>\n>>\n>>\n>>\n>>         val catalogName = \"my_catalog\"\n>>         val catalog = new HiveCatalog(\n>>             catalogName,              // catalog name\n>>             \"default\",                // default database\n>>\n>> \"D:\\\\ideaspace\\\\data-integrate-bigdata\\\\flink-restart\\\\flink-sql\\\\src\\\\main\\\\resources\",\n>> // Hive config (hive-site.xml) directory\n>>             \"1.1.0\"                   // Hive version\n>>         )\n>>         tableEnv.registerCatalog(catalogName, catalog)\n>>         tableEnv.useCatalog(catalogName)\n>>\n>>\n>>\n>>\n>>         //删除流表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |DROP TABLE IF EXISTS `stream_db`.`datagen_user`\n>>             \"\"\".stripMargin)\n>>\n>>\n>>         //创建流表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |CREATE TABLE `stream_db`.`datagen_user` (\n>>               | id INT,\n>>               | name STRING,\n>>               | dt AS localtimestamp,\n>>               | WATERMARK FOR dt AS dt\n>>               |) WITH (\n>>               | 'connector' = 'datagen',\n>>               | 'rows-per-second'='10',\n>>               | 'fields.id.kind'='random',\n>>               | 'fields.id.min'='1',\n>>               | 'fields.id.max'='1000',\n>>               | 'fields.name.length'='5'\n>>               |)\n>>             \"\"\".stripMargin)\n>>\n>>\n>>         //切换hive方言\n>>         tableEnv.getConfig.setSqlDialect(SqlDialect.HIVE)\n>>\n>>\n>>         //删除hive orc表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |DROP TABLE IF EXISTS `default`.`hive_user_orc`\n>>               |\n>>             \"\"\".stripMargin)\n>>\n>>\n>>         //创建hive orc表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |CREATE TABLE `default`.`hive_user_orc` (\n>>               |  id INT,\n>>               |  name STRING\n>>               |) PARTITIONED BY (ts_dt STRING, ts_hour STRING,ts_minute\n>> STRING ) STORED AS ORC TBLPROPERTIES (\n>>               |  'partition.time-extractor.timestamp-pattern'='$ts_dt\n>> $ts_hour:$ts_minute:00.000',\n>>               |  'sink.partition-commit.trigger'='partition-time',\n>>               |  'sink.partition-commit.delay'='1 min',\n>>               |\n>> 'sink.partition-commit.policy.kind'='metastore,success-file'\n>>               |)\n>>             \"\"\".stripMargin)\n>>\n>>\n>>         //删除hive parquet表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |DROP TABLE IF EXISTS `default`.`hive_user_parquet`\n>>             \"\"\".stripMargin)\n>>         //创建hive parquet表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |CREATE TABLE `default`.`hive_user_parquet` (\n>>               |  id INT,\n>>               |  name STRING\n>>               |) PARTITIONED BY (ts_dt STRING, ts_hour STRING,ts_minute\n>> STRING) STORED AS PARQUET TBLPROPERTIES (\n>>               |  'partition.time-extractor.timestamp-pattern'='$ts_dt\n>> $ts_hour:$ts_minute:00.000',\n>>               |  'sink.partition-commit.trigger'='partition-time',\n>>               |  'sink.partition-commit.delay'='1 min',\n>>               |\n>> 'sink.partition-commit.policy.kind'='metastore,success-file'\n>>               |)\n>>             \"\"\".stripMargin)\n>>         //设置flink方言\n>>         tableEnv.getConfig.setSqlDialect(SqlDialect.DEFAULT)\n>>         //流式写入orc表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |INSERT INTO `default`.`hive_user_orc`\n>>               |SELECT\n>>               |    id,name,\n>>               |    DATE_FORMAT(dt,'yyyy-MM-dd'),\n>>               |    DATE_FORMAT(dt,'HH'),\n>>               |    DATE_FORMAT(dt,'mm')\n>>               |FROM\n>>               |    stream_db.datagen_user\n>>             \"\"\".stripMargin)\n>>         //流式写入parquet表\n>>         tableEnv.executeSql(\n>>             \"\"\"\n>>               |INSERT INTO `default`.`hive_user_parquet`\n>>               |SELECT\n>>               |    id,name,\n>>               |    DATE_FORMAT(dt,'yyyy-MM-dd'),\n>>               |    DATE_FORMAT(dt,'HH'),\n>>               |    DATE_FORMAT(dt,'mm')\n>>               |FROM\n>>               |    stream_db.datagen_user\n>>             \"\"\".stripMargin)\n>>\n>>\n>>     }\n>>\n>>\n>> }\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>> 在 2020-08-13 10:08:55，\"flink小猪\" <18579099920@163.com> 写道：\n>>\n>>\n>>\n>>\n>> 尴尬，我明明上传了附件但是找不到了- -，我又上传了一次。\n>>\n>>\n>>\n>>\n>>\n>> 1.写orc和写parquet的作业在同一个作业中，并没有报错，但是hive中查不到数据，在hdfs目录里面有但是并没有按照checkpoint间隔生成，也没有生成_success文件。\n>> 2.没有设置table.exec.hive.fallback-mapred-writer。\n>> 以下是我的几个疑问\n>> 1.为什么hive streaming 生成orc文件需要导入flink-orc_2.11jar包，而parquet不需要？\n>> 2.sql client 我想要设置checkpoint生成间隔我应该在哪里设置？ 以下是hdfs目录图片\n>>\n>> 这是orc生成的文件\n>>\n>> 这是parquet生成的文件\n>>\n>>\n>>\n>>\n>>\n>> 在 2020-08-12 17:33:30，\"Rui Li\" <lirui.fudan@gmail.com> 写道：\n>> >Hi，\n>> >\n>> >写orc表的作业有报错么？还是成功执行但是hive查不到数据呢？\n>> >看不到你贴的代码，有没有设置table.exec.hive.fallback-mapred-writer?\n>> >\n>> >On Wed, Aug 12, 2020 at 5:14 PM 18579099920@163.com <18579099920@163.com>\n>> >wrote:\n>> >\n>> >>\n>> >>\n>> >>\n>> 我通过datagen作为流表，分别写入两个表结构相同，存储格式不同的hive表（一个orc，一个parquet）中，其中parquet表正常写入并且生成了_SUCCESS文件，hive也能查询到，\n>> >>\n>> >>\n>> 但是orc表没有生成_SUCCESS文件，并且hive中无法查询到，我是在本地ide上直接运行的，hive版本是1.2.1，flink版本是1.11.1，同时我发现orc表的分区中生成的文件数量比parquet多，\n>> >>\n>> 而且不会根据checkpoint间隔生成（parquet符合checkpoint间隔）。而且需要导入flink-orc_2.11jar包(parquet不需要),否则报Exception\n>> >> in thread \"main\" java.lang.NoClassDefFoundError:\n>> >> org/apache/orc/TypeDescription错误。并且parquet每间隔checkpoint interval\n>> >> 会输出parquet相关的日志，而orc的并没有日志产生，请问是什么原因？我已贴上代码。\n>> >> ------------------------------\n>> >> 18579099920@163.com\n>> >>\n>> >\n>> >\n>> >--\n>> >Best regards!\n>> >Rui Li\n>>\n>>\n>>\n>>\n>>\n>>\n>>\n>\n>\n>\n>-- \n>Best regards!\n>Rui Li\n",
        "Attach": []
    },
    "2020-08-14 01:44:03": {
        "From": "17610775726@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: 用hive streaming写 orc文件的问题",
        "Content": "hi\r\n\r\n我这边测试了ORC的,只需要把stored as pauquet 改成stored as\norc即可,success文件能生成,hive里面也能查看数据,但是有一个问题是,Flink Web UI上面显示的数据量是不对的 UI\n上面的records send 一直在增大 即使我已经停止向kafka写入数据了 但是hive 里面的数据是对的 我写了30条\nhive里面查出来的确实是30条 但UI上面已经显示480条了 且还在增加\n\n\n\r\n--\r\nSent from: http://apache-flink.147419.n8.nabble.com/",
        "Attach": []
    },
    "2020-08-14 16:35:01": {
        "From": "hinobleyd@gmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: 如何设置FlinkSQL并行度",
        "Content": "有结论了。貌似通过sql-client就是不支持。需要通过java/scala写代码方式，基于tableEnv提交sql执行，这种情况下只需要设置好env的检查点即可。\r\n同时本身这种情况执行也是使用flink命令提交的任务，自然也可以基于flink触发保存点，或启动任务且基于检查点。\r\n\r\n赵一旦 <hinobleyd@gmail.com> 于2020年8月14日周五 下午12:03写道：\r\n\r\n> 检查点呢，大多数用FlinkSQL的同学们，你们的任务是随时可运行那种吗，不是必须保证不可间断的准确性级别吗？\r\n>\r\n> Xingbo Huang <hxbks2ks@gmail.com> 于2020年8月14日周五 下午12:01写道：\r\n>\r\n>> Hi,\r\n>>\r\n>> 关于并行度的问题，据我所知，目前Table API上还没法对每一个算子单独设置并行度\r\n>>\r\n>> Best,\r\n>> Xingbo\r\n>>\r\n>> Zhao,Yi(SEC) <zhaoyi09@baidu.com> 于2020年8月14日周五 上午10:49写道：\r\n>>\r\n>> > 并行度问题有人帮忙解答下吗，此外补充个相关问题，除了并行度，flink-sql情况下，能做检查点/保存点，并基于检查点/保存点重启sql任务吗。\r\n>> >\r\n>> > 发件人: \"Zhao,Yi(SEC)\" <zhaoyi09@baidu.com>\r\n>> > 日期: 2020年8月13日 星期四 上午11:44\r\n>> > 收件人: \"user-zh@flink.apache.org\" <user-zh@flink.apache.org>\r\n>> > 主题: 如何设置FlinkSQL并行度\r\n>> >\r\n>> > 看配置文件有 execution. Parallelism，但这个明显是全局类型的配置。\r\n>> > 如何给Sql生成的数据源结点，window结点，sink结点等设置不同的并行度呢？\r\n>> >\r\n>> > 比如数据源理论上应该和kafka分区数一致比较好，window则需要根据数据量考虑计算压力，sink也应该有相应的场景考虑。\r\n>> >\r\n>> >\r\n>>\r\n>\r\n",
        "Attach": []
    },
    "2020-08-14 16:33:01": {
        "From": "fskmine@gmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: TableColumn为啥不包含comment",
        "Content": "hi, 我已经建了一个issue[1]跟踪这个情况，有兴趣的话可以帮忙修复下这个bug。\r\n\r\n[1] https://issues.apache.org/jira/browse/FLINK-18958\r\n\r\nHarold.Miao <miaohonghit@gmail.com> 于2020年8月13日周四 上午11:08写道：\r\n\r\n> hi all\r\n> 我发现TableColumn class不包含column comment  ， 给开发带来了一点麻烦，请教大家一下，谢谢\r\n>\r\n>\r\n> --\r\n>\r\n> Best Regards,\r\n> Harold Miao\r\n>\r\n",
        "Attach": []
    },
    "2020-08-14 01:28:02": {
        "From": "610493544@qq.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: flink 1.11 SQL idea调试无数据也无报错",
        "Content": "hi,\r\n\r\n之前查看邮件列表确实有看到很多地方提到executeSql是一个异步接口.但是我对这部分还是有一些疑惑\r\n\r\n\r\n1.当inset into 的逻辑是简单逻辑的时候可以看到代码有输出,但替换为我最初发的有聚合逻辑的insert into sql\r\n就无法显示输出了,为什么?\r\n代码\r\n...\r\n        tEnv.executeSql(sourceDDL);\r\n        tEnv.executeSql(sinkDDL);\r\n\r\n        tEnv.executeSql(\"INSERT INTO print_sink SELECT  user_id\r\n,item_id,category_id ,behavior ,ts,proctime FROM user_behavior\");\r\n...\r\n控制台\r\n3>\r\n+I(1014646,2869046,4022701,pv,2017-11-27T00:38:15,2020-08-14T08:20:23.847)\r\n3> +I(105950,191177,3975787,pv,2017-11-27T00:38:15,2020-08-14T08:20:23.847)\r\n3>\r\n+I(128322,5013356,4066962,buy,2017-11-27T00:38:15,2020-08-14T08:20:23.847)\r\n3> +I(225652,3487948,2462567,pv,2017-11-27T00:38:15,2020-08-14T08:20:23.847)\r\n\r\n聚合逻辑代码(source不变,sink 对应变更列)\r\n>         String transformationDDL= \"INSERT INTO buy_cnt_per_hour\\n\" +\r\n>                 \"SELECT HOUR(TUMBLE_START(ts, INTERVAL '1' HOUR)) as\r\n> hour_of_day , COUNT(*) as buy_cnt\\n\" +\r\n>                 \"FROM user_behavior\\n\" +\r\n>                 \"WHERE behavior = 'buy'\\n\" +\r\n>                 \"GROUP BY TUMBLE(ts, INTERVAL '1' HOUR)\";\r\n>\r\n>\r\n>\r\n>         //注册source和sink\r\n>         tEnv.executeSql(sourceDDL);\r\n>         tEnv.executeSql(sinkDDL);\r\n> //        tableResult.print();\r\n>\r\n>        tEnv.executeSql(transformationDDL);\r\n\r\n2.没有太理解您说的   手动拿到那个executeSql的返回的TableResult，然后去 ....  wait job finished\r\n代码修改为如下 运行控制台还是没有结果打印\r\n        //注册source和sink\r\n        tEnv.executeSql(sourceDDL);\r\n        tEnv.executeSql(sinkDDL);\r\n\r\n        TableResult tableResult = tEnv.executeSql(transformationDDL);\r\n\r\n        tableResult.getJobClient()\r\n                .get()\r\n               \r\n.getJobExecutionResult(Thread.currentThread().getContextClassLoader())\r\n                .get().wait();\r\n\r\nBest,\r\nDanielGu\r\n\r\n\r\n\r\n--\r\nSent from: http://apache-flink.147419.n8.nabble.com/",
        "Attach": []
    },
    "2020-08-14 13:48:04": {
        "From": "wuleiflink@foxmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "\"=?gb18030?B?eWo1MTgy?=\" <yj5182@gmail.com>",
        "Subject": "回复：请教关于Flink算子FINISHED状态时无法保存Checkpoint的问题",
        "Content": "在我们的生产环境最常用的做法都是通过维表关联的方式进行赋值的；\r\n或者可以先将字典数据写进redis，然后再在第一次使用的时候去访问redis，并加载到State中。\r\n\r\n\r\n\r\n\r\n------------------&nbsp;原始邮件&nbsp;------------------\r\n发件人:                                                                                                                        \"user-zh\"                                                                                    <yj5182@gmail.com&gt;;\r\n发送时间:&nbsp;2020年8月13日(星期四) 中午1:49\r\n收件人:&nbsp;\"user-zh\"<user-zh@flink.apache.org&gt;;\r\n\r\n主题:&nbsp;请教关于Flink算子FINISHED状态时无法保存Checkpoint的问题\r\n\r\n\r\n\r\n请教大佬一个我最近在配置Flink流的过程中遇到问题，\r\nflink作业中关联使用了物理表（字典表），在flinkjob启动后，会对字典表进行一次读取，然后该算子会变成FINISHED状态，导致该flinkjob无法保存checkpoint和savepoint。一般大家遇到这种问题都是怎么处理的，我这个作业在数据加工过程中必须用到字典表赋值。",
        "Attach": []
    },
    "2020-08-14 15:01:00": {
        "From": "sunfulin0321@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re:Re: flink sql作业state size一直增加",
        "Content": "hi, benchao,\n感谢回复，那我是不是可以理解为：去掉minibatch，就可以状态过期清理了哈？\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n在 2020-08-14 14:09:33，\"Benchao Li\" <libenchao@apache.org> 写道：\n>Hi,\n>现在group agg + mini batch 还没有支持状态过期清理，已经有工作[1] 在解决这个问题了。\n>\n>[1] https://issues.apache.org/jira/browse/FLINK-17096\n>\n>sunfulin <sunfulin0321@163.com> 于2020年8月14日周五 下午2:06写道：\n>\n>> hi，我的一个flink sql作业，在启用了idlestateretentiontime设置后，观察到web ui上的state\n>> size还是一直在增大，超过maximum retention time之后state大小也没有减小的情况，请问这个可能是啥原因哈？\n>>\n>>\n>> 使用的flink 版本：flink 1.10.1，启用的state\n>> ttl配置：tableEnv.getConfig.setIdleStateRetentionTime(Time.minutes(5),\n>> Time.minutes(10));\n>> 我的作业逻辑是：统计每个userId每天第一次出现的记录，类似：select userId, first_value(xxx) from\n>> source group by userId, date_format(eventtime, 'yyyy-MM-dd');\n>\n>\n>\n>-- \n>\n>Best,\n>Benchao Li\n",
        "Attach": []
    },
    "2020-08-14 15:08:05": {
        "From": "libenchao@apache.org",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: Re: flink sql作业state size一直增加",
        "Content": "是的。\r\n\r\nsunfulin <sunfulin0321@163.com> 于2020年8月14日周五 下午3:01写道：\r\n\r\n> hi, benchao,\r\n> 感谢回复，那我是不是可以理解为：去掉minibatch，就可以状态过期清理了哈？\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n>\r\n> 在 2020-08-14 14:09:33，\"Benchao Li\" <libenchao@apache.org> 写道：\r\n> >Hi,\r\n> >现在group agg + mini batch 还没有支持状态过期清理，已经有工作[1] 在解决这个问题了。\r\n> >\r\n> >[1] https://issues.apache.org/jira/browse/FLINK-17096\r\n> >\r\n> >sunfulin <sunfulin0321@163.com> 于2020年8月14日周五 下午2:06写道：\r\n> >\r\n> >> hi，我的一个flink sql作业，在启用了idlestateretentiontime设置后，观察到web ui上的state\r\n> >> size还是一直在增大，超过maximum retention time之后state大小也没有减小的情况，请问这个可能是啥原因哈？\r\n> >>\r\n> >>\r\n> >> 使用的flink 版本：flink 1.10.1，启用的state\r\n> >> ttl配置：tableEnv.getConfig.setIdleStateRetentionTime(Time.minutes(5),\r\n> >> Time.minutes(10));\r\n> >> 我的作业逻辑是：统计每个userId每天第一次出现的记录，类似：select userId, first_value(xxx) from\r\n> >> source group by userId, date_format(eventtime, 'yyyy-MM-dd');\r\n> >\r\n> >\r\n> >\r\n> >--\r\n> >\r\n> >Best,\r\n> >Benchao Li\r\n>\r\n\r\n\r\n-- \r\n\r\nBest,\r\nBenchao Li\r\n",
        "Attach": []
    },
    "2020-08-14 14:50:04": {
        "From": "yangpengklf007@gmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: Flink任务写入kafka 开启了EOS，statebackend为rocksdb，任务反压source端日志量堆积从cp恢复失败",
        "Content": "我们这个任务的operator没有分配uid，之前也没有分配uid但是从cp恢复过好几次都成功了 就这次没有成功\r\n\r\n宇 <freeman52018@qq.com> 于2020年8月14日周五 下午1:57写道：\r\n\r\n> 有没有可能是没分配uid，然后dag发生了变化，导致的恢复不了状态\r\n>\r\n>\r\n>\r\n> ---原始邮件---\r\n> 发件人: \"Yang Peng\"<yangpengklf007@gmail.com&gt;\r\n> 发送时间: 2020年8月14日(周五) 中午1:02\r\n> 收件人: \"user-zh\"<user-zh@flink.apache.org&gt;;\r\n> 主题: Flink任务写入kafka 开启了EOS，statebackend为rocksdb，任务反压source端日志量堆积从cp恢复失败\r\n>\r\n>\r\n> Hi,咨询各位一个问题我们有个任务，statebackend为rocksdb\r\n>\r\n> 增量执行cp，flink读取kafka经过处理然后写入到kafka，producer开启了EOS，最近发现任务有反压，source端日志量有积压，然后准备改一下资源分配多加一些资源（没有修改并行度，代码未做修改）从cp恢复任务，任务被cancel之后然后从cp恢复发现起不来了连续两次都不行，由于客户端日志保存时间太短当时没来得及去查看客户端日志，所以没有找到客户端日志，\r\n",
        "Attach": []
    },
    "2020-08-13 23:14:03": {
        "From": "1578166061@qq.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: 请问在 flink sql 中建立的多张表应当怎样分辨接收 kafka 传来的 canal-json？",
        "Content": "谢谢，我明白了。\r\n\r\n祝好\r\n\r\n\r\n\r\n--\r\nSent from: http://apache-flink.147419.n8.nabble.com/",
        "Attach": []
    },
    "2020-08-14 14:09:03": {
        "From": "libenchao@apache.org",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: flink sql作业state size一直增加",
        "Content": "Hi,\r\n现在group agg + mini batch 还没有支持状态过期清理，已经有工作[1] 在解决这个问题了。\r\n\r\n[1] https://issues.apache.org/jira/browse/FLINK-17096\r\n\r\nsunfulin <sunfulin0321@163.com> 于2020年8月14日周五 下午2:06写道：\r\n\r\n> hi，我的一个flink sql作业，在启用了idlestateretentiontime设置后，观察到web ui上的state\r\n> size还是一直在增大，超过maximum retention time之后state大小也没有减小的情况，请问这个可能是啥原因哈？\r\n>\r\n>\r\n> 使用的flink 版本：flink 1.10.1，启用的state\r\n> ttl配置：tableEnv.getConfig.setIdleStateRetentionTime(Time.minutes(5),\r\n> Time.minutes(10));\r\n> 我的作业逻辑是：统计每个userId每天第一次出现的记录，类似：select userId, first_value(xxx) from\r\n> source group by userId, date_format(eventtime, 'yyyy-MM-dd');\r\n\r\n\r\n\r\n-- \r\n\r\nBest,\r\nBenchao Li\r\n",
        "Attach": []
    },
    "2020-08-14 14:05:04": {
        "From": "sunfulin0321@163.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "flink sql作业state size一直增加",
        "Content": "hi，我的一个flink sql作业，在启用了idlestateretentiontime设置后，观察到web ui上的state size还是一直在增大，超过maximum retention time之后state大小也没有减小的情况，请问这个可能是啥原因哈？\n\n\n使用的flink 版本：flink 1.10.1，启用的state ttl配置：tableEnv.getConfig.setIdleStateRetentionTime(Time.minutes(5), Time.minutes(10));\n我的作业逻辑是：统计每个userId每天第一次出现的记录，类似：select userId, first_value(xxx) from source group by userId, date_format(eventtime, 'yyyy-MM-dd');",
        "Attach": []
    },
    "2020-08-14 13:57:03": {
        "From": "freeman52018@qq.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "回复：Flink任务写入kafka 开启了EOS，statebackend为rocksdb，任务反压source端日志量堆积从cp恢复失败",
        "Content": "有没有可能是没分配uid，然后dag发生了变化，导致的恢复不了状态\r\n\r\n\r\n\r\n---原始邮件---\r\n发件人: \"Yang Peng\"<yangpengklf007@gmail.com&gt;\r\n发送时间: 2020年8月14日(周五) 中午1:02\r\n收件人: \"user-zh\"<user-zh@flink.apache.org&gt;;\r\n主题: Flink任务写入kafka 开启了EOS，statebackend为rocksdb，任务反压source端日志量堆积从cp恢复失败\r\n\r\n\r\nHi,咨询各位一个问题我们有个任务，statebackend为rocksdb\r\n增量执行cp，flink读取kafka经过处理然后写入到kafka，producer开启了EOS，最近发现任务有反压，source端日志量有积压，然后准备改一下资源分配多加一些资源（没有修改并行度，代码未做修改）从cp恢复任务，任务被cancel之后然后从cp恢复发现起不来了连续两次都不行，由于客户端日志保存时间太短当时没来得及去查看客户端日志，所以没有找到客户端日志，",
        "Attach": []
    },
    "2020-08-14 13:43:01": {
        "From": "yj5182@gmail.com",
        "To": "user-zh@flink.apache.org",
        "Cc": "",
        "Subject": "Re: 请教大佬一个在flink调用kafka数据源时'scan.startup.mode'参数的使用问题",
        "Content": "好的，谢谢，我试一下！\r\n\r\n魏子涵 <wzh1007181398@163.com> 于2020年8月14日周五 下午1:35写道：\r\n\r\n> 建议先不使用flink的Kafka来消费，先自己编码写一个kafka消费看是否还是有这个问题，作个对比，看是否是flink提供的kafka接口的问题。\r\n>\r\n>\r\n> | |\r\n> 魏子涵\r\n> |\r\n> |\r\n> 邮箱：wzh1007181398@163.com\r\n> |\r\n>\r\n> 签名由 网易邮箱大师 定制\r\n>\r\n> 在2020年08月14日 13:27，yulu yang 写道：\r\n>  我这个flink作业和和分组都是新创建的，没有抽取历史\r\n> group是新的\r\n>\r\n> 魏子涵 <wzh1007181398@163.com> 于2020年8月14日周五 下午1:20写道：\r\n>\r\n> > Kafka客户端的group. id参数有改吗？\r\n> >\r\n> >\r\n> > | |\r\n> > 魏子涵\r\n> > |\r\n> > |\r\n> > 邮箱：wzh1007181398@163.com\r\n> > |\r\n> >\r\n> > 签名由 网易邮箱大师 定制\r\n> >\r\n> > 在2020年08月14日 12:44，yulu yang 写道：\r\n> > 在flink作业中从kafka数据源获取数据，将 参数设置为'scan.startup.mode' = 'earliest-offset',\r\n> > 检测flink运行结果时，发现只抽取了kafka中的newest数据，没有获取到oldest数据。\r\n> > 不知道是不是我这里'scan.startup.mode' 参数用的不对。\r\n> > Flink 版本1.11.1 kafka版本 2.6.0\r\n> >\r\n>\r\n",
        "Attach": []
    },
    "2020-08-14 05:41:05": {
        "From": "user-zh-help@flink.apache.org",
        "To": "liwen00812@163.com",
        "Cc": "",
        "Subject": "WELCOME to user-zh@flink.apache.org",
        "Content": "Hi! This is the ezmlm program. I'm managing the\r\nuser-zh@flink.apache.org mailing list.\r\n\r\nI'm working for my owner, who can be reached\r\nat user-zh-owner@flink.apache.org.\r\n\r\nAcknowledgment: I have added the address\r\n\r\n   liwen00812@163.com\r\n\r\nto the user-zh mailing list.\r\n\r\nWelcome to user-zh@flink.apache.org!\r\n\r\nPlease save this message so that you know the address you are\r\nsubscribed under, in case you later want to unsubscribe or change your\r\nsubscription address.\r\n\r\n\r\n--- Administrative commands for the user-zh list ---\r\n\r\nI can handle administrative requests automatically. Please\r\ndo not send them to the list address! Instead, send\r\nyour message to the correct command address:\r\n\r\nTo subscribe to the list, send a message to:\r\n   <user-zh-subscribe@flink.apache.org>\r\n\r\nTo remove your address from the list, send a message to:\r\n   <user-zh-unsubscribe@flink.apache.org>\r\n\r\nSend mail to the following for info and FAQ for this list:\r\n   <user-zh-info@flink.apache.org>\r\n   <user-zh-faq@flink.apache.org>\r\n\r\nSimilar addresses exist for the digest list:\r\n   <user-zh-digest-subscribe@flink.apache.org>\r\n   <user-zh-digest-unsubscribe@flink.apache.org>\r\n\r\nTo get messages 123 through 145 (a maximum of 100 per request), mail:\r\n   <user-zh-get.123_145@flink.apache.org>\r\n\r\nTo get an index with subject and author for messages 123-456 , mail:\r\n   <user-zh-index.123_456@flink.apache.org>\r\n\r\nThey are always returned as sets of 100, max 2000 per request,\r\nso you'll actually get 100-499.\r\n\r\nTo receive all messages with the same subject as message 12345,\r\nsend a short message to:\r\n   <user-zh-thread.12345@flink.apache.org>\r\n\r\nThe messages should contain one line or word of text to avoid being\r\ntreated as sp@m, but I will ignore their content.\r\nOnly the ADDRESS you send to is important.\r\n\r\nYou can start a subscription for an alternate address,\r\nfor example \"john@host.domain\", just add a hyphen and your\r\naddress (with '=' instead of '@') after the command word:\r\n<user-zh-subscribe-john=host.domain@flink.apache.org>\r\n\r\nTo stop subscription for this address, mail:\r\n<user-zh-unsubscribe-john=host.domain@flink.apache.org>\r\n\r\nIn both cases, I'll send a confirmation message to that address. When\r\nyou receive it, simply reply to it to complete your subscription.\r\n\r\nIf despite following these instructions, you do not get the\r\ndesired results, please contact my owner at\r\nuser-zh-owner@flink.apache.org. Please be patient, my owner is a\r\nlot slower than I am ;-)\r\n\r\n--- Enclosed is a copy of the request I received.\r\n\r\nReturn-Path: <liwen00812@163.com>\r\nReceived: (qmail 70405 invoked by uid 99); 14 Aug 2020 05:41:59 -0000\r\nReceived: from pnap-us-west-generic-nat.apache.org (HELO spamd1-us-west.apache.org) (209.188.14.142)\r\n    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 14 Aug 2020 05:41:59 +0000\r\nReceived: from localhost (localhost [127.0.0.1])\r\n\tby spamd1-us-west.apache.org (ASF Mail Server at spamd1-us-west.apache.org) with ESMTP id 2083EC07DD\r\n\tfor <user-zh-sc.1597383642.omoneopicgbmbieljdno-liwen00812=163.com@flink.apache.org>; Fri, 14 Aug 2020 05:41:59 +0000 (UTC)\r\nX-Virus-Scanned: Debian amavisd-new at spamd1-us-west.apache.org\r\nX-Spam-Flag: NO\r\nX-Spam-Score: 0.251\r\nX-Spam-Level:\r\nX-Spam-Status: No, score=0.251 tagged_above=-999 required=6.31\r\n\ttests=[DKIM_SIGNED=0.1, DKIM_VALID=-0.1, DKIM_VALID_AU=-0.1,\r\n\tDKIM_VALID_EF=-0.1, FREEMAIL_ENVFROM_END_DIGIT=0.25, HTML_MESSAGE=0.2,\r\n\tSPF_HELO_NONE=0.001, SPF_PASS=-0.001, URIBL_BLOCKED=0.001]\r\n\tautolearn=disabled\r\nAuthentication-Results: spamd1-us-west.apache.org (amavisd-new);\r\n\tdkim=pass (1024-bit key) header.d=163.com\r\nReceived: from mx1-ec2-va.apache.org ([10.40.0.8])\r\n\tby localhost (spamd1-us-west.apache.org [10.40.0.7]) (amavisd-new, port 10024)\r\n\twith ESMTP id V0ysCw7IXrzO\r\n\tfor <user-zh-sc.1597383642.omoneopicgbmbieljdno-liwen00812=163.com@flink.apache.org>;\r\n\tFri, 14 Aug 2020 05:41:54 +0000 (UTC)\r\nReceived-SPF: Pass (mailfrom) identity=mailfrom; client-ip=220.181.13.46; helo=m1346.mail.163.com; envelope-from=liwen00812@163.com; receiver=<UNKNOWN> \r\nReceived: from m1346.mail.163.com (m1346.mail.163.com [220.181.13.46])\r\n\tby mx1-ec2-va.apache.org (ASF Mail Server at mx1-ec2-va.apache.org) with ESMTPS id B9FE8BE36B\r\n\tfor <user-zh-sc.1597383642.omoneopicgbmbieljdno-liwen00812=163.com@flink.apache.org>; Fri, 14 Aug 2020 05:41:52 +0000 (UTC)\r\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;\r\n\ts=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=CHgjb\r\n\twyFR0eH42jVcB2o74VhOw6MszuCz0GySeakv4s=; b=YUOlhb3TW6oNZMKzdJtay\r\n\toTx9mxcsjxXDpNJQq50tGCRTkSmkv0GyzgcpHghIjFGnNa8Ojh8tTCjbhxIb/wj9\r\n\tIJzPNkn/fH9XByrXJwrtzVVg/z943nB68mX41igosCvjW189KJjLY6BEpx2bVu4W\r\n\tHEbiTeKlgnrABMY9JkZusY=\r\nReceived: from liwen00812$163.com ( [116.236.73.133] ) by\r\n ajax-webmail-wmsvr46 (Coremail) ; Fri, 14 Aug 2020 13:41:42 +0800 (CST)\r\nX-Originating-IP: [116.236.73.133]\r\nDate: Fri, 14 Aug 2020 13:41:42 +0800 (CST)\r\nFrom: =?GBK?B?wO7OxA==?= <liwen00812@163.com>\r\nTo: \r\n\tuser-zh-sc.1597383642.omoneopicgbmbieljdno-liwen00812=163.com@flink.apache.org\r\nSubject: Re:confirm subscribe to user-zh@flink.apache.org\r\nX-Priority: 3\r\nX-Mailer: Coremail Webmail Server Version XT5.0.10 build 20190724(ac680a23)\r\n Copyright (c) 2002-2020 www.mailtech.cn 163com\r\nIn-Reply-To: <1597383642.68659.ezmlm@flink.apache.org>\r\nReferences: <1597383642.68659.ezmlm@flink.apache.org>\r\nX-CM-CTRLDATA: oTj1IWZvb3Rlcl9odG09NzQ3NDo1Ng==\r\nContent-Type: multipart/alternative; \r\n\tboundary=\"----=_Part_51354_685325745.1597383702287\"\r\nMIME-Version: 1.0\r\nMessage-ID: <7fa64b00.373a.173eb7cf70f.Coremail.liwen00812@163.com>\r\nX-Coremail-Locale: zh_CN\r\nX-CM-TRANSID:LsGowACHSmUWJDZf7TR0AA--.10840W\r\nX-CM-SenderInfo: polzv0iqqyiji6rwjhhfrp/1tbiMgyANlWBqP9lNwABsY\r\nX-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==",
        "Attach": []
    },
    "2020-08-14 05:40:04": {
        "From": "user-zh-help@flink.apache.org",
        "To": "liwen00812@163.com",
        "Cc": "",
        "Subject": "confirm subscribe to user-zh@flink.apache.org",
        "Content": "Hi! This is the ezmlm program. I'm managing the\r\nuser-zh@flink.apache.org mailing list.\r\n\r\nI'm working for my owner, who can be reached\r\nat user-zh-owner@flink.apache.org.\r\n\r\nTo confirm that you would like\r\n\r\n   liwen00812@163.com\r\n\r\nadded to the user-zh mailing list, please send\r\na short reply to this address:\r\n\r\n   user-zh-sc.1597383642.omoneopicgbmbieljdno-liwen00812=163.com@flink.apache.org\r\n\r\nUsually, this happens when you just hit the \"reply\" button.\r\nIf this does not work, simply copy the address and paste it into\r\nthe \"To:\" field of a new message.\r\n\r\nor click here:\r\n\tmailto:user-zh-sc.1597383642.omoneopicgbmbieljdno-liwen00812=163.com@flink.apache.org\r\n\r\nThis confirmation serves two purposes. First, it verifies that I am able\r\nto get mail through to you. Second, it protects you in case someone\r\nforges a subscription request in your name.\r\n\r\nPlease note that ALL Apache dev- and user- mailing lists are publicly\r\narchived.  Do familiarize yourself with Apache's public archive policy at\r\n\r\n    http://www.apache.org/foundation/public-archives.html\r\n\r\nprior to subscribing and posting messages to user-zh@flink.apache.org.\r\nIf you're not sure whether or not the policy applies to this mailing list,\r\nassume it does unless the list name contains the word \"private\" in it.\r\n\r\nSome mail programs are broken and cannot handle long addresses. If you\r\ncannot reply to this request, instead send a message to\r\n<user-zh-request@flink.apache.org> and put the\r\nentire address listed above into the \"Subject:\" line.\r\n\r\n\r\n--- Administrative commands for the user-zh list ---\r\n\r\nI can handle administrative requests automatically. Please\r\ndo not send them to the list address! Instead, send\r\nyour message to the correct command address:\r\n\r\nTo subscribe to the list, send a message to:\r\n   <user-zh-subscribe@flink.apache.org>\r\n\r\nTo remove your address from the list, send a message to:\r\n   <user-zh-unsubscribe@flink.apache.org>\r\n\r\nSend mail to the following for info and FAQ for this list:\r\n   <user-zh-info@flink.apache.org>\r\n   <user-zh-faq@flink.apache.org>\r\n\r\nSimilar addresses exist for the digest list:\r\n   <user-zh-digest-subscribe@flink.apache.org>\r\n   <user-zh-digest-unsubscribe@flink.apache.org>\r\n\r\nTo get messages 123 through 145 (a maximum of 100 per request), mail:\r\n   <user-zh-get.123_145@flink.apache.org>\r\n\r\nTo get an index with subject and author for messages 123-456 , mail:\r\n   <user-zh-index.123_456@flink.apache.org>\r\n\r\nThey are always returned as sets of 100, max 2000 per request,\r\nso you'll actually get 100-499.\r\n\r\nTo receive all messages with the same subject as message 12345,\r\nsend a short message to:\r\n   <user-zh-thread.12345@flink.apache.org>\r\n\r\nThe messages should contain one line or word of text to avoid being\r\ntreated as sp@m, but I will ignore their content.\r\nOnly the ADDRESS you send to is important.\r\n\r\nYou can start a subscription for an alternate address,\r\nfor example \"john@host.domain\", just add a hyphen and your\r\naddress (with '=' instead of '@') after the command word:\r\n<user-zh-subscribe-john=host.domain@flink.apache.org>\r\n\r\nTo stop subscription for this address, mail:\r\n<user-zh-unsubscribe-john=host.domain@flink.apache.org>\r\n\r\nIn both cases, I'll send a confirmation message to that address. When\r\nyou receive it, simply reply to it to complete your subscription.\r\n\r\nIf despite following these instructions, you do not get the\r\ndesired results, please contact my owner at\r\nuser-zh-owner@flink.apache.org. Please be patient, my owner is a\r\nlot slower than I am ;-)\r\n\r\n--- Enclosed is a copy of the request I received.\r\n\r\nReturn-Path: <liwen00812@163.com>\r\nReceived: (qmail 68630 invoked by uid 99); 14 Aug 2020 05:40:41 -0000\r\nReceived: from pnap-us-west-generic-nat.apache.org (HELO spamd2-us-west.apache.org) (209.188.14.142)\r\n    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 14 Aug 2020 05:40:41 +0000\r\nReceived: from localhost (localhost [127.0.0.1])\r\n\tby spamd2-us-west.apache.org (ASF Mail Server at spamd2-us-west.apache.org) with ESMTP id 4E10C1A3312\r\n\tfor <user-zh-subscribe@flink.apache.org>; Fri, 14 Aug 2020 05:40:41 +0000 (UTC)\r\nX-Virus-Scanned: Debian amavisd-new at spamd2-us-west.apache.org\r\nX-Spam-Flag: NO\r\nX-Spam-Score: 0.25\r\nX-Spam-Level:\r\nX-Spam-Status: No, score=0.25 tagged_above=-999 required=6.31\r\n\ttests=[DKIM_SIGNED=0.1, DKIM_VALID=-0.1, DKIM_VALID_AU=-0.1,\r\n\tDKIM_VALID_EF=-0.1, FREEMAIL_ENVFROM_END_DIGIT=0.25, HTML_MESSAGE=0.2,\r\n\tSPF_HELO_NONE=0.001, SPF_PASS=-0.001] autolearn=disabled\r\nAuthentication-Results: spamd2-us-west.apache.org (amavisd-new);\r\n\tdkim=pass (1024-bit key) header.d=163.com\r\nReceived: from mx1-ec2-va.apache.org ([10.40.0.8])\r\n\tby localhost (spamd2-us-west.apache.org [10.40.0.9]) (amavisd-new, port 10024)\r\n\twith ESMTP id qFyBmau_jHhJ for <user-zh-subscribe@flink.apache.org>;\r\n\tFri, 14 Aug 2020 05:40:39 +0000 (UTC)\r\nReceived-SPF: Pass (mailfrom) identity=mailfrom; client-ip=220.181.13.46; helo=m1346.mail.163.com; envelope-from=liwen00812@163.com; receiver=<UNKNOWN> \r\nReceived: from m1346.mail.163.com (m1346.mail.163.com [220.181.13.46])\r\n\tby mx1-ec2-va.apache.org (ASF Mail Server at mx1-ec2-va.apache.org) with ESMTPS id C92C0BE3EF\r\n\tfor <user-zh-subscribe@flink.apache.org>; Fri, 14 Aug 2020 05:40:37 +0000 (UTC)\r\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;\r\n\ts=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=7hvB+\r\n\tKrFrn+Ql6HyMK0JPo+XY9liXgL0ZIMsnSHdwEE=; b=ibyLfJ68kGjtxL51qWMyU\r\n\tCAZne3LW2MV9ag4vPGweI2dGFAGtIJw7hrlyOr98pFlL5Wpk0Er3czjJV8CwoyLr\r\n\trapAJkAk/PXLMNDijWLl44vRkdq+M0G5La6lEUybt0fkr7JHk7CyojsZYbozdXeP\r\n\twZaNZ+LK5YsDCc8RO5lRVs=\r\nReceived: from liwen00812$163.com ( [116.236.73.133] ) by\r\n ajax-webmail-wmsvr46 (Coremail) ; Fri, 14 Aug 2020 13:40:23 +0800 (CST)\r\nX-Originating-IP: [116.236.73.133]\r\nDate: Fri, 14 Aug 2020 13:40:23 +0800 (CST)\r\nFrom: =?GBK?B?wO7OxA==?= <liwen00812@163.com>\r\nTo: user-zh-subscribe@flink.apache.org\r\nSubject: =?GBK?B?wLTX1MDuzsS1xNPKvP4=?=\r\nX-Priority: 3\r\nX-Mailer: Coremail Webmail Server Version XT5.0.10 build 20190724(ac680a23)\r\n Copyright (c) 2002-2020 www.mailtech.cn 163com\r\nX-CM-CTRLDATA: vvrErGZvb3Rlcl9odG09MTExOjU2\r\nContent-Type: multipart/alternative; \r\n\tboundary=\"----=_Part_51203_358748405.1597383623829\"\r\nMIME-Version: 1.0\r\nMessage-ID: <51bc98d8.370f.173eb7bc495.Coremail.liwen00812@163.com>\r\nX-Coremail-Locale: zh_CN\r\nX-CM-TRANSID:LsGowAAHu2XHIzZfujR0AA--.12544W\r\nX-CM-SenderInfo: polzv0iqqyiji6rwjhhfrp/1tbi6wF-NlXlsSm25gABsa\r\nX-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==",
        "Attach": []
    }
}